<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Papers - 2026-02-12</title>
<link>https://arxiv.org</link>
<description>AI papers as of 2026-02-12 - 50 papers</description>
<lastBuildDate>Thu, 12 Feb 2026 04:05:23 +0000</lastBuildDate>
<item>
<title><![CDATA[Towards Autonomous Mathematics Research]]></title>
<link>https://huggingface.co/papers/2602.10177</link>
<guid>2602.10177</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Tony Feng, Trieu H. Trinh, Garrett Bingham, Dawsen Hwang, Yuri Chervonyi
Institution: Amazon
Published: 2026-02-10
Score: 9/10
Citations: 0
Upvotes: 9
GitHub: 
Stars: 0

Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.]]></description>
<pubDate>Tue, 10 Feb 2026 18:50:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning]]></title>
<link>https://huggingface.co/papers/2602.08382</link>
<guid>2602.08382</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zhuoen Chen, Dongfang Li, Meishan Zhang, Baotian Hu, Min Zhang
Institution: 
Published: 2026-02-09
Score: 9/10
Citations: 0
Upvotes: 9
GitHub: 
Stars: 0

Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.]]></description>
<pubDate>Mon, 09 Feb 2026 08:33:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[GENIUS: Generative Fluid Intelligence Evaluation Suite]]></title>
<link>https://huggingface.co/papers/2602.11144</link>
<guid>2602.11144</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ruichuan An, Sihan Yang, Ziyu Guo, Wei Dai, Zijun Shen
Institution: 
Published: 2026-02-11
Score: 9/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.]]></description>
<pubDate>Wed, 11 Feb 2026 18:55:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters]]></title>
<link>https://huggingface.co/papers/2602.10604</link>
<guid>2602.10604</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ailin Huang, Ang Li, Aobo Kong, Bin Wang, Binxing Jiao
Institution: 
Published: 2026-02-11
Score: 8/10
Citations: 0
Upvotes: 46
GitHub: 
Stars: 0

We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.]]></description>
<pubDate>Wed, 11 Feb 2026 07:53:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[TodoEvolve: Learning to Architect Agent Planning Systems]]></title>
<link>https://huggingface.co/papers/2602.07839</link>
<guid>2602.07839</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jiaxi Liu, Yanzuo Jiang, Guibin Zhang, Zihan Zhang, Heng Chang
Institution: 
Published: 2026-02-08
Score: 8/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.]]></description>
<pubDate>Sun, 08 Feb 2026 06:37:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning]]></title>
<link>https://huggingface.co/papers/2602.11089</link>
<guid>2602.11089</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yicheng Chen, Zerun Ma, Xinchen Xie, Yining Li, Kai Chen
Institution: 
Published: 2026-02-11
Score: 8/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the data recipe, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate end-to-end data recipe generation for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.]]></description>
<pubDate>Wed, 11 Feb 2026 17:56:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Self-Correction in Vision-Language Models via Rollout Augmentation]]></title>
<link>https://huggingface.co/papers/2602.08503</link>
<guid>2602.08503</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yi Ding, Ziliang Qiu, Bolian Li, Ruqi Zhang
Institution: 
Published: 2026-02-09
Score: 8/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only 0.72times training time per step.]]></description>
<pubDate>Mon, 09 Feb 2026 10:55:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution]]></title>
<link>https://huggingface.co/papers/2602.07918</link>
<guid>2602.07918</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Minbeom Kim, Mihir Parmar, Phillip Wallis, Lesly Miculicich, Kyomin Jung
Institution: 
Published: 2026-02-08
Score: 8/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

AI agents equipped with tool-calling capabilities are susceptible to Indirect Prompt Injection (IPI) attacks. In this attack scenario, malicious commands hidden within untrusted content trick the agent into performing unauthorized actions. Existing defenses can reduce attack success but often suffer from the over-defense dilemma: they deploy expensive, always-on sanitization regardless of actual threat, thereby degrading utility and latency even in benign scenarios. We revisit IPI through a causal ablation perspective: a successful injection manifests as a dominance shift where the user request no longer provides decisive support for the agent's privileged action, while a particular untrusted segment, such as a retrieved document or tool output, provides disproportionate attributable influence. Based on this signature, we propose CausalArmor, a selective defense framework that (i) computes lightweight, leave-one-out ablation-based attributions at privileged decision points, and (ii) triggers targeted sanitization only when an untrusted segment dominates the user intent. Additionally, CausalArmor employs retroactive Chain-of-Thought masking to prevent the agent from acting on ``poisoned'' reasoning traces. We present a theoretical analysis showing that sanitization based on attribution margins conditionally yields an exponentially small upper bound on the probability of selecting malicious actions. Experiments on AgentDojo and DoomArena demonstrate that CausalArmor matches the security of aggressive defenses while improving explainability and preserving utility and latency of AI agents.]]></description>
<pubDate>Sun, 08 Feb 2026 11:34:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?]]></title>
<link>https://huggingface.co/papers/2602.04802</link>
<guid>2602.04802</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Qing'an Liu, Juntong Feng, Yuhao Wang, Xinzhe Han, Yujie Cheng
Institution: 
Published: 2026-02-04
Score: 8/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.]]></description>
<pubDate>Wed, 04 Feb 2026 17:48:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders]]></title>
<link>https://huggingface.co/papers/2602.10099</link>
<guid>2602.10099</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Amandeep Kumar, Vishal M. Patel
Institution: 
Published: 2026-02-10
Score: 8/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF]]></description>
<pubDate>Tue, 10 Feb 2026 18:58:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs]]></title>
<link>https://huggingface.co/papers/2602.07276</link>
<guid>2602.07276</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Pengrui Han, Xueqiang Xu, Keyang Xuan, Peiyang Song, Siru Ouyang
Institution: 
Published: 2026-02-07
Score: 7/10
Citations: 0
Upvotes: 10
GitHub: 
Stars: 0

Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.]]></description>
<pubDate>Sat, 07 Feb 2026 00:00:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.10224</link>
<guid>2602.10224</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Shiting Huang, Zecheng Li, Yu Zeng, Qingnan Ren, Zhen Fang
Institution: 
Published: 2026-02-10
Score: 7/10
Citations: 0
Upvotes: 8
GitHub: 
Stars: 0

Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.]]></description>
<pubDate>Tue, 10 Feb 2026 19:16:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[MIND: Benchmarking Memory Consistency and Action Control in World Models]]></title>
<link>https://huggingface.co/papers/2602.08025</link>
<guid>2602.08025</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yixuan Ye, Xuanyu Lu, Yuxin Jiang, Yuchao Gu, Rui Zhao
Institution: 
Published: 2026-02-08
Score: 7/10
Citations: 1
Upvotes: 7
GitHub: 
Stars: 0

World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/]]></description>
<pubDate>Sun, 08 Feb 2026 15:57:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion]]></title>
<link>https://huggingface.co/papers/2602.10999</link>
<guid>2602.10999</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yusong Lin, Haiyang Wang, Shuzhe Wu, Lue Fan, Feiyang Pan
Institution: 
Published: 2026-02-11
Score: 7/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.]]></description>
<pubDate>Wed, 11 Feb 2026 16:22:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model]]></title>
<link>https://huggingface.co/papers/2602.07422</link>
<guid>2602.07422</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Tianyi Wu, Mingzhe Du, Yue Liu, Chengran Yang, Terry Yue Zhuo
Institution: 
Published: 2026-02-07
Score: 7/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.]]></description>
<pubDate>Sat, 07 Feb 2026 07:42:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management]]></title>
<link>https://huggingface.co/papers/2602.07398</link>
<guid>2602.07398</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ruoyao Wen, Hao Li, Chaowei Xiao, Ning Zhang
Institution: 
Published: 2026-02-07
Score: 7/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack.
  We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length.
  On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.]]></description>
<pubDate>Sat, 07 Feb 2026 06:28:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Optimal Reasoning Length for RL-Trained Language Models]]></title>
<link>https://huggingface.co/papers/2602.09591</link>
<guid>2602.09591</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Daisuke Nohara, Taishi Nakamura, Rio Yokota
Institution: 
Published: 2026-02-10
Score: 7/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.]]></description>
<pubDate>Tue, 10 Feb 2026 09:45:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation]]></title>
<link>https://huggingface.co/papers/2602.09014</link>
<guid>2602.09014</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zihan Yang, Shuyuan Tu, Licheng Zhang, Qi Dai, Yu-Gang Jiang
Institution: 
Published: 2026-02-09
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.]]></description>
<pubDate>Mon, 09 Feb 2026 18:56:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation]]></title>
<link>https://huggingface.co/papers/2602.07670</link>
<guid>2602.07670</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jarrod Barnes
Institution: 
Published: 2026-02-07
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.]]></description>
<pubDate>Sat, 07 Feb 2026 19:29:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering]]></title>
<link>https://huggingface.co/papers/2602.08519</link>
<guid>2602.08519</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yunhui Liu, Pengyu Qiu, Yu Xing, Yongchao Liu, Peng Du
Institution: 
Published: 2026-02-09
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).]]></description>
<pubDate>Mon, 09 Feb 2026 11:07:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[SafePred: A Predictive Guardrail for Computer-Using Agents via World Models]]></title>
<link>https://huggingface.co/papers/2602.01725</link>
<guid>2602.01725</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yurun Chen, Zeyi Liao, Ping Yin, Taotao Xie, Keting Yin
Institution: 
Published: 2026-02-02
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.]]></description>
<pubDate>Mon, 02 Feb 2026 07:04:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation]]></title>
<link>https://huggingface.co/papers/2602.10699</link>
<guid>2602.10699</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jie Jiang, Yangru Huang, Zeyu Wang, Changping Wang, Yuling Xiong
Institution: 
Published: 2026-02-11
Score: 7/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.]]></description>
<pubDate>Wed, 11 Feb 2026 09:57:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation]]></title>
<link>https://huggingface.co/papers/2602.00268</link>
<guid>2602.00268</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ariel Shaulov, Eitan Shaar, Amit Edenzon, Lior Wolf
Institution: MIT
Published: 2026-01-30
Score: 6/10
Citations: 0
Upvotes: 18
GitHub: 
Stars: 0

Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.]]></description>
<pubDate>Fri, 30 Jan 2026 19:44:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments]]></title>
<link>https://huggingface.co/papers/2602.01244</link>
<guid>2602.01244</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Siwei Wu, Yizhi Li, Yuyang Song, Wei Zhang, Yang Wang
Institution: 
Published: 2026-02-01
Score: 6/10
Citations: 0
Upvotes: 12
GitHub: 
Stars: 0

Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \emph{Executability}, since each instance requires a suitable and often distinct Docker environment; and \emph{Verifiability}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.]]></description>
<pubDate>Sun, 01 Feb 2026 14:09:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models]]></title>
<link>https://huggingface.co/papers/2602.09017</link>
<guid>2602.09017</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zichen Jeff Cui, Omar Rayyan, Haritheja Etukuru, Bowen Tan, Zavier Andrianarivo
Institution: 
Published: 2026-02-09
Score: 6/10
Citations: 0
Upvotes: 11
GitHub: 
Stars: 0

The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/]]></description>
<pubDate>Mon, 09 Feb 2026 18:58:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models]]></title>
<link>https://huggingface.co/papers/2602.09713</link>
<guid>2602.09713</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ruisi Zhao, Haoren Zheng, Zongxin Yang, Hehe Fan, Yi Yang
Institution: 
Published: 2026-02-10
Score: 6/10
Citations: 0
Upvotes: 7
GitHub: 
Stars: 0

Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.]]></description>
<pubDate>Tue, 10 Feb 2026 12:17:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning]]></title>
<link>https://huggingface.co/papers/2602.10560</link>
<guid>2602.10560</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Leheng Sheng, Yongtao Zhang, Wenchang Ma, Yaorui Shi, Ting Huang
Institution: 
Published: 2026-02-11
Score: 6/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals r^{update} and r^{exit} within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\% times inference speed acceleration.]]></description>
<pubDate>Wed, 11 Feb 2026 06:14:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models]]></title>
<link>https://huggingface.co/papers/2602.10179</link>
<guid>2602.10179</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jiacheng Hou, Yining Sun, Ruochong Jin, Haochen Han, Fangming Liu
Institution: 
Published: 2026-02-10
Score: 6/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.]]></description>
<pubDate>Tue, 10 Feb 2026 18:59:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories]]></title>
<link>https://huggingface.co/papers/2602.05085</link>
<guid>2602.05085</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Sidi Lu, Zhenwen Liang, Dongyang Ma, Yan Wang, Haitao Mi
Institution: 
Published: 2026-02-04
Score: 6/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.]]></description>
<pubDate>Wed, 04 Feb 2026 22:09:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[ASA: Training-Free Representation Engineering for Tool-Calling Agents]]></title>
<link>https://huggingface.co/papers/2602.04935</link>
<guid>2602.04935</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Youjin Wang, Run Zhou, Rong Fu, Shuaishuai Cao, Hongwei Zeng
Institution: 
Published: 2026-02-04
Score: 6/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers, while continual parameter-efficient fine-tuning improves reliability at the cost of training, maintenance, and potential forgetting. We identify a critical Lazy Agent failure mode where tool necessity is nearly perfectly decodable from mid-layer activations, yet the model remains conservative in entering tool mode, revealing a representation-behavior gap. We propose Activation Steering Adapter (ASA), a training-free, inference-time controller that performs a single-shot mid-layer intervention and targets tool domains via a router-conditioned mixture of steering vectors with a probe-guided signed gate to amplify true intent while suppressing spurious triggers. On MTU-Bench with Qwen2.5-1.5B, ASA improves strict tool-use F1 from 0.18 to 0.50 while reducing the false positive rate from 0.15 to 0.05, using only about 20KB of portable assets and no weight updates.]]></description>
<pubDate>Wed, 04 Feb 2026 14:20:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[Online Causal Kalman Filtering for Stable and Effective Policy Optimization]]></title>
<link>https://huggingface.co/papers/2602.10609</link>
<guid>2602.10609</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Shuo He, Lang Feng, Xin Cheng, Lei Feng, Bo An
Institution: 
Published: 2026-02-11
Score: 6/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.]]></description>
<pubDate>Wed, 11 Feb 2026 07:57:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding]]></title>
<link>https://huggingface.co/papers/2602.06161</link>
<guid>2602.06161</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yanzheng Xiang, Lan Wei, Yizhen Yao, Qinglin Zhu, Hanqi Yan
Institution: 
Published: 2026-02-05
Score: 6/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.]]></description>
<pubDate>Thu, 05 Feb 2026 19:58:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[From Directions to Regions: Decomposing Activations in Language Models via Local Geometry]]></title>
<link>https://huggingface.co/papers/2602.02464</link>
<guid>2602.02464</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Or Shafran, Shaked Ronen, Omri Fahn, Shauli Ravfogel, Atticus Geiger
Institution: 
Published: 2026-02-02
Score: 6/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.]]></description>
<pubDate>Mon, 02 Feb 2026 18:49:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions]]></title>
<link>https://huggingface.co/papers/2602.06008</link>
<guid>2602.06008</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Xianyang Liu, Shangding Gu, Dawn Song
Institution: 
Published: 2026-02-05
Score: 6/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.]]></description>
<pubDate>Thu, 05 Feb 2026 18:50:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[ContextBench: A Benchmark for Context Retrieval in Coding Agents]]></title>
<link>https://huggingface.co/papers/2602.05892</link>
<guid>2602.05892</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang
Institution: 
Published: 2026-02-05
Score: 6/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval ("The Bitter Lesson" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.]]></description>
<pubDate>Thu, 05 Feb 2026 17:10:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning to Continually Learn via Meta-learning Agentic Memory Designs]]></title>
<link>https://huggingface.co/papers/2602.07755</link>
<guid>2602.07755</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yiming Xiong, Shengran Hu, Jeff Clune
Institution: 
Published: 2026-02-08
Score: 6/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.]]></description>
<pubDate>Sun, 08 Feb 2026 01:20:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes]]></title>
<link>https://huggingface.co/papers/2602.09153</link>
<guid>2602.09153</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Nicholas Pfaff, Thomas Cohn, Sergey Zakharov, Rick Cory, Russ Tedrake
Institution: 
Published: 2026-02-09
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stagesx2013from architectural layout to furniture placement to small object populationx2013each implemented as an interaction among VLM agents: designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.]]></description>
<pubDate>Mon, 09 Feb 2026 19:56:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations]]></title>
<link>https://huggingface.co/papers/2602.09924</link>
<guid>2602.09924</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: William Lugoloobi, Thomas Foster, William Bankes, Chris Russell
Institution: 
Published: 2026-02-10
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty]]></description>
<pubDate>Tue, 10 Feb 2026 15:57:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[iGRPO: Self-Feedback-Driven LLM Reasoning]]></title>
<link>https://huggingface.co/papers/2602.09000</link>
<guid>2602.09000</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han
Institution: 
Published: 2026-02-09
Score: 5/10
Citations: 0
Upvotes: 10
GitHub: 
Stars: 0

Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.]]></description>
<pubDate>Mon, 09 Feb 2026 18:45:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[Rethinking Global Text Conditioning in Diffusion Transformers]]></title>
<link>https://huggingface.co/papers/2602.09268</link>
<guid>2602.09268</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Nikita Starodubcev, Daniil Pakhomov, Zongze Wu, Ilya Drobyshevskiy, Yuchen Liu
Institution: 
Published: 2026-02-09
Score: 5/10
Citations: 0
Upvotes: 8
GitHub: 
Stars: 0

Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.]]></description>
<pubDate>Mon, 09 Feb 2026 23:06:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[Effective Reasoning Chains Reduce Intrinsic Dimensionality]]></title>
<link>https://huggingface.co/papers/2602.09276</link>
<guid>2602.09276</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Archiki Prasad, Mandar Joshi, Kenton Lee, Mohit Bansal, Peter Shaw
Institution: 
Published: 2026-02-09
Score: 5/10
Citations: 0
Upvotes: 7
GitHub: 
Stars: 0

Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.]]></description>
<pubDate>Mon, 09 Feb 2026 23:32:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design]]></title>
<link>https://huggingface.co/papers/2602.08253</link>
<guid>2602.08253</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Baoyun Zhao, He Wang, Liang Zeng
Institution: 
Published: 2026-02-09
Score: 5/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.]]></description>
<pubDate>Mon, 09 Feb 2026 04:13:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution]]></title>
<link>https://huggingface.co/papers/2602.09662</link>
<guid>2602.09662</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Deyang Jiang, Jing Huang, Xuanle Zhao, Lei Chen, Liming Zheng
Institution: 
Published: 2026-02-10
Score: 5/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (i.e., trajectory difficulty) and breadth (i.e., trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.]]></description>
<pubDate>Tue, 10 Feb 2026 11:16:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search]]></title>
<link>https://huggingface.co/papers/2602.09901</link>
<guid>2602.09901</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jianzhao Huang, Xiaorui Huang, Fei Zhao, Yunpeng Liu, Hui Zhang
Institution: 
Published: 2026-02-10
Score: 5/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.]]></description>
<pubDate>Tue, 10 Feb 2026 15:38:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[Stable Velocity: A Variance Perspective on Flow Matching]]></title>
<link>https://huggingface.co/papers/2602.05435</link>
<guid>2602.05435</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Donglin Yang, Yongxing Zhang, Xin Yu, Liang Hou, Xin Tao
Institution: 
Published: 2026-02-05
Score: 5/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet 256times256 and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than 2times faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.]]></description>
<pubDate>Thu, 05 Feb 2026 08:25:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory]]></title>
<link>https://huggingface.co/papers/2602.10652</link>
<guid>2602.10652</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yongshi Ye, Hui Jiang, Feihu Jiang, Tian Lan, Yichao Du
Institution: 
Published: 2026-02-11
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.]]></description>
<pubDate>Wed, 11 Feb 2026 08:58:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents]]></title>
<link>https://huggingface.co/papers/2602.08995</link>
<guid>2602.08995</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yuting Ning, Jaylen Jones, Zhehao Zhang, Chentao Ye, Weitong Ruan
Institution: 
Published: 2026-02-09
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.]]></description>
<pubDate>Mon, 09 Feb 2026 18:41:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[C-: Circuit-Restricted Weight Arithmetic for Selective Refusal]]></title>
<link>https://huggingface.co/papers/2602.04521</link>
<guid>2602.04521</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Aditya Kasliwal, Pratinav Seth, Vinay Kumar Sankarapu
Institution: 
Published: 2026-02-04
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update C supported only on that circuit (typically <5% of parameters). Applying C yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.]]></description>
<pubDate>Wed, 04 Feb 2026 13:10:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss]]></title>
<link>https://huggingface.co/papers/2602.07022</link>
<guid>2602.07022</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yucheng Zhou, Hao Li, Jianbing Shen
Institution: 
Published: 2026-02-02
Score: 4/10
Citations: 0
Upvotes: 18
GitHub: 
Stars: 0

Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.]]></description>
<pubDate>Mon, 02 Feb 2026 07:48:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs]]></title>
<link>https://huggingface.co/papers/2602.00462</link>
<guid>2602.00462</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Benno Krojer, Shravan Nayak, Oscar Maas, Vaibhav Adlakha, Desmond Elliott
Institution: 
Published: 2026-01-31
Score: 4/10
Citations: 0
Upvotes: 14
GitHub: 
Stars: 0

Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.]]></description>
<pubDate>Sat, 31 Jan 2026 02:33:07 +0000</pubDate>
</item>
</channel>
</rss>