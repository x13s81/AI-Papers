<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Papers - 2026-02-10</title>
<link>https://arxiv.org</link>
<description>AI papers as of 2026-02-10 - 238 papers</description>
<lastBuildDate>Tue, 10 Feb 2026 14:55:30 +0000</lastBuildDate>
<item>
<title><![CDATA[InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery]]></title>
<link>https://huggingface.co/papers/2602.08990</link>
<guid>2602.08990</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Shiyang Feng, Runmin Ma, Xiangchao Yan, Yue Fan, Yusong Hu
Institution: 
Published: 2026-02-09
Score: 9/10
Citations: 0
Upvotes: 15
GitHub: 
Stars: 0

We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.]]></description>
<pubDate>Mon, 09 Feb 2026 18:36:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[MOVA: Towards Scalable and Synchronized Video-Audio Generation]]></title>
<link>https://huggingface.co/papers/2602.08794</link>
<guid>2602.08794</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: SII-OpenMOSS Team, Donghua Yu, Mingshu Chen, Qi Chen, Qi Luo
Institution: 
Published: 2026-02-09
Score: 8/10
Citations: 0
Upvotes: 119
GitHub: 
Stars: 0

Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.]]></description>
<pubDate>Mon, 09 Feb 2026 15:31:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[LLaDA2.1: Speeding Up Text Diffusion via Token Editing]]></title>
<link>https://huggingface.co/papers/2602.08676</link>
<guid>2602.08676</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Tiwei Bie, Maosong Cao, Xiang Cao, Bingsen Chen, Fuyuan Chen
Institution: 
Published: 2026-02-09
Score: 8/10
Citations: 0
Upvotes: 36
GitHub: 
Stars: 0

While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.]]></description>
<pubDate>Mon, 09 Feb 2026 14:00:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning]]></title>
<link>https://huggingface.co/papers/2602.07075</link>
<guid>2602.07075</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Xinwu Ye, Yicheng Mao, Jia Zhang, Yimeng Liu, Li Hao
Institution: 
Published: 2026-02-06
Score: 8/10
Citations: 0
Upvotes: 15
GitHub: 
Stars: 0

Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.]]></description>
<pubDate>Fri, 06 Feb 2026 01:28:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models]]></title>
<link>https://huggingface.co/papers/2602.08658</link>
<guid>2602.08658</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Mingzi Cao, Xingwei Tan, Mahmud Akhter, Marco Valentino, Maria Liakata
Institution: 
Published: 2026-02-09
Score: 8/10
Citations: 0
Upvotes: 11
GitHub: 
Stars: 0

Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to 14.60) across realistic tasks.]]></description>
<pubDate>Mon, 09 Feb 2026 13:51:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.06694</link>
<guid>2602.06694</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Hyochan Chong, Dongkyu Kim, Changdong Kim, Minseop Choi
Institution: 
Published: 2026-02-06
Score: 8/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.]]></description>
<pubDate>Fri, 06 Feb 2026 13:26:44 +0000</pubDate>
</item>
<item>
<title><![CDATA[MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE]]></title>
<link>https://huggingface.co/papers/2602.08961</link>
<guid>2602.08961</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ruijie Zhu, Jiahao Lu, Wenbo Hu, Xiaoguang Han, Jianfei Cai
Institution: 
Published: 2026-02-09
Score: 8/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page]]></description>
<pubDate>Mon, 09 Feb 2026 17:58:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[CauScale: Neural Causal Discovery at Scale]]></title>
<link>https://huggingface.co/papers/2602.08629</link>
<guid>2602.08629</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Bo Peng, Sirui Chen, Jiaguo Tian, Yu Qiao, Chaochao Lu
Institution: 
Published: 2026-02-09
Score: 8/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.]]></description>
<pubDate>Mon, 09 Feb 2026 13:21:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods]]></title>
<link>https://huggingface.co/papers/2602.07040</link>
<guid>2602.07040</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Emmett Bicker
Institution: 
Published: 2026-02-03
Score: 8/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.
  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.
  Aster is accessible via a web interface and API at asterlab.ai.]]></description>
<pubDate>Tue, 03 Feb 2026 19:01:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Weak-Driven Learning: How Weak Agents make Strong Agents Stronger]]></title>
<link>https://huggingface.co/papers/2602.08222</link>
<guid>2602.08222</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zehao Chen, Gongxun Li, Tianxiang Ai, Yifei Li, Zixuan Huang
Institution: 
Published: 2026-02-09
Score: 7/10
Citations: 0
Upvotes: 95
GitHub: 
Stars: 0

As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.]]></description>
<pubDate>Mon, 09 Feb 2026 02:50:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning]]></title>
<link>https://huggingface.co/papers/2602.07845</link>
<guid>2602.07845</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yalcin Tur, Jalal Naghiyev, Haoquan Fang, Wei-Chuan Tsai, Jiafei Duan
Institution: 
Published: 2026-02-08
Score: 7/10
Citations: 0
Upvotes: 44
GitHub: 
Stars: 0

Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/]]></description>
<pubDate>Sun, 08 Feb 2026 07:21:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth]]></title>
<link>https://huggingface.co/papers/2602.07962</link>
<guid>2602.07962</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Weihao Zeng, Yuzhen Huang, Junxian He
Institution: 
Published: 2026-02-08
Score: 7/10
Citations: 0
Upvotes: 21
GitHub: 
Stars: 0

Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench]]></description>
<pubDate>Sun, 08 Feb 2026 13:20:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?]]></title>
<link>https://huggingface.co/papers/2602.07055</link>
<guid>2602.07055</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Pingyue Zhang, Zihan Huang, Yue Wang, Jieyu Zhang, Letian Xue
Institution: 
Published: 2026-02-04
Score: 7/10
Citations: 0
Upvotes: 16
GitHub: 
Stars: 0

Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.]]></description>
<pubDate>Wed, 04 Feb 2026 19:06:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[WorldCompass: Reinforcement Learning for Long-Horizon World Models]]></title>
<link>https://huggingface.co/papers/2602.09022</link>
<guid>2602.09022</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zehan Wang, Tengfei Wang, Haiyu Zhang, Xuhui Zuo, Junta Wu
Institution: 
Published: 2026-02-09
Score: 7/10
Citations: 0
Upvotes: 15
GitHub: 
Stars: 0

This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.]]></description>
<pubDate>Mon, 09 Feb 2026 18:59:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models]]></title>
<link>https://huggingface.co/papers/2602.04649</link>
<guid>2602.04649</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Binghai Wang, Yantao Liu, Yuxuan Liu, Tianyi Tang, Shenzhi Wang
Institution: 
Published: 2026-02-04
Score: 7/10
Citations: 0
Upvotes: 7
GitHub: 
Stars: 0

Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.]]></description>
<pubDate>Wed, 04 Feb 2026 15:24:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs]]></title>
<link>https://huggingface.co/papers/2602.08808</link>
<guid>2602.08808</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yapei Chang, Kyle Lo, Mohit Iyyer, Luca Soldaini
Institution: 
Published: 2026-02-09
Score: 7/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.]]></description>
<pubDate>Mon, 09 Feb 2026 15:47:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[WildReward: Learning Reward Models from In-the-Wild Human Interactions]]></title>
<link>https://huggingface.co/papers/2602.08829</link>
<guid>2602.08829</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Lei Hou
Institution: 
Published: 2026-02-09
Score: 7/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.]]></description>
<pubDate>Mon, 09 Feb 2026 16:00:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration]]></title>
<link>https://huggingface.co/papers/2602.05708</link>
<guid>2602.05708</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Chuangtao Ma, Zeyu Zhang, Arijit Khan, Sebastian Schelter, Paul Groth
Institution: 
Published: 2026-02-05
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.]]></description>
<pubDate>Thu, 05 Feb 2026 14:33:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design]]></title>
<link>https://huggingface.co/papers/2602.07491</link>
<guid>2602.07491</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Isabella A. Stewart, Tarjei Paule Hage, Yu-Chuan Hsu, Markus J. Buehler
Institution: 
Published: 2026-02-07
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.]]></description>
<pubDate>Sat, 07 Feb 2026 10:50:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment]]></title>
<link>https://huggingface.co/papers/2602.05946</link>
<guid>2602.05946</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Rajdeep Haldar, Lantao Mei, Guang Lin, Yue Xing, Qifan Song
Institution: 
Published: 2026-02-05
Score: 7/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.]]></description>
<pubDate>Thu, 05 Feb 2026 18:01:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.07026</link>
<guid>2602.07026</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Xiaomin Yu, Yi Xin, Wenjie Zhang, Chonghan Liu, Hanzhen Zhao
Institution: 
Published: 2026-02-02
Score: 6/10
Citations: 0
Upvotes: 111
GitHub: 
Stars: 0

Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.]]></description>
<pubDate>Mon, 02 Feb 2026 13:59:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[GEBench: Benchmarking Image Generation Models as GUI Environments]]></title>
<link>https://huggingface.co/papers/2602.09007</link>
<guid>2602.09007</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Haodong Li, Jingwei Wu, Quan Sun, Guopeng Li, Juanxi Tian
Institution: 
Published: 2026-02-09
Score: 6/10
Citations: 0
Upvotes: 30
GitHub: 
Stars: 0

Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.]]></description>
<pubDate>Mon, 09 Feb 2026 18:52:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition]]></title>
<link>https://huggingface.co/papers/2602.08439</link>
<guid>2602.08439</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yuhao Dong, Shulin Tian, Shuai Liu, Shuangrui Ding, Yuhang Zang
Institution: 
Published: 2026-02-09
Score: 6/10
Citations: 0
Upvotes: 28
GitHub: 
Stars: 0

Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.]]></description>
<pubDate>Mon, 09 Feb 2026 09:51:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents]]></title>
<link>https://huggingface.co/papers/2602.06855</link>
<guid>2602.06855</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Alisia Lupidi, Bhavul Gauri, Thomas Simon Foster, Bassel Al Omari, Despoina Magka
Institution: 
Published: 2026-02-06
Score: 6/10
Citations: 0
Upvotes: 19
GitHub: 
Stars: 0

LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.]]></description>
<pubDate>Fri, 06 Feb 2026 16:45:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[Context Compression via Explicit Information Transmission]]></title>
<link>https://huggingface.co/papers/2602.03784</link>
<guid>2602.03784</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jiangnan Ye, Hanqi Yan, Zhenyi Shen, Heng Chang, Ye Mao
Institution: 
Published: 2026-02-03
Score: 6/10
Citations: 0
Upvotes: 13
GitHub: 
Stars: 0

Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.]]></description>
<pubDate>Tue, 03 Feb 2026 17:44:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning]]></title>
<link>https://huggingface.co/papers/2602.08236</link>
<guid>2602.08236</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao
Institution: 
Published: 2026-02-09
Score: 6/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.]]></description>
<pubDate>Mon, 09 Feb 2026 03:21:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control]]></title>
<link>https://huggingface.co/papers/2601.21363</link>
<guid>2601.21363</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su
Institution: 
Published: 2026-01-29
Score: 6/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.]]></description>
<pubDate>Thu, 29 Jan 2026 07:43:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[Reliable and Responsible Foundation Models: A Comprehensive Survey]]></title>
<link>https://huggingface.co/papers/2602.08145</link>
<guid>2602.08145</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Xinyu Yang, Junlin Han, Rishi Bommasani, Jinqi Luo, Wenjie Qu
Institution: 
Published: 2026-02-04
Score: 6/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.]]></description>
<pubDate>Wed, 04 Feb 2026 17:25:03 +0000</pubDate>
</item>
<item>
<title><![CDATA[SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis]]></title>
<link>https://huggingface.co/papers/2602.07803</link>
<guid>2602.07803</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jiale Qian, Hao Meng, Tian Zheng, Pengcheng Zhu, Haopeng Lin
Institution: 
Published: 2026-02-08
Score: 6/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.]]></description>
<pubDate>Sun, 08 Feb 2026 03:51:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion]]></title>
<link>https://huggingface.co/papers/2602.07775</link>
<guid>2602.07775</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Haodong Li, Shaoteng Liu, Zhe Lin, Manmohan Chandraker
Institution: 
Published: 2026-02-08
Score: 6/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/]]></description>
<pubDate>Sun, 08 Feb 2026 02:16:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs]]></title>
<link>https://huggingface.co/papers/2602.05929</link>
<guid>2602.05929</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jian Chen, Zhuoran Wang, Jiayu Qin, Ming Li, Meng Wang
Institution: 
Published: 2026-02-05
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.]]></description>
<pubDate>Thu, 05 Feb 2026 17:41:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.08818</link>
<guid>2602.08818</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Annemette Brok Pirchert, Jacob Nielsen, Mogens Henrik From, Lukas Galke Poech, Peter Schneider-Kamp
Institution: 
Published: 2026-02-09
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating 6 experts with ranks 2^0 to 2^{14} resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across 120 tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score 47.18) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score 45.46) at less than one third the parameters (10.75B for FlexMoRE vs. 33.27B for FlexOlmo). All code will be made available.]]></description>
<pubDate>Mon, 09 Feb 2026 15:54:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[Statistical Learning Theory in Lean 4: Empirical Processes from Scratch]]></title>
<link>https://huggingface.co/papers/2602.02285</link>
<guid>2602.02285</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yuanhe Zhang, Jason D. Lee, Fanghui Liu
Institution: 
Published: 2026-02-02
Score: 6/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory]]></description>
<pubDate>Mon, 02 Feb 2026 16:24:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining]]></title>
<link>https://huggingface.co/papers/2602.07085</link>
<guid>2602.07085</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jun Han, Shuo Zhang, Wei Li, Zhi Yang, Yifan Dong
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 94
GitHub: 
Stars: 0

Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.]]></description>
<pubDate>Fri, 06 Feb 2026 08:08:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO]]></title>
<link>https://huggingface.co/papers/2602.06422</link>
<guid>2602.06422</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yunze Tong, Mushui Liu, Canyu Zhao, Wanggui He, Shiyi Zhang
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 36
GitHub: 
Stars: 0

Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.]]></description>
<pubDate>Fri, 06 Feb 2026 06:37:10 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory]]></title>
<link>https://huggingface.co/papers/2602.06025</link>
<guid>2602.06025</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao
Institution: 
Published: 2026-02-05
Score: 5/10
Citations: 0
Upvotes: 27
GitHub: 
Stars: 0

Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.]]></description>
<pubDate>Thu, 05 Feb 2026 18:57:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[GISA: A Benchmark for General Information-Seeking Assistant]]></title>
<link>https://huggingface.co/papers/2602.08543</link>
<guid>2602.08543</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yutao Zhu, Xingshuo Zhang, Maosen Zhang, Jiajie Jin, Liancheng Zhang
Institution: 
Published: 2026-02-09
Score: 5/10
Citations: 0
Upvotes: 18
GitHub: 
Stars: 0

The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.]]></description>
<pubDate>Mon, 09 Feb 2026 11:44:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research]]></title>
<link>https://huggingface.co/papers/2602.06540</link>
<guid>2602.06540</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yishan Li, Wentong Chen, Yukun Yan, Mingwei Li, Sen Mei
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 11
GitHub: 
Stars: 0

Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.]]></description>
<pubDate>Fri, 06 Feb 2026 09:45:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[RelayGen: Intra-Generation Model Switching for Efficient Reasoning]]></title>
<link>https://huggingface.co/papers/2602.06454</link>
<guid>2602.06454</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jiwon Song, Yoongon Kim, Jae-Joon Kim
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 11
GitHub: 
Stars: 0

Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.]]></description>
<pubDate>Fri, 06 Feb 2026 07:35:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay]]></title>
<link>https://huggingface.co/papers/2602.06942</link>
<guid>2602.06942</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Duygu Altinok
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.]]></description>
<pubDate>Fri, 06 Feb 2026 18:41:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Data Science and Technology Towards AGI Part I: Tiered Data Management]]></title>
<link>https://huggingface.co/papers/2602.09003</link>
<guid>2602.09003</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yudong Wang, Zixuan Fu, Hengyu Zhao, Chen Zhao, Chuyue Zhou
Institution: 
Published: 2026-02-09
Score: 5/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.]]></description>
<pubDate>Mon, 09 Feb 2026 18:47:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning]]></title>
<link>https://huggingface.co/papers/2602.06600</link>
<guid>2602.06600</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zhuoyuan Hao, Zhuo Li, Wu Li, Fangming Liu, Min Zhang
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the spontaneous repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the Echo of Prompt (EOP), as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the Echo Likelihood Gap L as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop Echo-Distilled SFT (ED-SFT) to instill an ``echo-then-reason'' pattern through supervised finetuning, and Echoic Prompting (EP) to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an attention refocusing mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.]]></description>
<pubDate>Fri, 06 Feb 2026 10:53:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization]]></title>
<link>https://huggingface.co/papers/2602.07054</link>
<guid>2602.07054</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ashutosh Chaubey, Jiacheng Pang, Maksim Siniukov, Mohammad Soleymani
Institution: 
Published: 2026-02-04
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.]]></description>
<pubDate>Wed, 04 Feb 2026 18:24:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity]]></title>
<link>https://huggingface.co/papers/2602.07970</link>
<guid>2602.07970</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zheyuan Hu, Weitao Chen, Cengiz ztireli, Chenliang Zhou, Fangcheng Zhong
Institution: 
Published: 2026-02-08
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.]]></description>
<pubDate>Sun, 08 Feb 2026 13:44:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks]]></title>
<link>https://huggingface.co/papers/2602.07090</link>
<guid>2602.07090</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yu-Che Tsai, Hsiang Hsiao, Kuan-Yu Chen, Shou-De Lin
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.]]></description>
<pubDate>Fri, 06 Feb 2026 09:28:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Uncertainty Drives Social Bias Changes in Quantized Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.06181</link>
<guid>2602.06181</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Stanley Z. Hua, Sanae Lotfi, Irene Y. Chen
Institution: 
Published: 2026-02-05
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.]]></description>
<pubDate>Thu, 05 Feb 2026 20:37:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking]]></title>
<link>https://huggingface.co/papers/2602.06445</link>
<guid>2602.06445</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Weidong Huang, Jingwen Zhang, Jiongye Li, Shibowen Zhang, Jiayang Wu
Institution: 
Published: 2026-02-06
Score: 4/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.]]></description>
<pubDate>Fri, 06 Feb 2026 07:14:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents]]></title>
<link>https://huggingface.co/papers/2602.07796</link>
<guid>2602.07796</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jiatong Li, Changdae Oh, Hyeong Kyu Choi, Jindong Wang, Sharon Li
Institution: 
Published: 2026-02-08
Score: 4/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.]]></description>
<pubDate>Sun, 08 Feb 2026 03:23:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[On Randomness in Agentic Evals]]></title>
<link>https://huggingface.co/papers/2602.07150</link>
<guid>2602.07150</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Bjarni Haukur Bjarnason, Andr Silva, Martin Monperrus
Institution: 
Published: 2026-02-06
Score: 4/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.]]></description>
<pubDate>Fri, 06 Feb 2026 19:49:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[dewi-kadita: A Python Library for Idealized Fish Schooling Simulation with Entropy-Based Diagnostics]]></title>
<link>https://huggingface.co/papers/2602.07948</link>
<guid>2602.07948</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Sandy H. S. Herho, Iwan P. Anwar, Faruq Khadami, Alfita P. Handayani, Karina A. Sujatmiko
Institution: 
Published: 2026-02-08
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Couzin zone-based model with comprehensive entropy diagnostics tailored for marine collective behavior research. The library introduces seven information-theoretic metrics -- school cohesion entropy, polarization entropy, depth stratification entropy, angular momentum entropy, nearest-neighbor entropy, velocity correlation entropy, and school shape entropy -- that characterize distinct organizational features inaccessible to classical order parameters. These metrics combine into an Oceanic Schooling Index (OSI) providing a single scalar measure of collective disorder. Validation across four canonical configurations (swarm, torus, dynamic parallel, highly parallel) confirms correct reproduction of known phase behaviors: the swarm maintains disorder with polarization P < 0.1 and OSI approx 0.71, while the highly parallel state achieves P = 0.998 with OSI = 0.24 and velocity correlation entropy vanishing to zero. The entropy framework successfully discriminates the torus and dynamic parallel configurations that exhibit comparable order parameter magnitudes through different organizational mechanisms. Numba just-in-time (JIT) compilation accelerates pairwise interaction calculations by 10--100times, enabling simulations of 150--250 agents over 1000--2000 time steps within five minutes on standard workstation hardware. NetCDF4 output ensures interoperability with oceanographic analysis tools. The library addresses the need for standardized, reproducible infrastructure in collective behavior modeling analogous to established molecular dynamics codes.]]></description>
<pubDate>Sun, 08 Feb 2026 12:43:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[Autoregressive Image Generation with Masked Bit Modeling]]></title>
<link>http://arxiv.org/abs/2602.09024v1</link>
<guid>2602.09024v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Qihang Yu, Qihao Liu, Ju He, Xinyang Zhang, Yang Liu et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/]]></description>
<pubDate>Mon, 09 Feb 2026 18:59:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[$_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies]]></title>
<link>http://arxiv.org/abs/2602.09021v1</link>
<guid>2602.09021v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.CV
Authors: Checheng Yu, Chonghao Sima, Gangcheng Jiang, Hai Zhang, Haoguang Mai et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $_{0}$ surpasses the state-of-the-art $_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.]]></description>
<pubDate>Mon, 09 Feb 2026 18:59:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving]]></title>
<link>http://arxiv.org/abs/2602.09018v1</link>
<guid>2602.09018v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.AI, cs.CV, cs.LG
Authors: Amir Mallak, Alaa Maalouf
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.]]></description>
<pubDate>Mon, 09 Feb 2026 18:59:03 +0000</pubDate>
</item>
<item>
<title><![CDATA[Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models]]></title>
<link>http://arxiv.org/abs/2602.09017v1</link>
<guid>2602.09017v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.LG
Authors: Zichen Jeff Cui, Omar Rayyan, Haritheja Etukuru, Bowen Tan, Zavier Andrianarivo et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/]]></description>
<pubDate>Mon, 09 Feb 2026 18:58:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction]]></title>
<link>http://arxiv.org/abs/2602.09016v1</link>
<guid>2602.09016v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Hao Phung, Hadar Averbuch-Elor
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.]]></description>
<pubDate>Mon, 09 Feb 2026 18:58:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection]]></title>
<link>http://arxiv.org/abs/2602.09015v1</link>
<guid>2602.09015v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CR, cs.AI
Authors: Fatemeh Nejati, Mahdi Rabbani, Mansur Mirani, Gunjan Piya, Igor Opushnyev et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibility of malicious email attachments makes them stand out as a preferred vector for attackers as they can embed harmful content such as malware or malicious URLs inside standard document formats. Although phishing email defenses have improved a lot, attackers continue to abuse attachments, enabling malicious content to bypass security measures. Moreover, another challenge that researches face in training advance models, is lack of an unified and comprehensive dataset that covers the most prevalent data types. To address this gap, we generated CIC-Trap4Phish, a multi-format dataset containing both malicious and benign samples across five categories commonly used in phishing campaigns: Microsoft Word documents, Excel spreadsheets, PDF files, HTML pages, and QR code images. For the first four file types, a set of execution-free static feature pipeline was proposed, designed to capture structural, lexical, and metadata-based indicators without the need to open or execute files. Feature selection was performed using a combination of SHAP analysis and feature importance, yielding compact, discriminative feature subsets for each file type. The selected features were evaluated by using lightweight machine learning models, including Random Forest, XGBoost, and Decision Tree. All models demonstrate high detection accuracy across formats. For QR code-based phishing (quishing), two complementary methods were implemented: image-based detection by employing Convolutional Neural Networks (CNNs) and lexical analysis of decoded URLs using recent lightweight language models.]]></description>
<pubDate>Mon, 09 Feb 2026 18:57:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation]]></title>
<link>http://arxiv.org/abs/2602.09014v1</link>
<guid>2602.09014v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Zihan Yang, Shuyuan Tu, Licheng Zhang, Qi Dai, Yu-Gang Jiang et al.
Institution: Microsoft
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.]]></description>
<pubDate>Mon, 09 Feb 2026 18:56:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction]]></title>
<link>http://arxiv.org/abs/2602.09013v1</link>
<guid>2602.09013v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.CV
Authors: Hongyi Chen, Tony Dong, Tiancheng Wu, Liquan Wang, Yash Jangir et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.]]></description>
<pubDate>Mon, 09 Feb 2026 18:56:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense]]></title>
<link>http://arxiv.org/abs/2602.09012v1</link>
<guid>2602.09012v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CL
Authors: Jiacheng Liu, Yaxin Luo, Jiacheng Cui, Xinyi Shang, Xiaohan Zhao et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.]]></description>
<pubDate>Mon, 09 Feb 2026 18:55:33 +0000</pubDate>
</item>
<item>
<title><![CDATA[ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling]]></title>
<link>http://arxiv.org/abs/2602.09009v1</link>
<guid>2602.09009v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Yilang Zhang, Bingcong Li, Niao He, Georgios B. Giannakis
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.]]></description>
<pubDate>Mon, 09 Feb 2026 18:54:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification]]></title>
<link>http://arxiv.org/abs/2602.09008v1</link>
<guid>2602.09008v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Sijia Peng, Yun Xiong, Xi Chen, Yi Xie, Guanzhi Li et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.]]></description>
<pubDate>Mon, 09 Feb 2026 18:53:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[ARO: A New Lens On Matrix Optimization For Large Models]]></title>
<link>http://arxiv.org/abs/2602.09006v1</link>
<guid>2602.09006v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Wenbo Gong, Javier Zazo, Qijun Luo, Puqian Wang, James Hensman et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.]]></description>
<pubDate>Mon, 09 Feb 2026 18:51:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection]]></title>
<link>http://arxiv.org/abs/2602.09002v1</link>
<guid>2602.09002v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.AI
Authors: Zilin Fang, Anxing Xiao, David Hsu, Gim Hee Lee
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io]]></description>
<pubDate>Mon, 09 Feb 2026 18:46:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[DirMoE: Dirichlet-routed Mixture of Experts]]></title>
<link>http://arxiv.org/abs/2602.09001v1</link>
<guid>2602.09001v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Amirhossein Vahidi, Hesam Asadollahzadeh, Navid Akhavan Attar, Marie Moullet, Kevin Ly et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.]]></description>
<pubDate>Mon, 09 Feb 2026 18:45:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[iGRPO: Self-Feedback-Driven LLM Reasoning]]></title>
<link>http://arxiv.org/abs/2602.09000v1</link>
<guid>2602.09000v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.]]></description>
<pubDate>Mon, 09 Feb 2026 18:45:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[Universal Coefficients and Mayer-Vietoris Sequence for Groupoid Homology]]></title>
<link>http://arxiv.org/abs/2602.08998v1</link>
<guid>2602.08998v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Luciano Melodia
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study homology of ample groupoids via the compactly supported Moore complex of the nerve. Let $A$ be a topological abelian group. For $n\ge 0$ set $C_n(\mathcal G;A) := C_c(\mathcal G_n,A)$ and define $\partial_n^A=\sum_{i=0}^n(-1)^i(d_i)_*$. This defines $H_n(\mathcal G;A)$. The theory is functorial for continuous tale homomorphisms. It is compatible with standard reductions, including restriction to saturated clopen subsets. In the ample setting it is invariant under Kakutani equivalence. We reprove Matui type long exact sequences and identify the comparison maps at chain level. For discrete $A$ we prove a natural universal coefficient short exact sequence $$0\to H_n(\mathcal G)\otimes_{\mathbb Z}A\xrightarrow{\ _n^{\mathcal G}\ }H_n(\mathcal G;A)\xrightarrow{\ _n^{\mathcal G}\ }\operatorname{Tor}_1^{\mathbb Z}\bigl(H_{n-1}(\mathcal G),A\bigr)\to 0.$$ The key input is the chain level isomorphism $C_c(\mathcal G_n,\mathbb Z)\otimes_{\mathbb Z}A\cong C_c(\mathcal G_n,A)$, which reduces the groupoid statement to the classical algebraic UCT for the free complex $C_c(\mathcal G_\bullet,\mathbb Z)$. We also isolate the obstruction for non-discrete coefficients. For a locally compact totally disconnected Hausdorff space $X$ with a basis of compact open sets, the image of $_X:C_c(X,\mathbb Z)\otimes_{\mathbb Z}A\to C_c(X,A)$ is exactly the compactly supported functions with finite image. Thus $_X$ is surjective if and only if every $f\in C_c(X,A)$ has finite image, and for suitable $X$ one can produce compactly supported continuous maps $X\to A$ with infinite image. Finally, for a clopen saturated cover $\mathcal G_0=U_1\cup U_2$ we construct a short exact sequence of Moore complexes and derive a Mayer-Vietoris long exact sequence for $H_\bullet(\mathcal G;A)$ for explicit computations.]]></description>
<pubDate>Mon, 09 Feb 2026 18:43:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[Paradox of De-identification: A Critique of HIPAA Safe Harbour in the Age of LLMs]]></title>
<link>http://arxiv.org/abs/2602.08997v1</link>
<guid>2602.08997v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CY, cs.CL
Authors: Lavender Y. Jiang, Xujin Chris Liu, Kyunghyun Cho, Eric K. Oermann
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Privacy is a human right that sustains patient-provider trust. Clinical notes capture a patient's private vulnerability and individuality, which are used for care coordination and research. Under HIPAA Safe Harbor, these notes are de-identified to protect patient privacy. However, Safe Harbor was designed for an era of categorical tabular data, focusing on the removal of explicit identifiers while ignoring the latent information found in correlations between identity and quasi-identifiers, which can be captured by modern LLMs. We first formalize these correlations using a causal graph, then validate it empirically through individual re-identification of patients from scrubbed notes. The paradox of de-identification is further shown through a diagnosis ablation: even when all other information is removed, the model can predict the patient's neighborhood based on diagnosis alone. This position paper raises the question of how we can act as a community to uphold patient-provider trust when de-identification is inherently imperfect. We aim to raise awareness and discuss actionable recommendations.]]></description>
<pubDate>Mon, 09 Feb 2026 18:43:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study]]></title>
<link>http://arxiv.org/abs/2602.08996v1</link>
<guid>2602.08996v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Arushi Rai, Adriana Kovashka
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.]]></description>
<pubDate>Mon, 09 Feb 2026 18:41:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents]]></title>
<link>http://arxiv.org/abs/2602.08995v1</link>
<guid>2602.08995v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Yuting Ning, Jaylen Jones, Zhehao Zhang, Chentao Ye, Weitong Ruan et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.]]></description>
<pubDate>Mon, 09 Feb 2026 18:41:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning]]></title>
<link>http://arxiv.org/abs/2602.08986v1</link>
<guid>2602.08986v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Isaac Xu, Martin Gillis, Ayushi Sharma, Benjamin Misiuk, Craig J. Brown et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data.]]></description>
<pubDate>Mon, 09 Feb 2026 18:34:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models]]></title>
<link>http://arxiv.org/abs/2602.08984v1</link>
<guid>2602.08984v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Yuliang Liu, Yunchong Song, Yixuan Wang, Kewen Ge, Alex Lamb et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.]]></description>
<pubDate>Mon, 09 Feb 2026 18:33:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[StretchTime: Adaptive Time Series Forecasting via Symplectic Attention]]></title>
<link>http://arxiv.org/abs/2602.08983v1</link>
<guid>2602.08983v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Yubin Kim, Viresh Pati, Jevon Twitty, Vinh Pham, Shihao Yang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit "time-warped" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\mathrm{SO}(2)$ to the symplectic group $\mathrm{Sp}(2,\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.]]></description>
<pubDate>Mon, 09 Feb 2026 18:29:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[When do neural ordinary differential equations generalize on complex networks?]]></title>
<link>http://arxiv.org/abs/2602.08980v1</link>
<guid>2602.08980v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.SI, stat.ML
Authors: Moritz Laber, Tina Eliassi-Rad, Brennan Klein
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Neural ordinary differential equations (neural ODEs) can effectively learn dynamical systems from time series data, but their behavior on graph-structured data remains poorly understood, especially when applied to graphs with different size or structure than encountered during training. We study neural ODEs ($\mathtt{nODE}$s) with vector fields following the Barabsi-Barzel form, trained on synthetic data from five common dynamical systems on graphs. Using the $\mathbb{S}^1$-model to generate graphs with realistic and tunable structure, we find that degree heterogeneity and the type of dynamical system are the primary factors in determining $\mathtt{nODE}$s' ability to generalize across graph sizes and properties. This extends to $\mathtt{nODE}$s' ability to capture fixed points and maintain performance amid missing data. Average clustering plays a secondary role in determining $\mathtt{nODE}$ performance. Our findings highlight $\mathtt{nODE}$s as a powerful approach to understanding complex systems but underscore challenges emerging from degree heterogeneity and clustering in realistic graphs.]]></description>
<pubDate>Mon, 09 Feb 2026 18:28:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Beyond Transcripts: A Renewed Perspective on Audio Chaptering]]></title>
<link>http://arxiv.org/abs/2602.08979v1</link>
<guid>2602.08979v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SD, cs.CL
Authors: Fabian Retkowski, Maike Zfle, Thai Binh Nguyen, Jan Niehues, Alexander Waibel
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Audio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributions: (1) a systematic comparison between text-based models with acoustic features, a novel audio-only architecture (AudioSeg) operating on learned audio representations, and multimodal LLMs; (2) empirical analysis of factors affecting performance, including transcript quality, acoustic features, duration, and speaker composition; and (3) formalized evaluation protocols contrasting transcript-dependent text-space protocols with transcript-invariant time-space protocols. Our experiments on YTSeg reveal that AudioSeg substantially outperforms text-based approaches, pauses provide the largest acoustic gains, and MLLMs remain limited by context length and weak instruction following, yet MLLMs are promising on shorter audio.]]></description>
<pubDate>Mon, 09 Feb 2026 18:28:10 +0000</pubDate>
</item>
<item>
<title><![CDATA[Distributionally Robust Optimization via Generative Ambiguity Modeling]]></title>
<link>http://arxiv.org/abs/2602.08976v1</link>
<guid>2602.08976v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jiaqi Wen, Jianyi Yang
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.]]></description>
<pubDate>Mon, 09 Feb 2026 18:20:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models]]></title>
<link>http://arxiv.org/abs/2602.08971v1</link>
<guid>2602.08971v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.RO
Authors: Yu Shang, Zhuohang Li, Yiding Ma, Weikang Su, Xin Jin et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.]]></description>
<pubDate>Mon, 09 Feb 2026 18:09:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation]]></title>
<link>http://arxiv.org/abs/2602.08968v1</link>
<guid>2602.08968v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Lucas Maes, Quentin Le Lidec, Dan Haramati, Nassim Massaudi, Damien Scieur et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.]]></description>
<pubDate>Mon, 09 Feb 2026 18:04:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.08965v1</link>
<guid>2602.08965v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.MA, cs.LG
Authors: John Gardiner, Orlando Romero, Brendan Tivnan, Nicol Dal Fabbro, George J. Pappas
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).]]></description>
<pubDate>Mon, 09 Feb 2026 18:01:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents]]></title>
<link>http://arxiv.org/abs/2602.08964v1</link>
<guid>2602.08964v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CL, cs.CY
Authors: Raghu Arghal, Fade Chen, Niall Dalton, Evgenii Kortukov, Calum McNamara et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.]]></description>
<pubDate>Mon, 09 Feb 2026 18:00:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting]]></title>
<link>http://arxiv.org/abs/2602.08962v1</link>
<guid>2602.08962v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.RO
Authors: Guangxun Zhu, Xuan Liu, Nicolas Pugeault, Chongfeng Wei, Edmond S. L. Ho
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D]]></description>
<pubDate>Mon, 09 Feb 2026 17:58:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields]]></title>
<link>http://arxiv.org/abs/2602.08958v1</link>
<guid>2602.08958v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Weihan Luo, Lily Goli, Sherwin Bahmani, Felix Taubner, Andrea Tagliasacchi et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.]]></description>
<pubDate>Mon, 09 Feb 2026 17:55:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[How Should We Model the Probability of a Language?]]></title>
<link>http://arxiv.org/abs/2602.08951v1</link>
<guid>2602.08951v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Rasul Dent, Pedro Ortiz Suarez, Thibault Clrice, Benot Sagot
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.]]></description>
<pubDate>Mon, 09 Feb 2026 17:46:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room]]></title>
<link>http://arxiv.org/abs/2602.08949v1</link>
<guid>2602.08949v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.SE
Authors: Mohammad Morsali, Siavash H. Khajavi
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.]]></description>
<pubDate>Mon, 09 Feb 2026 17:44:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute]]></title>
<link>http://arxiv.org/abs/2602.08948v1</link>
<guid>2602.08948v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.CL
Authors: Chen Jin, Ryutaro Tanno, Tom Diethe, Philip Teare
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.]]></description>
<pubDate>Mon, 09 Feb 2026 17:44:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search]]></title>
<link>http://arxiv.org/abs/2602.08945v1</link>
<guid>2602.08945v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.CY
Authors: Sahajpreet Singh, Kokil Jaidka, Min-Yen Kan
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in "cold start" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.]]></description>
<pubDate>Mon, 09 Feb 2026 17:42:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[pixelLOG: Logging of Online Gameplay for Cognitive Research]]></title>
<link>http://arxiv.org/abs/2602.08941v1</link>
<guid>2602.08941v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI
Authors: Zeyu Lu, Dennis L. Barbour
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Traditional cognitive assessments often rely on isolated, output-focused measurements that may fail to capture the complexity of human cognition in naturalistic settings. We present pixelLOG, a high-performance data collection framework for Spigot-based Minecraft servers designed specifically for process-based cognitive research. Unlike existing frameworks tailored only for artificial intelligence agents, pixelLOG also enables human behavioral tracking in multi-player/multi-agent environments. Operating at configurable frequencies up to and exceeding 20 updates per second, the system captures comprehensive behavioral data through a hybrid approach of active state polling and passive event monitoring. By leveraging Spigot's extensible API, pixelLOG facilitates robust session isolation and produces structured JSON outputs integrable with standard analytical pipelines. This framework bridges the gap between decontextualized laboratory assessments and richer, more ecologically valid tasks, enabling high-resolution analysis of cognitive processes as they unfold in complex, virtual environments.]]></description>
<pubDate>Mon, 09 Feb 2026 17:38:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse]]></title>
<link>http://arxiv.org/abs/2602.08939v1</link>
<guid>2602.08939v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Longling Geng, Andy Ouyang, Theodore Wu, Daphne Barretto, Matthew John Hayes et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench]]></description>
<pubDate>Mon, 09 Feb 2026 17:36:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors]]></title>
<link>http://arxiv.org/abs/2602.08934v1</link>
<guid>2602.08934v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CR
Authors: Suraj Ranganath, Atharv Ramesh
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.]]></description>
<pubDate>Mon, 09 Feb 2026 17:33:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[Provably robust learning of regression neural networks using $$-divergences]]></title>
<link>http://arxiv.org/abs/2602.08933v1</link>
<guid>2602.08933v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG, cs.NE, stat.ME
Authors: Abhik Ghosh, Suryasis Jana
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Regression neural networks (NNs) are most commonly trained by minimizing the mean squared prediction error, which is highly sensitive to outliers and data contamination. Existing robust training methods for regression NNs are often limited in scope and rely primarily on empirical validation, with only a few offering partial theoretical guarantees. In this paper, we propose a new robust learning framework for regression NNs based on the $$-divergence (also known as the density power divergence) which we call `rRNet'. It applies to a broad class of regression NNs, including models with non-smooth activation functions and error densities, and recovers the classical maximum likelihood learning as a special case. The rRNet is implemented via an alternating optimization scheme, for which we establish convergence guarantees to stationary points under mild, verifiable conditions. The (local) robustness of rRNet is theoretically characterized through the influence functions of both the parameter estimates and the resulting rRNet predictor, which are shown to be bounded for suitable choices of the tuning parameter $$, depending on the error density. We further prove that rRNet attains the optimal 50\% asymptotic breakdown point at the assumed model for all $\in(0, 1]$, providing a strong global robustness guarantee that is largely absent for existing NN learning methods. Our theoretical results are complemented by simulation experiments and real-data analyses, illustrating practical advantages of rRNet over existing approaches in both function approximation problems and prediction tasks with noisy observations.]]></description>
<pubDate>Mon, 09 Feb 2026 17:32:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Online monotone density estimation and log-optimal calibration]]></title>
<link>http://arxiv.org/abs/2602.08927v1</link>
<guid>2602.08927v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG, stat.ME
Authors: Rohan Hore, Ruodu Wang, Aaditya Ramdas
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study the problem of online monotone density estimation, where density estimators must be constructed in a predictable manner from sequentially observed data. We propose two online estimators: an online analogue of the classical Grenander estimator, and an expert aggregation estimator inspired by exponential weighting methods from the online learning literature. In the well-specified stochastic setting, where the underlying density is monotone, we show that the expected cumulative log-likelihood gap between the online estimators and the true density admits an $O(n^{1/3})$ bound. We further establish a $\sqrt{n\log{n}}$ pathwise regret bound for the expert aggregation estimator relative to the best offline monotone estimator chosen in hindsight, under minimal regularity assumptions on the observed sequence. As an application of independent interest, we show that the problem of constructing log-optimal p-to-e calibrators for sequential hypothesis testing can be formulated as an online monotone density estimation problem. We adapt the proposed estimators to build empirically adaptive p-to-e calibrators and establish their optimality. Numerical experiments illustrate the theoretical results.]]></description>
<pubDate>Mon, 09 Feb 2026 17:29:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce]]></title>
<link>http://arxiv.org/abs/2602.08923v1</link>
<guid>2602.08923v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.DC, cs.NI
Authors: Wenchen Han, Shay Vargaftik, Michael Mitzenmacher, Ran Ben Basat
Institution: MIT
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.
  This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.
  We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.]]></description>
<pubDate>Mon, 09 Feb 2026 17:25:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration]]></title>
<link>http://arxiv.org/abs/2602.08920v1</link>
<guid>2602.08920v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Manh Cuong Dao, Quang Hung Pham, Phi Le Nguyen, Thao Nguyen Truong, Bryan Kian Hsiang Low et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.]]></description>
<pubDate>Mon, 09 Feb 2026 17:24:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion]]></title>
<link>http://arxiv.org/abs/2602.08917v1</link>
<guid>2602.08917v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IR, cs.AI
Authors: Minghan Li, Ercong Nie, Siqi Zhao, Tongna Chen, Huiping Huang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.]]></description>
<pubDate>Mon, 09 Feb 2026 17:16:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[AMS-HD: Hyperdimensional Computing for Real-Time and Energy-Efficient Acute Mountain Sickness Detection]]></title>
<link>http://arxiv.org/abs/2602.08916v1</link>
<guid>2602.08916v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SC, cs.ET, cs.LG
Authors: Abu Masum, Mehran Moghadam, M. Hassan Najafi, Bige Unluturk, Ulkuhan Guler et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Altitude sickness is a potentially life-threatening condition that impacts many individuals traveling to elevated altitudes. Timely detection is critical as symptoms can escalate rapidly. Early recognition enables simple interventions such as descent, oxygen, or medication, and prompt treatment can save lives by significantly lowering the risk of severe complications. Although conventional machine learning (ML) techniques have been applied to identify altitude sickness using physiological signals, such as heart rate, oxygen saturation, respiration rate, blood pressure, and body temperature, they often struggle to balance predictive performance with low hardware demands. In contrast, hyperdimensional computing (HDC) remains under-explored for this task with limited biomedical features, where it may offer a compelling alternative to existing classification models. Its vector symbolic framework is inherently suited to hardware-efficient design, making it a strong candidate for low-power systems like wearables. Leveraging lightweight computation and efficient streamlined memory usage, HDC enables real-time detection of altitude sickness from physiological parameters collected by wearable devices, achieving accuracy comparable to that of traditional ML models. We present AMS-HD, a novel system that integrates tailored feature extraction and Hadamard HV encoding to enhance both the precision and efficiency of HDC-based detection. This framework is well-positioned for deployment in wearable health monitoring platforms, enabling continuous, on-the-go tracking of acute altitude sickness.]]></description>
<pubDate>Mon, 09 Feb 2026 17:16:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[Gesturing Toward Abstraction: Multimodal Convention Formation in Collaborative Physical Tasks]]></title>
<link>http://arxiv.org/abs/2602.08914v1</link>
<guid>2602.08914v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI
Authors: Kiyosu Maeda, William P. McCarthy, Ching-Yi Tsai, Jeffrey Mu, Haoliang Wang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

A quintessential feature of human intelligence is the ability to create ad hoc conventions over time to achieve shared goals efficiently. We investigate how communication strategies evolve through repeated collaboration as people coordinate on shared procedural abstractions. To this end, we conducted an online unimodal study (n = 98) using natural language to probe abstraction hierarchies. In a follow-up lab study (n = 40), we examined how multimodal communication (speech and gestures) changed during physical collaboration. Pairs used augmented reality to isolate their partner's hand and voice; one participant viewed a 3D virtual tower and sent instructions to the other, who built the physical tower. Participants became faster and more accurate by establishing linguistic and gestural abstractions and using cross-modal redundancy to emphasize key changes from previous interactions. Based on these findings, we extend probabilistic models of convention formation to multimodal settings, capturing shifts in modality preferences. Our findings and model provide building blocks for designing convention-aware intelligent agents situated in the physical world.]]></description>
<pubDate>Mon, 09 Feb 2026 17:13:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems]]></title>
<link>http://arxiv.org/abs/2602.08913v1</link>
<guid>2602.08913v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Kateina Henclov, Vclav mdl
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Selecting interpretable feature sets in underdetermined ($n \ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.
  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.
  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.
  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.]]></description>
<pubDate>Mon, 09 Feb 2026 17:13:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit]]></title>
<link>http://arxiv.org/abs/2602.08909v1</link>
<guid>2602.08909v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Zhendong Wang, Cihan Ruan, Jingchuan Xiao, Chuqing Shi, Wei Jiang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.]]></description>
<pubDate>Mon, 09 Feb 2026 17:09:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[Positive Distribution Shift as a Framework for Understanding Tractable Learning]]></title>
<link>http://arxiv.org/abs/2602.08907v1</link>
<guid>2602.08907v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Marko Medvedev, Idan Attias, Elisabetta Cornacchia, Theodor Misiakiewicz, Gal Vardi et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.]]></description>
<pubDate>Mon, 09 Feb 2026 17:08:10 +0000</pubDate>
</item>
<item>
<title><![CDATA[Efficient and Stable Reinforcement Learning for Diffusion Language Models]]></title>
<link>http://arxiv.org/abs/2602.08905v1</link>
<guid>2602.08905v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Jiawei Liu, Xiting Wang, Yuanyuan Zhong, Defu Lian, Yu Yang
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.]]></description>
<pubDate>Mon, 09 Feb 2026 17:04:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs]]></title>
<link>http://arxiv.org/abs/2602.08901v1</link>
<guid>2602.08901v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Xuanqi Zhang, Haoyang Shang, Xiaoxiao Li
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.]]></description>
<pubDate>Mon, 09 Feb 2026 17:02:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation]]></title>
<link>http://arxiv.org/abs/2602.08896v1</link>
<guid>2602.08896v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IR, cs.AI
Authors: Yehua Huang, Penglei Sun, Zebin Chen, Zhenheng Tang, Xiaowen Chu
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity of large-scale, verified benchmarks and oversimplified evaluation metrics that fail to reflect real-world editorial workflows. To bridge this gap, we present OmniReview, a comprehensive dataset constructed by integrating multi-source academic platforms encompassing comprehensive scholarly profiles through the disambiguation pipeline, yielding 202, 756 verified review records. Based on this data, we introduce a three-tier hierarchical evaluaion framework to assess recommendations from recall to precise expert identification. From the method perspective, existing embedding-based approaches suffer from the information bottleneck of semantic compression and limited interpretability. To resolve these method limitations, we propose Profiling Scholars with Multi-gate Mixture-of-Experts (Pro-MMoE), a novel framework that synergizes Large Language Models (LLMs) with Multi-task Learning. Specifically, it utilizes LLM-generated semantic profiles to preserve fine-grained expertise nuances and interpretability, while employing a Task-Adaptive MMoE architecture to dynamically balance conflicting evaluation goals. Comprehensive experiments demonstrate that Pro-MMoE achieves state-of-the-art performance across six of seven metrics, establishing a new benchmark for realistic reviewer recommendation.]]></description>
<pubDate>Mon, 09 Feb 2026 16:57:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[Discrete Bridges for Mutual Information Estimation]]></title>
<link>http://arxiv.org/abs/2602.08894v1</link>
<guid>2602.08894v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Iryna Zabarianska, Sergei Kholkin, Grigoriy Ksenofontov, Ivan Butakov, Alexander Korotin
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.]]></description>
<pubDate>Mon, 09 Feb 2026 16:55:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[Winner's Curse Drives False Promises in Data-Driven Decisions: A Case Study in Refugee Matching]]></title>
<link>http://arxiv.org/abs/2602.08892v1</link>
<guid>2602.08892v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Hamsa Bastani, Osbert Bastani, Bryce McLaughlin
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

A major challenge in data-driven decision-making is accurate policy evaluation-i.e., guaranteeing that a learned decision-making policy achieves the promised benefits. A popular strategy is model-based policy evaluation, which estimates a model from data to infer counterfactual outcomes. This strategy is known to produce unwarrantedly optimistic estimates of the true benefit due to the winner's curse. We searched the recent literature on data-driven decision-making, identifying a sample of 55 papers published in the Management Science in the past decade; all but two relied on this flawed methodology. Several common justifications are provided: (1) the estimated models are accurate, stable, and well-calibrated, (2) the historical data uses random treatment assignment, (3) the model family is well-specified, and (4) the evaluation methodology uses sample splitting. Unfortunately, we show that no combination of these justifications avoids the winner's curse. First, we provide a theoretical analysis demonstrating that the winner's curse can cause large, spurious reported benefits even when all these justifications hold. Second, we perform a simulation study based on the recent and consequential data-driven refugee matching problem. We construct a synthetic refugee matching environment (calibrated to closely match the real setting) but designed so that no assignment policy can improve expected employment compared to random assignment. Model-based methods report large, stable gains of around 60% even when the true effect is zero; these gains are on par with improvements of 22-75% reported in the literature. Our results provide strong evidence against model-based evaluation.]]></description>
<pubDate>Mon, 09 Feb 2026 16:54:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Scalable Delphi: Large Language Models for Structured Risk Estimation]]></title>
<link>http://arxiv.org/abs/2602.08889v1</link>
<guid>2602.08889v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Tobias Lorenz, Mario Fritz
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Quantitative risk assessment in high-stakes domains relies on structured expert elicitation to estimate unobservable properties. The gold standard - the Delphi method - produces calibrated, auditable judgments but requires months of coordination and specialist time, placing rigorous risk assessment out of reach for most applications. We investigate whether Large Language Models (LLMs) can serve as scalable proxies for structured expert elicitation. We propose Scalable Delphi, adapting the classical protocol for LLMs with diverse expert personas, iterative refinement, and rationale sharing. Because target quantities are typically unobservable, we develop an evaluation framework based on necessary conditions: calibration against verifiable proxies, sensitivity to evidence, and alignment with human expert judgment. We evaluate in the domain of AI-augmented cybersecurity risk, using three capability benchmarks and independent human elicitation studies. LLM panels achieve strong correlations with benchmark ground truth (Pearson r=0.87-0.95), improve systematically as evidence is added, and align with human expert panels - in one comparison, closer to a human panel than the two human panels are to each other. This demonstrates that LLM-based elicitation can extend structured expert judgment to settings where traditional methods are infeasible, reducing elicitation time from months to minutes.]]></description>
<pubDate>Mon, 09 Feb 2026 16:52:03 +0000</pubDate>
</item>
<item>
<title><![CDATA[DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories]]></title>
<link>http://arxiv.org/abs/2602.08887v1</link>
<guid>2602.08887v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SE, cs.AI
Authors: Adam Trendowicz, Daniel Seifert, Andreas Jedlitschka, Marcus Ciolkowski, Anton Strahilov
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach "DeepQuali", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.]]></description>
<pubDate>Mon, 09 Feb 2026 16:49:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[Contrastive Learning for Diversity-Aware Product Recommendations in Retail]]></title>
<link>http://arxiv.org/abs/2602.08886v1</link>
<guid>2602.08886v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IR, cs.LG
Authors: Vasileios Karlis, Ezgi Yldrm, David Vos, Maarten de Rijke
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recommender systems often struggle with long-tail distributions and limited item catalog exposure, where a small subset of popular items dominates recommendations. This challenge is especially critical in large-scale online retail settings with extensive and diverse product assortments. This paper introduces an approach to enhance catalog coverage without compromising recommendation quality in the existing digital recommendation pipeline at IKEA Retail. Drawing inspiration from recent advances in negative sampling to address popularity bias, we integrate contrastive learning with carefully selected negative samples. Through offline and online evaluations, we demonstrate that our method improves catalog coverage, ensuring a more diverse set of recommendations yet preserving strong recommendation performance.]]></description>
<pubDate>Mon, 09 Feb 2026 16:48:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression]]></title>
<link>http://arxiv.org/abs/2602.08885v1</link>
<guid>2602.08885v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.SC
Authors: Paul Saegert, Ullrich Kthe
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.]]></description>
<pubDate>Mon, 09 Feb 2026 16:47:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[Designing Multi-Robot Ground Video Sensemaking with Public Safety Professionals]]></title>
<link>http://arxiv.org/abs/2602.08882v1</link>
<guid>2602.08882v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.CV
Authors: Puqi Zhou, Ali Asgarov, Aafiya Hussain, Wonjoon Park, Amit Paudyal et al.
Institution: Meta AI
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Videos from fleets of ground robots can advance public safety by providing scalable situational awareness and reducing professionals' burden. Yet little is known about how to design and integrate multi-robot videos into public safety workflows. Collaborating with six police agencies, we examined how such videos could be made practical. In Study 1, we presented the first testbed for multi-robot ground video sensemaking. The testbed includes 38 events-of-interest (EoI) relevant to public safety, a dataset of 20 robot patrol videos (10 day/night pairs) covering EoI types, and 6 design requirements aimed at improving current video sensemaking practices. In Study 2, we built MRVS, a tool that augments multi-robot patrol video streams with a prompt-engineered video understanding model. Participants reported reduced manual workload and greater confidence with LLM-based explanations, while noting concerns about false alarms and privacy. We conclude with implications for designing future multi-robot video sensemaking tools. The testbed is available at https://github.com/Puqi7/MRVS\_VideoSensemaking]]></description>
<pubDate>Mon, 09 Feb 2026 16:43:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[Differentiable Logical Programming for Quantum Circuit Discovery and Optimization]]></title>
<link>http://arxiv.org/abs/2602.08880v1</link>
<guid>2602.08880v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Antonin Sulc
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Designing high-fidelity quantum circuits remains challenging, and current paradigms often depend on heuristic, fixed-ansatz structures or rule-based compilers that can be suboptimal or lack generality. We introduce a neuro-symbolic framework that reframes quantum circuit design as a differentiable logic programming problem. Our model represents a scaffold of potential quantum gates and parameterized operations as a set of learnable, continuous ``truth values'' or ``switches,'' $s \in [0, 1]^N$. These switches are optimized via standard gradient descent to satisfy a user-defined set of differentiable, logical axioms (e.g., correctness, simplicity, robustness). We provide a theoretical formulation bridging continuous logic (via T-norms) and unitary evolution (via geodesic interpolation), while addressing the barren plateau problem through biased initialization. We illustrate the approach on tasks including discovery of a 4-qubit Quantum Fourier Transform (QFT) from a scaffold of 21 candidate gates. We also report a hardware-aware adaptation experiment on the 133-qubit IBM Torino processor, where the method improved fidelity by 59.3 percentage points in a localized routing task while adapting to hardware failures.]]></description>
<pubDate>Mon, 09 Feb 2026 16:40:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Potentials for Dynamic Matching and Application to Heart Transplantation]]></title>
<link>http://arxiv.org/abs/2602.08878v1</link>
<guid>2602.08878v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Itai Zilberstein, Ioannis Anagnostides, Zachary W. Sollie, Arman Kilic, Tuomas Sandholm
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.]]></description>
<pubDate>Mon, 09 Feb 2026 16:39:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[Stress-Testing Alignment Audits With Prompt-Level Strategic Deception]]></title>
<link>http://arxiv.org/abs/2602.08877v1</link>
<guid>2602.08877v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Oliver Daniels, Perusha Moodley, Ben Marlin, David Lindner
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.]]></description>
<pubDate>Mon, 09 Feb 2026 16:38:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[Is Reasoning Capability Enough for Safety in Long-Context Language Models?]]></title>
<link>http://arxiv.org/abs/2602.08874v1</link>
<guid>2602.08874v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.CR
Authors: Yu Fu, Haz Sameen Shahgir, Huanli Gong, Zhipeng Wei, N. Benjamin Erichson et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.]]></description>
<pubDate>Mon, 09 Feb 2026 16:35:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Whose Name Comes Up? Benchmarking and Intervention-Based Auditing of LLM-Based Scholar Recommendation]]></title>
<link>http://arxiv.org/abs/2602.08873v1</link>
<guid>2602.08873v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IR, cs.AI, cs.CY, cs.SI
Authors: Lisette Espin-Noboa, Gonzalo Gabriel Mendez
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models (LLMs) are increasingly used for academic expert recommendation. Existing audits typically evaluate model outputs in isolation, largely ignoring end-user inference-time interventions. As a result, it remains unclear whether failures such as refusals, hallucinations, and uneven coverage stem from model choice or deployment decisions. We introduce LLMScholarBench, a benchmark for auditing LLM-based scholar recommendation that jointly evaluates model infrastructure and end-user interventions across multiple tasks. LLMScholarBench measures both technical quality and social representation using nine metrics. We instantiate the benchmark in physics expert recommendation and audit 22 LLMs under temperature variation, representation-constrained prompting, and retrieval-augmented generation (RAG) via web search. Our results show that end-user interventions do not yield uniform improvements but instead redistribute error across dimensions. Higher temperature degrades validity, consistency, and factuality. Representation-constrained prompting improves diversity at the expense of factuality, while RAG primarily improves technical quality while reducing diversity and parity. Overall, end-user interventions reshape trade-offs rather than providing a general fix. We release code and data that can be adapted to other disciplines by replacing domain-specific ground truth and metrics.]]></description>
<pubDate>Mon, 09 Feb 2026 16:34:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[Large Language Models for Geolocation Extraction in Humanitarian Crisis Response]]></title>
<link>http://arxiv.org/abs/2602.08872v1</link>
<guid>2602.08872v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.IR
Authors: G. Cafferata, T. Demarco, K. Kalimeri, Y. Mejova, M. G. Beir
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.]]></description>
<pubDate>Mon, 09 Feb 2026 16:34:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection]]></title>
<link>http://arxiv.org/abs/2602.08868v1</link>
<guid>2602.08868v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Junru Zhang, Lang Feng, Haoran Shi, Xu Guo, Han Yu et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.]]></description>
<pubDate>Mon, 09 Feb 2026 16:30:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[Understanding Dynamic Compute Allocation in Recurrent Transformers]]></title>
<link>http://arxiv.org/abs/2602.08864v1</link>
<guid>2602.08864v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI, cs.LG
Authors: Ibraheem Muhammad Moosa, Suhas Lohit, Ye Wang, Moitreya Chatterjee, Wenpeng Yin
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.]]></description>
<pubDate>Mon, 09 Feb 2026 16:27:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[Near-optimal Swap Regret Minimization for Convex Losses]]></title>
<link>http://arxiv.org/abs/2602.08862v1</link>
<guid>2602.08862v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.DS, stat.ML
Authors: Lunjia Hu, Jon Schneider, Yifan Wu
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We give a randomized online algorithm that guarantees near-optimal $\widetilde O(\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\widetilde O(\sqrt T)$ calibration error guarantee.]]></description>
<pubDate>Mon, 09 Feb 2026 16:26:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models]]></title>
<link>http://arxiv.org/abs/2602.08861v1</link>
<guid>2602.08861v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Xiangtian Zheng, Zishuo Wang, Yuxin Peng
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.]]></description>
<pubDate>Mon, 09 Feb 2026 16:24:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Magnitude Distance: A Geometric Measure of Dataset Similarity]]></title>
<link>http://arxiv.org/abs/2602.08859v1</link>
<guid>2602.08859v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Sahel Torkamani, Henry Gouk, Rik Sarkar
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.]]></description>
<pubDate>Mon, 09 Feb 2026 16:23:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[FlattenGPT: Depth Compression for Transformer with Layer Flattening]]></title>
<link>http://arxiv.org/abs/2602.08858v1</link>
<guid>2602.08858v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Ruihan Xu, Qingpei Guo, Yao Zhu, Xiangyang Ji, Ming Yang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\% of zero-shot performance with a compression ratio of 20\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.]]></description>
<pubDate>Mon, 09 Feb 2026 16:22:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[Discovering Interpretable Algorithms by Decompiling Transformers to RASP]]></title>
<link>http://arxiv.org/abs/2602.08857v1</link>
<guid>2602.08857v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CL
Authors: Xinting Huang, Aleksandra Bakalova, Satwik Bhattamishra, William Merrill, Michael Hahn
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.]]></description>
<pubDate>Mon, 09 Feb 2026 16:22:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization]]></title>
<link>http://arxiv.org/abs/2602.08855v1</link>
<guid>2602.08855v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Yang Qiu, Yixiong Zou, Jun Wang
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.]]></description>
<pubDate>Mon, 09 Feb 2026 16:20:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[Cutting Through the Noise: On-the-fly Outlier Detection for Robust Training of Machine Learning Interatomic Potentials]]></title>
<link>http://arxiv.org/abs/2602.08849v1</link>
<guid>2602.08849v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Terry C. W. Lam, Niamh O'Neill, Christoph Schran, Lars L. Schaaf
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The accuracy of machine learning interatomic potentials suffers from reference data that contains numerical noise. Often originating from unconverged or inconsistent electronic-structure calculations, this noise is challenging to identify. Existing mitigation strategies such as manual filtering or iterative refinement of outliers, require either substantial expert effort or multiple expensive retraining cycles, making them difficult to scale to large datasets. Here, we introduce an on-the-fly outlier detection scheme that automatically down-weights noisy samples, without requiring additional reference calculations. By tracking the loss distribution via an exponential moving average, this unsupervised method identifies outliers throughout a single training run. We show that this approach prevents overfitting and matches the performance of iterative refinement baselines with significantly reduced overhead. The method's effectiveness is demonstrated by recovering accurate physical observables for liquid water from unconverged reference data, including diffusion coefficients. Furthermore, we validate its scalability by training a foundation model for organic chemistry on the SPICE dataset, where it reduces energy errors by a factor of three. This framework provides a simple, automated solution for training robust models on imperfect datasets across dataset sizes.]]></description>
<pubDate>Mon, 09 Feb 2026 16:16:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[Deciding the Satisfiability of Combined Qualitative Constraint Networks]]></title>
<link>http://arxiv.org/abs/2602.08848v1</link>
<guid>2602.08848v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Quentin Cohen-Solal, Alexandre Niveau, Maroua Bouzid
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.]]></description>
<pubDate>Mon, 09 Feb 2026 16:14:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems]]></title>
<link>http://arxiv.org/abs/2602.08847v1</link>
<guid>2602.08847v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Lang Feng, Longtao Zheng, Shuo He, Fuxiang Zhang, Bo An
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.]]></description>
<pubDate>Mon, 09 Feb 2026 16:13:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[AMEM4Rec: Leveraging Cross-User Similarity for Memory Evolution in Agentic LLM Recommenders]]></title>
<link>http://arxiv.org/abs/2602.08837v1</link>
<guid>2602.08837v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IR, cs.LG
Authors: Minh-Duc Nguyen, Hai-Dang Kieu, Dung D. Le
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Agentic systems powered by Large Language Models (LLMs) have shown strong potential in recommender systems but remain hindered by several challenges. Fine-tuning LLMs is parameter-inefficient, and prompt-based agentic reasoning is limited by context length and hallucination risk. Moreover, existing agentic recommendation systems predominantly leverages semantic knowledge while neglecting the collaborative filtering (CF) signals essential for implicit preference modeling. To address these limitations, we propose AMEM4Rec, an agentic LLM-based recommender that learns collaborative signals in an end-to-end manner through cross-user memory evolution. AMEM4Rec stores abstract user behavior patterns from user histories in a global memory pool. Within this pool, memories are linked to similar existing ones and iteratively evolved to reinforce shared cross-user patterns, enabling the system to become aware of CF signals without relying on a pre-trained CF model. Extensive experiments on Amazon and MIND datasets show that AMEM4Rec consistently outperforms state-of-the-art LLM-based recommenders, demonstrating the effectiveness of evolving memory-guided collaborative filtering.]]></description>
<pubDate>Mon, 09 Feb 2026 16:06:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.08835v1</link>
<guid>2602.08835v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.CY, cs.LG
Authors: Andrs Holgado-Snchez, Peter Vamplew, Richard Dazeley, Sascha Ossowski, Holger Billhardt
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.]]></description>
<pubDate>Mon, 09 Feb 2026 16:06:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.08828v1</link>
<guid>2602.08828v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Hao Tan, Jun Lan, Senyuan Shi, Zichang Tan, Zijian Yu et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.]]></description>
<pubDate>Mon, 09 Feb 2026 16:00:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[Affective Flow Language Model for Emotional Support Conversation]]></title>
<link>http://arxiv.org/abs/2602.08826v1</link>
<guid>2602.08826v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Chenghui Zou, Ning Wang, Tiesunlong Shen, Luwei Xiao, Chuan Ma et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.]]></description>
<pubDate>Mon, 09 Feb 2026 15:58:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Methodology for Effective Surrogate Learning in Complex Optimization]]></title>
<link>http://arxiv.org/abs/2602.08825v1</link>
<guid>2602.08825v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.NE
Authors: Tomohiro Harada, Enrique Alba, Gabriel Luque
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Solving complex problems requires continuous effort in developing theory and practice to cope with larger, more difficult scenarios. Working with surrogates is normal for creating a proxy that realistically models the problem into the computer. Thus, the question of how to best define and characterize such a surrogate model is of the utmost importance. In this paper, we introduce the PTME methodology to study deep learning surrogates by analyzing their Precision, Time, Memory, and Energy consumption. We argue that only a combination of numerical and physical performance can lead to a surrogate that is both a trusted scientific substitute for the real problem and an efficient experimental artifact for scalable studies. Here, we propose different surrogates for a real problem in optimally organizing the network of traffic lights in European cities and perform a PTME study on the surrogates' sampling methods, dataset sizes, and resource consumption. We further use the built surrogates in new optimization metaheuristics for decision-making in real cities. We offer better techniques and conclude that the PTME methodology can be used as a guideline for other applications and solvers.]]></description>
<pubDate>Mon, 09 Feb 2026 15:57:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications]]></title>
<link>http://arxiv.org/abs/2602.08822v1</link>
<guid>2602.08822v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yao Pu, Yiming Shi, Zhenxi Zhang, Peixin Yu, Yitao Zhuang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.]]></description>
<pubDate>Mon, 09 Feb 2026 15:56:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing]]></title>
<link>http://arxiv.org/abs/2602.08820v1</link>
<guid>2602.08820v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Hao Yang, Zhiyu Tan, Jia Gong, Luozheng Qin, Hesen Chen et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.]]></description>
<pubDate>Mon, 09 Feb 2026 15:56:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Bayesian Preference Learning for Test-Time Steerable Reward Models]]></title>
<link>http://arxiv.org/abs/2602.08819v1</link>
<guid>2602.08819v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CL
Authors: Jiwoo Hong, Shao Tang, Zhipeng Wang
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.]]></description>
<pubDate>Mon, 09 Feb 2026 15:55:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Kirin: Improving ANN efficiency with SNN Hybridization]]></title>
<link>http://arxiv.org/abs/2602.08817v1</link>
<guid>2602.08817v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Chenyu Wang, Zhanglu Yan, Zhi Zhou, Xu Chen, Weng-Fai Wong
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.]]></description>
<pubDate>Mon, 09 Feb 2026 15:53:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity]]></title>
<link>http://arxiv.org/abs/2602.08816v1</link>
<guid>2602.08816v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CY, cs.SE
Authors: James Jewitt, Gopi Krishnan Rajbahadur, Hao Li, Bram Adams, Ahmed E. Hassan
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\rightarrow$ model $\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\% of datasets and 95.8\% of models lack the required license text, only 2.3\% of datasets and 3.2\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\% of models preserve compliant dataset notices and only 5.75\% of applications preserve compliant model notices (with just 6.38\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.]]></description>
<pubDate>Mon, 09 Feb 2026 15:51:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation]]></title>
<link>http://arxiv.org/abs/2602.08815v1</link>
<guid>2602.08815v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Yanglei Gan, Peng He, Yuxiang Cai, Run Lin, Guanyu Zhou et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.]]></description>
<pubDate>Mon, 09 Feb 2026 15:50:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Robust Policy Optimization to Prevent Catastrophic Forgetting]]></title>
<link>http://arxiv.org/abs/2602.08813v1</link>
<guid>2602.08813v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Mahdi Sabbaghi, George Pappas, Adel Javanmard, Hamed Hassani
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.
  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.]]></description>
<pubDate>Mon, 09 Feb 2026 15:50:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[$\texttt{lrnnx}$: A library for Linear RNNs]]></title>
<link>http://arxiv.org/abs/2602.08810v1</link>
<guid>2602.08810v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Karan Bania, Soham Kalburgi, Manit Tanwar,  Dhruthi, Aditya Nagarsekar et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architectures have been proposed, each introducing distinct parameterizations, discretization schemes, and implementation constraints. However, existing implementations are fragmented across different software frameworks, often rely on framework-specific optimizations, and in some cases require custom CUDA kernels or lack publicly available code altogether. As a result, using, comparing, or extending LRNNs requires substantial implementation effort. To address this, we introduce $\texttt{lrnnx}$, a unified software library that implements several modern LRNN architectures under a common interface. The library exposes multiple levels of control, allowing users to work directly with core components or higher-level model abstractions. $\texttt{lrnnx}$ aims to improve accessibility, reproducibility, and extensibility of LRNN research and applications. We make our code available under a permissive MIT license.]]></description>
<pubDate>Mon, 09 Feb 2026 15:48:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI]]></title>
<link>http://arxiv.org/abs/2602.08809v1</link>
<guid>2602.08809v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Karim Haroun, Aya Zitouni, Aicha Zenakhri, Meriem Amel Guessoum, Larbi Boubchir
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.]]></description>
<pubDate>Mon, 09 Feb 2026 15:48:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures]]></title>
<link>http://arxiv.org/abs/2602.08804v1</link>
<guid>2602.08804v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Liming Zhou, Ailing Liu, Hongwei Liu, Min He, Heng Zhang
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.]]></description>
<pubDate>Mon, 09 Feb 2026 15:41:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework]]></title>
<link>http://arxiv.org/abs/2602.08797v1</link>
<guid>2602.08797v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Jiaming Liu, Cheng Ding, Daoqiang Zhang
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.]]></description>
<pubDate>Mon, 09 Feb 2026 15:37:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[The Use of AI Tools to Develop and Validate Q-Matrices]]></title>
<link>http://arxiv.org/abs/2602.08796v1</link>
<guid>2602.08796v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.CL
Authors: Kevin Fan, Jacquelyn A. Bialo, Hongli Li
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.]]></description>
<pubDate>Mon, 09 Feb 2026 15:36:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation]]></title>
<link>http://arxiv.org/abs/2602.08793v1</link>
<guid>2602.08793v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.DB
Authors: Yushi Sun, Xujia Li, Nan Tang, Quanqing Xu, Chuanhui Yang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.]]></description>
<pubDate>Mon, 09 Feb 2026 15:30:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems]]></title>
<link>http://arxiv.org/abs/2602.08792v1</link>
<guid>2602.08792v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI, cs.LG
Authors: Hao Dong, Eleni Chatzi, Olga Fink
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.]]></description>
<pubDate>Mon, 09 Feb 2026 15:29:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[Empirically Understanding the Value of Prediction in Allocation]]></title>
<link>http://arxiv.org/abs/2602.08786v1</link>
<guid>2602.08786v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CY, cs.LG
Authors: Unai Fischer-Abaigar, Emily Aiken, Christoph Kern, Juan Carlos Perdomo
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Institutions increasingly use prediction to allocate scarce resources. From a design perspective, better predictions compete with other investments, such as expanding capacity or improving treatment quality. Here, the big question is not how to solve a specific allocation problem, but rather which problem to solve. In this work, we develop an empirical toolkit to help planners form principled answers to this question and quantify the bottom-line welfare impact of investments in prediction versus other policy levers such as expanding capacity and improving treatment quality. Applying our framework in two real-world case studies on German employment services and poverty targeting in Ethiopia, we illustrate how decision-makers can reliably derive context-specific conclusions about the relative value of prediction in their allocation problem. We make our software toolkit, rvp, and parts of our data available in order to enable future empirical work in this area.]]></description>
<pubDate>Mon, 09 Feb 2026 15:25:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Graphop Analysis of Graph Neural Networks on Sparse Graphs: Generalization and Universal Approximation]]></title>
<link>http://arxiv.org/abs/2602.08785v1</link>
<guid>2602.08785v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Ofek Amran, Tom Gilat, Ron Levie
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generalization and approximation capabilities of message passing graph neural networks (MPNNs) are often studied by defining a compact metric on a space of input graphs under which MPNNs are Hlder continuous. Such analyses are of two varieties: 1) when the metric space includes graphs of unbounded sizes, the theory is only appropriate for dense graphs, and, 2) when studying sparse graphs, the metric space only includes graphs of uniformly bounded size. In this work, we present a unified approach, defining a compact metric on the space of graphs of all sizes, both sparse and dense, under which MPNNs are Hlder continuous. This leads to more powerful universal approximation theorems and generalization bounds than previous works. The theory is based on, and extends, a recent approach to graph limit theory called graphop analysis.]]></description>
<pubDate>Mon, 09 Feb 2026 15:25:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure]]></title>
<link>http://arxiv.org/abs/2602.08783v1</link>
<guid>2602.08783v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.CL
Authors: Zirui Li, Xuefeng Bai, Kehai Chen, Yizhi Li, Jian Yang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.]]></description>
<pubDate>Mon, 09 Feb 2026 15:25:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[Amortising Inference and Meta-Learning Priors in Neural Networks]]></title>
<link>http://arxiv.org/abs/2602.08782v1</link>
<guid>2602.08782v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Tommy Rochussen, Vincent Fortuin
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

One of the core facets of Bayesianism is in the updating of prior beliefs in light of new evidence$\text{ -- }$so how can we maintain a Bayesian approach if we have no prior beliefs in the first place? This is one of the central challenges in the field of Bayesian deep learning, where it is not clear how to represent beliefs about a prediction task by prior distributions over model parameters. Bridging the fields of Bayesian deep learning and probabilistic meta-learning, we introduce a way to $\textit{learn}$ a weights prior from a collection of datasets by introducing a way to perform per-dataset amortised variational inference. The model we develop can be viewed as a neural process whose latent variable is the set of weights of a BNN and whose decoder is the neural network parameterised by a sample of the latent variable itself. This unique model allows us to study the behaviour of Bayesian neural networks under well-specified priors, use Bayesian neural networks as flexible generative models, and perform desirable but previously elusive feats in neural processes such as within-task minibatching or meta-learning under extreme data-starvation.]]></description>
<pubDate>Mon, 09 Feb 2026 15:24:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars]]></title>
<link>http://arxiv.org/abs/2602.08775v1</link>
<guid>2602.08775v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.CG
Authors: Vineet Kumar Rakesh, Ahana Bhattacharjee, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg]]></description>
<pubDate>Mon, 09 Feb 2026 15:17:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization]]></title>
<link>http://arxiv.org/abs/2602.08774v1</link>
<guid>2602.08774v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Nicols Villagrn Prieto, Eduardo C. Garrido-Merchn
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.]]></description>
<pubDate>Mon, 09 Feb 2026 15:15:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[FreqLens: Interpretable Frequency Attribution for Time Series Forecasting]]></title>
<link>http://arxiv.org/abs/2602.08768v1</link>
<guid>2602.08768v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Chi-Sheng Chen, Xinyu Zhang, En-Jui Kuo, Guan-Ying Chen, Qiuzhe Xie et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \textsc{FreqLens} introduces two key innovations: (1) \emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \pm 0.1$h, 2.5\% error) and 12-hour half-daily cycle ($11.8 \pm 0.1$h, 1.6\% error) on Traffic, and weekly cycles ($10\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.]]></description>
<pubDate>Mon, 09 Feb 2026 15:08:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas]]></title>
<link>http://arxiv.org/abs/2602.08765v1</link>
<guid>2602.08765v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SE, cs.AI
Authors: Micah Villmow
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.]]></description>
<pubDate>Mon, 09 Feb 2026 15:06:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[Efficient Brain Extraction of MRI Scans with Mild to Moderate Neuropathology]]></title>
<link>http://arxiv.org/abs/2602.08764v1</link>
<guid>2602.08764v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.CV
Authors: Hjalti Thrastarson, Lotta M. Ellingsen
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Skull stripping magnetic resonance images (MRI) of the human brain is an important process in many image processing techniques, such as automatic segmentation of brain structures. Numerous methods have been developed to perform this task, however, they often fail in the presence of neuropathology and can be inconsistent in defining the boundary of the brain mask. Here, we propose a novel approach to skull strip T1-weighted images in a robust and efficient manner, aiming to consistently segment the outer surface of the brain, including the sulcal cerebrospinal fluid (CSF), while excluding the full extent of the subarachnoid space and meninges. We train a modified version of the U-net on silver-standard ground truth data using a novel loss function based on the signed-distance transform (SDT). We validate our model both qualitatively and quantitatively using held-out data from the training dataset, as well as an independent external dataset. The brain masks used for evaluation partially or fully include the subarachnoid space, which may introduce bias into the comparison; nonetheless, our model demonstrates strong performance on the held-out test data, achieving a consistent mean Dice similarity coefficient (DSC) of 0.964$\pm$0.006 and an average symmetric surface distance (ASSD) of 1.4mm$\pm$0.2mm. Performance on the external dataset is comparable, with a DSC of 0.958$\pm$0.006 and an ASSD of 1.7$\pm$0.2mm. Our method achieves performance comparable to or better than existing state-of-the-art methods for brain extraction, particularly in its highly consistent preservation of the brain's outer surface. The method is publicly available on GitHub.]]></description>
<pubDate>Mon, 09 Feb 2026 15:03:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training]]></title>
<link>http://arxiv.org/abs/2602.08762v1</link>
<guid>2602.08762v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CR
Authors: Wen Xu, Zhetao Li, Yong Xiao, Pengpeng Qiao, Mianxiong Dong et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.]]></description>
<pubDate>Mon, 09 Feb 2026 15:03:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views]]></title>
<link>http://arxiv.org/abs/2602.08755v1</link>
<guid>2602.08755v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Duc-Anh Nguyen, Nhien-An Le-Khac
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.]]></description>
<pubDate>Mon, 09 Feb 2026 14:58:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[Belief Offloading in Human-AI Interaction]]></title>
<link>http://arxiv.org/abs/2602.08754v1</link>
<guid>2602.08754v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.CY, cs.HC
Authors: Rose E. Guingrich, Dvija Mehta, Umang Bhatt
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.]]></description>
<pubDate>Mon, 09 Feb 2026 14:56:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[MVAnimate: Enhancing Character Animation with Multi-View Optimization]]></title>
<link>http://arxiv.org/abs/2602.08753v1</link>
<guid>2602.08753v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Tianyu Sun, Zhoujie Fu, Bang Zhang, Guosheng Lin
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.]]></description>
<pubDate>Mon, 09 Feb 2026 14:55:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms]]></title>
<link>http://arxiv.org/abs/2602.08751v1</link>
<guid>2602.08751v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Nobuyuki Ota
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that
  researchers can examine. Here we present CDT-II, an "AI microscope" whose attention maps are directly interpretable as regulatory structure.
  By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA
  self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional
  control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in
  their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B
  regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Two distinct attention mechanisms converge on an RNA
  processing module ($P = 1 \times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing
  regulatory structure rather than merely optimizing predictions.]]></description>
<pubDate>Mon, 09 Feb 2026 14:54:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[Shifting the Breaking Point of Flow Matching for Multi-Instance Editing]]></title>
<link>http://arxiv.org/abs/2602.08749v1</link>
<guid>2602.08749v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Carmine Zaccagnino, Fabio Quattrini, Enis Simsar, Marta Tintor Gazulla, Rita Cucchiara et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.]]></description>
<pubDate>Mon, 09 Feb 2026 14:52:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Expressive Power of GNNs for Boolean Satisfiability]]></title>
<link>http://arxiv.org/abs/2602.08745v1</link>
<guid>2602.08745v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Saku Peltonen, Roger Wattenhofer
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Machine learning approaches to solving Boolean Satisfiability (SAT) aim to replace handcrafted heuristics with learning-based models. Graph Neural Networks have emerged as the main architecture for SAT solving, due to the natural graph representation of Boolean formulas. We analyze the expressive power of GNNs for SAT solving through the lens of the Weisfeiler-Leman (WL) test. As our main result, we prove that the full WL hierarchy cannot, in general, distinguish between satisfiable and unsatisfiable instances. We show that indistinguishability under higher-order WL carries over to practical limitations for WL-bounded solvers that set variables sequentially. We further study the expressivity required for several important families of SAT instances, including regular, random and planar instances. To quantify expressivity needs in practice, we conduct experiments on random instances from the G4SAT benchmark and industrial instances from the International SAT Competition. Our results suggest that while random instances are largely distinguishable, industrial instances often require more expressivity to predict a satisfying assignment.]]></description>
<pubDate>Mon, 09 Feb 2026 14:48:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Welfarist Formulations for Diverse Similarity Search]]></title>
<link>http://arxiv.org/abs/2602.08742v1</link>
<guid>2602.08742v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.DS, cs.CG, cs.GT, cs.IR, cs.LG
Authors: Siddharth Barman, Nirjhar Das, Shivam Gupta, Kirankumar Shiragur
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Nearest Neighbor Search (NNS) is a fundamental problem in data structures with wide-ranging applications, such as web search, recommendation systems, and, more recently, retrieval-augmented generations (RAG). In such recent applications, in addition to the relevance (similarity) of the returned neighbors, diversity among the neighbors is a central requirement. In this paper, we develop principled welfare-based formulations in NNS for realizing diversity across attributes. Our formulations are based on welfare functions -- from mathematical economics -- that satisfy central diversity (fairness) and relevance (economic efficiency) axioms. With a particular focus on Nash social welfare, we note that our welfare-based formulations provide objective functions that adaptively balance relevance and diversity in a query-dependent manner. Notably, such a balance was not present in the prior constraint-based approach, which forced a fixed level of diversity and optimized for relevance. In addition, our formulation provides a parametric way to control the trade-off between relevance and diversity, providing practitioners with flexibility to tailor search results to task-specific requirements. We develop efficient nearest neighbor algorithms with provable guarantees for the welfare-based objectives. Notably, our algorithm can be applied on top of any standard ANN method (i.e., use standard ANN method as a subroutine) to efficiently find neighbors that approximately maximize our welfare-based objectives. Experimental results demonstrate that our approach is practical and substantially improves diversity while maintaining high relevance of the retrieved neighbors.]]></description>
<pubDate>Mon, 09 Feb 2026 14:42:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy]]></title>
<link>http://arxiv.org/abs/2602.08740v1</link>
<guid>2602.08740v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Gaifan Zhang, Danushka Bollegala
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.]]></description>
<pubDate>Mon, 09 Feb 2026 14:41:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models]]></title>
<link>http://arxiv.org/abs/2602.08735v1</link>
<guid>2602.08735v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.]]></description>
<pubDate>Mon, 09 Feb 2026 14:39:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.08734v1</link>
<guid>2602.08734v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: David Hudk, Maris F. L. Galesloot, Martin Tappler, Martin Kureka, Nils Jansen et al.
Institution: Apple
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.]]></description>
<pubDate>Mon, 09 Feb 2026 14:39:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Foundation Inference Models for Ordinary Differential Equations]]></title>
<link>http://arxiv.org/abs/2602.08733v1</link>
<guid>2602.08733v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Maximilian Mauel, Johannes R. Hbers, David Berghaus, Patrick Seifner, Ramses J. Sanchez
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.]]></description>
<pubDate>Mon, 09 Feb 2026 14:39:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation]]></title>
<link>http://arxiv.org/abs/2602.08730v1</link>
<guid>2602.08730v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Shanshan Wang, Ziying Feng, Xiaozheng Shen, Xun Yang, Pichao Wang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA]]></description>
<pubDate>Mon, 09 Feb 2026 14:37:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework]]></title>
<link>http://arxiv.org/abs/2602.08727v1</link>
<guid>2602.08727v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Johannes Thalhammer, Tina Dorosti, Sebastian Peterhansl, Daniela Pfeiffer, Franz Pfeiffer et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.]]></description>
<pubDate>Mon, 09 Feb 2026 14:36:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training]]></title>
<link>http://arxiv.org/abs/2602.08726v1</link>
<guid>2602.08726v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Khadija Iddrisu, Waseem Shariff, Suzanne Little, Noel OConnor
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.]]></description>
<pubDate>Mon, 09 Feb 2026 14:34:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing]]></title>
<link>http://arxiv.org/abs/2602.08725v1</link>
<guid>2602.08725v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yongwen Lai, Chaoqun Wang, Shaobo Min
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.]]></description>
<pubDate>Mon, 09 Feb 2026 14:34:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering]]></title>
<link>http://arxiv.org/abs/2602.08724v1</link>
<guid>2602.08724v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.GR
Authors: Geng Lin, Matthias Zwicker
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.]]></description>
<pubDate>Mon, 09 Feb 2026 14:34:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[Data Reconstruction: Identifiability and Optimization with Sample Splitting]]></title>
<link>http://arxiv.org/abs/2602.08723v1</link>
<guid>2602.08723v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CR, stat.ML
Authors: Yujie Shen, Zihan Wang, Jian Qian, Qi Lei
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Training data reconstruction from KKT conditions has shown striking empirical success, yet it remains unclear when the resulting KKT equations have unique solutions and, even in identifiable regimes, how to reliably recover solutions by optimization. This work hereby focuses on these two complementary questions: identifiability and optimization. On the identifiability side, we discuss the sufficient conditions for KKT system of two-layer networks with polynomial activations to uniquely determine the training data, providing a theoretical explanation of when and why reconstruction is possible. On the optimization side, we introduce sample splitting, a curvature-aware refinement step applicable to general reconstruction objectives (not limited to KKT-based formulations): it creates additional descent directions to escape poor stationary points and refine solutions. Experiments demonstrate that augmenting several existing reconstruction methods with sample splitting consistently improves reconstruction performance.]]></description>
<pubDate>Mon, 09 Feb 2026 14:33:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill]]></title>
<link>http://arxiv.org/abs/2602.08722v1</link>
<guid>2602.08722v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Dalton Jones, Junyoung Park, Matthew Morse, Mingu Lee, Chris Lott et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.]]></description>
<pubDate>Mon, 09 Feb 2026 14:32:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images]]></title>
<link>http://arxiv.org/abs/2602.08717v1</link>
<guid>2602.08717v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Farnaz Khun Jush, Grit Werner, Mark Klemens, Matthias Lenga
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.]]></description>
<pubDate>Mon, 09 Feb 2026 14:26:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments]]></title>
<link>http://arxiv.org/abs/2602.08716v1</link>
<guid>2602.08716v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Shangrui Nie, Kian Omoomi, Lucie Flek, Zhixue Zhao, Charles Welch
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.]]></description>
<pubDate>Mon, 09 Feb 2026 14:25:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[Exploring SAIG Methods for an Objective Evaluation of XAI]]></title>
<link>http://arxiv.org/abs/2602.08715v1</link>
<guid>2602.08715v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Miquel Mir-Nicolau, Gabriel Moy-Alcover, Anna Arias-Duart
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.]]></description>
<pubDate>Mon, 09 Feb 2026 14:24:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions]]></title>
<link>http://arxiv.org/abs/2602.08711v1</link>
<guid>2602.08711v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Linli Yao, Yuancheng Wei, Yaojie Zhang, Lei Li, Xinlong Chen et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.]]></description>
<pubDate>Mon, 09 Feb 2026 14:21:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[FactSim: Fact-Checking for Opinion Summarization]]></title>
<link>http://arxiv.org/abs/2602.08709v1</link>
<guid>2602.08709v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Leandro Anghinoni, Jorge Sanchez
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.]]></description>
<pubDate>Mon, 09 Feb 2026 14:21:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[Intermediate Results on the Complexity of STRIPS$_{1}^{1}$]]></title>
<link>http://arxiv.org/abs/2602.08708v1</link>
<guid>2602.08708v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Stefan Edelkamp, Ji Fink, Petr Gregor, Anders Jonsson, Bernhard Nebel
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.]]></description>
<pubDate>Mon, 09 Feb 2026 14:21:10 +0000</pubDate>
</item>
<item>
<title><![CDATA[Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers]]></title>
<link>http://arxiv.org/abs/2602.08707v1</link>
<guid>2602.08707v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.CY, cs.HC
Authors: Aditya Gulati, Nuria Oliver
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.]]></description>
<pubDate>Mon, 09 Feb 2026 14:21:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[Technosocial risks of ideal emotion recognition technologies: A defense of the (social) value of emotional expressions]]></title>
<link>http://arxiv.org/abs/2602.08706v1</link>
<guid>2602.08706v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI, cs.ET
Authors: Alexandra Pregent
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The prospect of AI systems that I call ideal emotion recognition technologies (ERTs) is often defended on the assumption that social life would benefit from increased affective transparency. This paper challenges that assumption by examining the technosocial risks posed by ideal ERTs, understood as multimodal systems capable of reliably inferring inner affective states in real time. Drawing on philosophical accounts of emotional expression and social practice, as well as empirical work in affective science and social psychology, I argue that the appeal of such systems rests on a misunderstanding of the social functions of emotional expression. Emotional expressions function not only as read-outs of inner states, but also as tools for coordinating action, enabling moral repair, sustaining interpersonal trust, and supporting collective norms. These functions depend on a background of partial opacity and epistemic friction. When deployed in socially authoritative or evaluative contexts, ideal ERTs threaten this expressive space by collapsing epistemic friction, displacing relational meaning with technology-mediated affective profiles, and narrowing the space for aspirational and role-sensitive expressions. The result is a drift towards affective determinism and ambient forms of affective auditing, which undermine both social cohesion and individual agency. I argue that, although it is intuitive to think that increasing accuracy would legitimise such systems, in the case of ERTs accuracy does not straightforwardly justify their deployment, and may, in some contexts, provide a reason for regulatory restraint. I conclude by defending a function-first regulatory approach that treats expressive discretion and intentional emotional expression as constitutive of certain social goods, and that accordingly seeks to protect these goods from excessive affective legibility.]]></description>
<pubDate>Mon, 09 Feb 2026 14:20:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search]]></title>
<link>http://arxiv.org/abs/2602.08700v1</link>
<guid>2602.08700v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.HC, cs.IR
Authors: Clemencia Siro, Zahra Abbasiantaeb, Yifei Yuan, Mohammad Aliannejadi, Maarten de Rijke
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.]]></description>
<pubDate>Mon, 09 Feb 2026 14:16:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm]]></title>
<link>http://arxiv.org/abs/2602.08699v1</link>
<guid>2602.08699v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Xiaogang Xu, Kun Zhou, Tao Hu, Jiafei Wu, Ruixing Wang et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.]]></description>
<pubDate>Mon, 09 Feb 2026 14:15:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Challenges in Translating Technical Lectures: Insights from the NPTEL]]></title>
<link>http://arxiv.org/abs/2602.08698v1</link>
<guid>2602.08698v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Basudha Raje, Sadanand Venkatraman, Nandana TP, Soumyadeepa Das, Polkam Poojitha et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.]]></description>
<pubDate>Mon, 09 Feb 2026 14:15:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[Prototype-Based Disentanglement for Controllable Dysarthric Speech Synthesis]]></title>
<link>http://arxiv.org/abs/2602.08696v1</link>
<guid>2602.08696v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SD, cs.CL
Authors: Haoshen Wang, Xueli Zhong, Bingbing Lin, Jia Huang, Xingduo Pan et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Dysarthric speech exhibits high variability and limited labeled data, posing major challenges for both automatic speech recognition (ASR) and assistive speech technologies. Existing approaches rely on synthetic data augmentation or speech reconstruction, yet often entangle speaker identity with pathological articulation, limiting controllability and robustness.
  In this paper, we propose ProtoDisent-TTS, a prototype-based disentanglement TTS framework built on a pre-trained text-to-speech backbone that factorizes speaker timbre and dysarthric articulation within a unified latent space. A pathology prototype codebook provides interpretable and controllable representations of healthy and dysarthric speech patterns, while a dual-classifier objective with a gradient reversal layer enforces invariance of speaker embeddings to pathological attributes. Experiments on the TORGO dataset demonstrate that this design enables bidirectional transformation between healthy and dysarthric speech, leading to consistent ASR performance gains and robust, speaker-aware speech reconstruction.]]></description>
<pubDate>Mon, 09 Feb 2026 14:14:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[Trapped by simplicity: When Transformers fail to learn from noisy features]]></title>
<link>http://arxiv.org/abs/2602.08695v1</link>
<guid>2602.08695v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Evan Peters, Ando Deng, Matheus H. Zambianco, Devin Blankespoor, Achim Kempf
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.]]></description>
<pubDate>Mon, 09 Feb 2026 14:14:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Reasoning aligns language models to human cognition]]></title>
<link>http://arxiv.org/abs/2602.08693v1</link>
<guid>2602.08693v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Gonalo Guiomar, Elia Torre, Pehuen Moure, Victoria Shavina, Mario Giulianelli et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.]]></description>
<pubDate>Mon, 09 Feb 2026 14:13:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[PBLean: Pseudo-Boolean Proof Certificates for Lean 4]]></title>
<link>http://arxiv.org/abs/2602.08692v1</link>
<guid>2602.08692v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LO, cs.AI
Authors: Stefan Szeider
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present PBLean, a method for importing VeriPB pseudo-Boolean (PB) proof certificates into Lean 4. Key to our approach is reflection: a Boolean checker function whose soundness is fully proved in Lean and executed as compiled native code. Our method scales to proofs with tens of thousands of steps that would exhaust memory under explicit proof-term construction. Our checker supports all VeriPB kernel rules, including cutting-plane derivations and proof-by-contradiction subproofs. In contrast to external verified checkers that produce verdicts, our integration yields Lean theorems that can serve as composable lemmas in larger formal developments. To derive theorems about the original combinatorial problems rather than about PB constraints alone, we support verified encodings. This closes the trust gap between solver output and problem semantics since the constraint translation and its correctness proof are both formalized in Lean. We demonstrate the approach on various combinatorial problems.]]></description>
<pubDate>Mon, 09 Feb 2026 14:13:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity]]></title>
<link>http://arxiv.org/abs/2602.08690v1</link>
<guid>2602.08690v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CR
Authors: Shae McFadden, Myles Foley, Elizabeth Bates, Ilias Tsingenopoulos, Sanyam Vyas et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.]]></description>
<pubDate>Mon, 09 Feb 2026 14:12:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.08689v1</link>
<guid>2602.08689v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Constant Bourdrez, Alexandre Vrine, Olivier Capp
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.]]></description>
<pubDate>Mon, 09 Feb 2026 14:10:44 +0000</pubDate>
</item>
<item>
<title><![CDATA[Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement]]></title>
<link>http://arxiv.org/abs/2602.08688v1</link>
<guid>2602.08688v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.CY
Authors: Hossein Kermani, Fatemeh Oudlajani, Pardis Yarahmadi, Hamideh Mahdi Soltani, Mohammad Makki et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.]]></description>
<pubDate>Mon, 09 Feb 2026 14:10:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation]]></title>
<link>http://arxiv.org/abs/2602.08686v1</link>
<guid>2602.08686v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Ning Yang, Chengzhi Wang, Yibo Liu, Baoliang Tian, Haijun Zhang
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.]]></description>
<pubDate>Mon, 09 Feb 2026 14:07:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence]]></title>
<link>http://arxiv.org/abs/2602.08683v1</link>
<guid>2602.08683v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Feilong Tang, Xiang An, Yunyao Yan, Yin Xie, Bin Qin et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.]]></description>
<pubDate>Mon, 09 Feb 2026 14:06:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[ALIVE: Animate Your World with Lifelike Audio-Video Generation]]></title>
<link>http://arxiv.org/abs/2602.08682v1</link>
<guid>2602.08682v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Ying Guo, Qijun Gan, Yifu Zhang, Jinlai Liu, Yifei Hu et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.]]></description>
<pubDate>Mon, 09 Feb 2026 14:06:03 +0000</pubDate>
</item>
<item>
<title><![CDATA[The Theory and Practice of MAP Inference over Non-Convex Constraints]]></title>
<link>http://arxiv.org/abs/2602.08681v1</link>
<guid>2602.08681v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Leander Kurscheidt, Gabriele Masina, Roberto Sebastiani, Antonio Vergari
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.
  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.
  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.
  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.
  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.
  We evaluate both methods on synthetic and real-world benchmarks, showing our %
  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.]]></description>
<pubDate>Mon, 09 Feb 2026 14:05:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks]]></title>
<link>http://arxiv.org/abs/2602.08679v1</link>
<guid>2602.08679v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CR
Authors: Yanzhang Fu, Zizheng Guo, Jizhou Luo
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.]]></description>
<pubDate>Mon, 09 Feb 2026 14:02:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[6G-Bench: An Open Benchmark for Semantic Communication and Network-Level Reasoning with Foundation Models in AI-Native 6G Networks]]></title>
<link>http://arxiv.org/abs/2602.08675v1</link>
<guid>2602.08675v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.NI, cs.AI
Authors: Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper introduces 6G-Bench, an open benchmark for evaluating semantic communication and network-level reasoning in AI-native 6G networks. 6G-Bench defines a taxonomy of 30 decision-making tasks (T1--T30) extracted from ongoing 6G and AI-agent standardization activities in 3GPP, IETF, ETSI, ITU-T, and the O-RAN Alliance, and organizes them into five standardization-aligned capability categories. Starting from 113,475 scenarios, we generate a balanced pool of 10,000 very-hard multiple-choice questions using task-conditioned prompts that enforce multi-step quantitative reasoning under uncertainty and worst-case regret minimization over multi-turn horizons. After automated filtering and expert human validation, 3,722 questions are retained as a high-confidence evaluation set, while the full pool is released to support training and fine-tuning of 6G-specialized models. Using 6G-Bench, we evaluate 22 foundation models spanning dense and mixture-of-experts architectures, short- and long-context designs (up to 1M tokens), and both open-weight and proprietary systems. Across models, deterministic single-shot accuracy (pass@1) spans a wide range from 0.22 to 0.82, highlighting substantial variation in semantic reasoning capability. Leading models achieve intent and policy reasoning accuracy in the range 0.87--0.89, while selective robustness analysis on reasoning-intensive tasks shows pass@5 values ranging from 0.20 to 0.91. To support open science and reproducibility, we release the 6G-Bench dataset on GitHub: https://github.com/maferrag/6G-Bench]]></description>
<pubDate>Mon, 09 Feb 2026 13:57:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning to Judge: LLMs Designing and Applying Evaluation Rubrics]]></title>
<link>http://arxiv.org/abs/2602.08672v1</link>
<guid>2602.08672v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.LG
Authors: Clemencia Siro, Pourya Aliannejadi, Mohammad Aliannejadi
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.]]></description>
<pubDate>Mon, 09 Feb 2026 13:56:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Machine Learning accelerated geophysical fluid solver]]></title>
<link>http://arxiv.org/abs/2602.08670v1</link>
<guid>2602.08670v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.CE, cs.PF
Authors: Yang Bai
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.]]></description>
<pubDate>Mon, 09 Feb 2026 13:55:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion]]></title>
<link>http://arxiv.org/abs/2602.08668v1</link>
<guid>2602.08668v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CR, cs.IR, cs.LG
Authors: Scott Thornton
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved "seed" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure.
  We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition.
  We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.]]></description>
<pubDate>Mon, 09 Feb 2026 13:55:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling]]></title>
<link>http://arxiv.org/abs/2602.08661v1</link>
<guid>2602.08661v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yi Dao, Lankai Zhang, Hao Liu, Haiwei Zhang, Wenbo Wang
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.]]></description>
<pubDate>Mon, 09 Feb 2026 13:52:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models]]></title>
<link>http://arxiv.org/abs/2602.08660v1</link>
<guid>2602.08660v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Alexandre Verine, Rafael Pinot, Florian Le Bronnec
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.]]></description>
<pubDate>Mon, 09 Feb 2026 13:52:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction]]></title>
<link>http://arxiv.org/abs/2602.08657v1</link>
<guid>2602.08657v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ME
Authors: Xiaotong Liu, Shao-Bo Lin, Jun Fan, Ding-Xuan Zhou
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.]]></description>
<pubDate>Mon, 09 Feb 2026 13:49:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism]]></title>
<link>http://arxiv.org/abs/2602.08655v1</link>
<guid>2602.08655v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Sarthak Wanjari
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.]]></description>
<pubDate>Mon, 09 Feb 2026 13:48:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology]]></title>
<link>http://arxiv.org/abs/2602.08652v1</link>
<guid>2602.08652v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Oskar Thaeter, Tanja Niedermair, Johannes Raffler, Ralf Huss, Peter J. Schffler
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.]]></description>
<pubDate>Mon, 09 Feb 2026 13:46:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models]]></title>
<link>http://arxiv.org/abs/2602.08646v1</link>
<guid>2602.08646v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jisung Hwang, Minhyuk Sung
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.]]></description>
<pubDate>Mon, 09 Feb 2026 13:43:38 +0000</pubDate>
</item>
<item>
<title><![CDATA[LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection]]></title>
<link>http://arxiv.org/abs/2602.08638v1</link>
<guid>2602.08638v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Dezheng Wang, Tong Chen, Guansong Pang, Congyan Chen, Shihua Li et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.]]></description>
<pubDate>Mon, 09 Feb 2026 13:33:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[We Should Separate Memorization from Copyright]]></title>
<link>http://arxiv.org/abs/2602.08632v1</link>
<guid>2602.08632v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CY, cs.AI, cs.CL, cs.CV, cs.LG
Authors: Adi Haviv, Niva Elkin-Koren, Uri Hacohen, Roi Livni, Shay Moran
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The widespread use of foundation models has introduced a new risk factor of copyright issue. This issue is leading to an active, lively and on-going debate amongst the data-science community as well as amongst legal scholars. Where claims and results across both sides are often interpreted in different ways and leading to different implications. Our position is that much of the technical literature relies on traditional reconstruction techniques that are not designed for copyright analysis. As a result, memorization and copying have been conflated across both technical and legal communities and in multiple contexts. We argue that memorization, as commonly studied in data science, should not be equated with copying and should not be used as a proxy for copyright infringement. We distinguish technical signals that meaningfully indicate infringement risk from those that instead reflect lawful generalization or high-frequency content. Based on this analysis, we advocate for an output-level, risk-based evaluation process that aligns technical assessments with established copyright standards and provides a more principled foundation for research, auditing, and policy.]]></description>
<pubDate>Mon, 09 Feb 2026 13:24:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[Debate is efficient with your time]]></title>
<link>http://arxiv.org/abs/2602.08630v1</link>
<guid>2602.08630v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.CC
Authors: Jonah Brown-Cohen, Geoffrey Irving, Simon C. Marshall, Ilan Newman, Georgios Piliouras et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.]]></description>
<pubDate>Mon, 09 Feb 2026 13:21:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Revisiting [CLS] and Patch Token Interaction in Vision Transformers]]></title>
<link>http://arxiv.org/abs/2602.08626v1</link>
<guid>2602.08626v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Alexis Marouani, Oriane Simoni, Herv Jgou, Piotr Bojanowski, Huy V. Vo
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.]]></description>
<pubDate>Mon, 09 Feb 2026 13:16:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[Do Multilingual LLMs have specialized language heads?]]></title>
<link>http://arxiv.org/abs/2602.08625v1</link>
<guid>2602.08625v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Muhammad Naufil
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.]]></description>
<pubDate>Mon, 09 Feb 2026 13:15:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs]]></title>
<link>http://arxiv.org/abs/2602.08621v1</link>
<guid>2602.08621v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CR
Authors: Yukun Jiang, Hai Huang, Mingjie Li, Yage Zhang, Michael Backes et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.]]></description>
<pubDate>Mon, 09 Feb 2026 13:12:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[Improving Reconstruction of Representation Autoencoder]]></title>
<link>http://arxiv.org/abs/2602.08620v1</link>
<guid>2602.08620v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Siyu Liu, Chujie Qin, Hubery Yin, Qixin Yan, Zheng-Peng Duan et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.]]></description>
<pubDate>Mon, 09 Feb 2026 13:12:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[Enhancing Genetic Algorithms with Graph Neural Networks: A Timetabling Case Study]]></title>
<link>http://arxiv.org/abs/2602.08619v1</link>
<guid>2602.08619v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.NE, cs.AI, cs.LG
Authors: Laura-Maria Cornei, Mihaela-Elena Breabn
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper investigates the impact of hybridizing a multi-modal Genetic Algorithm with a Graph Neural Network for timetabling optimization. The Graph Neural Network is designed to encapsulate general domain knowledge to improve schedule quality, while the Genetic Algorithm explores different regions of the search space and integrates the deep learning model as an enhancement operator to guide the solution search towards optimality. Initially, both components of the hybrid technique were designed, developed, and optimized independently to solve the tackled task. Multiple experiments were conducted on Staff Rostering, a well-known timetabling problem, to compare the proposed hybridization with the standalone optimized versions of the Genetic Algorithm and Graph Neural Network. The experimental results demonstrate that the proposed hybridization brings statistically significant improvements in both the time efficiency and solution quality metrics, compared to the standalone methods. To the best of our knowledge, this work proposes the first hybridization of a Genetic Algorithm with a Graph Neural Network for solving timetabling problems.]]></description>
<pubDate>Mon, 09 Feb 2026 13:10:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning]]></title>
<link>http://arxiv.org/abs/2602.08617v1</link>
<guid>2602.08617v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Dario Fenoglio, Pasquale Polverino, Jacopo Quizi, Martin Gjoreski, Marc Langheinrich
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.]]></description>
<pubDate>Mon, 09 Feb 2026 13:05:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces]]></title>
<link>http://arxiv.org/abs/2602.08616v1</link>
<guid>2602.08616v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Heiko Hoppe, Fabian Akkerman, Wouter van Heeswijk, Maximilian Schiffer
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.]]></description>
<pubDate>Mon, 09 Feb 2026 13:05:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration]]></title>
<link>http://arxiv.org/abs/2602.08615v1</link>
<guid>2602.08615v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Kfir Goldberg, Elad Richardson, Yael Vinker
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.]]></description>
<pubDate>Mon, 09 Feb 2026 13:00:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Overview and Comparison of AVS Point Cloud Compression Standard]]></title>
<link>http://arxiv.org/abs/2602.08613v1</link>
<guid>2602.08613v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Wei Gao, Wenxu Gao, Xingming Mu, Changhao Peng, Ge Li
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.]]></description>
<pubDate>Mon, 09 Feb 2026 12:58:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling]]></title>
<link>http://arxiv.org/abs/2602.08607v1</link>
<guid>2602.08607v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.SD
Authors: Ziyang Cheng, Yuhao Wang, Heyang Liu, Ronghua Wu, Qunshan Gu et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\times$--10$\times$ decoding speedup and reduces first-chunk latency by 34\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.]]></description>
<pubDate>Mon, 09 Feb 2026 12:52:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[Constructive conditional normalizing flows]]></title>
<link>http://arxiv.org/abs/2602.08606v1</link>
<guid>2602.08606v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Borjan Geshkovski, Domnec Ruiz-Balet
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Motivated by applications in conditional sampling, given a probability measure $$ and a diffeomorphism $$, we consider the problem of simultaneously approximating $$ and the pushforward $_{\#}$ by means of the flow of a continuity equation whose velocity field is a perceptron neural network with piecewise constant weights. We provide an explicit construction based on a polar-like decomposition of the Lagrange interpolant of $$. The latter involves a compressible component, given by the gradient of a particular convex function, which can be realized exactly, and an incompressible component, which -- after approximating via permutations -- can be implemented through shear flows intrinsic to the continuity equation. For more regular maps $$ -- such as the Knthe-Rosenblatt rearrangement -- we provide an alternative, probabilistic construction inspired by the Maurey empirical method, in which the number of discontinuities in the weights doesn't scale inversely with the ambient dimension.]]></description>
<pubDate>Mon, 09 Feb 2026 12:52:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval]]></title>
<link>http://arxiv.org/abs/2602.08603v1</link>
<guid>2602.08603v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Teng Wang, Rong Shan, Jianghao Lin, Junjie Wu, Tianyi Xu et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.]]></description>
<pubDate>Mon, 09 Feb 2026 12:44:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation]]></title>
<link>http://arxiv.org/abs/2602.08600v1</link>
<guid>2602.08600v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Archchana Sindhujan, Girish A. Koushik, Shenbin Qian, Diptesh Kanojia, Constantin Orsan
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.]]></description>
<pubDate>Mon, 09 Feb 2026 12:42:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture]]></title>
<link>http://arxiv.org/abs/2602.08597v1</link>
<guid>2602.08597v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Roland Bertin-Johannet, Lara Scipio, Leopold Mayti, Rufin VanRullen
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.]]></description>
<pubDate>Mon, 09 Feb 2026 12:38:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Kissan-Dost: Bridging the Last Mile in Smallholder Precision Agriculture with Conversational IoT]]></title>
<link>http://arxiv.org/abs/2602.08593v1</link>
<guid>2602.08593v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI
Authors: Muhammad Saad Ali, Daanish U. Khan, Laiba Intizar Ahmad, Umer Irfan, Maryam Mustafa et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present Kissan-Dost, a multilingual, sensor-grounded conversational system that turns live on-farm measurements and weather into plain-language guidance delivered over WhatsApp text or voice. The system couples commodity soil and climate sensors with retrieval-augmented generation, then enforces grounding, traceability, and proactive alerts through a modular pipeline. In a 90-day, two-site pilot with five participants, we ran three phases (baseline, dashboard only, chatbot only). Dashboard engagement was sporadic and faded, while the chatbot was used nearly daily and informed concrete actions. Controlled tests on 99 sensor-grounded crop queries achieved over 90 percent correctness with subsecond end-to-end latency, alongside high-quality translation outputs. Results show that careful last-mile integration, not novel circuitry, unlocks the latent value of existing Agri-IoT for smallholders.]]></description>
<pubDate>Mon, 09 Feb 2026 12:34:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models]]></title>
<link>http://arxiv.org/abs/2602.08592v1</link>
<guid>2602.08592v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Tianyin Liao, Chunyu Hu, Yicheng Sui, Xingxuan Zhang, Peng Cui et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.]]></description>
<pubDate>Mon, 09 Feb 2026 12:33:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning]]></title>
<link>http://arxiv.org/abs/2602.08590v1</link>
<guid>2602.08590v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.DB
Authors: Yicheng Di, Wei Yuan, Tieke He, Zhanjie Zhang, Ao Ma et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.]]></description>
<pubDate>Mon, 09 Feb 2026 12:33:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[FairRARI: A Plug and Play Framework for Fairness-Aware PageRank]]></title>
<link>http://arxiv.org/abs/2602.08589v1</link>
<guid>2602.08589v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.SI
Authors: Emmanouil Kariotakis, Aritra Konar
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.]]></description>
<pubDate>Mon, 09 Feb 2026 12:30:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition]]></title>
<link>http://arxiv.org/abs/2602.08586v1</link>
<guid>2602.08586v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Yiming Yang, Zhuoyuan Li, Fanxiang Zeng, Hao Fu, Yue Liu
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.
  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.]]></description>
<pubDate>Mon, 09 Feb 2026 12:24:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction]]></title>
<link>http://arxiv.org/abs/2602.08585v1</link>
<guid>2602.08585v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Ziyao Tang, Pengkun Jiao, Xinhang Chen, Wei Liu, Shiyong Li et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.]]></description>
<pubDate>Mon, 09 Feb 2026 12:23:38 +0000</pubDate>
</item>
<item>
<title><![CDATA[Conditional Sequence Modeling for Safe Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.08584v1</link>
<guid>2602.08584v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Wensong Bai, Chao Zhang, Qihang Xu, Chufan Chen, Chenhao Zhou et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.]]></description>
<pubDate>Mon, 09 Feb 2026 12:22:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.08582v1</link>
<guid>2602.08582v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Melany Yang, Yuhang Yu, Diwang Weng, Jinwei Chen, Wei Dong
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.]]></description>
<pubDate>Mon, 09 Feb 2026 12:20:33 +0000</pubDate>
</item>
<item>
<title><![CDATA[retinalysis-vascx: An explainable software toolbox for the extraction of retinal vascular biomarkers]]></title>
<link>http://arxiv.org/abs/2602.08580v1</link>
<guid>2602.08580v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Jose D. Vargas Quiros, Michael J. Beyeler, Sofia Ortin Vela, EyeNED Reading Center, Sven Bergmann et al.
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The automatic extraction of retinal vascular biomarkers from color fundus images (CFI) is essential for large-scale studies of the retinal vasculature. We present VascX, an open-source Python toolbox designed for the automated extraction of biomarkers from artery and vein segmentations. The VascX workflow processes vessel segmentation masks into skeletons to build undirected and directed vessel graphs, which are then used to resolve segments into continuous vessels. This architecture enables the calculation of a comprehensive suite of biomarkers, including vascular density, bifurcation angles, central retinal equivalents (CREs), tortuosity, and temporal angles, alongside image quality metrics.
  A distinguishing feature of VascX is its region awareness; by utilizing the fovea, optic disc, and CFI boundaries as anatomical landmarks, the tool ensures spatially standardized measurements and identifies when specific biomarkers are not computable. Spatially localized biomarkers are calculated over grids relative to these landmarks, facilitating precise clinical analysis. Released via GitHub and PyPI, VascX provides an explainable and modifiable framework that supports reproducible vascular research through integrated visualizations. By enabling the rapid extraction of established biomarkers and the development of new ones, VascX advances the field of oculomics, offering a robust, computationally efficient solution for scalable deployment in large-scale clinical and epidemiological databases.]]></description>
<pubDate>Mon, 09 Feb 2026 12:19:33 +0000</pubDate>
</item>
<item>
<title><![CDATA[Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs]]></title>
<link>http://arxiv.org/abs/2602.08579v1</link>
<guid>2602.08579v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Junsu Seo
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.]]></description>
<pubDate>Mon, 09 Feb 2026 12:17:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources]]></title>
<link>http://arxiv.org/abs/2602.08577v1</link>
<guid>2602.08577v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.CO
Authors: Theodoros Anagnostopoulos, Evanthia Zervoudi, Christos Anagnostopoulos, Apostolos Christopoulos, Bogdan Wierzbinski
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.]]></description>
<pubDate>Mon, 09 Feb 2026 12:17:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[ValueFlow: Measuring the Propagation of Value Perturbations in Multi-Agent LLM Systems]]></title>
<link>http://arxiv.org/abs/2602.08567v1</link>
<guid>2602.08567v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.MA, cs.CL
Authors: Jinnuo Liu, Chuke Liu, Hua Shen
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multi-agent large language model (LLM) systems increasingly consist of agents that observe and respond to one another's outputs. While value alignment is typically evaluated for isolated models, how value perturbations propagate through agent interactions remains poorly understood. We present ValueFlow, a perturbation-based evaluation framework for measuring and analyzing value drift in multi-agent systems. ValueFlow introduces a 56-value evaluation dataset derived from the Schwartz Value Survey and quantifies agents' value orientations during interaction using an LLM-as-a-judge protocol. Building on this measurement layer, ValueFlow decomposes value drift into agent-level response behavior and system-level structural effects, operationalized by two metrics: beta-susceptibility, which measures an agent's sensitivity to perturbed peer signals, and system susceptibility (SS), which captures how node-level perturbations affect final system outputs. Experiments across multiple model backbones, prompt personas, value dimensions, and network structures show that susceptibility varies widely across values and is strongly shaped by structural topology.]]></description>
<pubDate>Mon, 09 Feb 2026 12:06:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[Agent-Supported Foresight for AI Systemic Risks: AI Agents for Breadth, Experts for Judgment]]></title>
<link>http://arxiv.org/abs/2602.08565v1</link>
<guid>2602.08565v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI
Authors: Leon Frhling, Alessandro Giaconia, Edyta Paulina Bogucka, Daniele Quercia
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

AI impact assessments often stress near-term risks because human judgment degrades over longer horizons, exemplifying the Collingridge dilemma: foresight is most needed when knowledge is scarcest. To address long-term systemic risks, we introduce a scalable approach that simulates in-silico agents using the strategic foresight method of the Futures Wheel. We applied it to four AI uses spanning Technology Readiness Levels (TRLs): Chatbot Companion (TRL 9, mature), AI Toy (TRL 7, medium), Griefbot (TRL 5, low), and Death App (TRL 2, conceptual). Across 30 agent runs per use, agents produced 86-110 consequences, condensed into 27-47 unique risks. To benchmark the agent outputs against human perspectives, we collected evaluations from 290 domain experts and 7 leaders, and conducted Futures Wheel sessions with 42 experts and 42 laypeople. Agents generated many systemic consequences across runs. Compared with these outputs, experts identified fewer risks, typically less systemic but judged more likely, whereas laypeople surfaced more emotionally salient concerns that were generally less systemic. We propose a hybrid foresight workflow, wherein agents broaden systemic coverage, and humans provide contextual grounding. Our dataset is available at: https://social-dynamics.net/ai-risks/foresight.]]></description>
<pubDate>Mon, 09 Feb 2026 12:03:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data]]></title>
<link>http://arxiv.org/abs/2602.08564v1</link>
<guid>2602.08564v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Tiantong Wang, Yiyang Duan, Haoyu Chen, Tiantong Wu, Wei Yang Bryan Lim
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.]]></description>
<pubDate>Mon, 09 Feb 2026 12:03:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs]]></title>
<link>http://arxiv.org/abs/2602.08563v1</link>
<guid>2602.08563v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CR
Authors: Ahmed Salem, Andrew Paverd, Sahar Abdelnabi
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.]]></description>
<pubDate>Mon, 09 Feb 2026 12:01:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches]]></title>
<link>http://arxiv.org/abs/2602.08561v1</link>
<guid>2602.08561v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SE, cs.CL
Authors: Syed Mehtab Hussain Shah, Frank Hopfgartner, Arnim Bleier
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.]]></description>
<pubDate>Mon, 09 Feb 2026 11:59:59 +0000</pubDate>
</item>
</channel>
</rss>