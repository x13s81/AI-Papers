<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Papers - 2026-02-23</title>
<link>https://arxiv.org</link>
<description>AI papers as of 2026-02-23 - 246 papers</description>
<lastBuildDate>Mon, 23 Feb 2026 14:42:47 +0000</lastBuildDate>
<item>
<title><![CDATA[RynnBrain: Open Embodied Foundation Models]]></title>
<link>https://huggingface.co/papers/2602.14979</link>
<guid>2602.14979</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ronghao Dang, Jiayan Guo, Bohan Hou, Sicong Leng, Kehan Li
Institution: 
Published: 2026-02-13
Score: 8/10
Citations: 0
Upvotes: 37
GitHub: 
Stars: 0

Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.]]></description>
<pubDate>Fri, 13 Feb 2026 18:59:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5]]></title>
<link>https://huggingface.co/papers/2602.14457</link>
<guid>2602.14457</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Dongrui Liu, Yi Yu, Jie Zhang, Guanxu Chen, Qihao Lin
Institution: 
Published: 2026-02-16
Score: 8/10
Citations: 0
Upvotes: 26
GitHub: 
Stars: 0

To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.]]></description>
<pubDate>Mon, 16 Feb 2026 04:30:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[MAEB: Massive Audio Embedding Benchmark]]></title>
<link>https://huggingface.co/papers/2602.16008</link>
<guid>2602.16008</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Adnan El Assadi, Isaac Chung, Chenghao Xiao, Roman Solomatin, Animesh Jha
Institution: 
Published: 2026-02-17
Score: 8/10
Citations: 0
Upvotes: 18
GitHub: 
Stars: 0

We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.]]></description>
<pubDate>Tue, 17 Feb 2026 21:00:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control]]></title>
<link>https://huggingface.co/papers/2602.18422</link>
<guid>2602.18422</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Linxi Xie, Lisong C. Sun, Ashley Neall, Tong Wu, Shengqu Cai
Institution: 
Published: 2026-02-20
Score: 8/10
Citations: 0
Upvotes: 9
GitHub: 
Stars: 0

Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.]]></description>
<pubDate>Fri, 20 Feb 2026 18:45:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing]]></title>
<link>https://huggingface.co/papers/2602.15823</link>
<guid>2602.15823</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zarif Ikram, Arad Firouzkouhi, Stephen Tu, Mahdi Soltanolkotabi, Paria Rashidinejad
Institution: 
Published: 2026-02-17
Score: 8/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.]]></description>
<pubDate>Tue, 17 Feb 2026 18:58:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[VidEoMT: Your ViT is Secretly Also a Video Segmentation Model]]></title>
<link>https://huggingface.co/papers/2602.17807</link>
<guid>2602.17807</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Narges Norouzi, Idil Esen Zulfikar, Niccol`o Cavagnero, Tommie Kerssies, Bastian Leibe
Institution: 
Published: 2026-02-19
Score: 8/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/]]></description>
<pubDate>Thu, 19 Feb 2026 20:14:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents]]></title>
<link>https://huggingface.co/papers/2602.16699</link>
<guid>2602.16699</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Wenxuan Ding, Nicholas Tomlin, Greg Durrett
Institution: 
Published: 2026-02-18
Score: 7/10
Citations: 0
Upvotes: 13
GitHub: 
Stars: 0

LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.]]></description>
<pubDate>Wed, 18 Feb 2026 18:46:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Discovering Multiagent Learning Algorithms with Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.16928</link>
<guid>2602.16928</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zun Li, John Schultz, Daniel Hennes, Marc Lanctot
Institution: 
Published: 2026-02-18
Score: 7/10
Citations: 0
Upvotes: 9
GitHub: 
Stars: 0

Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.]]></description>
<pubDate>Wed, 18 Feb 2026 22:41:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment]]></title>
<link>https://huggingface.co/papers/2602.17259</link>
<guid>2602.17259</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Han Zhao, Jingbo Wang, Wenxuan Song, Shuai Chen, Yang Liu
Institution: 
Published: 2026-02-19
Score: 7/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.]]></description>
<pubDate>Thu, 19 Feb 2026 11:00:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Situated Awareness in the Real World]]></title>
<link>https://huggingface.co/papers/2602.16682</link>
<guid>2602.16682</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Chuhan Li, Ruilin Han, Joy Hsu, Yongyuan Liang, Rajiv Dhawan
Institution: 
Published: 2026-02-18
Score: 7/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.]]></description>
<pubDate>Wed, 18 Feb 2026 18:22:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[Visual Memory Injection Attacks for Multi-Turn Conversations]]></title>
<link>https://huggingface.co/papers/2602.15927</link>
<guid>2602.15927</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Christian Schlarmann, Matthias Hein
Institution: 
Published: 2026-02-17
Score: 7/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection]]></description>
<pubDate>Tue, 17 Feb 2026 18:34:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[SARAH: Spatially Aware Real-time Agentic Humans]]></title>
<link>https://huggingface.co/papers/2602.18432</link>
<guid>2602.18432</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Evonne Ng, Siwei Zhang, Zhang Chen, Michael Zollhoefer, Alexander Richard
Institution: 
Published: 2026-02-20
Score: 7/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.]]></description>
<pubDate>Fri, 20 Feb 2026 18:59:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs]]></title>
<link>https://huggingface.co/papers/2602.10377</link>
<guid>2602.10377</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Luoyang Sun, Jiwen Jiang, Yifeng Ding, Fengfa Li, Yan Song
Institution: 
Published: 2026-02-10
Score: 7/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.]]></description>
<pubDate>Tue, 10 Feb 2026 23:51:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.08392</link>
<guid>2602.08392</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin
Institution: 
Published: 2026-02-09
Score: 7/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.]]></description>
<pubDate>Mon, 09 Feb 2026 08:47:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning]]></title>
<link>https://huggingface.co/papers/2602.16742</link>
<guid>2602.16742</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Haoxiang Sun, Lizhen Xu, Bing Zhao, Wotao Yin, Wei Wang
Institution: 
Published: 2026-02-18
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce DeepVision-103K, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: https://huggingface.co/datasets/skylenage/DeepVision-103K{this url}.]]></description>
<pubDate>Wed, 18 Feb 2026 01:51:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[NeST: Neuron Selective Tuning for LLM Safety]]></title>
<link>https://huggingface.co/papers/2602.16835</link>
<guid>2602.16835</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Sasha Behrouzi, Lichao Wu, Mohamadreza Rostami, Ahmad-Reza Sadeghi
Institution: 
Published: 2026-02-18
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.
  We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.]]></description>
<pubDate>Wed, 18 Feb 2026 20:01:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[CADEvolve: Creating Realistic CAD via Program Evolution]]></title>
<link>https://huggingface.co/papers/2602.16317</link>
<guid>2602.16317</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Maksim Elistratov, Marina Barannikov, Gregory Ivanov, Valentin Khrulkov, Anton Konushin
Institution: 
Published: 2026-02-18
Score: 6/10
Citations: 0
Upvotes: 25
GitHub: 
Stars: 0

Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.]]></description>
<pubDate>Wed, 18 Feb 2026 09:54:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality]]></title>
<link>https://huggingface.co/papers/2602.14080</link>
<guid>2602.14080</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Nitay Calderon, Eyal Ben-David, Zorik Gekhman, Eran Ofek, Gal Yona
Institution: 
Published: 2026-02-15
Score: 6/10
Citations: 0
Upvotes: 20
GitHub: 
Stars: 0

Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.]]></description>
<pubDate>Sun, 15 Feb 2026 10:13:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[2Mamba2Furious: Linear in Complexity, Competitive in Accuracy]]></title>
<link>https://huggingface.co/papers/2602.17363</link>
<guid>2602.17363</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Gabriel Mongaras, Eric C. Larson
Institution: 
Published: 2026-02-19
Score: 6/10
Citations: 0
Upvotes: 7
GitHub: 
Stars: 0

Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments]]></description>
<pubDate>Thu, 19 Feb 2026 13:45:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers]]></title>
<link>https://huggingface.co/papers/2602.18292</link>
<guid>2602.18292</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou-Ammar
Institution: 
Published: 2026-02-20
Score: 6/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.]]></description>
<pubDate>Fri, 20 Feb 2026 15:38:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[MMA: Multimodal Memory Agent]]></title>
<link>https://huggingface.co/papers/2602.16493</link>
<guid>2602.16493</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yihao Lu, Wanru Cheng, Zeyu Zhang, Hao Tang
Institution: 
Published: 2026-02-18
Score: 6/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the "Visual Placebo Effect", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.]]></description>
<pubDate>Wed, 18 Feb 2026 14:30:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[World Models for Policy Refinement in StarCraft II]]></title>
<link>https://huggingface.co/papers/2602.14857</link>
<guid>2602.14857</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yixin Zhang, Ziyi Wang, Yiming Rong, Haoxi Wang, Jinling Jiang
Institution: 
Published: 2026-02-16
Score: 6/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.]]></description>
<pubDate>Mon, 16 Feb 2026 15:51:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots]]></title>
<link>https://huggingface.co/papers/2602.18071</link>
<guid>2602.18071</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Boyuan An, Zhexiong Wang, Yipeng Wang, Jiaqi Li, Sihang Li
Institution: 
Published: 2026-02-20
Score: 6/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.]]></description>
<pubDate>Fri, 20 Feb 2026 08:54:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist]]></title>
<link>https://huggingface.co/papers/2602.16756</link>
<guid>2602.16756</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Johannes Bertram, Jonas Geiping
Institution: 
Published: 2026-02-18
Score: 6/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security, NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. NESSiE is intended as a lightweight, easy-to-use sanity check for language model safety and, as such, is not sufficient for guaranteeing safety in general -- but we argue that passing this test is necessary for any deployment. However, even state-of-the-art LLMs do not reach 100% on NESSiE and thus fail our necessary condition of language model safety, even in the absence of adversarial attacks. Our Safe & Helpful (SH) metric allows for direct comparison of the two requirements, showing models are biased toward being helpful rather than safe. We further find that disabled reasoning for some models, but especially a benign distraction context degrade model performance. Overall, our results underscore the critical risks of deploying such models as autonomous agents in the wild. We make the dataset, package and plotting code publicly available.]]></description>
<pubDate>Wed, 18 Feb 2026 09:41:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[Selective Training for Large Vision Language Models via Visual Information Gain]]></title>
<link>https://huggingface.co/papers/2602.17186</link>
<guid>2602.17186</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Seulbi Lee, Sangheum Hwang
Institution: 
Published: 2026-02-19
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.]]></description>
<pubDate>Thu, 19 Feb 2026 09:12:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[References Improve LLM Alignment in Non-Verifiable Domains]]></title>
<link>https://huggingface.co/papers/2602.16802</link>
<guid>2602.16802</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Kejian Shi, Yixin Liu, Peifeng Wang, Alexander R. Fabbri, Shafiq Joty
Institution: 
Published: 2026-02-18
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.]]></description>
<pubDate>Wed, 18 Feb 2026 19:03:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty]]></title>
<link>https://huggingface.co/papers/2602.18312</link>
<guid>2602.18312</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zhaoming Xie, Kevin Karol, Jessica Hodgins
Institution: 
Published: 2026-02-20
Score: 6/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.]]></description>
<pubDate>Fri, 20 Feb 2026 16:11:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation]]></title>
<link>https://huggingface.co/papers/2602.16915</link>
<guid>2602.16915</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zeyu Ren, Xiang Li, Yiran Wang, Zeyu Zhang, Hao Tang
Institution: 
Published: 2026-02-18
Score: 6/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.]]></description>
<pubDate>Wed, 18 Feb 2026 22:12:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training]]></title>
<link>https://huggingface.co/papers/2602.10693</link>
<guid>2602.10693</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Guobin Shen, Chenxiao Zhao, Xiang Cheng, Lei Huang, Xing Yu
Institution: 
Published: 2026-02-11
Score: 5/10
Citations: 0
Upvotes: 154
GitHub: 
Stars: 0

Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO]]></description>
<pubDate>Wed, 11 Feb 2026 09:48:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[SLA2: Sparse-Linear Attention with Learnable Routing and QAT]]></title>
<link>https://huggingface.co/papers/2602.12675</link>
<guid>2602.12675</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jintao Zhang, Haoxu Wang, Kai Jiang, Kaiwen Zheng, Youhe Jiang
Institution: 
Published: 2026-02-13
Score: 5/10
Citations: 0
Upvotes: 49
GitHub: 
Stars: 0

Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.]]></description>
<pubDate>Fri, 13 Feb 2026 07:16:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning]]></title>
<link>https://huggingface.co/papers/2602.13515</link>
<guid>2602.13515</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jintao Zhang, Kai Jiang, Chendong Xiang, Weiqi Feng, Yuezhou Hu
Institution: 
Published: 2026-02-13
Score: 5/10
Citations: 0
Upvotes: 41
GitHub: 
Stars: 0

Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.]]></description>
<pubDate>Fri, 13 Feb 2026 23:01:42 +0000</pubDate>
</item>
<item>
<title><![CDATA["What Are You Doing?": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing]]></title>
<link>https://huggingface.co/papers/2602.15569</link>
<guid>2602.15569</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Johannes Kirmayr, Raphael Wennmacher, Khanh Huynh, Lukas Stappen, Elisabeth Andr
Institution: 
Published: 2026-02-17
Score: 5/10
Citations: 0
Upvotes: 13
GitHub: 
Stars: 0

Agentic AI assistants that autonomously perform multi-step tasks raise open questions for user experience: how should such systems communicate progress and reasoning during extended operations, especially in attention-critical contexts such as driving? We investigate feedback timing and verbosity from agentic LLM-based in-car assistants through a controlled, mixed-methods study (N=45) comparing planned steps and intermediate results feedback against silent operation with final-only response. Using a dual-task paradigm with an in-car voice assistant, we found that intermediate feedback significantly improved perceived speed, trust, and user experience while reducing task load - effects that held across varying task complexities and interaction contexts. Interviews further revealed user preferences for an adaptive approach: high initial transparency to establish trust, followed by progressively reducing verbosity as systems prove reliable, with adjustments based on task stakes and situational context. We translate our empirical findings into design implications for feedback timing and verbosity in agentic assistants, balancing transparency and efficiency.]]></description>
<pubDate>Tue, 17 Feb 2026 13:27:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Computer-Using World Model]]></title>
<link>https://huggingface.co/papers/2602.17365</link>
<guid>2602.17365</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yiming Guan, Rui Yu, John Zhang, Lu Wang, Chaoyun Zhang
Institution: 
Published: 2026-02-19
Score: 5/10
Citations: 0
Upvotes: 12
GitHub: 
Stars: 0

Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.]]></description>
<pubDate>Thu, 19 Feb 2026 13:48:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[ArXiv-to-Model: A Practical Study of Scientific LM Training]]></title>
<link>https://huggingface.co/papers/2602.17288</link>
<guid>2602.17288</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Anuj Gupta
Institution: 
Published: 2026-02-19
Score: 5/10
Citations: 0
Upvotes: 7
GitHub: 
Stars: 0

While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.]]></description>
<pubDate>Thu, 19 Feb 2026 11:47:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking]]></title>
<link>https://huggingface.co/papers/2602.16849</link>
<guid>2602.16849</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jianliang He, Leda Wang, Siyu Chen, Zhuoran Yang
Institution: 
Published: 2026-02-18
Score: 5/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the "winner" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay.]]></description>
<pubDate>Wed, 18 Feb 2026 20:25:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[Optimizing Few-Step Generation with Adaptive Matching Distillation]]></title>
<link>https://huggingface.co/papers/2602.07345</link>
<guid>2602.07345</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Lichen Bai, Zikai Zhou, Shitong Shao, Wenliang Zhong, Shuo Yang
Institution: 
Published: 2026-02-07
Score: 5/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.]]></description>
<pubDate>Sat, 07 Feb 2026 04:00:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[Modeling Distinct Human Interaction in Web Agents]]></title>
<link>https://huggingface.co/papers/2602.17588</link>
<guid>2602.17588</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Faria Huq, Zora Zhiruo Wang, Zhanqiu Guo, Venu Arvind Arangarajan, Tianyue Ou
Institution: 
Published: 2026-02-19
Score: 5/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.]]></description>
<pubDate>Thu, 19 Feb 2026 18:11:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Sink-Aware Pruning for Diffusion Language Models]]></title>
<link>https://huggingface.co/papers/2602.17664</link>
<guid>2602.17664</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen
Institution: 
Published: 2026-02-19
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose {bf Sink-Aware Pruning}, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.]]></description>
<pubDate>Thu, 19 Feb 2026 18:59:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Does Your Reasoning Model Implicitly Know When to Stop Thinking?]]></title>
<link>https://huggingface.co/papers/2602.08354</link>
<guid>2602.08354</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuanda Wang
Institution: 
Published: 2026-02-09
Score: 4/10
Citations: 0
Upvotes: 81
GitHub: 
Stars: 0

Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.]]></description>
<pubDate>Mon, 09 Feb 2026 07:38:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines]]></title>
<link>https://huggingface.co/papers/2602.14296</link>
<guid>2602.14296</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yifan Wu, Yiran Peng, Yiyu Chen, Jianhao Ruan, Zijie Zhuang
Institution: 
Published: 2026-02-15
Score: 4/10
Citations: 0
Upvotes: 42
GitHub: 
Stars: 0

The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.]]></description>
<pubDate>Sun, 15 Feb 2026 20:03:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[Unified Latents (UL): How to train your latents]]></title>
<link>https://huggingface.co/papers/2602.17270</link>
<guid>2602.17270</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jonathan Heek, Emiel Hoogeboom, Thomas Mensink, Tim Salimans
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 38
GitHub: 
Stars: 0

We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.]]></description>
<pubDate>Thu, 19 Feb 2026 11:18:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents]]></title>
<link>https://huggingface.co/papers/2602.16855</link>
<guid>2602.16855</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Haiyang Xu, Xi Zhang, Haowei Liu, Junyang Wang, Zhaozai Zhu
Institution: 
Published: 2026-02-15
Score: 4/10
Citations: 0
Upvotes: 37
GitHub: 
Stars: 0

The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.]]></description>
<pubDate>Sun, 15 Feb 2026 01:52:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[Arcee Trinity Large Technical Report]]></title>
<link>https://huggingface.co/papers/2602.17004</link>
<guid>2602.17004</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Varun Singh, Lucas Krauss, Sami Jaghouar, Matej Sirovatka, Charles Goddard
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 16
GitHub: 
Stars: 0

We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.]]></description>
<pubDate>Thu, 19 Feb 2026 01:58:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Reinforced Fast Weights with Next-Sequence Prediction]]></title>
<link>https://huggingface.co/papers/2602.16704</link>
<guid>2602.16704</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Hee Seung Hwang, Xindi Wu, Sanghyuk Chun, Olga Russakovsky
Institution: 
Published: 2026-02-18
Score: 4/10
Citations: 0
Upvotes: 11
GitHub: 
Stars: 0

Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.]]></description>
<pubDate>Wed, 18 Feb 2026 18:53:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers]]></title>
<link>https://huggingface.co/papers/2602.16968</link>
<guid>2602.16968</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Dahye Kim, Deepti Ghadiyaram, Raghudeep Gadde
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 10
GitHub: 
Stars: 0

Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.]]></description>
<pubDate>Thu, 19 Feb 2026 00:15:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment]]></title>
<link>https://huggingface.co/papers/2602.13579</link>
<guid>2602.13579</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Youngsun Wi, Jessica Yin, Elvis Xiang, Akash Sharma, Jitendra Malik
Institution: 
Published: 2026-02-14
Score: 4/10
Citations: 0
Upvotes: 10
GitHub: 
Stars: 0

Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).]]></description>
<pubDate>Sat, 14 Feb 2026 03:31:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Uncertainty-Aware Vision-Language Segmentation for Medical Imaging]]></title>
<link>https://huggingface.co/papers/2602.14498</link>
<guid>2602.14498</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Aryan Das, Tanishq Rachamalla, Koushik Biswas, Swalpa Kumar Roy, Vinay Kumar Verma
Institution: 
Published: 2026-02-16
Score: 4/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

We introduce a novel uncertainty-aware multimodal segmentation framework that leverages both radiological images and associated clinical text for precise medical diagnosis. We propose a Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) to enable efficient cross-modal fusion and long-range dependency modelling. To guide learning under ambiguity, we propose the Spectral-Entropic Uncertainty (SEU) Loss, which jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective. In complex clinical circumstances with poor image quality, this formulation improves model reliability. Extensive experiments on various publicly available medical datasets, QATA-COVID19, MosMed++, and Kvasir-SEG, demonstrate that our method achieves superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art (SoTA) approaches. Our results highlight the importance of incorporating uncertainty modelling and structured modality alignment in vision-language medical segmentation tasks. Code: https://github.com/arya-domain/UA-VLS]]></description>
<pubDate>Mon, 16 Feb 2026 06:27:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[Efficient Text-Guided Convolutional Adapter for the Diffusion Model]]></title>
<link>https://huggingface.co/papers/2602.14514</link>
<guid>2602.14514</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Aryan Das, Koushik Biswas, Swalpa Kumar Roy, Badri Narayana Patro, Vinay Kumar Verma
Institution: 
Published: 2026-02-16
Score: 4/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning. Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter. Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter, which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters]]></description>
<pubDate>Mon, 16 Feb 2026 06:51:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Personalized Agents from Human Feedback]]></title>
<link>https://huggingface.co/papers/2602.16173</link>
<guid>2602.16173</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Kaiqu Liang, Julia Kruk, Shengyi Qian, Xianjun Yang, Shengjie Bi
Institution: 
Published: 2026-02-18
Score: 4/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.]]></description>
<pubDate>Wed, 18 Feb 2026 04:18:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[OPBench: A Graph Benchmark to Combat the Opioid Crisis]]></title>
<link>https://huggingface.co/papers/2602.14602</link>
<guid>2602.14602</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Tianyi Ma, Yiyang Li, Yiyue Qian, Zheyuan Zhang, Zehong Wang
Institution: 
Published: 2026-02-16
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.]]></description>
<pubDate>Mon, 16 Feb 2026 10:04:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[Assigning Confidence: K-partition Ensembles]]></title>
<link>http://arxiv.org/abs/2602.18435v1</link>
<guid>2602.18435v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Aggelos Semoglou, John Pavlopoulos
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.]]></description>
<pubDate>Fri, 20 Feb 2026 18:59:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory]]></title>
<link>http://arxiv.org/abs/2602.18434v1</link>
<guid>2602.18434v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Vatsal Agarwal, Saksham Suri, Matthew Gwilliam, Pulkit Kumar, Abhinav Shrivastava
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.]]></description>
<pubDate>Fri, 20 Feb 2026 18:59:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning]]></title>
<link>http://arxiv.org/abs/2602.18429v1</link>
<guid>2602.18429v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.IR
Authors: Harshul Raj Surana, Arijit Maji, Aryan Vats, Akash Ghosh, Sriparna Saha et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.]]></description>
<pubDate>Fri, 20 Feb 2026 18:53:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning]]></title>
<link>http://arxiv.org/abs/2602.18428v1</link>
<guid>2602.18428v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CV
Authors: Mojtaba Sahraee-Ardakan, Mauricio Delbracio, Peyman Milanfar
Institution: Mila
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.]]></description>
<pubDate>Fri, 20 Feb 2026 18:49:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[Spatio-Spectroscopic Representation Learning using Unsupervised Convolutional Long-Short Term Memory Networks]]></title>
<link>http://arxiv.org/abs/2602.18426v1</link>
<guid>2602.18426v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Kameswara Bharadwaj Mantha, Lucy Fortson, Ramanakumar Sankar, Claudia Scarlata, Chris Lintott et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Integral Field Spectroscopy (IFS) surveys offer a unique new landscape in which to learn in both spatial and spectroscopic dimensions and could help uncover previously unknown insights into galaxy evolution. In this work, we demonstrate a new unsupervised deep learning framework using Convolutional Long-Short Term Memory Network Autoencoders to encode generalized feature representations across both spatial and spectroscopic dimensions spanning $19$ optical emission lines (3800A $< <$ 8000A) among a sample of $\sim 9000$ galaxies from the MaNGA IFS survey. As a demonstrative exercise, we assess our model on a sample of $290$ Active Galactic Nuclei (AGN) and highlight scientifically interesting characteristics of some highly anomalous AGN.]]></description>
<pubDate>Fri, 20 Feb 2026 18:48:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering]]></title>
<link>http://arxiv.org/abs/2602.18425v1</link>
<guid>2602.18425v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.IR
Authors: Deniz Qian, Hung-Ting Chen, Eunsol Choi
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.]]></description>
<pubDate>Fri, 20 Feb 2026 18:48:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation]]></title>
<link>http://arxiv.org/abs/2602.18424v1</link>
<guid>2602.18424v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.RO
Authors: Xia Su, Ruiqi Chen, Benlin Liu, Jingwei Ma, Zonglin Di et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav]]></description>
<pubDate>Fri, 20 Feb 2026 18:46:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[SPQ: An Ensemble Technique for Large Language Model Compression]]></title>
<link>http://arxiv.org/abs/2602.18420v1</link>
<guid>2602.18420v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Jiamin Yao, Eren Gultepe
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/]]></description>
<pubDate>Fri, 20 Feb 2026 18:44:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Benchmarking Graph Neural Networks in Solving Hard Constraint Satisfaction Problems]]></title>
<link>http://arxiv.org/abs/2602.18419v1</link>
<guid>2602.18419v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Geri Skenderi, Lorenzo Buffoni, Francesco D'Amico, David Machado, Raffaele Marino et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Graph neural networks (GNNs) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics. However, such claims risk being unsolid due to a lack of standard benchmarks on truly hard instances. From a statistical physics perspective, we propose new hard benchmarks based on random problems. We provide these benchmarks, along with performance results from both classical heuristics and GNNs. Our fair comparison shows that classical algorithms still outperform GNNs. We discuss the challenges for neural networks in this domain. Future claims of superiority can be made more robust using our benchmarks, available at https://github.com/ArtLabBocconi/RandCSPBench.]]></description>
<pubDate>Fri, 20 Feb 2026 18:41:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures]]></title>
<link>http://arxiv.org/abs/2602.18417v1</link>
<guid>2602.18417v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CL
Authors: Joshua Nunley
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.]]></description>
<pubDate>Fri, 20 Feb 2026 18:35:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Unifying approach to uniform expressivity of graph neural networks]]></title>
<link>http://arxiv.org/abs/2602.18409v1</link>
<guid>2602.18409v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.LO
Authors: Huan Luo, Jonni Virtema
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.]]></description>
<pubDate>Fri, 20 Feb 2026 18:18:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges]]></title>
<link>http://arxiv.org/abs/2602.18406v1</link>
<guid>2602.18406v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Minh Dinh, Stphane Deny
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.]]></description>
<pubDate>Fri, 20 Feb 2026 18:14:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study]]></title>
<link>http://arxiv.org/abs/2602.18403v1</link>
<guid>2602.18403v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Orfeas Bourchas, George Papalambrou
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.]]></description>
<pubDate>Fri, 20 Feb 2026 18:12:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay]]></title>
<link>http://arxiv.org/abs/2602.18401v1</link>
<guid>2602.18401v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, stat.ML
Authors: Josue Casco-Rodriguez, Nanda H. Krishna, Richard G. Baraniuk
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Biological neural networks (like the hippocampus) can internally generate "replay" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.]]></description>
<pubDate>Fri, 20 Feb 2026 18:07:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[Exploiting Completeness Perception with Diffusion Transformer for Unified 3D MRI Synthesis]]></title>
<link>http://arxiv.org/abs/2602.18400v1</link>
<guid>2602.18400v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Junkai Liu, Nay Aung, Theodoros N. Arvanitis, Joao A. C. Lima, Steffen E. Petersen et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Missing data problems, such as missing modalities in multi-modal brain MRI and missing slices in cardiac MRI, pose significant challenges in clinical practice. Existing methods rely on external guidance to supply detailed missing state for instructing generative models to synthesize missing MRIs. However, manual indicators are not always available or reliable in real-world scenarios due to the unpredictable nature of clinical environments. Moreover, these explicit masks are not informative enough to provide guidance for improving semantic consistency. In this work, we argue that generative models should infer and recognize missing states in a self-perceptive manner, enabling them to better capture subtle anatomical and pathological variations. Towards this goal, we propose CoPeDiT, a general-purpose latent diffusion model equipped with completeness perception for unified synthesis of 3D MRIs. Specifically, we incorporate dedicated pretext tasks into our tokenizer, CoPeVAE, empowering it to learn completeness-aware discriminative prompts, and design MDiT3D, a specialized diffusion transformer architecture for 3D MRI synthesis, that effectively uses the learned prompts as guidance to enhance semantic consistency in 3D space. Comprehensive evaluations on three large-scale MRI datasets demonstrate that CoPeDiT significantly outperforms state-of-the-art methods, achieving superior robustness, generalizability, and flexibility. The code is available at https://github.com/JK-Liu7/CoPeDiT .]]></description>
<pubDate>Fri, 20 Feb 2026 18:05:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing]]></title>
<link>http://arxiv.org/abs/2602.18396v1</link>
<guid>2602.18396v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.AP, stat.ML
Authors: Ehsan Lari, Reza Arablouei, Stefan Werner
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.]]></description>
<pubDate>Fri, 20 Feb 2026 18:01:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[Self-Aware Object Detection via Degradation Manifolds]]></title>
<link>http://arxiv.org/abs/2602.18394v1</link>
<guid>2602.18394v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Stefan Becker, Simon Weiss, Wolfgang Hbner, Michael Arens
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.
  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.
  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.
  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.]]></description>
<pubDate>Fri, 20 Feb 2026 17:58:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO]]></title>
<link>http://arxiv.org/abs/2602.18386v1</link>
<guid>2602.18386v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.AI, cs.LG
Authors: Mohamed Elgouhary, Amr S. El-Wakeel
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.]]></description>
<pubDate>Fri, 20 Feb 2026 17:48:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[FedZMG: Efficient Client-Side Optimization in Federated Learning]]></title>
<link>http://arxiv.org/abs/2602.18384v1</link>
<guid>2602.18384v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Fotios Zantalis, Evangelos Zervas, Grigorios Koulouras
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the "intensity" or "bias" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.]]></description>
<pubDate>Fri, 20 Feb 2026 17:45:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Theory and interpretability of Quantum Extreme Learning Machines: a Pauli-transfer matrix approach]]></title>
<link>http://arxiv.org/abs/2602.18377v1</link>
<guid>2602.18377v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Markus Gross, Hans-Martin Rieser
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. Here, we consider n-qubit quantum extreme learning machines (QELMs) with continuous-time reservoir dynamics. QELMs are memoryless QRCs capable of various ML tasks, including image classification and time series forecasting. We apply the Pauli transfer matrix (PTM) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the QELM performance. This formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the QELM, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. Optimizing a QELM can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor. The PTM formalism allows one to identify the classical representation of a QELM and thereby guide its design towards a given training objective. As a specific application, we focus on learning nonlinear dynamical systems and show that a QELM trained on such trajectories learns a surrogate-approximation to the underlying flow map.]]></description>
<pubDate>Fri, 20 Feb 2026 17:33:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Zero-shot Interactive Perception]]></title>
<link>http://arxiv.org/abs/2602.18374v1</link>
<guid>2602.18374v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.AI
Authors: Venkatesh Sripada, Frank Guerin, Amir Ghalamzan
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.]]></description>
<pubDate>Fri, 20 Feb 2026 17:30:25 +0000</pubDate>
</item>
<item>
<title><![CDATA["How Do I ...?": Procedural Questions Predominate Student-LLM Chatbot Conversations]]></title>
<link>http://arxiv.org/abs/2602.18372v1</link>
<guid>2602.18372v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI
Authors: Alexandra Neagu, Marcus Messer, Peter Johnson, Rhodri Nelson
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Providing scaffolding through educational chatbots built on Large Language Models (LLM) has potential risks and benefits that remain an open area of research. When students navigate impasses, they ask for help by formulating impasse-driven questions. Within interactions with LLM chatbots, such questions shape the user prompts and drive the pedagogical effectiveness of the chatbot's response. This paper focuses on such student questions from two datasets of distinct learning contexts: formative self-study, and summative assessed coursework. We analysed 6,113 messages from both learning contexts, using 11 different LLMs and three human raters to classify student questions using four existing schemas. On the feasibility of using LLMs as raters, results showed moderate-to-good inter-rater reliability, with higher consistency than human raters. The data showed that 'procedural' questions predominated in both learning contexts, but more so when students prepare for summative assessment. These results provide a basis on which to use LLMs for classification of student questions. However, we identify clear limitations in both the ability to classify with schemas and the value of doing so: schemas are limited and thus struggle to accommodate the semantic richness of composite prompts, offering only partial understanding the wider risks and benefits of chatbot integration. In the future, we recommend an analysis approach that captures the nuanced, multi-turn nature of conversation, for example, by applying methods from conversation analysis in discursive psychology.]]></description>
<pubDate>Fri, 20 Feb 2026 17:27:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Quantum Maximum Likelihood Prediction via Hilbert Space Embeddings]]></title>
<link>http://arxiv.org/abs/2602.18364v1</link>
<guid>2602.18364v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IT, cs.LG, stat.ML
Authors: Sreejith Sreekumar, Nir Weinberger
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent works have proposed various explanations for the ability of modern large language models (LLMs) to perform in-context prediction. We propose an alternative conceptual viewpoint from an information-geometric and statistical perspective. Motivated by Bach[2023], we model training as learning an embedding of probability distributions into the space of quantum density operators, and in-context learning as maximum-likelihood prediction over a specified class of quantum models. We provide an interpretation of this predictor in terms of quantum reverse information projection and quantum Pythagorean theorem when the class of quantum models is sufficiently expressive. We further derive non-asymptotic performance guarantees in terms of convergence rates and concentration inequalities, both in trace norm and quantum relative entropy. Our approach provides a unified framework to handle both classical and quantum LLMs.]]></description>
<pubDate>Fri, 20 Feb 2026 17:16:38 +0000</pubDate>
</item>
<item>
<title><![CDATA[Validating Political Position Predictions of Arguments]]></title>
<link>http://arxiv.org/abs/2602.18351v1</link>
<guid>2602.18351v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Jordan Robinson, Angus R. Williams, Katie Atkinson, Anthony G. Cohn
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.]]></description>
<pubDate>Fri, 20 Feb 2026 17:03:44 +0000</pubDate>
</item>
<item>
<title><![CDATA[Quantum-enhanced satellite image classification]]></title>
<link>http://arxiv.org/abs/2602.18350v1</link>
<guid>2602.18350v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Qi Zhang, Anton Simen, Carlos Flores-Garrigs, Gabriel Alvarado Barrios, Paolo A. Erdman et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. By harnessing the dynamics of many-body spin Hamiltonians, the method generates expressive quantum features that, when combined with classical processing, lead to quantum-enhanced classification accuracy. Using a strong and well-established ResNet50 baseline, we achieved a maximum classical accuracy of 83%, which can be improved to 84% with a transfer learning approach. In contrast, applying our quantum-classical method the performance is increased to 87% accuracy, demonstrating a clear and reproducible improvement over robust classical approaches. Implemented on several of IBM's quantum processors, our hybrid quantum-classical approach delivers consistent gains of 2-3% in absolute accuracy. These results highlight the practical potential of current and near-term quantum processors in high-stakes, data-driven domains such as satellite imaging and remote sensing, while suggesting broader applicability in real-world machine learning tasks.]]></description>
<pubDate>Fri, 20 Feb 2026 17:02:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering]]></title>
<link>http://arxiv.org/abs/2602.18348v1</link>
<guid>2602.18348v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Matheus Camilo da Silva, Leonardo Arrighi, Ana Carolina Lorena, Sylvio Barbon Junior
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.]]></description>
<pubDate>Fri, 20 Feb 2026 17:01:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System]]></title>
<link>http://arxiv.org/abs/2602.18346v1</link>
<guid>2602.18346v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Pavithra PM Nair, Preethu Rose Anish
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.]]></description>
<pubDate>Fri, 20 Feb 2026 16:57:44 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the "Induction Bias" in Sequence Models]]></title>
<link>http://arxiv.org/abs/2602.18333v1</link>
<guid>2602.18333v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CL
Authors: M. Reza Ebrahimi, Michal Defferrard, Sunny Panchal, Roland Memisevic
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.]]></description>
<pubDate>Fri, 20 Feb 2026 16:39:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[G-LoG Bi-filtration for Medical Image Classification]]></title>
<link>http://arxiv.org/abs/2602.18329v1</link>
<guid>2602.18329v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Qingsong Wang, Jiaxing He, Bingzhe Hou, Tieru Wu, Yang Cao et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.]]></description>
<pubDate>Fri, 20 Feb 2026 16:35:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning]]></title>
<link>http://arxiv.org/abs/2602.18326v1</link>
<guid>2602.18326v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Tao Wu, Adam Kapelner
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We describe a modern deep learning system that automatically identifies informative contextual examples (\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.]]></description>
<pubDate>Fri, 20 Feb 2026 16:32:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[PsihoRo: Depression and Anxiety Romanian Text Corpus]]></title>
<link>http://arxiv.org/abs/2602.18324v1</link>
<guid>2602.18324v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Alexandra Ciobotaru, Ana-Maria Bucur, Liviu P. Dinu
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.]]></description>
<pubDate>Fri, 20 Feb 2026 16:24:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis]]></title>
<link>http://arxiv.org/abs/2602.18322v1</link>
<guid>2602.18322v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Ziteng Cui, Shuhong Liu, Xiaoyu Dong, Xuangeng Chu, Lin Gu et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.]]></description>
<pubDate>Fri, 20 Feb 2026 16:20:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Robo-Saber: Generating and Simulating Virtual Reality Players]]></title>
<link>http://arxiv.org/abs/2602.18319v1</link>
<guid>2602.18319v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.GR, cs.AI, cs.HC, cs.LG
Authors: Nam Hee Kim, Jingjing May Liu, Jaakko Lehtinen, Perttu Hmlinen, James F. O'Brien et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present the first motion generation system for playtesting virtual reality (VR) games. Our player model generates VR headset and handheld controller movements from in-game object arrangements, guided by style exemplars and aligned to maximize simulated gameplay score. We train on the large BOXRR-23 dataset and apply our framework on the popular VR game Beat Saber. The resulting model Robo-Saber produces skilled gameplay and captures diverse player behaviors, mirroring the skill levels and movement patterns specified by input style exemplars. Robo-Saber demonstrates promise in synthesizing rich gameplay data for predictive applications and enabling a physics-based whole-body VR playtesting agent.]]></description>
<pubDate>Fri, 20 Feb 2026 16:19:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting]]></title>
<link>http://arxiv.org/abs/2602.18314v1</link>
<guid>2602.18314v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.GR, cs.RO
Authors: Tianyi Song, Danail Stoyanov, Evangelos Mazomenos, Francisco Vasconcelos
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.]]></description>
<pubDate>Fri, 20 Feb 2026 16:14:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[Clapeyron Neural Networks for Single-Species Vapor-Liquid Equilibria]]></title>
<link>http://arxiv.org/abs/2602.18313v1</link>
<guid>2602.18313v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jan Pavek, Alexander Mitsos, Elvis J. Sim, Jan G. Rittig
Institution: MIT
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Machine learning (ML) approaches have shown promising results for predicting molecular properties relevant for chemical process design. However, they are often limited by scarce experimental property data and lack thermodynamic consistency. As such, thermodynamics-informed ML, i.e., incorporating thermodynamic relations into the loss function as regularization term for training, has been proposed. We herein transfer the concept of thermodynamics-informed graph neural networks (GNNs) from the Gibbs-Duhem to the Clapeyron equation, predicting several pure component properties in a multi-task manner, namely: vapor pressure, liquid molar volume, vapor molar volume and enthalpy of vaporization. We find improved prediction accuracy of the Clapeyron-GNN compared to the single-task learning setting, and improved approximation of the Clapeyron equation compared to the purely data-driven multi-task learning setting. In fact, we observe the largest improvement in prediction accuracy for the properties with the lowest availability of data, making our model promising for practical application in data scarce scenarios of chemical engineering practice.]]></description>
<pubDate>Fri, 20 Feb 2026 16:11:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation]]></title>
<link>http://arxiv.org/abs/2602.18309v1</link>
<guid>2602.18309v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Ziyue Liu, Davide Talon, Federico Girella, Zanxi Ruan, Mattia Mondo et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.]]></description>
<pubDate>Fri, 20 Feb 2026 16:07:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[JPmHC Dynamical Isometry via Orthogonal Hyper-Connections]]></title>
<link>http://arxiv.org/abs/2602.18308v1</link>
<guid>2602.18308v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Biswa Sengupta, Jinhua Wang, Leo Brunswic
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.]]></description>
<pubDate>Fri, 20 Feb 2026 16:06:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean]]></title>
<link>http://arxiv.org/abs/2602.18307v1</link>
<guid>2602.18307v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SE, cs.CL, cs.LG, cs.PL
Authors: Yutong Xin, Qiaochu Chen, Greg Durrett, Iil Dillig
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.]]></description>
<pubDate>Fri, 20 Feb 2026 16:05:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction]]></title>
<link>http://arxiv.org/abs/2602.18301v1</link>
<guid>2602.18301v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CL
Authors: Ivan Bondarenko, Egor Palkin, Fedor Tikunov
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for "imposing" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.]]></description>
<pubDate>Fri, 20 Feb 2026 15:54:10 +0000</pubDate>
</item>
<item>
<title><![CDATA[Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory]]></title>
<link>http://arxiv.org/abs/2602.18297v1</link>
<guid>2602.18297v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CL, cs.IT
Authors: Usman Anwar, Tim Bakker, Dana Kianfar, Cristina Pinneri, Christos Louizos
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.]]></description>
<pubDate>Fri, 20 Feb 2026 15:50:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies]]></title>
<link>http://arxiv.org/abs/2602.18291v1</link>
<guid>2602.18291v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Zhuoran Li, Hai Zhong, Xun Wang, Qingxin Xia, Lihua Zhang et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.]]></description>
<pubDate>Fri, 20 Feb 2026 15:38:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation]]></title>
<link>http://arxiv.org/abs/2602.18283v1</link>
<guid>2602.18283v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IR, cs.AI
Authors: Lei Xin, Yuhao Zheng, Ke Cheng, Changjiang Jiang, Zifan Zhang et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.]]></description>
<pubDate>Fri, 20 Feb 2026 15:11:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control]]></title>
<link>http://arxiv.org/abs/2602.18282v1</link>
<guid>2602.18282v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Shiyan Du, Conghan Yue, Xinyu Cheng, Dongyu Zhang
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.]]></description>
<pubDate>Fri, 20 Feb 2026 15:11:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[PRISM: Parallel Reward Integration with Symmetry for MORL]]></title>
<link>http://arxiv.org/abs/2602.18277v1</link>
<guid>2602.18277v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, stat.ML
Authors: Finn van der Knaap, Kejiang Qian, Zheng Xu, Fengxiang He
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\% over the baseline and up to 32\% over the oracle. The code is at \href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.]]></description>
<pubDate>Fri, 20 Feb 2026 15:02:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Probabilistic Framework for LLM-Based Model Discovery]]></title>
<link>http://arxiv.org/abs/2602.18266v1</link>
<guid>2602.18266v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Stefan Wahl, Raphaela Schenk, Ali Farnoud, Jakob H. Macke, Daniel Gedon
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.]]></description>
<pubDate>Fri, 20 Feb 2026 14:49:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Simplifying Outcomes of Language Model Component Analyses with ELIA]]></title>
<link>http://arxiv.org/abs/2602.18262v1</link>
<guid>2602.18262v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI, cs.LG
Authors: Aaron Louis Eidt, Nils Feldhus
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.]]></description>
<pubDate>Fri, 20 Feb 2026 14:45:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[RoEL: Robust Event-based 3D Line Reconstruction]]></title>
<link>http://arxiv.org/abs/2602.18258v1</link>
<guid>2602.18258v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.CV
Authors: Gwangtak Bae, Jaeho Shin, Seunggu Kang, Junho Kim, Ayoung Kim et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Event cameras in motion tend to detect object boundaries or texture edges, which produce lines of brightness changes, especially in man-made environments. While lines can constitute a robust intermediate representation that is consistently observed, the sparse nature of lines may lead to drastic deterioration with minor estimation errors. Only a few previous works, often accompanied by additional sensors, utilize lines to compensate for the severe domain discrepancies of event sensors along with unpredictable noise characteristics. We propose a method that can stably extract tracks of varying appearances of lines using a clever algorithmic process that observes multiple representations from various time slices of events, compensating for potential adversaries within the event data. We then propose geometric cost functions that can refine the 3D line maps and camera poses, eliminating projective distortions and depth ambiguities. The 3D line maps are highly compact and can be equipped with our proposed cost function, which can be adapted for any observations that can detect and extract line structures or projections of them, including 3D point cloud maps or image observations. We demonstrate that our formulation is powerful enough to exhibit a significant performance boost in event-based mapping and pose refinement across diverse datasets, and can be flexibly applied to multimodal scenarios. Our results confirm that the proposed line-based formulation is a robust and effective approach for the practical deployment of event-based perceptual modules. Project page: https://gwangtak.github.io/roel/]]></description>
<pubDate>Fri, 20 Feb 2026 14:43:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data]]></title>
<link>http://arxiv.org/abs/2602.18253v1</link>
<guid>2602.18253v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Xabier de Zuazo, Vincenzo Verbeni, Eva Navas, Ibon Saratxaga, Mathieu Bourguignon et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.]]></description>
<pubDate>Fri, 20 Feb 2026 14:39:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Adversarial Robustness of Discrete Image Tokenizers]]></title>
<link>http://arxiv.org/abs/2602.18252v1</link>
<guid>2602.18252v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Rishika Bhagwatkar, Irina Rish, Nicolas Flammarion, Francesco Croce
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.]]></description>
<pubDate>Fri, 20 Feb 2026 14:39:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[Variational Distributional Neuron]]></title>
<link>http://arxiv.org/abs/2602.18250v1</link>
<guid>2602.18250v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Yves Ruffenach
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This "contraction" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze "collapse" modes and the conditions for a "living neuron", then extend the contribution over time via autoregressive priors over the latent, per unit.]]></description>
<pubDate>Fri, 20 Feb 2026 14:35:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver]]></title>
<link>http://arxiv.org/abs/2602.18248v1</link>
<guid>2602.18248v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Pietro Sittoni, Emanuele Zangrando, Angelo A. Casulli, Nicola Guglielmi, Francesco Tudisco
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.]]></description>
<pubDate>Fri, 20 Feb 2026 14:31:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning]]></title>
<link>http://arxiv.org/abs/2602.18232v1</link>
<guid>2602.18232v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Lexiang Tang, Weihao Gao, Bingchen Zhao, Lu Ma, Qiao jin et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.]]></description>
<pubDate>Fri, 20 Feb 2026 14:13:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games]]></title>
<link>http://arxiv.org/abs/2602.18230v1</link>
<guid>2602.18230v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Jorge Carrasco Pollo, Ioannis Kapetangeorgis, Joshua Rosenthal, John Hua Yao
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.]]></description>
<pubDate>Fri, 20 Feb 2026 14:11:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction]]></title>
<link>http://arxiv.org/abs/2602.18227v1</link>
<guid>2602.18227v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Redwanul Karim, Changhun Kim, Timon Conrad, Nora Gourmelon, Julian Oelhaf et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.]]></description>
<pubDate>Fri, 20 Feb 2026 14:07:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[SimVLA: A Simple VLA Baseline for Robotic Manipulation]]></title>
<link>http://arxiv.org/abs/2602.18224v1</link>
<guid>2602.18224v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.LG
Authors: Yuankai Luo, Woping Chen, Tong Liang, Baiqiao Wang, Zhenguo Li
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA]]></description>
<pubDate>Fri, 20 Feb 2026 14:04:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Information-Theoretic Storage Cost in Sentence Comprehension]]></title>
<link>http://arxiv.org/abs/2602.18217v1</link>
<guid>2602.18217v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Kohei Kajikawa, Shinnosuke Isono, Ethan Gotlieb Wilcox
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.]]></description>
<pubDate>Fri, 20 Feb 2026 13:55:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Generative Model via Quantile Assignment]]></title>
<link>http://arxiv.org/abs/2602.18216v1</link>
<guid>2602.18216v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Georgi Hrusanov, Oliver Y. Chn, Julien S. Bodelet
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Deep Generative models (DGMs) play two key roles in modern machine learning: (i) producing new information (e.g., image synthesis) and (ii) reducing dimensionality. However, traditional architectures often rely on auxiliary networks such as encoders in Variational Autoencoders (VAEs) or discriminators in Generative Adversarial Networks (GANs), which introduce training instability, computational overhead, and risks like mode collapse. We present NeuroSQL, a new generative paradigm that eliminates the need for auxiliary networks by learning low-dimensional latent representations implicitly. NeuroSQL leverages an asymptotic approximation that expresses the latent variables as the solution to an optimal transportation problem. Specifically, NeuroSQL learns the latent variables by solving a linear assignment problem and then passes the latent information to a standalone generator. We benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on four datasets: handwritten digits (MNIST), faces (CelebA), animal faces (AFHQ), and brain images (OASIS). Compared to VAEs, GANs, and diffusion models: (1) in terms of image quality, NeuroSQL achieves overall lower mean pixel distance between synthetic and authentic images and stronger perceptual/structural fidelity; (2) computationally, NeuroSQL requires the least training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data with limited training samples. By embracing quantile assignment rather than an encoder, NeuroSQL provides a fast, stable, and robust way to generate synthetic data with minimal information loss.]]></description>
<pubDate>Fri, 20 Feb 2026 13:52:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Machine-learning force-field models for dynamical simulations of metallic magnets]]></title>
<link>http://arxiv.org/abs/2602.18213v1</link>
<guid>2602.18213v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Gia-Wei Chern, Yunhao Fan, Sheng Zhang, Puhan Zhang
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We review recent advances in machine learning (ML) force-field methods for Landau-Lifshitz-Gilbert (LLG) simulations of itinerant electron magnets, focusing on scalability and transferability. Built on the principle of locality, a deep neural network model is developed to efficiently and accurately predict the electron-mediated forces governing spin dynamics. Symmetry-aware descriptors constructed through a group-theoretical approach ensure rigorous incorporation of both lattice and spin-rotation symmetries. The framework is demonstrated using the prototypical s-d exchange model widely employed in spintronics. ML-enabled large-scale simulations reveal novel nonequilibrium phenomena, including anomalous coarsening of tetrahedral spin order on the triangular lattice and the freezing of phase separation dynamics in lightly hole-doped, strong-coupling square-lattice systems. These results establish ML force-field frameworks as scalable, accurate, and versatile tools for modeling nonequilibrium spin dynamics in itinerant magnets.]]></description>
<pubDate>Fri, 20 Feb 2026 13:51:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps]]></title>
<link>http://arxiv.org/abs/2602.18201v1</link>
<guid>2602.18201v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.LG
Authors: Joseph Bingham, Netanel Arussy, Dvir Aran
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime]]></description>
<pubDate>Fri, 20 Feb 2026 13:25:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion]]></title>
<link>http://arxiv.org/abs/2602.18199v1</link>
<guid>2602.18199v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Gahyeon Shim, Soogeun Park, Hyemin Ahn
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.]]></description>
<pubDate>Fri, 20 Feb 2026 13:17:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference]]></title>
<link>http://arxiv.org/abs/2602.18196v1</link>
<guid>2602.18196v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Xiuying Wei, Caglar Gulcehre
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.]]></description>
<pubDate>Fri, 20 Feb 2026 13:09:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification]]></title>
<link>http://arxiv.org/abs/2602.18195v1</link>
<guid>2602.18195v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Hairong Chen, Yicheng Feng, Ziyu Jia, Samir Bhatt, Hengguan Huang
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.]]></description>
<pubDate>Fri, 20 Feb 2026 13:03:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards]]></title>
<link>http://arxiv.org/abs/2602.18193v1</link>
<guid>2602.18193v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yiran Yang, Zhaowei Liu, Yuan Yuan, Yukun Song, Xiong Ma et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.]]></description>
<pubDate>Fri, 20 Feb 2026 12:59:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Box Thirding: Anytime Best Arm Identification under Insufficient Sampling]]></title>
<link>http://arxiv.org/abs/2602.18186v1</link>
<guid>2602.18186v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Seohwa Hwang, Junyong Park
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We introduce Box Thirding (B3), a flexible and efficient algorithm for Best Arm Identification (BAI) under fixed-budget constraints. It is designed for both anytime BAI and scenarios with large N, where the number of arms is too large for exhaustive evaluation within a limited budget T. The algorithm employs an iterative ternary comparison: in each iteration, three arms are compared--the best-performing arm is explored further, the median is deferred for future comparisons, and the weakest is discarded. Even without prior knowledge of T, B3 achieves an epsilon-best arm misidentification probability comparable to Successive Halving (SH), which requires T as a predefined parameter, applied to a randomly selected subset of c0 arms that fit within the budget. Empirical results show that B3 outperforms existing methods under limited-budget constraints in terms of simple regret, as demonstrated on the New Yorker Cartoon Caption Contest dataset.]]></description>
<pubDate>Fri, 20 Feb 2026 12:47:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[Capabilities Ain't All You Need: Measuring Propensities in AI]]></title>
<link>http://arxiv.org/abs/2602.18182v1</link>
<guid>2602.18182v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Daniel Romero-Alvarado, Fernando Martnez-Plumed, Lorenzo Pacchiardi, Hugo Save, Siddhesh Milind Pawar et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an "ideal band". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.]]></description>
<pubDate>Fri, 20 Feb 2026 12:40:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[SeedFlood: A Step Toward Scalable Decentralized Training of LLMs]]></title>
<link>http://arxiv.org/abs/2602.18181v1</link>
<guid>2602.18181v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jihun Kim, Namhoon Lee
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This work presents a new approach to decentralized training-SeedFlood-designed to scale for large models across complex network topologies and achieve global consensus with minimal communication overhead. Traditional gossip-based methods suffer from message communication costs that grow with model size, while information decay over network hops renders global consensus inefficient. SeedFlood departs from these practices by exploiting the seed-reconstructible structure of zeroth-order updates and effectively making the messages near-zero in size, allowing them to be flooded to every client in the network. This mechanism makes communication overhead negligible and independent of model size, removing the primary scalability bottleneck in decentralized training. Consequently, SeedFlood enables training in regimes previously considered impractical, such as billion-parameter models distributed across hundreds of clients. Our experiments on decentralized LLM fine-tuning demonstrate thatSeedFlood consistently outperforms gossip-based baselines in both generalization performance and communication efficiency, and even achieves results comparable to first-order methods in large scale settings.]]></description>
<pubDate>Fri, 20 Feb 2026 12:38:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[Evaluating Graphical Perception Capabilities of Vision Transformers]]></title>
<link>http://arxiv.org/abs/2602.18178v1</link>
<guid>2602.18178v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Poonam Poonam, Pere-Pau Vzquez, Timo Ropinski
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.]]></description>
<pubDate>Fri, 20 Feb 2026 12:32:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Improving Sampling for Masked Diffusion Models via Information Gain]]></title>
<link>http://arxiv.org/abs/2602.18176v1</link>
<guid>2602.18176v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Kaisen Yang, Jayden Teoh, Kaicheng Yang, Yitong Zhang, Alex Lamb
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.]]></description>
<pubDate>Fri, 20 Feb 2026 12:26:03 +0000</pubDate>
</item>
<item>
<title><![CDATA[Can AI Lower the Barrier to Cybersecurity? A Human-Centered Mixed-Methods Study of Novice CTF Learning]]></title>
<link>http://arxiv.org/abs/2602.18172v1</link>
<guid>2602.18172v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CR, cs.AI
Authors: Cathrin Schachner, Jasmin Wachter
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Capture-the-Flag (CTF) competitions serve as gateways into offensive cybersecurity, yet they often present steep barriers for novices due to complex toolchains and opaque workflows. Recently, agentic AI frameworks for cybersecurity promise to lower these barriers by automating and coordinating penetration testing tasks. However, their role in shaping novice learning remains underexplored.
  We present a human-centered, mixed-methods case study examining how agentic AI frameworks -- here Cybersecurity AI (CAI) -- mediates novice entry into CTF-based penetration testing. An undergraduate student without prior hacking experience attempted to approach performance benchmarks from a national cybersecurity challenge using CAI. Quantitative performance metrics were complemented by structured reflective analysis of learning progression and AI interaction patterns.
  Our thematic analysis suggest that agentic AI reduces initial entry barriers by providing overview, structure and guidance, thereby lowering the cognitive workload during early engagement. Quantitatively, the observed extensive exploration of strategies and low per-strategy execution time potetially facilitatates cybersecurity training on meta, i.e. strategic levels. At the same time, AI-assisted cybersecurity education introduces new challenges related to trust, dependency, and responsible use. We discuss implications for human-centered AI-supported cybersecurity education and outline open questions for future research.]]></description>
<pubDate>Fri, 20 Feb 2026 12:20:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models]]></title>
<link>http://arxiv.org/abs/2602.18171v1</link>
<guid>2602.18171v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Wojciech Michaluk, Tymoteusz Urban, Mateusz Kubita, Soveatin Kuntur, Anna Wroblewska
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.]]></description>
<pubDate>Fri, 20 Feb 2026 12:16:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Deep Surrogate Model for Robust and Generalizable Long-Term Blast Wave Prediction]]></title>
<link>http://arxiv.org/abs/2602.18168v1</link>
<guid>2602.18168v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Danning Jing, Xinhai Chen, Xifeng Pu, Jie Hu, Chao Huang et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurately modeling the spatio-temporal dynamics of blast wave propagation remains a longstanding challenge due to its highly nonlinear behavior, sharp gradients, and burdensome computational cost. While machine learning-based surrogate models offer fast inference as a promising alternative, they suffer from degraded accuracy, particularly evaluated on complex urban layouts or out-of-distribution scenarios. Moreover, autoregressive prediction strategies in such models are prone to error accumulation over long forecasting horizons, limiting their robustness for extended-time simulations. To address these limitations, we propose RGD-Blast, a robust and generalizable deep surrogate model for high-fidelity, long-term blast wave forecasting. RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, effectively mitigating error accumulation during autoregressive prediction. We introduce a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features, thereby enhancing out-of-distribution generalization. Experiments demonstrate that RGD-Blast achieves a two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, the model achieves an average RMSE below 0.01 and an R2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization, substantially advancing the state of the art in long-term blast wave modeling.]]></description>
<pubDate>Fri, 20 Feb 2026 12:14:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Unifying Formal Explanations: A Complexity-Theoretic Perspective]]></title>
<link>http://arxiv.org/abs/2602.18160v1</link>
<guid>2602.18160v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CC, cs.DS
Authors: Shahaf Bassan, Xuanxiang Huang, Guy Katz
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties of the value function: (1) *monotonicity*, (2) *submodularity*, and (3) *supermodularity* - which are three fundamental properties in *combinatorial optimization*. Our findings uncover some counterintuitive results regarding the nature of these properties within the explanation settings examined. For instance, although the *local* value functions do not exhibit monotonicity or submodularity/supermodularity whatsoever, we demonstrate that the *global* value functions do possess these properties. This distinction enables us to prove a series of novel polynomial-time results for computing various explanations with provable guarantees in the global explainability setting, across a range of ML models that span the interpretability spectrum, such as neural networks, decision trees, and tree ensembles. In contrast, we show that even highly simplified versions of these explanations become NP-hard to compute in the corresponding local explainability setting.]]></description>
<pubDate>Fri, 20 Feb 2026 11:52:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[FENCE: A Financial and Multimodal Jailbreak Detection Dataset]]></title>
<link>http://arxiv.org/abs/2602.18154v1</link>
<guid>2602.18154v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI, cs.DB
Authors: Mirae Kim, Seonghun Jeong, Youngjun Kwak
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.]]></description>
<pubDate>Fri, 20 Feb 2026 11:40:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[The Statistical Signature of LLMs]]></title>
<link>http://arxiv.org/abs/2602.18152v1</link>
<guid>2602.18152v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.CY
Authors: Ortal Hadad, Edoardo Loru, Jacopo Nudo, Niccol Di Marco, Matteo Cinelli et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.]]></description>
<pubDate>Fri, 20 Feb 2026 11:33:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[Rethinking Beam Management: Generalization Limits Under Hardware Heterogeneity]]></title>
<link>http://arxiv.org/abs/2602.18151v1</link>
<guid>2602.18151v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.NI, cs.IT, cs.LG
Authors: Nikita Zeulin, Olga Galinina, Ibrahim Kilinc, Sergey Andreev, Robert W. Heath
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Hardware heterogeneity across diverse user devices poses new challenges for beam-based communication in 5G and beyond. This heterogeneity limits the applicability of machine learning (ML)-based algorithms. This article highlights the critical need to treat hardware heterogeneity as a first-class design concern in ML-aided beam management. We analyze key failure modes in the presence of heterogeneity and present case studies demonstrating their performance impact. Finally, we discuss potential strategies to improve generalization in beam management.]]></description>
<pubDate>Fri, 20 Feb 2026 11:30:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[BONNI: Gradient-Informed Bayesian and Interior Point Optimization for Efficient Inverse Design in Nanophotonics]]></title>
<link>http://arxiv.org/abs/2602.18148v1</link>
<guid>2602.18148v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Yannik Mahlau, Yannick Augenstein, Tyler W. Hughes, Marius Lindauer, Bodo Rosenhahn
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Inverse design, particularly geometric shape optimization, provides a systematic approach for developing high-performance nanophotonic devices. While numerous optimization algorithms exist, previous global approaches exhibit slow convergence and conversely local search strategies frequently become trapped in local optima. To address the limitations inherent to both local and global approaches, we introduce BONNI: Bayesian optimization through neural network ensemble surrogates with interior point optimization. It augments global optimization with an efficient incorporation of gradient information to determine optimal sampling points. This capability allows BONNI to circumvent the local optima found in many nanophotonic applications, while capitalizing on the efficiency of gradient-based optimization. We demonstrate BONNI's capabilities in the design of a distributed Bragg reflector as well as a dual-layer grating coupler through an exhaustive comparison against other optimization algorithms commonly used in literature. Using BONNI, we were able to design a 10-layer distributed Bragg reflector with only 4.5% mean spectral error, compared to the previously reported results of 7.8% error with 16 layers. Further designs of a broadband waveguide taper and photonic crystal waveguide transition validate the capabilities of BONNI.]]></description>
<pubDate>Fri, 20 Feb 2026 11:26:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks]]></title>
<link>http://arxiv.org/abs/2602.18146v1</link>
<guid>2602.18146v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Lionel Salesses, Larbi Arbaoui, Tariq Benamara, Arnaud Francois, Caroline Sainvitu
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models operating at complementary time scales. Both models rely on a latent recurrent graph neural network to capture spatiotemporal dynamics on meshes, while a variational graph autoencoder provides a compact latent representation that reduces memory usage and improves training stability. Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baseline. Although evaluated in two dimensions, the framework is general and extensible to physics-driven systems with multiscale dynamics and to three-dimensional geometries.]]></description>
<pubDate>Fri, 20 Feb 2026 11:22:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention]]></title>
<link>http://arxiv.org/abs/2602.18145v1</link>
<guid>2602.18145v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Siya Qi, Yudong Chen, Runcong Zhao, Qinglin Zhu, Zhanghao Hu et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.]]></description>
<pubDate>Fri, 20 Feb 2026 11:18:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks]]></title>
<link>http://arxiv.org/abs/2602.18141v1</link>
<guid>2602.18141v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Pierre-Gabriel Berlureau, Ali Hariri, Victor Kawasaki-Borruat, Mia Zosso, Pierre Vandergheynst
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency. Our theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet delivers consistent gains on synthetic long-range reasoning tasks, as well as real-world benchmarks, while offering an interpretable routing field that reveals how information flows through the graph. This establishes the Bakry-Emery Laplacian as a principled and efficient foundation for adaptive spectral graph learning.]]></description>
<pubDate>Fri, 20 Feb 2026 11:01:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[Flexi-NeurA: A Configurable Neuromorphic Accelerator with Adaptive Bit-Precision Exploration for Edge SNNs]]></title>
<link>http://arxiv.org/abs/2602.18140v1</link>
<guid>2602.18140v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AR, cs.NE
Authors: Mohammad Farahani, Mohammad Rasoul Roshanshah, Saeed Safari
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Neuromorphic accelerators promise unparalleled energy efficiency and computational density for spiking neural networks (SNNs), especially in edge intelligence applications. However, most existing platforms exhibit rigid architectures with limited configurability, restricting their adaptability to heterogeneous workloads and diverse design objectives. To address these limitations, we present Flexi-NeurA -- a parameterizable neuromorphic accelerator (core) that unifies configurability, flexibility, and efficiency. Flexi-NeurA allows users to customize neuron models, network structures, and precision settings at design time. By pairing these design-time configurability and flexibility features with a time-multiplexed and event-driven processing approach, Flexi-NeurA substantially reduces the required hardware resources and total power while preserving high efficiency and low inference latency. Complementing this, we introduce Flex-plorer, a heuristic-guided design-space exploration (DSE) tool that determines cost-effective fixed-point precisions for critical parameters -- such as decay factors, synaptic weights, and membrane potentials -- based on user-defined trade-offs between accuracy and resource usage. Based on the configuration selected through the Flex-plorer process, RTL code is configured to match the specified design. Comprehensive evaluations across MNIST, SHD, and DVS benchmarks demonstrate that the Flexi-NeurA and Flex-plorer co-framework achieves substantial improvements in accuracy, latency, and energy efficiency. A three-layer 256--128--10 fully connected network with LIF neurons mapped onto two processing cores achieves 97.23% accuracy on MNIST with 1.1~ms inference latency, utilizing only 1,623 logic cells, 7 BRAMs, and 111~mW of total power -- establishing Flexi-NeurA as a scalable, edge-ready neuromorphic platform.]]></description>
<pubDate>Fri, 20 Feb 2026 11:01:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[Agentic Adversarial QA for Improving Domain-Specific LLMs]]></title>
<link>http://arxiv.org/abs/2602.18137v1</link>
<guid>2602.18137v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI, cs.LG
Authors: Vincent Grari, Ciprian Tomoiaga, Sylvain Lamprier, Tatsunori Hashimoto, Marcin Detyniecki
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.]]></description>
<pubDate>Fri, 20 Feb 2026 10:53:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Long-Range Dependencies with Temporal Predictive Coding]]></title>
<link>http://arxiv.org/abs/2602.18131v1</link>
<guid>2602.18131v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Tom Potter, Oliver Rhodes
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Predictive Coding (PC) is a biologically-inspired learning framework characterised by local, parallelisable operations, properties that enable energy-efficient implementation on neuromorphic hardware. Despite this, extending PC effectively to recurrent neural networks (RNNs) has been challenging, particularly for tasks involving long-range temporal dependencies. Backpropagation Through Time (BPTT) remains the dominant method for training RNNs, but its non-local computation, lack of spatial parallelism, and requirement to store extensive activation histories results in significant energy consumption. This work introduces a novel method combining Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL), enabling effective spatio-temporal credit assignment. Results indicate that the proposed method can closely match the performance of BPTT on both synthetic benchmarks and real-world tasks. On a challenging machine translation task, with a 15-million parameter model, the proposed method achieves a test perplexity of 7.62 (vs. 7.49 for BPTT), marking one of the first applications of tPC to tasks of this scale. These findings demonstrate the potential of this method to learn complex temporal dependencies whilst retaining the local, parallelisable, and flexible properties of the original PC framework, paving the way for more energy-efficient learning systems.]]></description>
<pubDate>Fri, 20 Feb 2026 10:46:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[RamanSeg: Interpretability-driven Deep Learning on Raman Spectra for Cancer Diagnosis]]></title>
<link>http://arxiv.org/abs/2602.18119v1</link>
<guid>2602.18119v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.CV, cs.LG
Authors: Chris Tomy, Mo Vali, David Pertzborn, Tammam Alamatouri, Anna Mhlig et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Histopathology, the current gold standard for cancer diagnosis, involves the manual examination of tissue samples after chemical staining, a time-consuming process requiring expert analysis. Raman spectroscopy is an alternative, stain-free method of extracting information from samples. Using nnU-Net, we trained a segmentation model on a novel dataset of spatial Raman spectra aligned with tumour annotations, achieving a mean foreground Dice score of 80.9%, surpassing previous work. Furthermore, we propose a novel, interpretable, prototype-based architecture called RamanSeg. RamanSeg classifies pixels based on discovered regions of the training set, generating a segmentation mask. Two variants of RamanSeg allow a trade-off between interpretability and performance: one with prototype projection and another projection-free version. The projection-free RamanSeg outperformed a U-Net baseline with a mean foreground Dice score of 67.3%, offering a meaningful improvement over a black-box training approach.]]></description>
<pubDate>Fri, 20 Feb 2026 10:18:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.18117v1</link>
<guid>2602.18117v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Yongjae Shin, Jongseong Chae, Jongeui Park, Youngchul Sung
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.]]></description>
<pubDate>Fri, 20 Feb 2026 10:14:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[Cut Less, Fold More: Model Compression through the Lens of Projection Geometry]]></title>
<link>http://arxiv.org/abs/2602.18116v1</link>
<guid>2602.18116v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Olga Saukh, Dong Wang, Haris iki, Yun Cheng, Lothar Thiele
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.]]></description>
<pubDate>Fri, 20 Feb 2026 10:09:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[Non-Stationary Online Resource Allocation: Learning from a Single Sample]]></title>
<link>http://arxiv.org/abs/2602.18114v1</link>
<guid>2602.18114v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.DS
Authors: Yiding Feng, Jiashuo Jiang, Yige Wang
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.
  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\tilde{O}(\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.]]></description>
<pubDate>Fri, 20 Feb 2026 10:07:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs]]></title>
<link>http://arxiv.org/abs/2602.18109v1</link>
<guid>2602.18109v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.OS
Authors: Rong Fu, Yibo Meng, Guangzhen Yao, Jiaxuan Lu, Zeyu Zhang et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.]]></description>
<pubDate>Fri, 20 Feb 2026 09:56:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[MeanVoiceFlow: One-step Nonparallel Voice Conversion with Mean Flows]]></title>
<link>http://arxiv.org/abs/2602.18104v1</link>
<guid>2602.18104v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SD, cs.AI, cs.LG
Authors: Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Yuto Kondo
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In voice conversion (VC) applications, diffusion and flow-matching models have exhibited exceptional speech quality and speaker similarity performances. However, they are limited by slow conversion owing to their iterative inference. Consequently, we propose MeanVoiceFlow, a novel one-step nonparallel VC model based on mean flows, which can be trained from scratch without requiring pretraining or distillation. Unlike conventional flow matching that uses instantaneous velocity, mean flows employ average velocity to more accurately compute the time integral along the inference path in a single step. However, training the average velocity requires its derivative to compute the target velocity, which can cause instability. Therefore, we introduce a structural margin reconstruction loss as a zero-input constraint, which moderately regularizes the input-output behavior of the model without harmful statistical averaging. Furthermore, we propose conditional diffused-input training in which a mixture of noise and source data is used as input to the model during both training and inference. This enables the model to effectively leverage source information while maintaining consistency between training and inference. Experimental results validate the effectiveness of these techniques and demonstrate that MeanVoiceFlow achieves performance comparable to that of previous multi-step and distillation-based models, even when trained from scratch. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/meanvoiceflow/.]]></description>
<pubDate>Fri, 20 Feb 2026 09:48:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning]]></title>
<link>http://arxiv.org/abs/2602.18097v1</link>
<guid>2602.18097v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.LG
Authors: Aarati Andrea Noronha, Jean Oh
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In this paper, we present a framework for enabling autonomous vehicles to interact with cyclists in a manner that balances safety and optimality. The approach integrates Hamilton-Jacobi reachability analysis with deep Q-learning to jointly address safety guarantees and time-efficient navigation. A value function is computed as the solution to a time-dependent Hamilton-Jacobi-Bellman inequality, providing a quantitative measure of safety for each system state. This safety metric is incorporated as a structured reward signal within a reinforcement learning framework. The method further models the cyclist's latent response to the vehicle, allowing disturbance inputs to reflect human comfort and behavioral adaptation. The proposed framework is evaluated through simulation and comparison with human driving behavior and an existing state-of-the-art method.]]></description>
<pubDate>Fri, 20 Feb 2026 09:38:38 +0000</pubDate>
</item>
<item>
<title><![CDATA[Neurosymbolic Language Reasoning as Satisfiability Modulo Theory]]></title>
<link>http://arxiv.org/abs/2602.18095v1</link>
<guid>2602.18095v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Hyunseok Oh, Sam Stern, Youngki Lee, Matthai Philipose
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Natural language understanding requires interleaving textual and logical reasoning, yet large language models often fail to perform such reasoning reliably. Existing neurosymbolic systems combine LLMs with solvers but remain limited to fully formalizable tasks such as math or program synthesis, leaving natural documents with only partial logical structure unaddressed. We introduce Logitext, a neurosymbolic language that represents documents as natural language text constraints (NLTCs), making partial logical structure explicit. We develop an algorithm that integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, enabling joint textual-logical reasoning. Experiments on a new content moderation benchmark, together with LegalBench and Super-Natural Instructions, show that Logitext improves both accuracy and coverage. This work is the first that treats LLM-based reasoning as an SMT theory, extending neurosymbolic methods beyond fully formalizable domains.]]></description>
<pubDate>Fri, 20 Feb 2026 09:35:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models]]></title>
<link>http://arxiv.org/abs/2602.18094v1</link>
<guid>2602.18094v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI, cs.DB
Authors: Ling Lin, Yang Bai, Heng Su, Congcong Zhu, Yaoxing Wang et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.]]></description>
<pubDate>Fri, 20 Feb 2026 09:34:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers]]></title>
<link>http://arxiv.org/abs/2602.18093v1</link>
<guid>2602.18093v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Hanshuai Cui, Zhiqing Tang, Qianli Ma, Zhi Yao, Weijia Jia
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.]]></description>
<pubDate>Fri, 20 Feb 2026 09:33:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[Perceived Political Bias in LLMs Reduces Persuasive Abilities]]></title>
<link>http://arxiv.org/abs/2602.18092v1</link>
<guid>2602.18092v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI, cs.CY
Authors: Matthew DiGiuseppe, Joshua Robison
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.]]></description>
<pubDate>Fri, 20 Feb 2026 09:33:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text]]></title>
<link>http://arxiv.org/abs/2602.18089v1</link>
<guid>2602.18089v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI, cs.LG
Authors: Kunwar Arpit Singh, Ankush Prakash, Haroon R Lone
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.]]></description>
<pubDate>Fri, 20 Feb 2026 09:25:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Balancing Symmetry and Efficiency in Graph Flow Matching]]></title>
<link>http://arxiv.org/abs/2602.18084v1</link>
<guid>2602.18084v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Benjamin Honor, Alba Carballo-Castro, Yiming Qin, Pascal Frossard
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\%$ of the baseline training epochs.]]></description>
<pubDate>Fri, 20 Feb 2026 09:17:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation]]></title>
<link>http://arxiv.org/abs/2602.18083v1</link>
<guid>2602.18083v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Ioannis Kontogiorgakis, Athanasios Askitopoulos, Iason Tsardanidis, Dimitrios Bormpoudakis, Ilias Tsoumas et al.
Institution: MIT
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.]]></description>
<pubDate>Fri, 20 Feb 2026 09:17:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[HiAER-Spike Software-Hardware Reconfigurable Platform for Event-Driven Neuromorphic Computing at Scale]]></title>
<link>http://arxiv.org/abs/2602.18072v1</link>
<guid>2602.18072v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AR, cs.AI
Authors: Gwenevere Frank, Gopabandhu Hota, Keli Wang, Christopher Deng, Krish Arora et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In this work, we present HiAER-Spike, a modular, reconfigurable, event-driven neuromorphic computing platform designed to execute large spiking neural networks with up to 160 million neurons and 40 billion synapses - roughly twice the neurons of a mouse brain at faster than real time. This system, assembled at the UC San Diego Supercomputer Center, comprises a co-designed hard- and software stack that is optimized for run-time massively parallel processing and hierarchical address-event routing (HiAER) of spikes while promoting memory-efficient network storage and execution. The architecture efficiently handles both sparse connectivity and sparse activity for robust and low-latency event-driven inference for both edge and cloud computing. A Python programming interface to HiAER-Spike, agnostic to hardware-level detail, shields the user from complexity in the configuration and execution of general spiking neural networks with minimal constraints in topology. The system is made easily available over a web portal for use by the wider community. In the following, we provide an overview of the hard- and software stack, explain the underlying design principles, demonstrate some of the system's capabilities and solicit feedback from the broader neuromorphic community. Examples are shown demonstrating HiAER-Spike's capabilities for event-driven vision on benchmark CIFAR-10, DVS event-based gesture, MNIST, and Pong tasks.]]></description>
<pubDate>Fri, 20 Feb 2026 08:55:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation]]></title>
<link>http://arxiv.org/abs/2602.18066v1</link>
<guid>2602.18066v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Daniel Busch, Christian Bohn, Thomas Kurbiel, Klaus Friedrichs, Richard Meyes et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.]]></description>
<pubDate>Fri, 20 Feb 2026 08:37:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis]]></title>
<link>http://arxiv.org/abs/2602.18064v1</link>
<guid>2602.18064v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Ziyue Wang, Linghan Cai, Chang Han Low, Haofeng Liu, Junde Wu et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.]]></description>
<pubDate>Fri, 20 Feb 2026 08:31:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Deepmechanics]]></title>
<link>http://arxiv.org/abs/2602.18060v1</link>
<guid>2602.18060v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Abhay Shinde, Aryan Amit Barsainyan, Jose Siguenza, Ankita Vaishnobi Bisoi, Rakshit Kr. Singh et al.
Institution: MIT
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Physics-informed deep learning models have emerged as powerful tools for learning dynamical systems. These models directly encode physical principles into network architectures. However, systematic benchmarking of these approaches across diverse physical phenomena remains limited, particularly in conservative and dissipative systems. In addition, benchmarking that has been done thus far does not integrate out full trajectories to check stability. In this work, we benchmark three prominent physics-informed architectures such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Symplectic Recurrent Neural Networks (SRNN) using the DeepChem framework, an open-source scientific machine learning library. We evaluate these models on six dynamical systems spanning classical conservative mechanics (mass-spring system, simple pendulum, double pendulum, and three-body problem, spring-pendulum) and non-conservative systems with contact (bouncing ball). We evaluate models by computing error on predicted trajectories and evaluate error both quantitatively and qualitatively. We find that all benchmarked models struggle to maintain stability for chaotic or nonconservative systems. Our results suggest that more research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems.]]></description>
<pubDate>Fri, 20 Feb 2026 08:27:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Temporal Consistency-Aware Text-to-Motion Generation]]></title>
<link>http://arxiv.org/abs/2602.18057v1</link>
<guid>2602.18057v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Hongsong Wang, Wenjing Yan, Qiuxia Lai, Xin Geng
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.]]></description>
<pubDate>Fri, 20 Feb 2026 08:17:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework]]></title>
<link>http://arxiv.org/abs/2602.18055v1</link>
<guid>2602.18055v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jingyang Qiao, Zhizhong Zhang, Xin Tan, Jingyu Gong, Yanyun Qu et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.]]></description>
<pubDate>Fri, 20 Feb 2026 08:15:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Generalization and Robustness in Conditional Value-at-Risk]]></title>
<link>http://arxiv.org/abs/2602.18053v1</link>
<guid>2602.18053v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Dinesh Karthik Mulumudi, Piyushi Manupriya, Gholamali Aminian, Anant Raj
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Conditional Value-at-Risk (CVaR) is a widely used risk-sensitive objective for learning under rare but high-impact losses, yet its statistical behavior under heavy-tailed data remains poorly understood. Unlike expectation-based risk, CVaR depends on an endogenous, data-dependent quantile, which couples tail averaging with threshold estimation and fundamentally alters both generalization and robustness properties. In this work, we develop a learning-theoretic analysis of CVaR-based empirical risk minimization under heavy-tailed and contaminated data. We establish sharp, high-probability generalization and excess risk bounds under minimal moment assumptions, covering fixed hypotheses, finite and infinite classes, and extending to $$-mixing dependent data; we further show that these rates are minimax optimal. To capture the intrinsic quantile sensitivity of CVaR, we derive a uniform Bahadur-Kiefer type expansion that isolates a threshold-driven error term absent in mean-risk ERM and essential in heavy-tailed regimes. We complement these results with robustness guarantees by proposing a truncated median-of-means CVaR estimator that achieves optimal rates under adversarial contamination. Finally, we show that CVaR decisions themselves can be intrinsically unstable under heavy tails, establishing a fundamental limitation on decision robustness even when the population optimum is well separated. Together, our results provide a principled characterization of when CVaR learning generalizes and is robust, and when instability is unavoidable due to tail scarcity.]]></description>
<pubDate>Fri, 20 Feb 2026 08:10:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras]]></title>
<link>http://arxiv.org/abs/2602.18047v1</link>
<guid>2602.18047v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Rong Fu, Wenxin Zhang, Yibo Meng, Jia Yee Tan, Jiaxuan Lu et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.]]></description>
<pubDate>Fri, 20 Feb 2026 08:00:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[Conformal Tradeoffs: Guarantees Beyond Coverage]]></title>
<link>http://arxiv.org/abs/2602.18045v1</link>
<guid>2602.18045v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ME, cs.AI
Authors: Petrus H. Zwart
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Deployed conformal predictors are long-lived decision infrastructure operating over finite operational windows. The real-world question is not only ``Does the true label lie in the prediction set at the target rate?'' (marginal coverage), but ``How often does the system commit versus defer? What error exposure does it induce when it acts? How do these rates trade off?'' Marginal coverage does not determine these deployment-facing quantities: the same calibrated thresholds can yield different operational profiles depending on score geometry. We provide a framework for operational certification and planning beyond coverage with three contributions. (1) Small-Sample Beta Correction (SSBC): we invert the exact finite-sample Beta/rank law for split conformal to map a user request $(^\star,)$ to a calibrated grid point with PAC-style semantics, yielding explicit finite-window coverage guarantees. (2) Calibrate-and-Audit: since no distribution-free pivot exists for rates beyond coverage, we introduce a two-stage design in which an independent audit set produces a reusable region -- label table and certified finite-window envelopes (Binomial/Beta-Binomial) for operational quantities -- commitment frequency, deferral, decisive error exposure, and commit purity -- via linear projection. (3) Geometric characterization: we describe feasibility constraints, regime boundaries (hedging vs.\ rejection), and cost-coherence conditions induced by a fixed conformal partition, explaining why operational rates are coupled and how calibration navigates their trade-offs. The output is an auditable operational menu: for a fixed scoring model, we trace attainable operational profiles across calibration settings and attach finite-window uncertainty envelopes. We demonstrate the approach on Tox21 toxicity prediction (12 endpoints) and aqueous solubility screening using AquaSolDB.]]></description>
<pubDate>Fri, 20 Feb 2026 07:58:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition]]></title>
<link>http://arxiv.org/abs/2602.18043v1</link>
<guid>2602.18043v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Hongyu Qu, Xiangbo Shu, Rui Yan, Hailiang Gao, Wenguan Wang et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.]]></description>
<pubDate>Fri, 20 Feb 2026 07:52:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[PINEAPPLE: Physics-Informed Neuro-Evolution Algorithm for Prognostic Parameter Inference in Lithium-Ion Battery Electrodes]]></title>
<link>http://arxiv.org/abs/2602.18042v1</link>
<guid>2602.18042v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CE, cs.NE
Authors: Karkulali Pugalenthi, Jian Cheng Wong, Qizheng Yang, Pao-Hsiung Chiu, My Ha Dao et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurate, real-time, yet non-destructive estimation of internal states in lithium-ion batteries is critical for predicting degradation, optimizing usage strategies, and extending operational lifespan. Here, we introduce PINEAPPLE (Physics-Informed Neuro-Evolution Algorithm for Prognostic Parameter inference in Lithium-ion battery Electrodes), a novel framework that integrates physics-informed neural networks (PINNs) with an evolutionary search algorithm to enable rapid, scalable, and interpretable parameter inference with potential for application to next-generation batteries. The meta-learned PINN utilizes fundamental physics principles to achieve accurate zero-shot prediction of electrode behavior with test errors below 0.1$\%$ while maintaining an order-of-magnitude speed-up over conventional solvers. PINEAPPLE demonstrates robust parameter inference solely from voltage-time discharge curves across multiple batteries from the open-source CALCE repository, recovering the evolution of key internal state parameters such as Li-ion diffusion coefficients across usage cycles. Notably, the inferred cycle-dependent evolution of these parameters exhibit consistent trends across different batteries without any customized degradation physics-embedded heuristic, highlighting the effective regularizing effect and robustness that can be conferred through incorporation of fundamental physics in PINEAPPLE. By enabling computationally efficient, real-time parameter estimation, PINEAPPLE offers a promising route towards the non-destructive, physics-based characterization of inter-cell and intra-cell variability of battery modules and battery packs, thereby unlocking new opportunities for downstream on-the-fly needs in next-generation battery management systems such as individual cell-scale state-of-health diagnostics.]]></description>
<pubDate>Fri, 20 Feb 2026 07:51:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards]]></title>
<link>http://arxiv.org/abs/2602.18037v1</link>
<guid>2602.18037v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CL
Authors: Johannes Ackermann, Michael Noukhovitch, Takashi Ishida, Masashi Sugiyama
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.]]></description>
<pubDate>Fri, 20 Feb 2026 07:32:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[Towards More Standardized AI Evaluation: From Models to Agents]]></title>
<link>http://arxiv.org/abs/2602.18029v1</link>
<guid>2602.18029v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Ali El Filali, Ins Bedar
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer "How good is the model?" but "Can we trust the system to behave as intended, under change, at scale?". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.]]></description>
<pubDate>Fri, 20 Feb 2026 06:54:44 +0000</pubDate>
</item>
<item>
<title><![CDATA[Mean-Field Reinforcement Learning without Synchrony]]></title>
<link>http://arxiv.org/abs/2602.18026v1</link>
<guid>2602.18026v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.MA, cs.AI, cs.LG
Authors: Shan Yang
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Mean-field reinforcement learning (MF-RL) scales multi-agent RL to large populations by reducing each agent's dependence on others to a single summary statistic -- the mean action. However, this reduction requires every agent to act at every time step; when some agents are idle, the mean action is simply undefined. Addressing asynchrony therefore requires a different summary statistic -- one that remains defined regardless of which agents act. The population distribution $\in (\mathcal{O})$ -- the fraction of agents at each observation -- satisfies this requirement: its dimension is independent of $N$, and under exchangeability it fully determines each agent's reward and transition. Existing MF-RL theory, however, is built on the mean action and does not extend to $$. We therefore construct the Temporal Mean Field (TMF) framework around the population distribution $$ from scratch, covering the full spectrum from fully synchronous to purely sequential decision-making within a single theory. We prove existence and uniqueness of TMF equilibria, establish an $O(1/\sqrt{N})$ finite-population approximation bound that holds regardless of how many agents act per step, and prove convergence of a policy gradient algorithm (TMF-PG) to the unique equilibrium. Experiments on a resource selection game and a dynamic queueing game confirm that TMF-PG achieves near-identical performance whether one agent or all $N$ act per step, with approximation error decaying at the predicted $O(1/\sqrt{N})$ rate.]]></description>
<pubDate>Fri, 20 Feb 2026 06:42:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets]]></title>
<link>http://arxiv.org/abs/2602.18025v1</link>
<guid>2602.18025v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.RO
Authors: Haruki Abe, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.]]></description>
<pubDate>Fri, 20 Feb 2026 06:39:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers]]></title>
<link>http://arxiv.org/abs/2602.18022v1</link>
<guid>2602.18022v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Guandong Li, Mengxia Ye
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(_k, _v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).]]></description>
<pubDate>Fri, 20 Feb 2026 06:24:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models]]></title>
<link>http://arxiv.org/abs/2602.18020v1</link>
<guid>2602.18020v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.RO
Authors: Jiabing Yang, Yixiang Chen, Yuan Xu, Peiyan Li, Xiangnan Wu et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.]]></description>
<pubDate>Fri, 20 Feb 2026 06:22:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE]]></title>
<link>http://arxiv.org/abs/2602.18019v1</link>
<guid>2602.18019v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Yujie Jin, Wenxin Zhang, Jingjing Wang, Guodong Zhou
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.]]></description>
<pubDate>Fri, 20 Feb 2026 06:18:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating]]></title>
<link>http://arxiv.org/abs/2602.18016v1</link>
<guid>2602.18016v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Jiamin Luo, Xuqian Gu, Jingjing Wang, Jiahong Lu
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.]]></description>
<pubDate>Fri, 20 Feb 2026 06:12:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Flow Actor-Critic for Offline Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.18015v1</link>
<guid>2602.18015v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Jongseong Chae, Jongeui Park, Yongjae Shin, Gyeongmin Kim, Seungyul Han et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.]]></description>
<pubDate>Fri, 20 Feb 2026 06:11:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[Quasi-Periodic Gaussian Process Predictive Iterative Learning Control]]></title>
<link>http://arxiv.org/abs/2602.18014v1</link>
<guid>2602.18014v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, stat.ML
Authors: Unnati Nigam, Radhendushka Srivastava, Faezeh Marzbanrad, Michael Burke
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\mathcal{O}(p^3)$ instead of $\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.]]></description>
<pubDate>Fri, 20 Feb 2026 06:10:10 +0000</pubDate>
</item>
<item>
<title><![CDATA[NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs]]></title>
<link>http://arxiv.org/abs/2602.18008v1</link>
<guid>2602.18008v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CL
Authors: Zihan Guan, Rituparna Datta, Mengxuan Hu, Shunshun Liu, Aiying Zhang et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.]]></description>
<pubDate>Fri, 20 Feb 2026 05:46:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method]]></title>
<link>http://arxiv.org/abs/2602.18006v1</link>
<guid>2602.18006v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Ahsan Baidar Bakht, Mohamad Alansari, Muhayy Ud Din, Muzammal Naseer, Sajid Javed et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.]]></description>
<pubDate>Fri, 20 Feb 2026 05:43:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Preconditioned Robust Neural Posterior Estimation for Misspecified Simulators]]></title>
<link>http://arxiv.org/abs/2602.18004v1</link>
<guid>2602.18004v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ME, stat.CO, stat.ML
Authors: Ryan P. Kelly, David T. Frazier, David J. Warne, Christopher C. Drovandi
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Simulation-based inference (SBI) enables parameter estimation for complex stochastic models with intractable likelihoods when model simulation is feasible. Neural posterior estimation (NPE) is a popular SBI approach that often achieves accurate inference with far fewer simulations than classical approaches. But in practice, neural approaches can be unreliable for two reasons: incompatible data summaries arising from model misspecification yield unreliable posteriors due to extrapolation, and prior-predictive draws can produce extreme summaries that lead to difficulties in obtaining an accurate posterior for the observed data of interest. Existing preconditioning schemes target well-specified settings, and their behaviour under misspecification remains unexplored. We study preconditioning under misspecification and propose preconditioned robust neural posterior estimation, which computes data-dependent weights that focus training near the observed summaries and fits a robust neural posterior approximation. We also introduce a forest-proximity preconditioning approach that uses tree-based proximity scores to down-weight outlying simulations and concentrate computation around the observed dataset. Across two synthetic examples and one real example with incompatible summaries and extreme prior-predictive behaviour, we demonstrate that preconditioning combined with robust NPE increases stability and improves accuracy, calibration, and posterior-predictive fit over standard baseline methods.]]></description>
<pubDate>Fri, 20 Feb 2026 05:32:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[Asynchronous Heavy-Tailed Optimization]]></title>
<link>http://arxiv.org/abs/2602.18002v1</link>
<guid>2602.18002v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Junfei Sun, Dixi Yao, Xuchen Gong, Tahseen Rabbani, Manzil Zaheer et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.]]></description>
<pubDate>Fri, 20 Feb 2026 05:28:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching]]></title>
<link>http://arxiv.org/abs/2602.18000v1</link>
<guid>2602.18000v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Xuting Lan, Mingliang Zhou, Xuekai Wei, Jielu Yan, Yueting Huang et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.]]></description>
<pubDate>Fri, 20 Feb 2026 05:27:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Aurora: Neuro-Symbolic AI Driven Advising Agent]]></title>
<link>http://arxiv.org/abs/2602.17999v1</link>
<guid>2602.17999v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI
Authors: Lorena Amanda Quincoso Lugones, Christopher Kverne, Nityam Sharadkumar Bhimani, Ana Carolina Oliveira, Agoritsa Polyzou et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Academic advising in higher education is under severe strain, with advisor-to-student ratios commonly exceeding 300:1. These structural bottlenecks limit timely access to guidance, increase the risk of delayed graduation, and contribute to inequities in student support. We introduce Aurora, a modular neuro-symbolic advising agent that unifies retrieval-augmented generation (RAG), symbolic reasoning, and normalized curricular databases to deliver policy-compliant, verifiable recommendations at scale. Aurora integrates three components: (i) a Boyce-Codd Normal Form (BCNF) catalog schema for consistent program rules, (ii) a Prolog engine for prerequisite and credit enforcement, and (iii) an instruction-tuned large language model for natural-language explanations of its recommendations. To assess performance, we design a structured evaluation suite spanning common and edge-case advising scenarios, including short-term scheduling, long-term roadmapping, skill-aligned pathways, and out-of-scope requests. Across this diverse set, Aurora improves semantic alignment with expert-crafted answers from 0.68 (Raw LLM baseline) to 0.93 (+36%), achieves perfect precision and recall in nearly half of in-scope cases, and consistently produces correct fallbacks for unanswerable prompts. On commodity hardware, Aurora delivers sub-second mean latency (0.71s across 20 queries), approximately 83X faster than a Raw LLM baseline (59.2s). By combining symbolic rigor with neural fluency, Aurora advances a paradigm for accurate, explainable, and scalable AI-driven advising.]]></description>
<pubDate>Fri, 20 Feb 2026 05:26:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting]]></title>
<link>http://arxiv.org/abs/2602.17998v1</link>
<guid>2602.17998v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, cs.CE
Authors: Shubham Bhardwaj, Chandrajit Bajaj
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\dot{x}=(J-R)\nabla H(x)$, guaranteeing $dH/dt\le 0$ when $R\succeq 0$. We introduce \textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.]]></description>
<pubDate>Fri, 20 Feb 2026 05:23:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly]]></title>
<link>http://arxiv.org/abs/2602.17997v1</link>
<guid>2602.17997v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.RO
Authors: Zehao Jin, Yaoye Zhu, Chen Zhang, Yanan Sui
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.]]></description>
<pubDate>Fri, 20 Feb 2026 05:09:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers]]></title>
<link>http://arxiv.org/abs/2602.17993v1</link>
<guid>2602.17993v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Mohan Tang, Sidi Lu
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Complex problems, whether in math, logic, or planning, are solved by humans through a sequence of steps where the result of one step informs the next. In this work, we adopt the perspective that the reasoning power of Transformers is fundamentally limited by a fixed maximum number of steps along any latent path of computation. To address this, we introduce Turbo Connection (TurboConn), a novel architecture that overcomes the fixed-depth constraint by routing multiple residual connections from the higher-layer hidden states of each token $t$ to the lower layers of token $t+1$. Fine-tuning pre-trained LLMs with our method not only yields accuracy gains of 0.9% to over 10% on benchmarks like GSM8K, Parity, and multi-step arithmetic, but also demonstrates that the density of these backward connections is critical; our dense interaction significantly outperforms "sparse" alternatives that only pass a single hidden state or vector. Notably, TurboConn can be integrated into pre-trained LLMs to overcome task-specific plateaus: while a fine-tuned Qwen-3-1.7B achieves only 53.78% on Parity, adding our architectural modification enables the model to reach 100% accuracy, all without the necessity to retrain the full model from scratch or sophisticated curriculum learning. Our results provide strong empirical evidence that the depth of the computational path is a key factor in reasoning ability, also offering a new mechanism to enhance LLMs without significantly affecting generation latency.]]></description>
<pubDate>Fri, 20 Feb 2026 05:01:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics]]></title>
<link>http://arxiv.org/abs/2602.17990v1</link>
<guid>2602.17990v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Madhav Kanda, Pedro Las-Casas, Alok Gautam Kumbhare, Rodrigo Fonseca, Sharad Agarwal
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.]]></description>
<pubDate>Fri, 20 Feb 2026 04:54:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[From Global Radiomics to Parametric Maps: A Unified Workflow Fusing Radiomics and Deep Learning for PDAC Detection]]></title>
<link>http://arxiv.org/abs/2602.17986v1</link>
<guid>2602.17986v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Zengtian Deng, Yimeng He, Yu Shi, Lixia Wang, Touseef Ahmad Qureshi et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Radiomics and deep learning both offer powerful tools for quantitative medical imaging, but most existing fusion approaches only leverage global radiomic features and overlook the complementary value of spatially resolved radiomic parametric maps. We propose a unified framework that first selects discriminative radiomic features and then injects them into a radiomics-enhanced nnUNet at both the global and voxel levels for pancreatic ductal adenocarcinoma (PDAC) detection. On the PANORAMA dataset, our method achieved AUC = 0.96 and AP = 0.84 in cross-validation. On an external in-house cohort, it achieved AUC = 0.95 and AP = 0.78, outperforming the baseline nnUNet; it also ranked second in the PANORAMA Grand Challenge. This demonstrates that handcrafted radiomics, when injected at both global and voxel levels, provide complementary signals to deep learning models for PDAC detection. Our code can be found at https://github.com/briandzt/dl-pdac-radiomics-global-n-paramaps]]></description>
<pubDate>Fri, 20 Feb 2026 04:46:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Without Training]]></title>
<link>http://arxiv.org/abs/2602.17985v1</link>
<guid>2602.17985v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Ryan O'Dowd
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.
  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\mathcal{D}=\{(x_j,f(x_j))\}_{j=1}^M$, can one build a model $F\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.
  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.
  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.]]></description>
<pubDate>Fri, 20 Feb 2026 04:42:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering]]></title>
<link>http://arxiv.org/abs/2602.17981v1</link>
<guid>2602.17981v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.IR
Authors: Amine Kobeissi, Philippe Langlais
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.]]></description>
<pubDate>Fri, 20 Feb 2026 04:31:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Optimal and Sample-Efficient Decision Policies with Guarantees]]></title>
<link>http://arxiv.org/abs/2602.17978v1</link>
<guid>2602.17978v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Daqian Shao
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.]]></description>
<pubDate>Fri, 20 Feb 2026 04:24:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[In-Context Learning for Pure Exploration in Continuous Spaces]]></title>
<link>http://arxiv.org/abs/2602.17976v1</link>
<guid>2602.17976v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Alessio Russo, Yin-Ching Lee, Ryan Welch, Aldo Pacchiano
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In active sequential testing, also termed pure exploration, a learner is tasked with the goal to adaptively acquire information so as to identify an unknown ground-truth hypothesis with as few queries as possible. This problem, originally studied by Chernoff in 1959, has several applications: classical formulations include Best-Arm Identification (BAI) in bandits, where actions index hypotheses, and generalized search problems, where strategically chosen queries reveal partial information about a hidden label. In many modern settings, however, the hypothesis space is continuous and naturally coincides with the query/action space: for example, identifying an optimal action in a continuous-armed bandit, localizing an $$-ball contained in a target region, or estimating the minimizer of an unknown function from a sequence of observations. In this work, we study pure exploration in such continuous spaces and introduce Continuous In-Context Pure Exploration for this regime. We introduce C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to (i) the next continuous query action and (ii) a predicted hypothesis, thereby learning transferable sequential testing strategies directly from data. At inference time, C-ICPE-TS actively gathers evidence on previously unseen tasks and infers the true hypothesis without parameter updates or explicit hand-crafted information models. We validate C-ICPE-TS across a range of benchmarks, spanning continuous best-arm identification, region localization, and function minimizer identification.]]></description>
<pubDate>Fri, 20 Feb 2026 04:20:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Generating adversarial inputs for a graph neural network model of AC power flow]]></title>
<link>http://arxiv.org/abs/2602.17975v1</link>
<guid>2602.17975v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Robert Parker
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This work formulates and solves optimization problems to generate input points that yield high errors between a neural network's predicted AC power flow solution and solutions to the AC power flow equations. We demonstrate this capability on an instance of the CANOS-PF graph neural network model, as implemented by the PF$$ benchmark library, operating on a 14-bus test grid. Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. When minimizing the perturbation from a training point necessary to satisfy adversarial constraints, we find that the constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus. This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.]]></description>
<pubDate>Fri, 20 Feb 2026 04:09:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[PenTiDef: Enhancing Privacy and Robustness in Decentralized Federated Intrusion Detection Systems against Poisoning Attacks]]></title>
<link>http://arxiv.org/abs/2602.17973v1</link>
<guid>2602.17973v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CR, cs.AI
Authors: Phan The Duy, Nghi Hoang Khoa, Nguyen Tran Anh Quan, Luong Ha Tien, Ngo Duc Hoang Son et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The increasing deployment of Federated Learning (FL) in Intrusion Detection Systems (IDS) introduces new challenges related to data privacy, centralized coordination, and susceptibility to poisoning attacks. While significant research has focused on protecting traditional FL-IDS with centralized aggregation servers, there remains a notable gap in addressing the unique challenges of decentralized FL-IDS (DFL-IDS). This study aims to address the limitations of traditional centralized FL-IDS by proposing a novel defense framework tailored for the decentralized FL-IDS architecture, with a focus on privacy preservation and robustness against poisoning attacks. We propose PenTiDef, a privacy-preserving and robust defense framework for DFL-IDS, which incorporates Distributed Differential Privacy (DDP) to protect data confidentiality and utilizes latent space representations (LSR) derived from neural networks to detect malicious updates in the decentralized model aggregation context. To eliminate single points of failure and enhance trust without a centralized aggregation server, PenTiDef employs a blockchain-based decentralized coordination mechanism that manages model aggregation, tracks update history, and supports trust enforcement through smart contracts. Experimental results on CIC-IDS2018 and Edge-IIoTSet demonstrate that PenTiDef consistently outperforms existing defenses (e.g., FLARE, FedCC) across various attack scenarios and data distributions. These findings highlight the potential of PenTiDef as a scalable and secure framework for deploying DFL-based IDS in adversarial environments. By leveraging privacy protection, malicious behavior detection in hidden data, and working without a central server, it provides a useful security solution against real-world attacks from untrust participants.]]></description>
<pubDate>Fri, 20 Feb 2026 03:58:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Student Flow Modeling for School Decongestion via Stochastic Gravity Estimation and Constrained Spatial Allocation]]></title>
<link>http://arxiv.org/abs/2602.17972v1</link>
<guid>2602.17972v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Sebastian Felipe R. Bundoc, Paula Joy B. Martinez, Sebastian C. Ibaez, Erika Fille T. Legara
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

School congestion, where student enrollment exceeds school capacity, is a major challenge in low- and middle-income countries. It highly impacts learning outcomes and deepens inequities in education. While subsidy programs that transfer students from public to private schools offer a mechanism to alleviate congestion without capital-intensive construction, they often underperform due to fragmented data systems that hinder effective implementation. The Philippine Educational Service Contracting program, one of the world's largest educational subsidy programs, exemplifies these challenges, falling short of its goal to decongest public schools. This prevents the science-based and data-driven analyses needed to understand what shapes student enrollment flows, particularly how families respond to economic incentives and spatial constraints. We introduce a computational framework for modeling student flow patterns and simulating policy scenarios. By synthesizing heterogeneous government data across nearly 3,000 institutions, we employ a stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These elasticities inform a doubly constrained spatial allocation mechanism that simulates student redistribution under varying subsidy amounts while respecting both origin candidate pools and destination slot capacities. We find that geographic proximity constrains school choice four times more strongly than tuition cost and that slot capacity, not subsidy amounts, is the binding constraint. Our work demonstrates that subsidy programs alone cannot resolve systemic overcrowding, and computational modeling can empower education policymakers to make equitable, data-driven decisions by revealing the structural constraints that shape effective resource allocation, even when resources are limited.]]></description>
<pubDate>Fri, 20 Feb 2026 03:57:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[Minimax optimal adaptive structured transfer learning through semi-parametric domain-varying coefficient model]]></title>
<link>http://arxiv.org/abs/2602.17967v1</link>
<guid>2602.17967v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ME, stat.ML
Authors: Hanxiao Chen, Debarghya Mukherjee
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Transfer learning aims to improve inference in a target domain by leveraging information from related source domains, but its effectiveness critically depends on how cross-domain heterogeneity is modeled and controlled. When the conditional mechanism linking covariates and responses varies across domains, indiscriminate information pooling can lead to negative transfer, degrading performance relative to target-only estimation. We study a multi-source, single-target transfer learning problem under conditional distributional drift and propose a semiparametric domain-varying coefficient model (DVCM), in which domain-relatedness is encoded through an observable domain identifier. This framework generalizes classical varying-coefficient models to structured transfer learning and interpolates between invariant and fully heterogeneous regimes. Building on this model, we develop an adaptive transfer learning estimator that selectively borrows strength from informative source domains while provably safeguarding against negative transfer. Our estimator is computationally efficient and easy to implement; we also show that it is minimax rate-optimal and derive its asymptotic distribution, enabling valid uncertainty quantification and hypothesis testing despite data-adaptive pooling and shrinkage. Our results precisely characterize the interplay among domain heterogeneity, the smoothness of the underlying mean function, and the number of source domains and are corroborated by comprehensive numerical experiments and two real-data applications.]]></description>
<pubDate>Fri, 20 Feb 2026 03:53:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts]]></title>
<link>http://arxiv.org/abs/2602.17962v1</link>
<guid>2602.17962v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Shuo Sun, Meiling Zhou, Chen Zhao, Joyce H. Keyak, Nancy E. Lane et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.]]></description>
<pubDate>Fri, 20 Feb 2026 03:37:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Anisotropic local law for non-separable sample covariance matrices]]></title>
<link>http://arxiv.org/abs/2602.17960v1</link>
<guid>2602.17960v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.AP, stat.ML
Authors: Zhou Fan, Renyuan Ma, Elliot Paquette, Zhichao Wang
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We establish local laws for sample covariance matrices $K = N^{-1}\sum_{i=1}^N \g_i\g_i^*$ where the random vectors $\g_1, \ldots, \g_N \in \R^n$ are independent with common covariance $$. Previous work has largely focused on the separable model $\g = ^{1/2}\w$ with $\w$ having independent entries, but this structure is rarely present in statistical applications involving dependent or nonlinearly transformed data. Under a concentration assumption for quadratic forms $\g^*A\g$, we prove an optimal averaged local law showing that the Stieltjes transform of $K$ converges to its deterministic limit uniformly down to the optimal scale $\geq N^{-1+\eps}$. Under an additional structural assumption on the cumulant tensors of $\g$ -- which interpolates between the highly structured case of independent entries and generic dependence -- we establish the full anisotropic local law, providing entrywise control of the resolvent $(K-zI)^{-1}$ in arbitrary directions. We discuss several classes of non-separable examples satisfying our assumptions, including conditionally mean-zero distributions, the random features model $\g = (X\w)$ arising in machine learning, and Gaussian measures with nonlinear tilting. The proofs introduce a tensor network framework for analyzing fluctuation averaging in the presence of higher-order cumulant structure.]]></description>
<pubDate>Fri, 20 Feb 2026 03:28:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[Bayesian Online Model Selection]]></title>
<link>http://arxiv.org/abs/2602.17958v1</link>
<guid>2602.17958v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Aida Afshar, Yuke Zhang, Aldo Pacchiano
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\left( d^* M \sqrt{T} + \sqrt{(MT)} \right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.]]></description>
<pubDate>Fri, 20 Feb 2026 03:23:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Hardware-Friendly Input Expansion for Accelerating Function Approximation]]></title>
<link>http://arxiv.org/abs/2602.17952v1</link>
<guid>2602.17952v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Hu Lou, Yin-Jun Gao, Dong-Xiao Zhang, Tai-Jiao Du, Jun-Jie Zhang et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

One-dimensional function approximation is a fundamental problem in scientific computing and engineering applications. While neural networks possess powerful universal approximation capabilities, their optimization process is often hindered by flat loss landscapes induced by parameter-space symmetries, leading to slow convergence and poor generalization, particularly for high-frequency components. Inspired by the principle of \emph{symmetry breaking} in physics, this paper proposes a hardware-friendly approach for function approximation through \emph{input-space expansion}. The core idea involves augmenting the original one-dimensional input (e.g., $x$) with constant values (e.g., $$) to form a higher-dimensional vector (e.g., $[, , x, , ]$), effectively breaking parameter symmetries without increasing the network's parameter count. We evaluate the method on ten representative one-dimensional functions, including smooth, discontinuous, high-frequency, and non-differentiable functions. Experimental results demonstrate that input-space expansion significantly accelerates training convergence (reducing LBFGS iterations by 12\% on average) and enhances approximation accuracy (reducing final MSE by 66.3\% for the optimal 5D expansion). Ablation studies further reveal the effects of different expansion dimensions and constant selections, with $$ consistently outperforming other constants. Our work proposes a low-cost, efficient, and hardware-friendly technique for algorithm design.]]></description>
<pubDate>Fri, 20 Feb 2026 03:07:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models]]></title>
<link>http://arxiv.org/abs/2602.17951v1</link>
<guid>2602.17951v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Guoheng Sun, Tingting Du, Kaixi Feng, Chenxiang Luo, Xingguo Ding et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, nave multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.]]></description>
<pubDate>Fri, 20 Feb 2026 03:06:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications]]></title>
<link>http://arxiv.org/abs/2602.17949v1</link>
<guid>2602.17949v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Victoria Blake, Mathew Miller, Jamie Novak, Sze-yuan Ooi, Blanca Gallego
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.]]></description>
<pubDate>Fri, 20 Feb 2026 03:00:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion]]></title>
<link>http://arxiv.org/abs/2602.17948v1</link>
<guid>2602.17948v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Yu Bai, Zhe Wang, Jiarui Zhang, Dong-Xiao Zhang, Yinjun Gao et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\%$ to $95.63\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.]]></description>
<pubDate>Fri, 20 Feb 2026 02:58:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition]]></title>
<link>http://arxiv.org/abs/2602.17947v1</link>
<guid>2602.17947v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Yubo Zhou, Jun Shu, Junmin Liu, Deyu Meng
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.]]></description>
<pubDate>Fri, 20 Feb 2026 02:52:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[Optimizing Graph Causal Classification Models: Estimating Causal Effects and Addressing Confounders]]></title>
<link>http://arxiv.org/abs/2602.17941v1</link>
<guid>2602.17941v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Simi Job, Xiaohui Tao, Taotao Cai, Haoran Xie, Jianming Yong et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Graph data is becoming increasingly prevalent due to the growing demand for relational insights in AI across various domains. Organizations regularly use graph data to solve complex problems involving relationships and connections. Causal learning is especially important in this context, since it helps to understand cause-effect relationships rather than mere associations. Since many real-world systems are inherently causal, graphs can efficiently model these systems. However, traditional graph machine learning methods including graph neural networks (GNNs), rely on correlations and are sensitive to spurious patterns and distribution changes. On the other hand, causal models enable robust predictions by isolating true causal factors, thus making them more stable under such shifts. Causal learning also helps in identifying and adjusting for confounders, ensuring that predictions reflect true causal relationships and remain accurate even under interventions. To address these challenges and build models that are robust and causally informed, we propose CCAGNN, a Confounder-Aware causal GNN framework that incorporates causal reasoning into graph learning, supporting counterfactual reasoning and providing reliable predictions in real-world settings. Comprehensive experiments on six publicly available datasets from diverse domains show that CCAGNN consistently outperforms leading state-of-the-art models.]]></description>
<pubDate>Fri, 20 Feb 2026 02:19:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere]]></title>
<link>http://arxiv.org/abs/2602.17940v1</link>
<guid>2602.17940v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Shogo Iwazaki
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $(\sqrt{T (\ln T)^{d} (\ln \ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $(^{-2}(\ln \frac{1})^d (\ln \ln \frac{1})^{-d})$ time steps to find an $$-optimal point. We also provide the improved $O((\ln T)^{d+1}(\ln \ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \emph{dimension-independent} logarithmic factors under a hyperspherical input domain.]]></description>
<pubDate>Fri, 20 Feb 2026 02:17:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Analyzing LLM Instruction Optimization for Tabular Fact Verification]]></title>
<link>http://arxiv.org/abs/2602.17937v1</link>
<guid>2602.17937v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.PL
Authors: Xiaotang Du, Giwon Hong, Wai-Chung Kwan, Rohit Saxena, Ivan Titov et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.]]></description>
<pubDate>Fri, 20 Feb 2026 01:56:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Causal Neighbourhood Learning for Invariant Graph Representations]]></title>
<link>http://arxiv.org/abs/2602.17934v1</link>
<guid>2602.17934v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Simi Job, Xiaohui Tao, Taotao Cai, Haoran Xie, Jianming Yong
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Graph data often contain noisy and spurious correlations that mask the true causal relationships, which are essential for enabling graph models to make predictions based on the underlying causal structure of the data. Dependence on spurious connections makes it challenging for traditional Graph Neural Networks (GNNs) to generalize effectively across different graphs. Furthermore, traditional aggregation methods tend to amplify these spurious patterns, limiting model robustness under distribution shifts. To address these issues, we propose Causal Neighbourhood Learning with Graph Neural Networks (CNL-GNN), a novel framework that performs causal interventions on graph structure. CNL-GNN effectively identifies and preserves causally relevant connections and reduces spurious influences through the generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and an attention-based mechanism. In addition, by combining structural-level interventions with the disentanglement of causal features from confounding factors, the model learns invariant node representations that are robust and generalize well across different graph structures. Our approach improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model. Extensive experiments on four publicly available datasets, including multiple domain variants of one dataset, demonstrate that CNL-GNN outperforms state-of-the-art GNN models.]]></description>
<pubDate>Fri, 20 Feb 2026 01:52:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.17931v1</link>
<guid>2602.17931v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Narjes Nourzad, Carlee Joe-Wong
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.]]></description>
<pubDate>Fri, 20 Feb 2026 01:44:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance]]></title>
<link>http://arxiv.org/abs/2602.17930v1</link>
<guid>2602.17930v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Narjes Nourzad, Carlee Joe-Wong
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/]]></description>
<pubDate>Fri, 20 Feb 2026 01:43:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging]]></title>
<link>http://arxiv.org/abs/2602.17929v1</link>
<guid>2602.17929v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Athanasios Angelakis
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.]]></description>
<pubDate>Fri, 20 Feb 2026 01:38:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[Data-driven configuration tuning of glmnet to balance accuracy and computation time]]></title>
<link>http://arxiv.org/abs/2602.17922v1</link>
<guid>2602.17922v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.CO, stat.ME, stat.ML
Authors: Shuhei Muroya, Kei Hirose
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

glmnet is a widely adopted R package for lasso estimation due to its computational efficiency. Despite its popularity, glmnet sometimes yields solutions that are substantially different from the true ones because of the inappropriate default configuration of the algorithm. The accuracy of the obtained solutions can be improved by appropriately tuning the configuration. However, improving accuracy typically increases computational time, resulting in a trade-off between accuracy and computational efficiency. Therefore, it is essential to establish a systematic approach to determine appropriate configuration. To address this need, we propose a unified data-driven framework specifically designed to optimize the configuration by balancing the trade-off between accuracy and computational efficiency. We generate large-scale simulated datasets and apply glmnet under various configurations to obtain accuracy and computation time. Based on these results, we construct neural networks that predict accuracy and computation time from data characteristics and configuration. Given a new dataset, our framework uses the neural networks to explore the configuration space and derive a Pareto front that represents the trade-off between accuracy and computational cost. This front allows us to automatically identify the configuration that maximize accuracy under a user-specified time constraint. The proposed method is implemented in the R package 'glmnetconf', available at https://github.com/Shuhei-Muroya/glmnetconf.]]></description>
<pubDate>Fri, 20 Feb 2026 00:58:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation]]></title>
<link>http://arxiv.org/abs/2602.17921v1</link>
<guid>2602.17921v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.LG
Authors: Kei Ikemura, Yifei Dong, Florian T. Pokorny
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.]]></description>
<pubDate>Fri, 20 Feb 2026 00:33:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[Distribution-Free Sequential Prediction with Abstentions]]></title>
<link>http://arxiv.org/abs/2602.17918v1</link>
<guid>2602.17918v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jialin Yu, Mose Blanchard
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\ instances, but at each round, the learner may also \emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $$ is \emph{unknown} and propose an algorithm \textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.]]></description>
<pubDate>Fri, 20 Feb 2026 00:28:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Interactions that reshape the interfaces of the interacting parties]]></title>
<link>http://arxiv.org/abs/2602.17917v1</link>
<guid>2602.17917v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: David I. Spivak
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Polynomial functors model systems with interfaces: each polynomial specifies the outputs a system can produce and, for each output, the inputs it accepts. The bicategory $\mathbb{O}\mathbf{rg}$ of dynamic organizations \cite{spivak2021learners} gives a notion of state-driven interaction patterns that evolves over time, but each system's interface remains fixed throughout the interaction. Yet in many systems, the outputs sent and inputs received can reshape the interface itself: a cell differentiating in response to chemical signals gains or loses receptors; a sensor damaged by its input loses a channel; a neural network may grow its output resolution during training.
  Here we introduce *polynomial trees*, elements of the terminal $(u\triangleleft u)$-coalgebra where $u$ is the polynomial associated to a universe of sets, to model such systems: a polynomial tree is a coinductive tree whose nodes carry polynomials, and in which each round of interaction -- an output chosen and an input received -- determines a child tree, hence the next interface. We construct a monoidal closed category $\mathbf{PolyTr}$ of polynomial trees, with coinductively-defined morphisms, tensor product, and internal hom. We then build a bicategory $\mathbb{O}\mathbf{rgTr}$ generalizing $\mathbb{O}\mathbf{rg}$, whose hom-categories parametrize morphisms by state sets with coinductive action-and-update data. We provide a locally fully faithful functor $\mathbb{O}\mathbf{rg}\to\mathbb{O}\mathbf{rgTr}$ via constant trees, those for which the interfaces do not change through time. We illustrate the generalization by suggesting a notion of progressive generative adversarial networks, where gradient feedback determines when the image-generation interface grows to a higher resolution.]]></description>
<pubDate>Fri, 20 Feb 2026 00:25:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[From Lossy to Verified: A Provenance-Aware Tiered Memory for Agents]]></title>
<link>http://arxiv.org/abs/2602.17913v1</link>
<guid>2602.17913v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.DB, cs.AI
Authors: Qiming Zhu, Shunian Chen, Rui Yu, Zhehao Wu, Benyou Wang
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Long-horizon agents often compress interaction histories into write-time summaries. This creates a fundamental write-before-query barrier: compression decisions are made before the system knows what a future query will hinge on. As a result, summaries can cause unverifiable omissions -- decisive constraints (e.g., allergies) may be dropped, leaving the agent unable to justify an answer with traceable evidence. Retaining raw logs restores an authoritative source of truth, but grounding on raw logs by default is expensive: many queries are answerable from summaries, yet raw grounding still requires processing far longer contexts, inflating token consumption and latency.
  We propose TierMem, a provenance-linked framework that casts retrieval as an inference-time evidence allocation problem. TierMem uses a two-tier memory hierarchy to answer with the cheapest sufficient evidence: it queries a fast summary index by default, and a runtime sufficiency router Escalates to an immutable raw-log store only when summary evidence is insufficient. TierMem then writes back verified findings as new summary units linked to their raw sources. On LoCoMo, TierMem achieves 0.851 accuracy (vs.0.873 raw-only) while reducing input tokens by 54.1\% and latency by 60.7%.]]></description>
<pubDate>Fri, 20 Feb 2026 00:21:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering]]></title>
<link>http://arxiv.org/abs/2602.17911v1</link>
<guid>2602.17911v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Jash Rajesh Parekh, Wonbin Kweon, Joey Chan, Rezarta Islamaj, Robert Leaman et al.
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.]]></description>
<pubDate>Fri, 20 Feb 2026 00:17:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems]]></title>
<link>http://arxiv.org/abs/2602.17910v1</link>
<guid>2602.17910v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Hanjing Shi, Dominic DiFranzo
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.]]></description>
<pubDate>Fri, 20 Feb 2026 00:16:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Single Image and Multimodality Is All You Need for Novel View Synthesis]]></title>
<link>http://arxiv.org/abs/2602.17909v1</link>
<guid>2602.17909v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Amirhosein Javadi, Chi-Shiang Gau, Konstantinos D. Polyzos, Tara Javidi
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.]]></description>
<pubDate>Fri, 20 Feb 2026 00:13:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions]]></title>
<link>http://arxiv.org/abs/2602.17907v1</link>
<guid>2602.17907v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Raymond Li, Amirhossein Abaskohi, Chuyuan Li, Gabriel Murray, Giuseppe Carenini
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.]]></description>
<pubDate>Fri, 20 Feb 2026 00:12:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[Games That Teach, Chats That Convince: Comparing Interactive and Static Formats for Persuasive Learning]]></title>
<link>http://arxiv.org/abs/2602.17905v1</link>
<guid>2602.17905v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI, cs.CL, cs.ET
Authors: Seyed Hossein Alavi, Zining Wang, Shruthi Chockkalingam, Raymond T. Ng, Vered Shwartz
Institution: 
Published: 2026-02-20
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Interactive systems such as chatbots and games are increasingly used to persuade and educate on sustainability-related topics, yet it remains unclear how different delivery formats shape learning and persuasive outcomes when content is held constant. Grounding on identical arguments and factual content across conditions, we present a controlled user study comparing three modes of information delivery: static essays, conversational chatbots, and narrative text-based games. Across subjective measures, the chatbot condition consistently outperformed the other modes and increased perceived importance of the topic. However, perceived learning did not reliably align with objective outcomes: participants in the text-based game condition reported learning less than those reading essays, yet achieved higher scores on a delayed (24-hour) knowledge quiz. Additional exploratory analyses further suggest that common engagement proxies, such as verbosity and interaction length, are more closely related to subjective experience than to actual learning. These findings highlight a dissociation between how persuasive experiences feel and what participants retain, and point to important design trade-offs between interactivity, realism, and learning in persuasive systems and serious games.]]></description>
<pubDate>Fri, 20 Feb 2026 00:07:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[El Agente Grfico: Structured Execution Graphs for Scientific Agents]]></title>
<link>http://arxiv.org/abs/2602.17902v1</link>
<guid>2602.17902v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.MA, cs.SE
Authors: Jiaru Bai, Abdulrahman Aldossary, Thomas Swanick, Marcel Mller, Yeonghun Kang et al.
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Grfico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.]]></description>
<pubDate>Thu, 19 Feb 2026 23:47:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[MeDUET: Disentangled Unified Pretraining for 3D Medical Image Synthesis and Analysis]]></title>
<link>http://arxiv.org/abs/2602.17901v1</link>
<guid>2602.17901v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.GT
Authors: Junkai Liu, Ling Shao, Le Zhang
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Self-supervised learning (SSL) and diffusion models have advanced representation learning and image synthesis. However, in 3D medical imaging, they remain separate: diffusion for synthesis, SSL for analysis. Unifying 3D medical image synthesis and analysis is intuitive yet challenging, as multi-center datasets exhibit dominant style shifts, while downstream tasks rely on anatomy, and site-specific style co-varies with anatomy across slices, making factors unreliable without explicit constraints. In this paper, we propose MeDUET, a 3D Medical image Disentangled UnifiEd PreTraining framework that performs SSL in the Variational Autoencoder (VAE) latent space which explicitly disentangles domain-invariant content from domain-specific style. The token demixing mechanism serves to turn disentanglement from a modeling assumption into an empirically identifiable property. Two novel proxy tasks, Mixed-Factor Token Distillation (MFTD) and Swap-invariance Quadruplet Contrast (SiQC), are devised to synergistically enhance disentanglement. Once pretrained, MeDUET is capable of (i) delivering higher fidelity, faster convergence, and improved controllability for synthesis, and (ii) demonstrating strong domain generalization and notable label efficiency for analysis across diverse medical benchmarks. In summary, MeDUET converts multi-source heterogeneity from an obstacle into a learning signal, enabling unified pretraining for 3D medical image synthesis and analysis. The code is available at https://github.com/JK-Liu7/MeDUET .]]></description>
<pubDate>Thu, 19 Feb 2026 23:45:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors]]></title>
<link>http://arxiv.org/abs/2602.17898v1</link>
<guid>2602.17898v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jingquan Yan, Yuwei Miao, Peiran Yu, Junzhou Huang
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.]]></description>
<pubDate>Thu, 19 Feb 2026 23:33:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning from Biased and Costly Data Sources: Minimax-optimal Data Collection under a Budget]]></title>
<link>http://arxiv.org/abs/2602.17894v1</link>
<guid>2602.17894v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Michael O. Harding, Vikas Singh, Kirthevasan Kandasamy
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Data collection is a critical component of modern statistical and machine learning pipelines, particularly when data must be gathered from multiple heterogeneous sources to study a target population of interest. In many use cases, such as medical studies or political polling, different sources incur different sampling costs. Observations often have associated group identities (for example, health markers, demographics, or political affiliations) and the relative composition of these groups may differ substantially, both among the source populations and between sources and target population.
  In this work, we study multi-source data collection under a fixed budget, focusing on the estimation of population means and group-conditional means. We show that naive data collection strategies (e.g. attempting to "match" the target distribution) or relying on standard estimators (e.g. sample mean) can be highly suboptimal. Instead, we develop a sampling plan which maximizes the effective sample size: the total sample size divided by $D_{^2}(q\mid\mid\overline{p}) + 1$, where $q$ is the target distribution, $\overline{p}$ is the aggregated source distribution, and $D_{^2}$ is the $^2$-divergence. We pair this sampling plan with a classical post-stratification estimator and upper bound its risk. We provide matching lower bounds, establishing that our approach achieves the budgeted minimax optimal risk. Our techniques also extend to prediction problems when minimizing the excess risk, providing a principled approach to multi-source learning with costly and heterogeneous data sources.]]></description>
<pubDate>Thu, 19 Feb 2026 23:17:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[COMBA: Cross Batch Aggregation for Learning Large Graphs with Context Gating State Space Models]]></title>
<link>http://arxiv.org/abs/2602.17893v1</link>
<guid>2602.17893v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jiajun Shen, Yufei Jin, Yi He, xingquan Zhu
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

State space models (SSMs) have recently emerged for modeling long-range dependency in sequence data, with much simplified computational costs than modern alternatives, such as transformers. Advancing SMMs to graph structured data, especially for large graphs, is a significant challenge because SSMs are sequence models and the shear graph volumes make it very expensive to convert graphs as sequences for effective learning. In this paper, we propose COMBA to tackle large graph learning using state space models, with two key innovations: graph context gating and cross batch aggregation. Graph context refers to different hops of neighborhood for each node, and graph context gating allows COMBA to use such context to learn best control of neighbor aggregation. For each graph context, COMBA samples nodes as batches, and train a graph neural network (GNN), with information being aggregated cross batches, allowing COMBA to scale to large graphs. Our theoretical study asserts that cross-batch aggregation guarantees lower error than training GNN without aggregation. Experiments on benchmark networks demonstrate significant performance gains compared to baseline approaches. Code and benchmark datasets will be released for public access.]]></description>
<pubDate>Thu, 19 Feb 2026 23:14:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data]]></title>
<link>http://arxiv.org/abs/2602.17888v1</link>
<guid>2602.17888v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Sayeed Shafayet Chowdhury, Karen D'Souza, V. Siva Kakumani, Snehasis Mukhopadhyay, Shiaofen Fang et al.
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.]]></description>
<pubDate>Thu, 19 Feb 2026 22:47:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations]]></title>
<link>http://arxiv.org/abs/2602.17881v1</link>
<guid>2602.17881v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI, cs.LG
Authors: Joschka Braun
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.]]></description>
<pubDate>Thu, 19 Feb 2026 22:37:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Interactive Learning of Single-Index Models via Stochastic Gradient Descent]]></title>
<link>http://arxiv.org/abs/2602.17876v1</link>
<guid>2602.17876v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Nived Rajaraman, Yanjun Han
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Stochastic gradient descent (SGD) is a cornerstone algorithm for high-dimensional optimization, renowned for its empirical successes. Recent theoretical advances have provided a deep understanding of how SGD enables feature learning in high-dimensional nonlinear models, most notably the \textit{single-index model} with i.i.d. data. In this work, we study the sequential learning problem for single-index models, also known as generalized linear bandits or ridge bandits, where SGD is a simple and natural solution, yet its learning dynamics remain largely unexplored. We show that, similar to the optimal interactive learner, SGD undergoes a distinct ``burn-in'' phase before entering the ``learning'' phase in this setting. Moreover, with an appropriately chosen learning rate schedule, a single SGD procedure simultaneously achieves near-optimal (or best-known) sample complexity and regret guarantees across both phases, for a broad class of link functions. Our results demonstrate that SGD remains highly competitive for learning single-index models under adaptive data.]]></description>
<pubDate>Thu, 19 Feb 2026 22:22:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[MultiVer: Zero-Shot Multi-Agent Vulnerability Detection]]></title>
<link>http://arxiv.org/abs/2602.17875v1</link>
<guid>2602.17875v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.MA, cs.AI
Authors: Shreshth Rajan
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present MultiVer, a zero-shot multi-agent system for vulnerability detection that achieves state-of-the-art recall without fine-tuning. A four-agent ensemble (security, correctness, performance, style) with union voting achieves 82.7% recall on PyVul, exceeding fine-tuned GPT-3.5 (81.3%) by 1.4 percentage points -- the first zeroshot system to surpass fine-tuned performance on this benchmark. On SecurityEval, the same architecture achieves 91.7% detection rate, matching specialized systems. The recall improvement comes at a precision cost: 48.8% precision versus 63.9% for fine-tuned baselines, yielding 61.4% F1. Ablation experiments isolate component contributions: the multi-agent ensemble adds 17 percentage points recall over single-agent security analysis. These results demonstrate that for security applications where false negatives are costlier than false positives, zero-shot multi-agent ensembles can match and exceed fine-tuned models on the metric that matters most.]]></description>
<pubDate>Thu, 19 Feb 2026 22:20:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models]]></title>
<link>http://arxiv.org/abs/2602.17871v1</link>
<guid>2602.17871v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI, cs.LG, cs.MM
Authors: Dhruba Ghosh, Yuhui Zhang, Ludwig Schmidt
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.]]></description>
<pubDate>Thu, 19 Feb 2026 22:07:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models]]></title>
<link>http://arxiv.org/abs/2602.17869v1</link>
<guid>2602.17869v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yuxiao Chen, Jue Wang, Zhikang Zhang, Jingru Yi, Xu Zhang et al.
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.]]></description>
<pubDate>Thu, 19 Feb 2026 22:04:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies]]></title>
<link>http://arxiv.org/abs/2602.17868v1</link>
<guid>2602.17868v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Vasilii Feofanov, Songkang Wen, Jianfeng Zhang, Lujia Pan, Ievgen Redko
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.]]></description>
<pubDate>Thu, 19 Feb 2026 22:04:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization]]></title>
<link>http://arxiv.org/abs/2602.17867v1</link>
<guid>2602.17867v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CL
Authors: Joo N. Cardoso, Arlindo L. Oliveira, Bruno Martins
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.]]></description>
<pubDate>Thu, 19 Feb 2026 22:03:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[Financial time series augmentation using transformer based GAN architecture]]></title>
<link>http://arxiv.org/abs/2602.17865v1</link>
<guid>2602.17865v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Andrzej Podobiski, Jarosaw A. Chudziak
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.]]></description>
<pubDate>Thu, 19 Feb 2026 22:02:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[JAX-Privacy: A library for differentially private machine learning]]></title>
<link>http://arxiv.org/abs/2602.17861v1</link>
<guid>2602.17861v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Ryan McKenna, Galen Andrew, Borja Balle, Vadym Doroshenko, Arun Ganesh et al.
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.]]></description>
<pubDate>Thu, 19 Feb 2026 21:55:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Enhancing Scientific Literature Chatbots with Retrieval-Augmented Generation: A Performance Evaluation of Vector and Graph-Based Systems]]></title>
<link>http://arxiv.org/abs/2602.17856v1</link>
<guid>2602.17856v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IR, cs.AI
Authors: Hamideh Ghanadian, Amin Kamali, Mohammad Hossein Tekieh
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper investigates the enhancement of scientific literature chatbots through retrieval-augmented generation (RAG), with a focus on evaluating vector- and graph-based retrieval systems. The proposed chatbot leverages both structured (graph) and unstructured (vector) databases to access scientific articles and gray literature, enabling efficient triage of sources according to research objectives. To systematically assess performance, we examine two use-case scenarios: retrieval from a single uploaded document and retrieval from a large-scale corpus. Benchmark test sets were generated using a GPT model, with selected outputs annotated for evaluation. The comparative analysis emphasizes retrieval accuracy and response relevance, providing insight into the strengths and limitations of each approach. The findings demonstrate the potential of hybrid RAG systems to improve accessibility to scientific knowledge and to support evidence-based decision making.]]></description>
<pubDate>Thu, 19 Feb 2026 21:42:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[TopoGate: Quality-Aware Topology-Stabilized Gated Fusion for Longitudinal Low-Dose CT New-Lesion Prediction]]></title>
<link>http://arxiv.org/abs/2602.17855v1</link>
<guid>2602.17855v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Seungik Cho
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Longitudinal low-dose CT follow-ups vary in noise, reconstruction kernels, and registration quality. These differences destabilize subtraction images and can trigger false new lesion alarms. We present TopoGate, a lightweight model that combines the follow-up appearance view with the subtraction view and controls their influence through a learned, quality-aware gate. The gate is driven by three case-specific signals: CT appearance quality, registration consistency, and stability of anatomical topology measured with topological metrics. On the NLST--New-Lesion--LongCT cohort comprising 152 pairs from 122 patients, TopoGate improves discrimination and calibration over single-view baselines, achieving an area under the ROC curve of 0.65 with a standard deviation of 0.05 and a Brier score of 0.14. Removing corrupted or low-quality pairs, identified by the quality scores, further increases the area under the ROC curve from 0.62 to 0.68 and reduces the Brier score from 0.14 to 0.12. The gate responds predictably to degradation, placing more weight on appearance when noise grows, which mirrors radiologist practice. The approach is simple, interpretable, and practical for reliable longitudinal LDCT triage.]]></description>
<pubDate>Thu, 19 Feb 2026 21:41:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective]]></title>
<link>http://arxiv.org/abs/2602.17854v1</link>
<guid>2602.17854v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Domonkos Varga
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirnyi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.]]></description>
<pubDate>Thu, 19 Feb 2026 21:37:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[Neural Prior Estimation: Learning Class Priors from Latent Representations]]></title>
<link>http://arxiv.org/abs/2602.17853v1</link>
<guid>2602.17853v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CV
Authors: Masoud Yavari, Payman Moallem
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Class imbalance induces systematic bias in deep neural networks by imposing a skewed effective class prior. This work introduces the Neural Prior Estimator (NPE), a framework that learns feature-conditioned log-prior estimates from latent representations. NPE employs one or more Prior Estimation Modules trained jointly with the backbone via a one-way logistic loss. Under the Neural Collapse regime, NPE is analytically shown to recover the class log-prior up to an additive constant, providing a theoretically grounded adaptive signal without requiring explicit class counts or distribution-specific hyperparameters. The learned estimate is incorporated into logit adjustment, forming NPE-LA, a principled mechanism for bias-aware prediction. Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) demonstrate consistent improvements, particularly for underrepresented classes. NPE thus offers a lightweight and theoretically justified approach to learned prior estimation and imbalance-aware prediction.]]></description>
<pubDate>Thu, 19 Feb 2026 21:36:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[Mind the Style: Impact of Communication Style on Human-Chatbot Interaction]]></title>
<link>http://arxiv.org/abs/2602.17850v1</link>
<guid>2602.17850v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI, cs.CL, cs.CY
Authors: Erik Derner, Dalibor Kuera, Aditya Gulati, Ayoub Bagheri, Nuria Oliver
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Conversational agents increasingly mediate everyday digital interactions, yet the effects of their communication style on user experience and task success remain unclear. Addressing this gap, we describe the results of a between-subject user study where participants interact with one of two versions of a chatbot called NAVI which assists users in an interactive map-based 2D navigation task. The two chatbot versions differ only in communication style: one is friendly and supportive, while the other is direct and task-focused. Our results show that the friendly style increases subjective satisfaction and significantly improves task completion rates among female participants only, while no baseline differences between female and male participants were observed in a control condition without the chatbot. Furthermore, we find little evidence of users mimicking the chatbot's style, suggesting limited linguistic accommodation. These findings highlight the importance of user- and task-sensitive conversational agents and support that communication style personalization can meaningfully enhance interaction quality and performance.]]></description>
<pubDate>Thu, 19 Feb 2026 21:32:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Dual Length Codes for Lossless Compression of BFloat16]]></title>
<link>http://arxiv.org/abs/2602.17849v1</link>
<guid>2602.17849v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.IT
Authors: Aditya Agrawal, Albert Magyar, Hiteshwar Eswaraiah, Patrick Sheridan, Pradeep Janedula et al.
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Dual Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. Analyzing BFloat16 tensors from the Gemma model, we observed that the top 8 most frequent symbols account for approximately 50% of the cumulative probability. These 8 symbols are assigned a short 4 bit code. The remaining 248 symbols are assigned a longer 9 bit code. The coding scheme uses a single prefix bit to distinguish between the two code lengths. The scheme uses a small Look Up Table with only 8 entries for encoding and decoding. The scheme achieves a compressibility of 18.6% in comparison to 21.3% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.]]></description>
<pubDate>Thu, 19 Feb 2026 21:31:33 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the scaling relationship between cloze probabilities and language model next-token prediction]]></title>
<link>http://arxiv.org/abs/2602.17848v1</link>
<guid>2602.17848v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Cassandra L. Jacobs, Morgan Grobol
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.]]></description>
<pubDate>Thu, 19 Feb 2026 21:29:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models]]></title>
<link>http://arxiv.org/abs/2602.17846v1</link>
<guid>2602.17846v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Nick Dodson, Xinyu Gao, Qingsong Wang, Yusu Wang, Zhengchao Wan
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.]]></description>
<pubDate>Thu, 19 Feb 2026 21:21:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[TFL: Targeted Bit-Flip Attack on Large Language Model]]></title>
<link>http://arxiv.org/abs/2602.17837v1</link>
<guid>2602.17837v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CR, cs.CL, cs.LG
Authors: Jingkai Guo, Chaitali Chakrabarti, Deliang Fan
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models (LLMs) are increasingly deployed in safety and security critical applications, raising concerns about their robustness to model parameter fault injection attacks. Recent studies have shown that bit-flip attacks (BFAs), which exploit computer main memory (i.e., DRAM) vulnerabilities to flip a small number of bits in model weights, can severely disrupt LLM behavior. However, existing BFA on LLM largely induce un-targeted failure or general performance degradation, offering limited control over manipulating specific or targeted outputs. In this paper, we present TFL, a novel targeted bit-flip attack framework that enables precise manipulation of LLM outputs for selected prompts while maintaining almost no or minor degradation on unrelated inputs. Within our TFL framework, we propose a novel keyword-focused attack loss to promote attacker-specified target tokens in generative outputs, together with an auxiliary utility score that balances attack effectiveness against collateral performance impact on benign data. We evaluate TFL on multiple LLMs (Qwen, DeepSeek, Llama) and benchmarks (DROP, GSM8K, and TriviaQA). The experiments show that TFL achieves successful targeted LLM output manipulations with less than 50 bit flips and significantly reduced effect on unrelated queries compared to prior BFA approaches. This demonstrates the effectiveness of TFL and positions it as a new class of stealthy and targeted LLM model attack.]]></description>
<pubDate>Thu, 19 Feb 2026 20:59:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Influence-Preserving Proxies for Gradient-Based Data Selection in LLM Fine-tuning]]></title>
<link>http://arxiv.org/abs/2602.17835v1</link>
<guid>2602.17835v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Sirui Chen, Yunzhe Qi, Mengting Ai, Yifan Sun, Ruizhong Qiu et al.
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits a model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce Iprox, a two-stage framework that derives influence-preserving proxies directly from the target model. It first applies a low-rank compression stage to preserve influence information of the target model, and then an aligning stage to align both model gradients and logits, thereby constructing proxies that flexibly control computational cost while retaining the target model's influence. Experimental results across diverse LLM families and evaluation tasks show that Iprox consistently outperforms off-the-shelf proxies and baseline methods. On Qwen3-4B, a 1.5B proxy constructed with Iprox achieves stronger performance than the larger 1.7B off-the-shelf proxy. Notably, on Llama3.2, Iprox achieves better performance than baselines while reducing computational cost by more than half relative to the full 3B model. These results show that Iprox provides effective influence-preserving proxies, making gradient-based data selection more scalable for LLMs.]]></description>
<pubDate>Thu, 19 Feb 2026 20:57:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[MePoly: Max Entropy Polynomial Policy Optimization]]></title>
<link>http://arxiv.org/abs/2602.17832v1</link>
<guid>2602.17832v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.RO
Authors: Hang Liu, Sangli Teng, Maani Ghaffari
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.]]></description>
<pubDate>Thu, 19 Feb 2026 20:52:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Drift Estimation for Stochastic Differential Equations with Denoising Diffusion Models]]></title>
<link>http://arxiv.org/abs/2602.17830v1</link>
<guid>2602.17830v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Marcos Tapia Costa, Nikolas Kantas, George Deligiannidis
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study the estimation of time-homogeneous drift functions in multivariate stochastic differential equations with known diffusion coefficient, from multiple trajectories observed at high frequency over a fixed time horizon. We formulate drift estimation as a denoising problem conditional on previous observations, and propose an estimator of the drift function which is a by-product of training a conditional diffusion model capable of simulating new trajectories dynamically. Across different drift classes, the proposed estimator was found to match classical methods in low dimensions and remained consistently competitive in higher dimensions, with gains that cannot be attributed to architectural design choices alone.]]></description>
<pubDate>Thu, 19 Feb 2026 20:49:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[The Token Games: Evaluating Language Model Reasoning with Puzzle Duels]]></title>
<link>http://arxiv.org/abs/2602.17831v1</link>
<guid>2602.17831v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Simon Henniger, Gabriel Poesia
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.]]></description>
<pubDate>Thu, 19 Feb 2026 20:49:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models]]></title>
<link>http://arxiv.org/abs/2602.17829v1</link>
<guid>2602.17829v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Preetom Biswas, Giulia Pedrielli, K. Seluk Candan
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.]]></description>
<pubDate>Thu, 19 Feb 2026 20:49:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[Avoid What You Know: Divergent Trajectory Balance for GFlowNets]]></title>
<link>http://arxiv.org/abs/2602.17827v1</link>
<guid>2602.17827v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Pedro Dall'Antonia, Tiago da Silva, Daniel Csillag, Salem Lahlou, Diego Mesquita
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generative Flow Networks (GFlowNets) are a flexible family of amortized samplers trained to generate discrete and compositional objects with probability proportional to a reward function. However, learning efficiency is constrained by the model's ability to rapidly explore diverse high-probability regions during training. To mitigate this issue, recent works have focused on incentivizing the exploration of unvisited and valuable states via curiosity-driven search and self-supervised random network distillation, which tend to waste samples on already well-approximated regions of the state space. In this context, we propose Adaptive Complementary Exploration (ACE), a principled algorithm for the effective exploration of novel and high-probability regions when learning GFlowNets. To achieve this, ACE introduces an exploration GFlowNet explicitly trained to search for high-reward states in regions underexplored by the canonical GFlowNet, which learns to sample from the target distribution. Through extensive experiments, we show that ACE significantly improves upon prior work in terms of approximation accuracy to the target distribution and discovery rate of diverse high-reward states.]]></description>
<pubDate>Thu, 19 Feb 2026 20:47:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge]]></title>
<link>http://arxiv.org/abs/2602.17826v1</link>
<guid>2602.17826v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.LG, cs.SC
Authors: Marcelo Labre
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.]]></description>
<pubDate>Thu, 19 Feb 2026 20:45:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Neural Synchrony Between Socially Interacting Language Models]]></title>
<link>http://arxiv.org/abs/2602.17815v1</link>
<guid>2602.17815v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Zhining Zhang, Wentao Zhu, Chi Han, Yizhou Wang, Heng Ji
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the "social minds" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.]]></description>
<pubDate>Thu, 19 Feb 2026 20:33:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[VQPP: Video Query Performance Prediction Benchmark]]></title>
<link>http://arxiv.org/abs/2602.17814v1</link>
<guid>2602.17814v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.IR, cs.LG
Authors: Adrian Catalin Lutu, Eduard Poesina, Radu Tudor Ionescu
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.]]></description>
<pubDate>Thu, 19 Feb 2026 20:32:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[Promptable segmentation with region exploration enables minimal-effort expert-level prostate cancer delineation]]></title>
<link>http://arxiv.org/abs/2602.17813v1</link>
<guid>2602.17813v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Junqing Yang, Natasha Thorley, Ahmed Nadeem Abbasi, Shonit Punwani, Zion Tse et al.
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Purpose: Accurate segmentation of prostate cancer on magnetic resonance (MR) images is crucial for planning image-guided interventions such as targeted biopsies, cryoablation, and radiotherapy. However, subtle and variable tumour appearances, differences in imaging protocols, and limited expert availability make consistent interpretation difficult. While automated methods aim to address this, they rely on large expertly-annotated datasets that are often inconsistent, whereas manual delineation remains labour-intensive. This work aims to bridge the gap between automated and manual segmentation through a framework driven by user-provided point prompts, enabling accurate segmentation with minimal annotation effort.
  Methods: The framework combines reinforcement learning (RL) with a region-growing segmentation process guided by user prompts. Starting from an initial point prompt, region-growing generates a preliminary segmentation, which is iteratively refined through RL. At each step, the RL agent observes the image and current segmentation to predict a new point, from which region growing updates the mask. A reward, balancing segmentation accuracy and voxel-wise uncertainty, encourages exploration of ambiguous regions, allowing the agent to escape local optima and perform sample-specific optimisation. Despite requiring fully supervised training, the framework bridges manual and fully automated segmentation at inference by substantially reducing user effort while outperforming current fully automated methods.
  Results: The framework was evaluated on two public prostate MR datasets (PROMIS and PICAI, with 566 and 1090 cases). It outperformed the previous best automated methods by 9.9% and 8.9%, respectively, with performance comparable to manual radiologist segmentation, reducing annotation time tenfold.]]></description>
<pubDate>Thu, 19 Feb 2026 20:29:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning]]></title>
<link>http://arxiv.org/abs/2602.17809v1</link>
<guid>2602.17809v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.]]></description>
<pubDate>Thu, 19 Feb 2026 20:17:54 +0000</pubDate>
</item>
</channel>
</rss>