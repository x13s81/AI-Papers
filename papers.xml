<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Papers - 2026-02-24</title>
<link>https://arxiv.org</link>
<description>AI papers as of 2026-02-24 - 245 papers</description>
<lastBuildDate>Tue, 24 Feb 2026 03:59:25 +0000</lastBuildDate>
<item>
<title><![CDATA[A Very Big Video Reasoning Suite]]></title>
<link>https://huggingface.co/papers/2602.20159</link>
<guid>2602.20159</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Maijunxian Wang, Ruisi Wang, Juyi Lin, Ran Ji, ThaddÃ¤us Wiedemer
Institution: 
Published: 2026-02-23
Score: 9/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .]]></description>
<pubDate>Mon, 23 Feb 2026 18:59:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control]]></title>
<link>https://huggingface.co/papers/2602.18422</link>
<guid>2602.18422</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Linxi Xie, Lisong C. Sun, Ashley Neall, Tong Wu, Shengqu Cai
Institution: 
Published: 2026-02-20
Score: 8/10
Citations: 0
Upvotes: 20
GitHub: 
Stars: 0

Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.]]></description>
<pubDate>Fri, 20 Feb 2026 18:45:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[Arcee Trinity Large Technical Report]]></title>
<link>https://huggingface.co/papers/2602.17004</link>
<guid>2602.17004</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Varun Singh, Lucas Krauss, Sami Jaghouar, Matej Sirovatka, Charles Goddard
Institution: 
Published: 2026-02-19
Score: 8/10
Citations: 0
Upvotes: 16
GitHub: 
Stars: 0

We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.]]></description>
<pubDate>Thu, 19 Feb 2026 01:58:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents]]></title>
<link>https://huggingface.co/papers/2602.16699</link>
<guid>2602.16699</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Wenxuan Ding, Nicholas Tomlin, Greg Durrett
Institution: 
Published: 2026-02-18
Score: 8/10
Citations: 0
Upvotes: 13
GitHub: 
Stars: 0

LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.]]></description>
<pubDate>Wed, 18 Feb 2026 18:46:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[Discovering Multiagent Learning Algorithms with Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.16928</link>
<guid>2602.16928</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zun Li, John Schultz, Daniel Hennes, Marc Lanctot
Institution: 
Published: 2026-02-18
Score: 8/10
Citations: 0
Upvotes: 11
GitHub: 
Stars: 0

Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.]]></description>
<pubDate>Wed, 18 Feb 2026 22:41:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[Uncertainty-Aware Vision-Language Segmentation for Medical Imaging]]></title>
<link>https://huggingface.co/papers/2602.14498</link>
<guid>2602.14498</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Aryan Das, Tanishq Rachamalla, Koushik Biswas, Swalpa Kumar Roy, Vinay Kumar Verma
Institution: 
Published: 2026-02-16
Score: 8/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

We introduce a novel uncertainty-aware multimodal segmentation framework that leverages both radiological images and associated clinical text for precise medical diagnosis. We propose a Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) to enable efficient cross-modal fusion and long-range dependency modelling. To guide learning under ambiguity, we propose the Spectral-Entropic Uncertainty (SEU) Loss, which jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective. In complex clinical circumstances with poor image quality, this formulation improves model reliability. Extensive experiments on various publicly available medical datasets, QATA-COVID19, MosMed++, and Kvasir-SEG, demonstrate that our method achieves superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art (SoTA) approaches. Our results highlight the importance of incorporating uncertainty modelling and structured modality alignment in vision-language medical segmentation tasks. Code: https://github.com/arya-domain/UA-VLS]]></description>
<pubDate>Mon, 16 Feb 2026 06:27:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics]]></title>
<link>https://huggingface.co/papers/2602.19313</link>
<guid>2602.19313</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Shirui Chen, Cole Harrison, Ying-Chun Lee, Angela Jin Yang, Zhongzheng Ren
Institution: 
Published: 2026-02-22
Score: 8/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.]]></description>
<pubDate>Sun, 22 Feb 2026 19:25:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[SARAH: Spatially Aware Real-time Agentic Humans]]></title>
<link>https://huggingface.co/papers/2602.18432</link>
<guid>2602.18432</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Evonne Ng, Siwei Zhang, Zhang Chen, Michael Zollhoefer, Alexander Richard
Institution: 
Published: 2026-02-20
Score: 8/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.]]></description>
<pubDate>Fri, 20 Feb 2026 18:59:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device]]></title>
<link>https://huggingface.co/papers/2602.20161</link>
<guid>2602.20161</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Abdelrahman Shaker, Ahmed Heakl, Jaseel Muhammad, Ritesh Thawkar, Omkar Thawakar
Institution: 
Published: 2026-02-23
Score: 8/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/]]></description>
<pubDate>Mon, 23 Feb 2026 18:59:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[VidEoMT: Your ViT is Secretly Also a Video Segmentation Model]]></title>
<link>https://huggingface.co/papers/2602.17807</link>
<guid>2602.17807</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Narges Norouzi, Idil Esen Zulfikar, Niccol`o Cavagnero, Tommie Kerssies, Bastian Leibe
Institution: 
Published: 2026-02-19
Score: 8/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/]]></description>
<pubDate>Thu, 19 Feb 2026 20:14:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist]]></title>
<link>https://huggingface.co/papers/2602.16756</link>
<guid>2602.16756</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Johannes Bertram, Jonas Geiping
Institution: 
Published: 2026-02-18
Score: 8/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security, NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. NESSiE is intended as a lightweight, easy-to-use sanity check for language model safety and, as such, is not sufficient for guaranteeing safety in general -- but we argue that passing this test is necessary for any deployment. However, even state-of-the-art LLMs do not reach 100% on NESSiE and thus fail our necessary condition of language model safety, even in the absence of adversarial attacks. Our Safe & Helpful (SH) metric allows for direct comparison of the two requirements, showing models are biased toward being helpful rather than safe. We further find that disabled reasoning for some models, but especially a benign distraction context degrade model performance. Overall, our results underscore the critical risks of deploying such models as autonomous agents in the wild. We make the dataset, package and plotting code publicly available.]]></description>
<pubDate>Wed, 18 Feb 2026 09:41:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere]]></title>
<link>https://huggingface.co/papers/2602.10094</link>
<guid>2602.10094</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yihang Luo, Shangchen Zhou, Yushi Lan, Xingang Pan, Chen Change Loy
Institution: 
Published: 2026-02-10
Score: 8/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.]]></description>
<pubDate>Tue, 10 Feb 2026 18:57:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[Does Your Reasoning Model Implicitly Know When to Stop Thinking?]]></title>
<link>https://huggingface.co/papers/2602.08354</link>
<guid>2602.08354</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuanda Wang
Institution: 
Published: 2026-02-09
Score: 7/10
Citations: 0
Upvotes: 100
GitHub: 
Stars: 0

Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.]]></description>
<pubDate>Mon, 09 Feb 2026 07:38:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5]]></title>
<link>https://huggingface.co/papers/2602.14457</link>
<guid>2602.14457</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Dongrui Liu, Yi Yu, Jie Zhang, Guanxu Chen, Qihao Lin
Institution: 
Published: 2026-02-16
Score: 7/10
Citations: 0
Upvotes: 26
GitHub: 
Stars: 0

To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.]]></description>
<pubDate>Mon, 16 Feb 2026 04:30:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers]]></title>
<link>https://huggingface.co/papers/2602.16968</link>
<guid>2602.16968</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Dahye Kim, Deepti Ghadiyaram, Raghudeep Gadde
Institution: 
Published: 2026-02-19
Score: 7/10
Citations: 0
Upvotes: 11
GitHub: 
Stars: 0

Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.]]></description>
<pubDate>Thu, 19 Feb 2026 00:15:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment]]></title>
<link>https://huggingface.co/papers/2602.13579</link>
<guid>2602.13579</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Youngsun Wi, Jessica Yin, Elvis Xiang, Akash Sharma, Jitendra Malik
Institution: 
Published: 2026-02-14
Score: 7/10
Citations: 0
Upvotes: 10
GitHub: 
Stars: 0

Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).]]></description>
<pubDate>Sat, 14 Feb 2026 03:31:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers]]></title>
<link>https://huggingface.co/papers/2602.18292</link>
<guid>2602.18292</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou-Ammar
Institution: 
Published: 2026-02-20
Score: 7/10
Citations: 0
Upvotes: 8
GitHub: 
Stars: 0

Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.]]></description>
<pubDate>Fri, 20 Feb 2026 15:38:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots]]></title>
<link>https://huggingface.co/papers/2602.18071</link>
<guid>2602.18071</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Boyuan An, Zhexiong Wang, Yipeng Wang, Jiaqi Li, Sihang Li
Institution: 
Published: 2026-02-20
Score: 7/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.]]></description>
<pubDate>Fri, 20 Feb 2026 08:54:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment]]></title>
<link>https://huggingface.co/papers/2602.17259</link>
<guid>2602.17259</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Han Zhao, Jingbo Wang, Wenxuan Song, Shuai Chen, Yang Liu
Institution: 
Published: 2026-02-19
Score: 7/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.]]></description>
<pubDate>Thu, 19 Feb 2026 11:00:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing]]></title>
<link>https://huggingface.co/papers/2602.15823</link>
<guid>2602.15823</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zarif Ikram, Arad Firouzkouhi, Stephen Tu, Mahdi Soltanolkotabi, Paria Rashidinejad
Institution: 
Published: 2026-02-17
Score: 7/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.]]></description>
<pubDate>Tue, 17 Feb 2026 18:58:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges]]></title>
<link>https://huggingface.co/papers/2602.13576</link>
<guid>2602.13576</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ruomeng Ding, Yifei Pang, He Sun, Yizhong Wang, Zhiwei Steven Wu
Institution: 
Published: 2026-02-14
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Evaluation and alignment pipelines for large language models increasingly rely on LLM-based judges, whose behavior is guided by natural-language rubrics and validated on benchmarks. We identify a previously under-recognized vulnerability in this workflow, which we term Rubric-Induced Preference Drift (RIPD). Even when rubric edits pass benchmark validation, they can still produce systematic and directional shifts in a judge's preferences on target domains. Because rubrics serve as a high-level decision interface, such drift can emerge from seemingly natural, criterion-preserving edits and remain difficult to detect through aggregate benchmark metrics or limited spot-checking. We further show this vulnerability can be exploited through rubric-based preference attacks, in which benchmark-compliant rubric edits steer judgments away from a fixed human or trusted reference on target domains, systematically inducing RIPD and reducing target-domain accuracy up to 9.5% (helpfulness) and 27.9% (harmlessness). When these judgments are used to generate preference labels for downstream post-training, the induced bias propagates through alignment pipelines and becomes internalized in trained policies. This leads to persistent and systematic drift in model behavior. Overall, our findings highlight evaluation rubrics as a sensitive and manipulable control interface, revealing a system-level alignment risk that extends beyond evaluator reliability alone. The code is available at: https://github.com/ZDCSlab/Rubrics-as-an-Attack-Surface. Warning: Certain sections may contain potentially harmful content that may not be appropriate for all readers.]]></description>
<pubDate>Sat, 14 Feb 2026 03:19:14 +0000</pubDate>
</item>
<item>
<title><![CDATA[ReIn: Conversational Error Recovery with Reasoning Inception]]></title>
<link>https://huggingface.co/papers/2602.17022</link>
<guid>2602.17022</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Takyoung Kim, Jinseok Nam, Chandrayee Basu, Xing Fan, Chengyuan Ma
Institution: 
Published: 2026-02-19
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.]]></description>
<pubDate>Thu, 19 Feb 2026 02:37:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[Selective Training for Large Vision Language Models via Visual Information Gain]]></title>
<link>https://huggingface.co/papers/2602.17186</link>
<guid>2602.17186</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Seulbi Lee, Sangheum Hwang
Institution: 
Published: 2026-02-19
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.]]></description>
<pubDate>Thu, 19 Feb 2026 09:12:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[NeST: Neuron Selective Tuning for LLM Safety]]></title>
<link>https://huggingface.co/papers/2602.16835</link>
<guid>2602.16835</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Sasha Behrouzi, Lichao Wu, Mohamadreza Rostami, Ahmad-Reza Sadeghi
Institution: 
Published: 2026-02-18
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.
  We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.]]></description>
<pubDate>Wed, 18 Feb 2026 20:01:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning]]></title>
<link>https://huggingface.co/papers/2602.19455</link>
<guid>2602.19455</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zelin He, Boran Han, Xiyuan Zhang, Shuai Zhang, Haotian Lin
Institution: 
Published: 2026-02-23
Score: 7/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM's reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection. We further release SenTSR-Bench, a multivariate time-series-based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.]]></description>
<pubDate>Mon, 23 Feb 2026 02:55:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training]]></title>
<link>https://huggingface.co/papers/2602.10693</link>
<guid>2602.10693</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Guobin Shen, Chenxiao Zhao, Xiang Cheng, Lei Huang, Xing Yu
Institution: 
Published: 2026-02-11
Score: 6/10
Citations: 0
Upvotes: 159
GitHub: 
Stars: 0

Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO]]></description>
<pubDate>Wed, 11 Feb 2026 09:48:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[Unified Latents (UL): How to train your latents]]></title>
<link>https://huggingface.co/papers/2602.17270</link>
<guid>2602.17270</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jonathan Heek, Emiel Hoogeboom, Thomas Mensink, Tim Salimans
Institution: 
Published: 2026-02-19
Score: 6/10
Citations: 0
Upvotes: 46
GitHub: 
Stars: 0

We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.]]></description>
<pubDate>Thu, 19 Feb 2026 11:18:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[Spanning the Visual Analogy Space with a Weight Basis of LoRAs]]></title>
<link>https://huggingface.co/papers/2602.15727</link>
<guid>2602.15727</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Hila Manor, Rinon Gal, Haggai Maron, Tomer Michaeli, Gal Chechik
Institution: 
Published: 2026-02-17
Score: 6/10
Citations: 0
Upvotes: 9
GitHub: 
Stars: 0

Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet {a, a', b}, the goal is to generate b' such that a : a' :: b : b'. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a "space of LoRAs". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb]]></description>
<pubDate>Tue, 17 Feb 2026 17:02:38 +0000</pubDate>
</item>
<item>
<title><![CDATA[2Mamba2Furious: Linear in Complexity, Competitive in Accuracy]]></title>
<link>https://huggingface.co/papers/2602.17363</link>
<guid>2602.17363</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Gabriel Mongaras, Eric C. Larson
Institution: 
Published: 2026-02-19
Score: 6/10
Citations: 0
Upvotes: 7
GitHub: 
Stars: 0

Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments]]></description>
<pubDate>Thu, 19 Feb 2026 13:45:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking]]></title>
<link>https://huggingface.co/papers/2602.16849</link>
<guid>2602.16849</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jianliang He, Leda Wang, Siyu Chen, Zhuoran Yang
Institution: 
Published: 2026-02-18
Score: 6/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the "winner" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay.]]></description>
<pubDate>Wed, 18 Feb 2026 20:25:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[World Models for Policy Refinement in StarCraft II]]></title>
<link>https://huggingface.co/papers/2602.14857</link>
<guid>2602.14857</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yixin Zhang, Ziyi Wang, Yiming Rong, Haoxi Wang, Jinling Jiang
Institution: 
Published: 2026-02-16
Score: 6/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.]]></description>
<pubDate>Mon, 16 Feb 2026 15:51:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[Modeling Distinct Human Interaction in Web Agents]]></title>
<link>https://huggingface.co/papers/2602.17588</link>
<guid>2602.17588</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Faria Huq, Zora Zhiruo Wang, Zhanqiu Guo, Venu Arvind Arangarajan, Tianyue Ou
Institution: 
Published: 2026-02-19
Score: 6/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.]]></description>
<pubDate>Thu, 19 Feb 2026 18:11:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Avey-B]]></title>
<link>https://huggingface.co/papers/2602.15814</link>
<guid>2602.15814</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Devang Acharya, Mohammad Hammoud
Institution: 
Published: 2026-02-17
Score: 6/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.]]></description>
<pubDate>Tue, 17 Feb 2026 18:50:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning]]></title>
<link>https://huggingface.co/papers/2602.16742</link>
<guid>2602.16742</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Haoxiang Sun, Lizhen Xu, Bing Zhao, Wotao Yin, Wei Wang
Institution: 
Published: 2026-02-18
Score: 6/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce DeepVision-103K, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: https://huggingface.co/datasets/skylenage/DeepVision-103K{this url}.]]></description>
<pubDate>Wed, 18 Feb 2026 01:51:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty]]></title>
<link>https://huggingface.co/papers/2602.18312</link>
<guid>2602.18312</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zhaoming Xie, Kevin Karol, Jessica Hodgins
Institution: 
Published: 2026-02-20
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.]]></description>
<pubDate>Fri, 20 Feb 2026 16:11:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[References Improve LLM Alignment in Non-Verifiable Domains]]></title>
<link>https://huggingface.co/papers/2602.16802</link>
<guid>2602.16802</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Kejian Shi, Yixin Liu, Peifeng Wang, Alexander R. Fabbri, Shafiq Joty
Institution: 
Published: 2026-02-18
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.]]></description>
<pubDate>Wed, 18 Feb 2026 19:03:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model]]></title>
<link>https://huggingface.co/papers/2602.19128</link>
<guid>2602.19128</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Shiyi Cao, Ziming Mao, Joseph E. Gonzalez, Ion Stoica
Institution: 
Published: 2026-02-22
Score: 6/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.]]></description>
<pubDate>Sun, 22 Feb 2026 11:06:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction]]></title>
<link>https://huggingface.co/papers/2602.20160</link>
<guid>2602.20160</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Chen Wang, Hao Tan, Wang Yifan, Zhiqin Chen, Yuheng Liu
Institution: 
Published: 2026-02-23
Score: 6/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.]]></description>
<pubDate>Mon, 23 Feb 2026 18:59:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning]]></title>
<link>https://huggingface.co/papers/2602.13515</link>
<guid>2602.13515</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jintao Zhang, Kai Jiang, Chendong Xiang, Weiqi Feng, Yuezhou Hu
Institution: 
Published: 2026-02-13
Score: 5/10
Citations: 0
Upvotes: 42
GitHub: 
Stars: 0

Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.]]></description>
<pubDate>Fri, 13 Feb 2026 23:01:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents]]></title>
<link>https://huggingface.co/papers/2602.16855</link>
<guid>2602.16855</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Haiyang Xu, Xi Zhang, Haowei Liu, Junyang Wang, Zhaozai Zhu
Institution: 
Published: 2026-02-15
Score: 5/10
Citations: 0
Upvotes: 38
GitHub: 
Stars: 0

The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.]]></description>
<pubDate>Sun, 15 Feb 2026 01:52:19 +0000</pubDate>
</item>
<item>
<title><![CDATA["What Are You Doing?": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing]]></title>
<link>https://huggingface.co/papers/2602.15569</link>
<guid>2602.15569</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Johannes Kirmayr, Raphael Wennmacher, Khanh Huynh, Lukas Stappen, Elisabeth AndrÃ©
Institution: 
Published: 2026-02-17
Score: 5/10
Citations: 0
Upvotes: 13
GitHub: 
Stars: 0

Agentic AI assistants that autonomously perform multi-step tasks raise open questions for user experience: how should such systems communicate progress and reasoning during extended operations, especially in attention-critical contexts such as driving? We investigate feedback timing and verbosity from agentic LLM-based in-car assistants through a controlled, mixed-methods study (N=45) comparing planned steps and intermediate results feedback against silent operation with final-only response. Using a dual-task paradigm with an in-car voice assistant, we found that intermediate feedback significantly improved perceived speed, trust, and user experience while reducing task load - effects that held across varying task complexities and interaction contexts. Interviews further revealed user preferences for an adaptive approach: high initial transparency to establish trust, followed by progressively reducing verbosity as systems prove reliable, with adjustments based on task stakes and situational context. We translate our empirical findings into design implications for feedback timing and verbosity in agentic assistants, balancing transparency and efficiency.]]></description>
<pubDate>Tue, 17 Feb 2026 13:27:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Computer-Using World Model]]></title>
<link>https://huggingface.co/papers/2602.17365</link>
<guid>2602.17365</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yiming Guan, Rui Yu, John Zhang, Lu Wang, Chaoyun Zhang
Institution: 
Published: 2026-02-19
Score: 5/10
Citations: 0
Upvotes: 12
GitHub: 
Stars: 0

Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.]]></description>
<pubDate>Thu, 19 Feb 2026 13:48:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning]]></title>
<link>https://huggingface.co/papers/2602.19895</link>
<guid>2602.19895</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zhongwei Wan, Yun Shen, Zhihao Dou, Donghao Zhou, Yu Zhang
Institution: 
Published: 2026-02-23
Score: 5/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.]]></description>
<pubDate>Mon, 23 Feb 2026 14:37:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[Sink-Aware Pruning for Diffusion Language Models]]></title>
<link>https://huggingface.co/papers/2602.17664</link>
<guid>2602.17664</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen
Institution: 
Published: 2026-02-19
Score: 5/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose {bf Sink-Aware Pruning}, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.]]></description>
<pubDate>Thu, 19 Feb 2026 18:59:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs]]></title>
<link>https://huggingface.co/papers/2602.10377</link>
<guid>2602.10377</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Luoyang Sun, Jiwen Jiang, Yifeng Ding, Fengfa Li, Yan Song
Institution: 
Published: 2026-02-10
Score: 5/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.]]></description>
<pubDate>Tue, 10 Feb 2026 23:51:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions]]></title>
<link>https://huggingface.co/papers/2602.14279</link>
<guid>2602.14279</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ruomeng Ding, Tianwei Gao, Thomas P. Zollo, Eitan Bachmat, Richard Zemel
Institution: 
Published: 2026-02-15
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.]]></description>
<pubDate>Sun, 15 Feb 2026 19:05:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum]]></title>
<link>https://huggingface.co/papers/2602.17080</link>
<guid>2602.17080</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Minxin Zhang, Yuxuan Liu, Hayden Scheaffer
Institution: 
Published: 2026-02-19
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation.]]></description>
<pubDate>Thu, 19 Feb 2026 05:00:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation]]></title>
<link>https://huggingface.co/papers/2602.16915</link>
<guid>2602.16915</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zeyu Ren, Xiang Li, Yiran Wang, Zeyu Zhang, Hao Tang
Institution: 
Published: 2026-02-18
Score: 5/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.]]></description>
<pubDate>Wed, 18 Feb 2026 22:12:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[ArXiv-to-Model: A Practical Study of Scientific LM Training]]></title>
<link>https://huggingface.co/papers/2602.17288</link>
<guid>2602.17288</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Anuj Gupta
Institution: 
Published: 2026-02-19
Score: 4/10
Citations: 0
Upvotes: 7
GitHub: 
Stars: 0

While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.]]></description>
<pubDate>Thu, 19 Feb 2026 11:47:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[Agents of Chaos]]></title>
<link>https://huggingface.co/papers/2602.20021</link>
<guid>2602.20021</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Natalie Shapira, Chris Wendler, Avery Yen, Gabriele Sarti, Koyena Pal
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.]]></description>
<pubDate>Mon, 23 Feb 2026 16:28:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning]]></title>
<link>http://arxiv.org/abs/2602.20157v1</link>
<guid>2602.20157v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Zhongxiao Cong, Qitao Zhao, Minsik Jeon, Shubham Tulsiani
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.]]></description>
<pubDate>Mon, 23 Feb 2026 18:59:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks]]></title>
<link>http://arxiv.org/abs/2602.20156v1</link>
<guid>2602.20156v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CR, cs.LG
Authors: David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.]]></description>
<pubDate>Mon, 23 Feb 2026 18:59:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks]]></title>
<link>http://arxiv.org/abs/2602.20153v1</link>
<guid>2602.20153v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG, stat.ME
Authors: Jakob Heiss, SÃ¶ren Lambrecht, Jakob Weissteiner, Hanna Wutte, Å½an Å½uriÄ et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.]]></description>
<pubDate>Mon, 23 Feb 2026 18:59:10 +0000</pubDate>
</item>
<item>
<title><![CDATA[Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data]]></title>
<link>http://arxiv.org/abs/2602.20152v1</link>
<guid>2602.20152v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, stat.ML
Authors: Zhenyao Ma, Yue Liang, Dongxu Li
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.]]></description>
<pubDate>Mon, 23 Feb 2026 18:59:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[Conformal Risk Control for Non-Monotonic Losses]]></title>
<link>http://arxiv.org/abs/2602.20151v1</link>
<guid>2602.20151v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ME, cs.LG, stat.ML
Authors: Anastasios N. Angelopoulos
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.]]></description>
<pubDate>Mon, 23 Feb 2026 18:58:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization]]></title>
<link>http://arxiv.org/abs/2602.20150v1</link>
<guid>2602.20150v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.CV
Authors: Wei-Cheng Huang, Jiaheng Han, Xiaohan Ye, Zherong Pan, Kris Hauser
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.]]></description>
<pubDate>Mon, 23 Feb 2026 18:58:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[Agentic AI for Scalable and Robust Optical Systems Control]]></title>
<link>http://arxiv.org/abs/2602.20144v1</link>
<guid>2602.20144v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.NI
Authors: Zehao Wang, Mingzhe Han, Wei Cheng, Yue-Kai Huang, Philip Ji et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.]]></description>
<pubDate>Mon, 23 Feb 2026 18:54:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Recurrent Structural Policy Gradient for Partially Observable Mean Field Games]]></title>
<link>http://arxiv.org/abs/2602.20141v1</link>
<guid>2602.20141v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Clarisse Wibault, Johannes Forkel, Sebastian Towers, Tiphaine Wibault, Juan Duque et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.]]></description>
<pubDate>Mon, 23 Feb 2026 18:53:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[Do Large Language Models Understand Data Visualization Rules?]]></title>
<link>http://arxiv.org/abs/2602.20137v1</link>
<guid>2602.20137v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Martin Sinnona, Valentin Bonas, Emmanuel Iarussi, Viviana Siless
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.]]></description>
<pubDate>Mon, 23 Feb 2026 18:47:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration]]></title>
<link>http://arxiv.org/abs/2602.20135v1</link>
<guid>2602.20135v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI, cs.IR
Authors: Mohammad Amanlou, Erfan Shafiee Moghaddam, Yasaman Amou Jafari, Mahdi Noori, Farhan Farsi et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.]]></description>
<pubDate>Mon, 23 Feb 2026 18:46:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Modeling Epidemiological Dynamics Under Adversarial Data and User Deception]]></title>
<link>http://arxiv.org/abs/2602.20134v1</link>
<guid>2602.20134v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.GT, cs.AI
Authors: Yiqi Su, Christo Kurisummoottil Thomas, Walid Saad, Bud Mishra, Naren Ramakrishnan
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the impact of non-pharmaceutical interventions (NPIs). While such data provide valuable real-time insights, they are often subject to strategic misreporting, driven by individual incentives to avoid penalties, access benefits, or express distrust in public health authorities. To account for such human behavior, in this paper, we introduce a game-theoretic framework that models the interaction between the population and a public health authority as a signaling game. Individuals (senders) choose how to report their behaviors, while the public health authority (receiver) updates their epidemiological model(s) based on potentially distorted signals. Focusing on deception around masking and vaccination, we characterize analytically game equilibrium outcomes and evaluate the degree to which deception can be tolerated while maintaining epidemic control through policy interventions. Our results show that separating equilibria-with minimal deception-drive infections to near zero over time. Remarkably, even under pervasive dishonesty in pooling equilibria, well-designed sender and receiver strategies can still maintain effective epidemic control. This work advances the understanding of adversarial data in epidemiology and offers tools for designing more robust public health models in the presence of strategic user behavior.]]></description>
<pubDate>Mon, 23 Feb 2026 18:45:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization]]></title>
<link>http://arxiv.org/abs/2602.20133v1</link>
<guid>2602.20133v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.NE, cs.AI, cs.CL
Authors: Mert Cemri, Shubham Agrawal, Akshat Gupta, Shu Liu, Audrey Cheng et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an "accumulated improvement signal" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.]]></description>
<pubDate>Mon, 23 Feb 2026 18:45:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[LAD: Learning Advantage Distribution for Reasoning]]></title>
<link>http://arxiv.org/abs/2602.20132v1</link>
<guid>2602.20132v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Wendi Li, Sharon Li
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.]]></description>
<pubDate>Mon, 23 Feb 2026 18:44:10 +0000</pubDate>
</item>
<item>
<title><![CDATA[To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering]]></title>
<link>http://arxiv.org/abs/2602.20130v1</link>
<guid>2602.20130v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Zaifu Zhan, Min Zeng, Shuang Zhou, Yiran Song, Xiaoyi Chen et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.
  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.
  Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\leq$4\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.
  Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.
  Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.]]></description>
<pubDate>Mon, 23 Feb 2026 18:42:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Adaptation to Intrinsic Dependence in Diffusion Language Models]]></title>
<link>http://arxiv.org/abs/2602.20126v1</link>
<guid>2602.20126v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.IT, stat.ML
Authors: Yunxiao Zhao, Changxiao Cai
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\widetilde O(\mathsf{TC}/K)$ and $\widetilde O(\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\mathsf{TC}$ and $\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K<L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.]]></description>
<pubDate>Mon, 23 Feb 2026 18:41:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[NanoKnow: How to Know What Your Language Model Knows]]></title>
<link>http://arxiv.org/abs/2602.20122v1</link>
<guid>2602.20122v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI, cs.IR, cs.LG
Authors: Lingwei Gu, Nour Jedidi, Jimmy Lin
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a "black box" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.]]></description>
<pubDate>Mon, 23 Feb 2026 18:37:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning]]></title>
<link>http://arxiv.org/abs/2602.20119v1</link>
<guid>2602.20119v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.AI, cs.CV
Authors: Jiahui Fu, Junyu Nan, Lingfeng Sun, Hongyu Li, Jianing Qian et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/]]></description>
<pubDate>Mon, 23 Feb 2026 18:35:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models]]></title>
<link>http://arxiv.org/abs/2602.20117v1</link>
<guid>2602.20117v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.LG
Authors: Andre He, Nathaniel Weir, Kaj Bostrom, Allen Nie, Darion Cassel et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs]]></description>
<pubDate>Mon, 23 Feb 2026 18:34:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[Benchmarking Unlearning for Vision Transformers]]></title>
<link>http://arxiv.org/abs/2602.20114v1</link>
<guid>2602.20114v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Kairan Zhao, Iurie Luca, Peter Triantafillou
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.]]></description>
<pubDate>Mon, 23 Feb 2026 18:33:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[StyleStream: Real-Time Zero-Shot Voice Style Conversion]]></title>
<link>http://arxiv.org/abs/2602.20113v1</link>
<guid>2602.20113v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SD, cs.AI
Authors: Yisi Liu, Nicholas Lee, Gopala Anumanchipalli
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Voice style conversion aims to transform an input utterance to match a target speaker's timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from style. While prior work has explored this problem, conversion quality remains limited, and real-time voice style conversion has not been addressed. We propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art performance. StyleStream consists of two components: a Destylizer, which removes style attributes while preserving linguistic content, and a Stylizer, a diffusion transformer (DiT) that reintroduces target style conditioned on reference speech. Robust content-style disentanglement is enforced through text supervision and a highly constrained information bottleneck. This design enables a fully non-autoregressive architecture, achieving real-time voice style conversion with an end-to-end latency of 1 second. Samples and real-time demo: https://berkeley-speech-group.github.io/StyleStream/.]]></description>
<pubDate>Mon, 23 Feb 2026 18:32:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds]]></title>
<link>http://arxiv.org/abs/2602.20111v1</link>
<guid>2602.20111v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Ezra Edelman, Surbhi Goel
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\ from an unknown distribution $\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\tilde{O}(\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental.
  We resolve this question by proving a matching $Î©(\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \emph{inference dimension}, yielding combined error $\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.]]></description>
<pubDate>Mon, 23 Feb 2026 18:30:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration]]></title>
<link>http://arxiv.org/abs/2602.20104v1</link>
<guid>2602.20104v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.HC, cs.LG
Authors: Hasan Amin, Ming Yin, Rajiv Khanna
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.]]></description>
<pubDate>Mon, 23 Feb 2026 18:22:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[BarrierSteer: LLM Safety via Learning Barrier Steering]]></title>
<link>http://arxiv.org/abs/2602.20102v1</link>
<guid>2602.20102v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Thanh Q. Tran, Arun Verma, Kiwan Wong, Bryan Kian Hsiang Low, Daniela Rus et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.]]></description>
<pubDate>Mon, 23 Feb 2026 18:19:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine]]></title>
<link>http://arxiv.org/abs/2602.20100v1</link>
<guid>2602.20100v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Soumick Chatterjee
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in "learning without labels," highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.]]></description>
<pubDate>Mon, 23 Feb 2026 18:15:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching]]></title>
<link>http://arxiv.org/abs/2602.20094v1</link>
<guid>2602.20094v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Yuzhe Wang, Yaochen Zhu, Jundong Li
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.]]></description>
<pubDate>Mon, 23 Feb 2026 18:06:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop]]></title>
<link>http://arxiv.org/abs/2602.20092v1</link>
<guid>2602.20092v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Leshem Choshen, Ryan Cotterell, Mustafa Omer Gul, Jaap Jumelet, Tal Linzen et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: Multilingual.
  We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.]]></description>
<pubDate>Mon, 23 Feb 2026 18:02:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[How Retrieved Context Shapes Internal Representations in RAG]]></title>
<link>http://arxiv.org/abs/2602.20091v1</link>
<guid>2602.20091v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Samuel Yeh, Sharon Li
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.]]></description>
<pubDate>Mon, 23 Feb 2026 18:02:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues]]></title>
<link>http://arxiv.org/abs/2602.20089v1</link>
<guid>2602.20089v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Zanxi Ruan, Qiuyu Kong, Songqun Gao, Yiming Wang, Marco Cristani
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them "structure-centric". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.]]></description>
<pubDate>Mon, 23 Feb 2026 17:57:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[Do Large Language Models Understand Data Visualization Principles?]]></title>
<link>http://arxiv.org/abs/2602.20084v1</link>
<guid>2602.20084v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Martin Sinnona, Valentin Bonas, Viviana Siless, Emmanuel Iarussi
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.]]></description>
<pubDate>Mon, 23 Feb 2026 17:51:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[SemanticNVS: Improving Semantic Scene Understanding in Generative Novel View Synthesis]]></title>
<link>http://arxiv.org/abs/2602.20079v1</link>
<guid>2602.20079v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Xinya Chen, Christopher Wewer, Jiahao Xie, Xinting Hu, Jan Eric Lenssen
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present SemanticNVS, a camera-conditioned multi-view diffusion model for novel view synthesis (NVS), which improves generation quality and consistency by integrating pre-trained semantic feature extractors. Existing NVS methods perform well for views near the input view, however, they tend to generate semantically implausible and distorted images under long-range camera motion, revealing severe degradation. We speculate that this degradation is due to current models failing to fully understand their conditioning or intermediate generated scene content. Here, we propose to integrate pre-trained semantic feature extractors to incorporate stronger scene semantics as conditioning to achieve high-quality generation even at distant viewpoints. We investigate two different strategies, (1) warped semantic features and (2) an alternating scheme of understanding and generation at each denoising step. Experimental results on multiple datasets demonstrate the clear qualitative and quantitative (4.69%-15.26% in FID) improvement over state-of-the-art alternatives.]]></description>
<pubDate>Mon, 23 Feb 2026 17:45:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning]]></title>
<link>http://arxiv.org/abs/2602.20078v1</link>
<guid>2602.20078v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.MA, cs.AI, cs.LG
Authors: Shan Yang, Yang Liu
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Î(N)$, yielding sample complexity $\mathcal{O}(N/Îµ)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Î(N)$ to $\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\mathcal{O}(1/Îµ)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.]]></description>
<pubDate>Mon, 23 Feb 2026 17:45:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[Robust Taylor-Lagrange Control for Safety-Critical Systems]]></title>
<link>http://arxiv.org/abs/2602.20076v1</link>
<guid>2602.20076v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.RO
Authors: Wei Xiao, Christos Cassandras, Anni Li
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method. However, the existence of a CBF is only a sufficient condition for system safety. The recently proposed Taylor-Lagrange Control (TLC) method addresses this limitation, but is vulnerable to the feasibility preservation problem (e.g., inter-sampling effect). In this paper, we propose a robust TLC (rTLC) method to address the feasibility preservation problem. Specifically, the rTLC method expands the safety function at an order higher than the relative degree of the function using Taylor's expansion with Lagrange remainder, which allows the control to explicitly show up at the current time instead of the future time in the TLC method. The rTLC method naturally addresses the feasibility preservation problem with only one hyper-parameter (the discretization time interval size during implementation), which is much less than its counterparts. Finally, we illustrate the effectiveness of the proposed rTLC method through an adaptive cruise control problem, and compare it with existing safety-critical control methods.]]></description>
<pubDate>Mon, 23 Feb 2026 17:40:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Training-Free Generative Modeling via Kernelized Stochastic Interpolants]]></title>
<link>http://arxiv.org/abs/2602.20070v1</link>
<guid>2602.20070v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Florentin Coeurdoux, Etienne Lempereur, NathanaÃ«l Cuvelle-Magar, Thomas Eboli, StÃ©phane Mallat et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\hat b_t(x) = \nablaÏ(x)^\topÎ·_t$, where $Î·_t\in\R^P$ solves a $P\times P$ system computable from data, with $P$ independent of the data dimension $d$. Since estimates are inexact, the diffusion coefficient $D_t$ affects sample quality; the optimal $D_t^*$ from Girsanov diverges at $t=0$, but this poses no difficulty and we develop an integrator that handles it seamlessly. The framework accommodates diverse feature maps -- scattering transforms, pretrained generative models etc. -- enabling training-free generation and model combination. We demonstrate the approach on financial time series, turbulence, and image generation.]]></description>
<pubDate>Mon, 23 Feb 2026 17:26:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[The Invisible Gorilla Effect in Out-of-distribution Detection]]></title>
<link>http://arxiv.org/abs/2602.20068v1</link>
<guid>2602.20068v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Harry Anthony, Ziyun Liang, Hermione Warr, Konstantinos Kamnitsas
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model's ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.]]></description>
<pubDate>Mon, 23 Feb 2026 17:24:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images]]></title>
<link>http://arxiv.org/abs/2602.20066v1</link>
<guid>2602.20066v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Kundan Thota, Xuanhao Mu, Thorsten Schlachter, Veit Hagenmeyer
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as an energy planner and extract the visual attributes such as roof age, building density, etc, from the RGB satellite image that correspond to the thermal load. A Multi-Layer Perceptron (MLP) regressor trained on these captions shows an $R^2$ uplift of 93.7% and shrinks the mean absolute error (MAE) by 30% compared to the baseline model. Qualitative analysis shows that high-impact tokens align with high-demand zones, offering lightweight support for heat planning in data-scarce regions.]]></description>
<pubDate>Mon, 23 Feb 2026 17:22:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[Multilingual Large Language Models do not comprehend all natural languages to equal degrees]]></title>
<link>http://arxiv.org/abs/2602.20065v1</link>
<guid>2602.20065v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Natalia Moskvina, Raquel Montero, Masaya Yoshida, Ferdy Hubers, Paolo Morosi et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.]]></description>
<pubDate>Mon, 23 Feb 2026 17:22:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[The LLMbda Calculus: AI Agents, Conversations, and Information Flow]]></title>
<link>http://arxiv.org/abs/2602.20064v1</link>
<guid>2602.20064v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.PL, cs.AI, cs.CR
Authors: Zac Garby, Andrew D. Gordon, David Sands
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming.]]></description>
<pubDate>Mon, 23 Feb 2026 17:22:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning]]></title>
<link>http://arxiv.org/abs/2602.20062v1</link>
<guid>2602.20062v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Nicolas Anguita, Francesco Locatello, Andrew M. Saxe, Marco Mondelli, Flavia Mancini et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning. However, an end-to-end theoretical understanding of how choices of initialization impact the ability to reuse and refine features during fine-tuning has remained elusive. Here we develop an analytical theory of the pretraining-fine-tuning pipeline in diagonal linear networks, deriving exact expressions for the generalization error as a function of initialization parameters and task statistics. We find that different initialization choices place the network into four distinct fine-tuning regimes that are distinguished by their ability to support feature learning and reuse, and therefore by the task statistics for which they are beneficial. In particular, a smaller initialization scale in earlier layers enables the network to both reuse and refine its features, leading to superior generalization on fine-tuning tasks that rely on a subset of pretraining features. We demonstrate empirically that the same initialization parameters impact generalization in nonlinear networks trained on CIFAR-100. Overall, our results demonstrate analytically how data and network initialization interact to shape fine-tuning generalization, highlighting an important role for the relative scale of initialization across different layers in enabling continued feature learning during fine-tuning.]]></description>
<pubDate>Mon, 23 Feb 2026 17:19:33 +0000</pubDate>
</item>
<item>
<title><![CDATA[MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving]]></title>
<link>http://arxiv.org/abs/2602.20060v1</link>
<guid>2602.20060v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.RO
Authors: Junli Wang, Xueyi Liu, Yinan Zheng, Zebing Xing, Pengfei Li et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser.]]></description>
<pubDate>Mon, 23 Feb 2026 17:17:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Interaction Theater: A case of LLM Agents Interacting at Scale]]></title>
<link>http://arxiv.org/abs/2602.20059v1</link>
<guid>2602.20059v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Sarath Shekkizhar, Adam Earle
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\%$) vary their output across contexts, $65\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\%$) and off-topic content ($22\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.]]></description>
<pubDate>Mon, 23 Feb 2026 17:14:29 +0000</pubDate>
</item>
<item>
<title><![CDATA[AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation]]></title>
<link>http://arxiv.org/abs/2602.20057v1</link>
<guid>2602.20057v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.AI
Authors: Ge Yuan, Qiyuan Qiao, Jing Zhang, Dong Xu
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.]]></description>
<pubDate>Mon, 23 Feb 2026 17:12:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation]]></title>
<link>http://arxiv.org/abs/2602.20055v1</link>
<guid>2602.20055v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.AI, cs.CV
Authors: Apoorva Vashisth, Manav Kulshrestha, Pranav Bakshi, Damon Conover, Guillaume Sartoretti et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.]]></description>
<pubDate>Mon, 23 Feb 2026 17:10:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[Decoupling Defense Strategies for Robust Image Watermarking]]></title>
<link>http://arxiv.org/abs/2602.20053v1</link>
<guid>2602.20053v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Jiahui Chen, Zehang Deng, Zeyu Zhang, Chaoyang Li, Lianchen Jia et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage fine-tuning framework that decouples the defense strategies. In stage 1, we address adversarial vulnerability via a tailored adversarial training paradigm that primarily fine-tunes the encoder while only conditionally updating the decoder. This approach learns to move the image into a non-attackable region, rather than modifying the decision boundary, thus preserving clean accuracy. In stage 2, we tackle distortion and regeneration attacks via direct image optimization. To preserve the adversarial robustness gained in stage 1, we formulate a principled, constrained image loss with theoretical guarantees, which balances the deviation from cover and previous encoded images. We also propose a quality-aware early-stop to further guarantee the lower bound of visual quality. Extensive experiments demonstrate AdvMark outperforms with the highest image quality and comprehensive robustness, i.e. up to 29\%, 33\% and 46\% accuracy improvement for distortion, regeneration and adversarial attacks, respectively.]]></description>
<pubDate>Mon, 23 Feb 2026 17:02:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Entropy in Large Language Models]]></title>
<link>http://arxiv.org/abs/2602.20052v1</link>
<guid>2602.20052v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Marco Scharringhausen
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.]]></description>
<pubDate>Mon, 23 Feb 2026 17:02:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[SEAL-pose: Enhancing 3D Human Pose Estimation via a Learned Loss for Structural Consistency]]></title>
<link>http://arxiv.org/abs/2602.20051v1</link>
<guid>2602.20051v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Yeonsung Kim, Junggeun Do, Seunguk Do, Sangmin Kim, Jaesik Park et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints. Conventional supervised losses are limited in capturing these correlations because they treat each joint independently. Previous studies have attempted to promote structural consistency through manually designed priors or rule-based constraints; however, these approaches typically require manual specification and are often non-differentiable, limiting their use as end-to-end training objectives. We propose SEAL-pose, a data-driven framework in which a learnable loss-net trains a pose-net by evaluating structural plausibility. Rather than relying on hand-crafted priors, our joint-graph-based design enables the loss-net to learn complex structural dependencies directly from data. Extensive experiments on three 3D HPE benchmarks with eight backbones show that SEAL-pose reduces per-joint errors and improves pose plausibility compared with the corresponding backbones across all settings. Beyond improving each backbone, SEAL-pose also outperforms models with explicit structural constraints, despite not enforcing any such constraints. Finally, we analyze the relationship between the loss-net and structural consistency, and evaluate SEAL-pose in cross-dataset and in-the-wild settings.]]></description>
<pubDate>Mon, 23 Feb 2026 17:00:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence]]></title>
<link>http://arxiv.org/abs/2602.20048v1</link>
<guid>2602.20048v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.SE
Authors: Tarakanath Paipuru
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools.]]></description>
<pubDate>Mon, 23 Feb 2026 16:58:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[Closing the gap in multimodal medical representation alignment]]></title>
<link>http://arxiv.org/abs/2602.20046v1</link>
<guid>2602.20046v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Eleonora Grassucci, Giordano Cicchetti, Danilo Comminiello
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.]]></description>
<pubDate>Mon, 23 Feb 2026 16:57:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously]]></title>
<link>http://arxiv.org/abs/2602.20042v1</link>
<guid>2602.20042v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Han Bao, Yue Huang, Xiaoda Wang, Zheyuan Zhang, Yujun Zhou et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \textbf{structural} value flattening, \textbf{normative} representation loss, and \textbf{cognitive} uncertainty blindness. We introduce Edge Alignment as a distinct approach in which systems preserve multi dimensional value structure, support plural and democratic representation, and incorporate epistemic mechanisms for interaction and clarification. To make this approach practical, we propose seven interdependent pillars organized into three phases. We identify key challenges in data collection, training objectives, and evaluation, outlining complementary technical and governance directions. Taken together, these measures reframe alignment as a lifecycle problem of dynamic normative governance rather than as a single instance optimization task.]]></description>
<pubDate>Mon, 23 Feb 2026 16:51:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover]]></title>
<link>http://arxiv.org/abs/2602.20041v1</link>
<guid>2602.20041v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.CV
Authors: Ghadah Alosaimi, Maha Alsayyari, Yixin Sun, Stamos Katsigiannis, Amir Atapour-Abarghouei et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Brain-computer interfaces (BCIs) provide a hands-free control modality for mobile robotics, yet decoding user intent during real-world navigation remains challenging. This work presents a brain-robot control framework for offline decoding of driving commands during robotic rover operation. A 4WD Rover Pro platform was remotely operated by 12 participants who navigated a predefined route using a joystick, executing the commands forward, reverse, left, right, and stop. Electroencephalogram (EEG) signals were recorded with a 16-channel OpenBCI cap and aligned with motor actions at Delta = 0 ms and future prediction horizons (Delta > 0 ms). After preprocessing, several deep learning models were benchmarked, including convolutional neural networks, recurrent neural networks, and Transformer architectures. ShallowConvNet achieved the highest performance for both action prediction and intent prediction. By combining real-world robotic control with multi-horizon EEG intention decoding, this study introduces a reproducible benchmark and reveals key design insights for predictive deep learning-based BCI systems.]]></description>
<pubDate>Mon, 23 Feb 2026 16:50:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization]]></title>
<link>http://arxiv.org/abs/2602.20040v1</link>
<guid>2602.20040v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Fahmida Liza Piya, Rahmatollah Beheshti
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.]]></description>
<pubDate>Mon, 23 Feb 2026 16:49:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[Latent Introspection: Models Can Detect Prior Concept Injections]]></title>
<link>http://arxiv.org/abs/2602.20031v1</link>
<guid>2602.20031v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.LG
Authors: Theia Pearson-Vogel, Martin Vanek, Raymond Douglas, Jan Kulveit
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.]]></description>
<pubDate>Mon, 23 Feb 2026 16:39:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[gencat: Generative computerized adaptive testing]]></title>
<link>http://arxiv.org/abs/2602.20020v1</link>
<guid>2602.20020v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Wanyong Feng, Andrew Lan
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Existing computerized Adaptive Testing (CAT) frameworks are typically built on predicting the correctness of a student response to a question. Although effective, this approach fails to leverage textual information in questions and responses, especially for open-ended questions. In this work, we propose GENCAT (\textbf{GEN}erative \textbf{CAT}), a novel CAT framework that leverages Large Language Models for knowledge estimate and question selection. First, we develop a Generative Item Response Theory (GIRT) model that enables us to estimate student knowledge from their open-ended responses and predict responses to unseen questions. We train the model in a two-step process, first via Supervised Fine-Tuning and then via preference optimization for knowledge-response alignment. Second, we introduce three question selection algorithms that leverage the generative capabilities of the GIRT model, based on the uncertainty, linguistic diversity, and information of sampled student responses. Third, we conduct experiments on two real-world programming datasets and demonstrate that GENCAT outperforms existing CAT baselines, achieving an AUC improvement of up to 4.32\% in the key early testing stages.]]></description>
<pubDate>Mon, 23 Feb 2026 16:28:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Discriminative and Generalizable Anomaly Detector for Dynamic Graph with Limited Supervision]]></title>
<link>http://arxiv.org/abs/2602.20019v1</link>
<guid>2602.20019v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Yuxing Tian, Yiyan Qi, Fengran Mo, Weixu Zhang, Jian Guo et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Dynamic graph anomaly detection (DGAD) is critical for many real-world applications but remains challenging due to the scarcity of labeled anomalies. Existing methods are either unsupervised or semi-supervised: unsupervised methods avoid the need for labeled anomalies but often produce ambiguous boundary, whereas semi-supervised methods can overfit to the limited labeled anomalies and generalize poorly to unseen anomalies. To address this gap, we consider a largely underexplored problem in DGAD: learning a discriminative boundary from normal/unlabeled data, while leveraging limited labeled anomalies \textbf{when available} without sacrificing generalization to unseen anomalies. To this end, we propose an effective, generalizable, and model-agnostic framework with three main components: (i) residual representation encoding that capture deviations between current interactions and their historical context, providing anomaly-relevant signals; (ii) a restriction loss that constrain the normal representations within an interval bounded by two co-centered hyperspheres, ensuring consistent scales while keeping anomalies separable; (iii) a bi-boundary optimization strategy that learns a discriminative and robust boundary using the normal log-likelihood distribution modeled by a normalizing flow. Extensive experiments demonstrate the superiority of our framework across diverse evaluation settings.]]></description>
<pubDate>Mon, 23 Feb 2026 16:25:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[QUIETT: Query-Independent Table Transformation for Robust Reasoning]]></title>
<link>http://arxiv.org/abs/2602.20017v1</link>
<guid>2602.20017v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Gaurav Najpande, Tampu Ravi Kumar, Manan Roy Choudhury, Neha Valeti, Yanjie Fu et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entangling table cleanup with reasoning and thus limiting generalization. We introduce QuIeTT, a query-independent table transformation framework that preprocesses raw tables into a single SQL-ready canonical representation before any test-time queries are observed. QuIeTT performs lossless schema and value normalization, exposes implicit relations, and preserves full provenance via raw table snapshots. By decoupling table transformation from reasoning, QuIeTT enables cleaner, more reliable, and highly efficient querying without modifying downstream models. Experiments on four benchmarks, WikiTQ, HiTab, NQ-Table, and SequentialQA show consistent gains across models and reasoning paradigms, with particularly strong improvements on a challenge set of structurally diverse, unseen questions.]]></description>
<pubDate>Mon, 23 Feb 2026 16:23:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation]]></title>
<link>http://arxiv.org/abs/2602.20008v1</link>
<guid>2602.20008v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Louis Fabrice Tshimanga, Andrea Zanola, Federico Del Pup, Manfredo Atzori
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.
  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.
  The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution.
  This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames.
  To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures.
  Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\%, 10\%, and 35\% of the SwinUNETR values, with better average performance (86.75\% $\pm 0.19\%$ Dice score for SwinUNETR vs our 87.21\% $\pm 0.35\%$).
  This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community.]]></description>
<pubDate>Mon, 23 Feb 2026 16:15:38 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Secure and Private Distributed Bayesian Federated Learning Design]]></title>
<link>http://arxiv.org/abs/2602.20003v1</link>
<guid>2602.20003v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Nuocheng Yang, Sihua Wang, Zhaohui Yang, Mingzhe Chen, Changchuan Yin et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergence acceleration. Within this framework, each device trains a local model using a Bayesian approach and independently selects an optimal subset of neighbors for posterior exchange. We formulate this neighbor selection as an optimization problem to minimize the global loss function under security and privacy constraints. Solving this problem is challenging because devices only possess partial network information, and the complex coupling between topology, security, and convergence remains unclear. To bridge this gap, we first analytically characterize the trade-offs between dynamic connectivity, Byzantine detection, privacy levels, and convergence speed. Leveraging these insights, we develop a fully distributed Graph Neural Network (GNN)-based Reinforcement Learning (RL) algorithm. This approach enables devices to make autonomous connection decisions based on local observations. Simulation results demonstrate that our method achieves superior robustness and efficiency with significantly lower overhead compared to traditional security and privacy schemes.]]></description>
<pubDate>Mon, 23 Feb 2026 16:12:02 +0000</pubDate>
</item>
<item>
<title><![CDATA[FairFS: Addressing Deep Feature Selection Biases for Recommender System]]></title>
<link>http://arxiv.org/abs/2602.20001v1</link>
<guid>2602.20001v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IR, cs.LG
Authors: Xianquan Wang, Zhaocheng Du, Jieming Zhu, Qinglin Jia, Zhenhua Dong et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance.]]></description>
<pubDate>Mon, 23 Feb 2026 16:08:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[RADE-Net: Robust Attention Network for Radar-Only Object Detection in Adverse Weather]]></title>
<link>http://arxiv.org/abs/2602.19994v1</link>
<guid>2602.19994v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Christof Leitgeb, Thomas Puchleitner, Max Peter Ronecker, Daniel Watzenig
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Automotive perception systems are obligated to meet high requirements. While optical sensors such as Camera and Lidar struggle in adverse weather conditions, Radar provides a more robust perception performance, effectively penetrating fog, rain, and snow. Since full Radar tensors have large data sizes and very few datasets provide them, most Radar-based approaches work with sparse point clouds or 2D projections, which can result in information loss. Additionally, deep learning methods show potential to extract richer and more dense features from low level Radar data and therefore significantly increase the perception performance. Therefore, we propose a 3D projection method for fast-Fourier-transformed 4D Range-Azimuth-Doppler-Elevation (RADE) tensors. Our method preserves rich Doppler and Elevation features while reducing the required data size for a single frame by 91.9% compared to a full tensor, thus achieving higher training and inference speed as well as lower model complexity. We introduce RADE-Net, a lightweight model tailored to 3D projections of the RADE tensor. The backbone enables exploitation of low-level and high-level cues of Radar tensors with spatial and channel-attention. The decoupled detection heads predict object center-points directly in the Range-Azimuth domain and regress rotated 3D bounding boxes from rich feature maps in the cartesian scene. We evaluate the model on scenes with multiple different road users and under various weather conditions on the large-scale K-Radar dataset and achieve a 16.7% improvement compared to their baseline, as well as 6.5% improvement over current Radar-only models. Additionally, we outperform several Lidar approaches in scenarios with adverse weather conditions. The code is available under https://github.com/chr-is-tof/RADE-Net.]]></description>
<pubDate>Mon, 23 Feb 2026 16:01:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[Cross-lingual Matryoshka Representation Learning across Speech and Text]]></title>
<link>http://arxiv.org/abs/2602.19991v1</link>
<guid>2602.19991v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Yaya Sy, Dioula DoucourÃ©, Christophe Cerisara, Irina Illina
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Speakers of under-represented languages face both a language barrier, as most online knowledge is in a few dominant languages, and a modality barrier, since information is largely text-based while many languages are primarily oral. We address this for French-Wolof by training the first bilingual speech-text Matryoshka embedding model, enabling efficient retrieval of French text from Wolof speech queries without relying on a costly ASR-translation pipelines. We introduce large-scale data curation pipelines and new benchmarks, compare modeling strategies, and show that modality fusion within a frozen text Matryoshka model performs best. Although trained only for retrieval, the model generalizes well to other tasks, such as speech intent detection, indicating the learning of general semantic representations. Finally, we analyze cost-accuracy trade-offs across Matryoshka dimensions and ranks, showing that information is concentrated only in a few components, suggesting potential for efficiency improvements.]]></description>
<pubDate>Mon, 23 Feb 2026 15:57:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction]]></title>
<link>http://arxiv.org/abs/2602.19987v1</link>
<guid>2602.19987v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.IR
Authors: Ha-Anh Hoang Nguyen, Tri-Duc Phan Le, Duc-Hoang Pham, Huy-Son Nguyen, Cam-Van Thi Nguyen et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics signals can be adaptively refined using a mixture-of-experts architecture, emphasizing the most informative omics components. Building upon this representation, CURE implicitly retrieves patient-specific latent subgroups that capture both baseline survival dynamics and treatment-dependent variations. Experimental results on METABRIC and TCGA-LUAD datasets demonstrate that proposed CURE model consistently outperforms strong baselines in survival analysis, evaluated using the Time-dependent Concordance Index ($C^{td}$) and Integrated Brier Score (IBS). These findings highlight the potential of CURE to enhance multimodal understanding and serve as a foundation for future treatment recommendation models. All code and related resources are publicly available to facilitate the reproducibility https://github.com/L2R-UET/CURE.]]></description>
<pubDate>Mon, 23 Feb 2026 15:53:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[Multivariate time-series forecasting of ASTRI-Horn monitoring data: A Normal Behavior Model]]></title>
<link>http://arxiv.org/abs/2602.19984v1</link>
<guid>2602.19984v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Federico Incardona, Alessandro Costa, Farida Farsian, Francesco Franchina, Giuseppe Leto et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This study presents a Normal Behavior Model (NBM) developed to forecast monitoring time-series data from the ASTRI-Horn Cherenkov telescope under normal operating conditions. The analysis focused on 15 physical variables acquired by the Telescope Control Unit between September 2022 and July 2024, representing sensor measurements from the Azimuth and Elevation motors. After data cleaning, resampling, feature selection, and correlation analysis, the dataset was segmented into fixed-length intervals, in which the first I samples represented the input sequence provided to the model, while the forecast length, T, indicated the number of future time steps to be predicted. A sliding-window technique was then applied to increase the number of intervals. A Multi-Layer Perceptron (MLP) was trained to perform multivariate forecasting across all features simultaneously. Model performance was evaluated using the Mean Squared Error (MSE) and the Normalized Median Absolute Deviation (NMAD), and it was also benchmarked against a Long Short-Term Memory (LSTM) network. The MLP model demonstrated consistent results across different features and I-T configurations, and matched the performance of the LSTM while converging faster. It achieved an MSE of 0.019+/-0.003 and an NMAD of 0.032+/-0.009 on the test set under its best configuration (4 hidden layers, 720 units per layer, and I-T lengths of 300 samples each, corresponding to 5 hours at 1-minute resolution). Extending the forecast horizon up to 6.5 hours-the maximum allowed by this configuration-did not degrade performance, confirming the model's effectiveness in providing reliable hour-scale predictions. The proposed NBM provides a powerful tool for enabling early anomaly detection in online ASTRI-Horn monitoring time series, offering a basis for the future development of a prognostics and health management system that supports predictive maintenance.]]></description>
<pubDate>Mon, 23 Feb 2026 15:51:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Contextual Safety Reasoning and Grounding for Open-World Robots]]></title>
<link>http://arxiv.org/abs/2602.19983v1</link>
<guid>2602.19983v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.AI
Authors: Zachary Ravichadran, David Snyder, Alexander Robey, Hamed Hassani, Vijay Kumar et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.]]></description>
<pubDate>Mon, 23 Feb 2026 15:51:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Computationally Efficient Multidimensional Vision Transformer]]></title>
<link>http://arxiv.org/abs/2602.19982v1</link>
<guid>2602.19982v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Alaa El Ichi, Khalide Jbilou
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision Transformers have achieved state-of-the-art performance in a wide range
  of computer vision tasks, but their practical deployment is limited by high
  computational and memory costs. In this paper, we introduce a novel tensor-based
  framework for Vision Transformers built upon the Tensor Cosine Product
  (Cproduct). By exploiting multilinear structures inherent in image data and the
  orthogonality of cosine transforms, the proposed approach enables efficient
  attention mechanisms and structured feature representations. We develop the
  theoretical foundations of the tensor cosine product, analyze its algebraic
  properties, and integrate it into a new Cproduct-based Vision Transformer
  architecture (TCP-ViT). Numerical experiments on standard classification and
  segmentation benchmarks demonstrate that the proposed method achieves a uniform
  1/C parameter reduction (where C is the number of channels) while
  maintaining competitive accuracy.]]></description>
<pubDate>Mon, 23 Feb 2026 15:49:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks]]></title>
<link>http://arxiv.org/abs/2602.19980v1</link>
<guid>2602.19980v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Itamar Trainin, Shauli Ravfogel, Omri Abend, Amir Feder
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

While Autoregressive (AR) Transformer-based Generative Language Models are frequently employed for lookahead tasks, recent research suggests a potential discrepancy in their ability to perform planning tasks that require multi-step lookahead. In this work, we investigate the distinct emergent mechanisms that arise when training AR versus Non-Autoregressive (NAR) models, such as Discrete Diffusion Models (dLLMs), on lookahead tasks. By requiring the models to plan ahead to reach the correct conclusion, we analyze how these two paradigms fundamentally differ in their approach to the problem. We identify a critical asymmetry in planning problems: while forward generation requires complex lookahead at branching junctions, reverse generation is often deterministic. This asymmetry creates an opportunity for NAR models. Through mechanistic analysis of training and inference dynamics, we demonstrate that NAR models learn to solve planning tasks by utilizing future tokens to decode backwards, avoiding the need to learn complex traversal mechanisms entirely. Consequently, we report that both AR and NAR models are able to achieve perfect accuracy on the lookahead task. However, NAR models require exponentially fewer training examples and shallower architectures compared to AR models, which often fail to converge without specific curriculum adjustments.]]></description>
<pubDate>Mon, 23 Feb 2026 15:47:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection]]></title>
<link>http://arxiv.org/abs/2602.19974v1</link>
<guid>2602.19974v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Tianyu Wang, Zhiyuan Ma, Qian Wang, Xinyi Zhang, Xinwei Long et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relationships from the prompt and correctly generate scenes with structural integrity. To mitigate this dilemma, we propose RL-RIG, a Reinforcement Learning framework for Reflection-based Image Generation. Our architecture comprises four primary components: Diffuser, Checker, Actor, and Inverse Diffuser, following a Generate-Reflect-Edit paradigm to spark the Chain of Thought reasoning ability in image generation for addressing the dilemma. To equip the model with better intuition over generation trajectories, we further develop Reflection-GRPO to train the VLM Actor for edit prompts and the Image Editor for better image quality under a given prompt, respectively. Unlike traditional approaches that solely produce visually stunning yet structurally unreasonable content, our evaluation metrics prioritize spatial accuracy, utilizing Scene Graph IoU and employing a VLM-as-a-Judge strategy to assess the spatial consistency of generated images on LAION-SG dataset. Experimental results show that RL-RIG outperforms existing state-of-the-art open-source models by up to 11% in terms of controllable and precise spatial reasoning in image generation.]]></description>
<pubDate>Mon, 23 Feb 2026 15:39:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting]]></title>
<link>http://arxiv.org/abs/2602.19969v1</link>
<guid>2602.19969v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI
Authors: Yuxing Tian, Fengran Mo, Weixu Zhang, Yiyan Qi, Jian-Yun Nie
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.]]></description>
<pubDate>Mon, 23 Feb 2026 15:30:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems]]></title>
<link>http://arxiv.org/abs/2602.19967v1</link>
<guid>2602.19967v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Yongsheng Chen, Yong Chen, Wei Guo, Xinghui Zhong
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Physics-informed neural networks (PINNs) provide a promising framework for solving inverse problems governed by partial differential equations (PDEs) by integrating observational data and physical constraints in a unified optimization objective. However, the ill-posed nature of PDE inverse problems makes them highly sensitive to noise. Even a small fraction of corrupted observations can distort internal neural representations, severely impairing accuracy and destabilizing training. Motivated by recent advances in machine unlearning and structured network pruning, we propose P-PINN, a selective pruning framework designed to unlearn the influence of corrupted data in a pretrained PINN. Specifically, starting from a PINN trained on the full dataset, P-PINN evaluates a joint residual--data fidelity indicator, a weighted combination of data misfit and PDE residuals, to partition the training set into reliable and corrupted subsets. Next, we introduce a bias-based neuron importance measure that quantifies directional activation discrepancies between the two subsets, identifying neurons whose representations are predominantly driven by corrupted samples. Building on this, an iterative pruning strategy then removes noise-sensitive neurons layer by layer. The resulting pruned network is fine-tuned on the reliable data subject to the original PDE constraints, acting as a lightweight post-processing stage rather than a complete retraining. Numerical experiments on extensive PDE inverse-problem benchmarks demonstrate that P-PINN substantially improves robustness, accuracy, and training stability under noisy conditions, achieving up to a 96.6\% reduction in relative error compared with baseline PINNs. These results indicate that activation-level post hoc pruning is a promising mechanism for enhancing the reliability of physics-informed learning in noise-contaminated settings.]]></description>
<pubDate>Mon, 23 Feb 2026 15:29:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference]]></title>
<link>http://arxiv.org/abs/2602.19964v1</link>
<guid>2602.19964v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, stat.ML
Authors: Moritz A. Zanger, Yijun Wu, Pascal R. Van der Vaart, Wendelin BÃ¶hmer, Matthijs T. J. Spaan
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods.]]></description>
<pubDate>Mon, 23 Feb 2026 15:28:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval]]></title>
<link>http://arxiv.org/abs/2602.19961v1</link>
<guid>2602.19961v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.IR
Authors: Yibo Yan, Jiahao Huo, Guanbo Feng, Mingdong Ou, Yi Cao et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through the lens of the Multimodal Large Language Model (MLLM) era. We begin by examining the benchmark landscape, and subsequently dive into the methodological evolution, categorizing approaches into three primary aspects: multimodal embedding models, multimodal reranker models, and the integration of Retrieval-Augmented Generation (RAG) and Agentic systems for complex document intelligence. Finally, we identify persistent challenges and outline promising future directions, aiming to provide a clear roadmap for future multimodal document intelligence.]]></description>
<pubDate>Mon, 23 Feb 2026 15:27:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Sparse Masked Attention Policies for Reliable Generalization]]></title>
<link>http://arxiv.org/abs/2602.19956v1</link>
<guid>2602.19956v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Caroline Horsch, Laurens Engwegen, Max Weltevrede, Matthijs T. J. Spaan, Wendelin BÃ¶hmer
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In reinforcement learning, abstraction methods that remove unnecessary information from the observation are commonly used to learn policies which generalize better to unseen tasks. However, these methods often overlook a crucial weakness: the function which extracts the reduced-information representation has unknown generalization ability in unseen observations. In this paper, we address this problem by presenting an information removal method which more reliably generalizes to new states. We accomplish this by using a learned masking function which operates on, and is integrated with, the attention weights within an attention-based policy network. We demonstrate that our method significantly improves policy generalization to unseen tasks in the Procgen benchmark compared to standard PPO and masking approaches.]]></description>
<pubDate>Mon, 23 Feb 2026 15:23:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Bayesian Framework for Post-disruption Travel Time Prediction in Metro Networks]]></title>
<link>http://arxiv.org/abs/2602.19952v1</link>
<guid>2602.19952v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.AP, stat.ME, stat.ML
Authors: Shayan Nazemi, AurÃ©lie Labbe, Stefan Steiner, Pratheepa Jeganathan, Martin TrÃ©panier et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Disruptions are an inherent feature of transportation systems, occurring unpredictably and with varying durations. Even after an incident is reported as resolved, disruptions can induce irregular train operations that generate substantial uncertainty in passenger waiting and travel times. Accurately forecasting post-disruption travel times therefore remains a critical challenge for transit operators and passenger information systems. This paper develops a Bayesian spatiotemporal modeling framework for post-disruption train travel times that explicitly captures train interactions, headway imbalance, and non-Gaussian distributional characteristics observed during recovery periods. The proposed model decomposes travel times into delay and journey components and incorporates a moving-average error structure to represent dependence between consecutive trains. Skew-normal and skew-$t$ distributions are employed to flexibly accommodate heteroskedasticity, skewness, and heavy-tailed behavior in post-disruption travel times. The framework is evaluated using high-resolution track-occupancy and disruption log data from the MontrÃ©al metro system, covering two lines in both travel directions. Empirical results indicate that post-disruption travel times exhibit pronounced distributional asymmetries that vary with traveled distance, as well as significant error dependence across trains. The proposed models consistently outperform baseline specifications in both point prediction accuracy and uncertainty quantification, with the skew-$t$ model demonstrating the most robust performance for longer journeys. These findings underscore the importance of incorporating both distributional flexibility and error dependence when forecasting post-disruption travel times in urban rail systems.]]></description>
<pubDate>Mon, 23 Feb 2026 15:20:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming]]></title>
<link>http://arxiv.org/abs/2602.19948v1</link>
<guid>2602.19948v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.AI, cs.CY, cs.HC, cs.MA
Authors: Ian Steenstra, Paola Pedrelli, Weiyan Shi, Stacy Marsella, Timothy W. Bickmore
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.
  Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions ("AI Psychosis") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the "black box" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.]]></description>
<pubDate>Mon, 23 Feb 2026 15:17:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators]]></title>
<link>http://arxiv.org/abs/2602.19946v1</link>
<guid>2602.19946v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Krzysztof Adamkiewicz, Brian Moser, Stanislav Frolov, Tobias Christian Nauen, Federico Raue et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.]]></description>
<pubDate>Mon, 23 Feb 2026 15:15:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models]]></title>
<link>http://arxiv.org/abs/2602.19945v1</link>
<guid>2602.19945v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Jin Liu, Yinbin Miao, Ning Xi, Junkang Liu
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Balancing convergence efficiency and robustness under Differential Privacy (DP) is a central challenge in Federated Learning (FL). While AdamW accelerates training and fine-tuning in large-scale models, we find that directly applying it to Differentially Private FL (DPFL) suffers from three major issues: (i) data heterogeneity and privacy noise jointly amplify the variance of second-moment estimator, (ii) DP perturbations bias the second-moment estimator, and (iii) DP amplify AdamW sensitivity to local overfitting, worsening client drift. We propose DP-FedAdamW, the first AdamW-based optimizer for DPFL. It restores AdamW under DP by stabilizing second-moment variance, removing DP-induced bias, and aligning local updates to the global descent to curb client drift. Theoretically, we establish an unbiased second-moment estimator and prove a linearly accelerated convergence rate without any heterogeneity assumption, while providing tighter $(\varepsilon,Î´)$-DP guarantees. Our empirical results demonstrate the effectiveness of DP-FedAdamW across language and vision Transformers and ResNet-18. On Tiny-ImageNet (Swin-Base, $\varepsilon=1$), DP-FedAdamW outperforms the state-of-the-art (SOTA) by 5.83\%. The code is available in Appendix.]]></description>
<pubDate>Mon, 23 Feb 2026 15:15:47 +0000</pubDate>
</item>
<item>
<title><![CDATA[Discover, Segment, and Select: A Progressive Mechanism for Zero-shot Camouflaged Object Segmentation]]></title>
<link>http://arxiv.org/abs/2602.19944v1</link>
<guid>2602.19944v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yilong Yang, Jianxin Tian, Shengchuan Zhang, Liujuan Cao
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Current zero-shot Camouflaged Object Segmentation methods typically employ a two-stage pipeline (discover-then-segment): using MLLMs to obtain visual prompts, followed by SAM segmentation. However, relying solely on MLLMs for camouflaged object discovery often leads to inaccurate localization, false positives, and missed detections. To address these issues, we propose the \textbf{D}iscover-\textbf{S}egment-\textbf{S}elect (\textbf{DSS}) mechanism, a progressive framework designed to refine segmentation step by step. The proposed method contains a Feature-coherent Object Discovery (FOD) module that leverages visual features to generate diverse object proposals, a segmentation module that refines these proposals through SAM segmentation, and a Semantic-driven Mask Selection (SMS) module that employs MLLMs to evaluate and select the optimal segmentation mask from multiple candidates. Without requiring any training or supervision, DSS achieves state-of-the-art performance on multiple COS benchmarks, especially in multiple-instance scenes.]]></description>
<pubDate>Mon, 23 Feb 2026 15:15:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs]]></title>
<link>http://arxiv.org/abs/2602.19938v1</link>
<guid>2602.19938v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Zijie Liu, Jie Peng, Jinhao Duan, Zirui Liu, Kaixiong Zhou et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.
  We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.
  We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference.]]></description>
<pubDate>Mon, 23 Feb 2026 15:11:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation]]></title>
<link>http://arxiv.org/abs/2602.19937v1</link>
<guid>2602.19937v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yifei Shi, Boyan Wan, Xin Xu, Kai Xu
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space-including unobserved regions in camera space-significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling (PIPS) strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The PIPS strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. Our method outperforms the state-of-the-art on three pose estimation datasets. Notably, it demonstrates significant improvements in challenging scenarios, such as objects captured with unseen pose, high occlusion, novel geometry, and severe noise.]]></description>
<pubDate>Mon, 23 Feb 2026 15:10:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[Expanding the Role of Diffusion Models for Robust Classifier Training]]></title>
<link>http://arxiv.org/abs/2602.19931v1</link>
<guid>2602.19931v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CV
Authors: Pin-Han Huang, Shang-Tse Chen, Hsuan-Tien Lin
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations that are both diverse and partially robust, and that explicitly incorporating diffusion representations as an auxiliary learning signal during AT consistently improves robustness across settings. Furthermore, our representation analysis indicates that incorporating diffusion models into AT encourages more disentangled features, while diffusion representations and diffusion-generated synthetic data play complementary roles in shaping representations. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate these findings, demonstrating the effectiveness of jointly leveraging diffusion representations and synthetic data within AT.]]></description>
<pubDate>Mon, 23 Feb 2026 15:06:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning]]></title>
<link>http://arxiv.org/abs/2602.19930v1</link>
<guid>2602.19930v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.LG
Authors: Nathan Gavenski, Felipe Meneguzzi, Odinaldo Rodrigues
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world.]]></description>
<pubDate>Mon, 23 Feb 2026 15:06:33 +0000</pubDate>
</item>
<item>
<title><![CDATA[Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models]]></title>
<link>http://arxiv.org/abs/2602.19926v1</link>
<guid>2602.19926v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Jin Liu, Yinbin Miao, Ning Xi, Junkang Liu
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\textbf{L}ocal \textbf{A}lternating \textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($Îµ= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\% in test accuracy. Code is provided in \repolink.]]></description>
<pubDate>Mon, 23 Feb 2026 15:05:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling]]></title>
<link>http://arxiv.org/abs/2602.19919v1</link>
<guid>2602.19919v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.LG
Authors: Xiang Li, Zikai Wei, Yiyan Qi, Wanyun Zhou, Xiang Liu et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.]]></description>
<pubDate>Mon, 23 Feb 2026 14:58:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[RobPI: Robust Private Inference against Malicious Client]]></title>
<link>http://arxiv.org/abs/2602.19918v1</link>
<guid>2602.19918v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CR, cs.LG
Authors: Jiaqi Xue, Mengxin Zheng, Qian Lou
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks.]]></description>
<pubDate>Mon, 23 Feb 2026 14:58:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.19917v1</link>
<guid>2602.19917v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.RO
Authors: Thanh Nguyen, Tung Luu, Tri Ton, Sungwoong Kim, Chang D. Yoo
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations such as being overly conservative in utilizing OOD data, imprecise OOD data characterization, and significant computational overhead. To address these challenges, this paper introduces an Uncertainty-Aware Rank-One Multi-Input Multi-Output (MIMO) Q Network framework. The framework aims to enhance Offline Reinforcement Learning by fully leveraging the potential of OOD data while still ensuring efficiency in the learning process. Specifically, the framework quantifies data uncertainty and harnesses it in the training losses, aiming to train a policy that maximizes the lower confidence bound of the corresponding Q-function. Furthermore, a Rank-One MIMO architecture is introduced to model the uncertainty-aware Q-function, \TP{offering the same ability for uncertainty quantification as an ensemble of networks but with a cost nearly equivalent to that of a single network}. Consequently, this framework strikes a harmonious balance between precision, speed, and memory efficiency, culminating in improved overall performance. Extensive experimentation on the D4RL benchmark demonstrates that the framework attains state-of-the-art performance while remaining computationally efficient. By incorporating the concept of uncertainty quantification, our framework offers a promising avenue to alleviate extrapolation errors and enhance the efficiency of offline RL.]]></description>
<pubDate>Mon, 23 Feb 2026 14:57:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[Augmented Radiance Field: A General Framework for Enhanced Gaussian Splatting]]></title>
<link>http://arxiv.org/abs/2602.19916v1</link>
<guid>2602.19916v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.GR
Authors: Yixin Yang, Bojian Wu, Yang Zhou, Hui Huang
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Due to the real-time rendering performance, 3D Gaussian Splatting (3DGS) has emerged as the leading method for radiance field reconstruction. However, its reliance on spherical harmonics for color encoding inherently limits its ability to separate diffuse and specular components, making it challenging to accurately represent complex reflections. To address this, we propose a novel enhanced Gaussian kernel that explicitly models specular effects through view-dependent opacity. Meanwhile, we introduce an error-driven compensation strategy to improve rendering quality in existing 3DGS scenes. Our method begins with 2D Gaussian initialization and then adaptively inserts and optimizes enhanced Gaussian kernels, ultimately producing an augmented radiance field. Experiments demonstrate that our method not only surpasses state-of-the-art NeRF methods in rendering performance but also achieves greater parameter efficiency. Project page at: https://xiaoxinyyx.github.io/augs.]]></description>
<pubDate>Mon, 23 Feb 2026 14:55:31 +0000</pubDate>
</item>
<item>
<title><![CDATA[Fully Convolutional Spatiotemporal Learning for Microstructure Evolution Prediction]]></title>
<link>http://arxiv.org/abs/2602.19915v1</link>
<guid>2602.19915v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Michael Trimboli, Mohammed Alsubaie, Sirani M. Perera, Ke-Gang Wang, Xianqi Li
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Understanding and predicting microstructure evolution is fundamental to materials science, as it governs the resulting properties and performance of materials. Traditional simulation methods, such as phase-field models, offer high-fidelity results but are computationally expensive due to the need to solve complex partial differential equations at fine spatiotemporal resolutions. To address this challenge, we propose a deep learning-based framework that accelerates microstructure evolution predictions while maintaining high accuracy. Our approach utilizes a fully convolutional spatiotemporal model trained in a self-supervised manner using sequential images generated from simulations of microstructural processes, including grain growth and spinodal decomposition. The trained neural network effectively learns the underlying physical dynamics and can accurately capture both short-term local behaviors and long-term statistical properties of evolving microstructures, while also demonstrating generalization to unseen spatiotemporal domains and variations in configuration and material parameters. Compared to recurrent neural architectures, our model achieves state-of-the-art predictive performance with significantly reduced computational cost in both training and inference. This work establishes a robust baseline for spatiotemporal learning in materials science and offers a scalable, data-driven alternative for fast and reliable microstructure simulations.]]></description>
<pubDate>Mon, 23 Feb 2026 14:55:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning]]></title>
<link>http://arxiv.org/abs/2602.19914v1</link>
<guid>2602.19914v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Thatchawin Leelawat, Lewis D Griffin
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.]]></description>
<pubDate>Mon, 23 Feb 2026 14:54:38 +0000</pubDate>
</item>
<item>
<title><![CDATA[De novo molecular structure elucidation from mass spectra via flow matching]]></title>
<link>http://arxiv.org/abs/2602.19912v1</link>
<guid>2602.19912v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Ghaith Mqawass, Tuan Le, Fabian Theis, Djork-ArnÃ© Clevert
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Mass spectrometry is a powerful and widely used tool for identifying molecular structures due to its sensitivity and ability to profile complex samples. However, translating spectra into full molecular structures is a difficult, under-defined inverse problem. Overcoming this problem is crucial for enabling biological insight, discovering new metabolites, and advancing chemical research across multiple fields. To this end, we develop MSFlow, a two-stage encoder-decoder flow-matching generative model that achieves state-of-the-art performance on the structure elucidation task for small molecules. In the first stage, we adopt a formula-restricted transformer model for encoding mass spectra into a continuous and chemically informative embedding space, while in the second stage, we train a decoder flow matching model to reconstruct molecules from latent embeddings of mass spectra. We present ablation studies demonstrating the importance of using information-preserving molecular descriptors for encoding mass spectra and motivate the use of our discrete flow-based decoder. Our rigorous evaluation demonstrates that MSFlow can accurately translate up to 45 percent of molecular mass spectra into their corresponding molecular representations - an improvement of up to fourteen-fold over the current state-of-the-art. A trained version of MSFlow is made publicly available on GitHub for non-commercial users.]]></description>
<pubDate>Mon, 23 Feb 2026 14:52:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery]]></title>
<link>http://arxiv.org/abs/2602.19910v1</link>
<guid>2602.19910v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Wei He, Xianghan Meng, Zhiyuan Huang, Xianbiao Qi, Rong Xiao et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this paper, we propose a novel and effective multi-modal representation learning framework for GCD via Semi-Supervised Rate Reduction, called SSR$^2$-GCD, to learn cross-modality representations with desired structural properties based on emphasizing to properly align intra-modality relationships. Moreover, to boost knowledge transfer, we integrate prompt candidates by leveraging the inter-modal alignment offered by Vision Language Models. We conduct extensive experiments on generic and fine-grained benchmark datasets demonstrating superior performance of our approach.]]></description>
<pubDate>Mon, 23 Feb 2026 14:51:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[Gradient based Severity Labeling for Biomarker Classification in OCT]]></title>
<link>http://arxiv.org/abs/2602.19907v1</link>
<guid>2602.19907v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: Kiran Kokilepersaud, Mohit Prabhushankar, Ghassan AlRegib, Stephanie Trejo Corona, Charles Wykoff
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In this paper, we propose a novel selection strategy for contrastive learning for medical images. On natural images, contrastive learning uses augmentations to select positive and negative pairs for the contrastive loss. However, in the medical domain, arbitrary augmentations have the potential to distort small localized regions that contain the biomarkers we are interested in detecting. A more intuitive approach is to select samples with similar disease severity characteristics, since these samples are more likely to have similar structures related to the progression of a disease. To enable this, we introduce a method that generates disease severity labels for unlabeled OCT scans on the basis of gradient responses from an anomaly detection algorithm. These labels are used to train a supervised contrastive learning setup to improve biomarker classification accuracy by as much as 6% above self-supervised baselines for key indicators of Diabetic Retinopathy.]]></description>
<pubDate>Mon, 23 Feb 2026 14:46:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[Rethinking Chronological Causal Discovery with Signal Processing]]></title>
<link>http://arxiv.org/abs/2602.19903v1</link>
<guid>2602.19903v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Kurt Butler, Damian Machlanski, Panagiotis Dimitrakopoulos, Sotirios A. Tsaftaris
Institution: MIT
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Causal discovery problems use a set of observations to deduce causality between variables in the real world, typically to answer questions about biological or physical systems. These observations are often recorded at regular time intervals, determined by a user or a machine, depending on the experiment design. There is generally no guarantee that the timing of these recordings matches the timing of the underlying biological or physical events. In this paper, we examine the sensitivity of causal discovery methods to this potential mismatch. We consider empirical and theoretical evidence to understand how causal discovery performance is impacted by changes of sampling rate and window length. We demonstrate that both classical and recent causal discovery methods exhibit sensitivity to these hyperparameters, and we discuss how ideas from signal processing may help us understand these phenomena.]]></description>
<pubDate>Mon, 23 Feb 2026 14:43:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[ExpPortrait: Expressive Portrait Generation via Personalized Representation]]></title>
<link>http://arxiv.org/abs/2602.19900v1</link>
<guid>2602.19900v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.GR
Authors: Junyi Wang, Yudong Guo, Boyang Guo, Shengming Yang, Juyong Zhang
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

While diffusion models have shown great potential in portrait generation, generating expressive, coherent, and controllable cinematic portrait videos remains a significant challenge. Existing intermediate signals for portrait generation, such as 2D landmarks and parametric models, have limited disentanglement capabilities and cannot express personalized details due to their sparse or low-rank representation. Therefore, existing methods based on these models struggle to accurately preserve subject identity and expressions, hindering the generation of highly expressive portrait videos. To overcome these limitations, we propose a high-fidelity personalized head representation that more effectively disentangles expression and identity. This representation captures both static, subject-specific global geometry and dynamic, expression-related details. Furthermore, we introduce an expression transfer module to achieve personalized transfer of head pose and expression details between different identities. We use this sophisticated and highly expressive head model as a conditional signal to train a diffusion transformer (DiT)-based generator to synthesize richly detailed portrait videos. Extensive experiments on self- and cross-reenactment tasks demonstrate that our method outperforms previous models in terms of identity preservation, expression accuracy, and temporal stability, particularly in capturing fine-grained details of complex motion.]]></description>
<pubDate>Mon, 23 Feb 2026 14:41:35 +0000</pubDate>
</item>
<item>
<title><![CDATA[Monocular Mesh Recovery and Body Measurement of Female Saanen Goats]]></title>
<link>http://arxiv.org/abs/2602.19896v1</link>
<guid>2602.19896v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Bo Jin, Shichao Zhao, Jin Lyu, Bin Zhang, Tao Yu et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The lactation performance of Saanen dairy goats, renowned for their high milk yield, is intrinsically linked to their body size, making accurate 3D body measurement essential for assessing milk production potential, yet existing reconstruction methods lack goat-specific authentic 3D data. To address this limitation, we establish the FemaleSaanenGoat dataset containing synchronized eight-view RGBD videos of 55 female Saanen goats (6-18 months). Using multi-view DynamicFusion, we fuse noisy, non-rigid point cloud sequences into high-fidelity 3D scans, overcoming challenges from irregular surfaces and rapid movement. Based on these scans, we develop SaanenGoat, a parametric 3D shape model specifically designed for female Saanen goats. This model features a refined template with 41 skeletal joints and enhanced udder representation, registered with our scan data. A comprehensive shape space constructed from 48 goats enables precise representation of diverse individual variations. With the help of SaanenGoat model, we get high-precision 3D reconstruction from single-view RGBD input, and achieve automated measurement of six critical body dimensions: body length, height, chest width, chest girth, hip width, and hip height. Experimental results demonstrate the superior accuracy of our method in both 3D reconstruction and body measurement, presenting a novel paradigm for large-scale 3D vision applications in precision livestock farming.]]></description>
<pubDate>Mon, 23 Feb 2026 14:37:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[Generalized Random Direction Newton Algorithms for Stochastic Optimization]]></title>
<link>http://arxiv.org/abs/2602.19893v1</link>
<guid>2602.19893v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Soumen Pachal, Prashanth L. A., Shalabh Bhatnagar, Avinash Achar
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present a family of generalized Hessian estimators of the objective using random direction stochastic approximation (RDSA) by utilizing only noisy function measurements. The form of each estimator and the order of the bias depend on the number of function measurements. In particular, we demonstrate that estimators with more function measurements exhibit lower-order estimation bias. We show the asymptotic unbiasedness of the estimators. We also perform asymptotic and non-asymptotic convergence analyses for stochastic Newton methods that incorporate our generalized Hessian estimators. Finally, we perform numerical experiments to validate our theoretical findings.]]></description>
<pubDate>Mon, 23 Feb 2026 14:33:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images]]></title>
<link>http://arxiv.org/abs/2602.19891v1</link>
<guid>2602.19891v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Wen-Liang Lin, Yun-Chien Cheng
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by "domain shift" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on enhancing pseudo-label reliability by learning deep structural information within the feature space. Specifically, three modules are integrated and designed for this task: (1) a Prototype Alignment (PA) mechanism to reduce category-level distribution discrepancies; (2) Global and Local Contrastive Learning (GLCL) to capture both pixel-level topological relationships and global semantic representations; and (3) an Attention-based Auxiliary Local Prediction (AALP) module designed to reinforce sensitivity to small PE lesions by automatically extracting high-information slices from Transformer attention maps. Experimental validation conducted on cross-center datasets (FUMPE and CAD-PE) demonstrates significant performance gains. In the FUMPE -> CAD-PE task, the IoU increased from 0.1152 to 0.4153, while the CAD-PE -> FUMPE task saw an improvement from 0.1705 to 0.4302. Furthermore, the proposed method achieved a 69.9% Dice score in the CT -> MRI cross-modality task on the MMWHS dataset without utilizing any target-domain labels for model selection, confirming its robustness and generalizability for diverse clinical environments.]]></description>
<pubDate>Mon, 23 Feb 2026 14:33:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[Denotational Semantics for ODRL: Knowledge-Based Constraint Conflict Detection]]></title>
<link>http://arxiv.org/abs/2602.19883v1</link>
<guid>2602.19883v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.LO
Authors: Daham Mustafa, Diego Collarana, Yixin Peng, Rafiqul Haque, Christoph Lange-Bever et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

ODRL's six set-based operators -- isA, isPartOf, hasPart, isAnyOf, isAllOf, isNoneOf -- depend on external domain knowledge that the W3C specification leaves unspecified. Without it, every cross-dataspace policy comparison defaults to Unknown. We present a denotational semantics that maps each ODRL constraint to the set of knowledge-base concepts satisfying it. Conflict detection reduces to denotation intersection under a three-valued verdict -- Conflict, Compatible, or Unknown -- that is sound under incomplete knowledge. The framework covers all three ODRL composition modes (and, or, xone) and all three semantic domains arising in practice: taxonomic (class subsumption), mereological (part-whole containment), and nominal (identity). For cross-dataspace interoperability, we define order-preserving alignments between knowledge bases and prove two guarantees: conflicts are preserved across different KB standards, and unmapped concepts degrade gracefully to Unknown -- never to false conflicts. A runtime soundness theorem ensures that design-time verdicts hold for all execution contexts. The encoding stays within the decidable EPR fragment of first-order logic. We validate it with 154 benchmarks across six knowledge base families (GeoNames, ISO 3166, W3C DPV, a GDPR-derived taxonomy, BCP 47, and ISO 639-3) and four structural KBs targeting adversarial edge cases. Both the Vampire theorem prover and the Z3 SMT solver agree on all 154 verdicts. A key finding is that exclusive composition (xone) requires strictly stronger KB axioms than conjunction or disjunction: open-world semantics blocks exclusivity even when positive evidence appears to satisfy exactly one branch.]]></description>
<pubDate>Mon, 23 Feb 2026 14:28:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[Make Some Noise: Unsupervised Remote Sensing Change Detection Using Latent Space Perturbations]]></title>
<link>http://arxiv.org/abs/2602.19881v1</link>
<guid>2602.19881v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: BlaÅ¾ Rolih, Matic FuÄka, Filip Wolf, Luka Äehovin Zajc
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Unsupervised change detection (UCD) in remote sensing aims to localise semantic changes between two images of the same region without relying on labelled data during training. Most recent approaches rely either on frozen foundation models in a training-free manner or on training with synthetic changes generated in pixel space. Both strategies inherently rely on predefined assumptions about change types, typically introduced through handcrafted rules, external datasets, or auxiliary generative models. Due to these assumptions, such methods fail to generalise beyond a few change types, limiting their real-world usage, especially in rare or complex scenarios. To address this, we propose MaSoN (Make Some Noise), an end-to-end UCD framework that synthesises diverse changes directly in the latent feature space during training. It generates changes that are dynamically estimated using feature statistics of target data, enabling diverse yet data-driven variation aligned with the target domain. It also easily extends to new modalities, such as SAR. MaSoN generalises strongly across diverse change types and achieves state-of-the-art performance on five benchmarks, improving the average F1 score by 14.1 percentage points. Project page: https://blaz-r.github.io/mason_ucd]]></description>
<pubDate>Mon, 23 Feb 2026 14:27:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[Axis Decomposition for ODRL: Resolving Dimensional Ambiguity in Policy Constraints through Interval Semantics]]></title>
<link>http://arxiv.org/abs/2602.19878v1</link>
<guid>2602.19878v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL, cs.LO
Authors: Daham Mustafa, Diego Collarana, Yixin Peng, Rafiqul Haque, Christoph Lange-Bever et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Every ODRL 2.2 constraint compares a single scalar value: (leftOperand, operator, rightOperand). Five of ODRL's approximately 34 left operands, however, denote multi-dimensional quantities--image dimensions, canvas positions, geographic coordinates--whose specification text explicitly references multiple axes. For these operands, a single scalar constraint admits one interpretation per axis, making policy evaluation non-deterministic.
  We classify ODRL's left operands by value-domain structure (scalar, dimensional, concept-valued), grounded in the ODRL 2.2 specification text, and show that dimensional ambiguity is intrinsic to the constraint syntax.
  We present an axis-decomposition framework that refines each dimensional operand into axis-specific scalar operands and prove four properties: deterministic interpretation, AABB completeness, sound over-approximation under projection, and conservative extension.
  Conflict detection operates in two layers: per-axis verdicts are always decidable; box-level verdicts compose through Strong Kleene conjunction into a three-valued logic (Conflict, Compatible, Unknown). For ODRL's disjunctive (odrl:or) and exclusive-or (odrl:xone) logical constraints, where per-axis decomposition does not apply, the framework encodes coupled multi-axis conjectures directly.
  We instantiate the framework as the ODRL Spatial Axis Profile--15 axis-specific left operands for the five affected base terms--and evaluate it on 117 benchmark problems spanning nine categories across both TPTP FOF (Vampire) and SMT-LIB (Z3) encodings, achieving full concordance between provers. Benchmark scenarios are inspired by constraints arising in cultural heritage dataspaces such as Datenraum Kultur. All meta-theorems are mechanically verified in Isabelle/HOL.]]></description>
<pubDate>Mon, 23 Feb 2026 14:24:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[BigMaQ: A Big Macaque Motion and Animation Dataset Bridging Image and 3D Pose Representations]]></title>
<link>http://arxiv.org/abs/2602.19874v1</link>
<guid>2602.19874v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Lucas Martini, Alexander Lappe, Anna BognÃ¡r, Rufin Vogels, Martin A. Giese
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The recognition of dynamic and social behavior in animals is fundamental for advancing ethology, ecology, medicine and neuroscience. Recent progress in deep learning has enabled automated behavior recognition from video, yet an accurate reconstruction of the three-dimensional (3D) pose and shape has not been integrated into this process. Especially for non-human primates, mesh-based tracking efforts lag behind those for other species, leaving pose descriptions restricted to sparse keypoints that are unable to fully capture the richness of action dynamics. To address this gap, we introduce the $\textbf{Big Ma}$ca$\textbf{Q}$ue 3D Motion and Animation Dataset ($\texttt{BigMaQ}$), a large-scale dataset comprising more than 750 scenes of interacting rhesus macaques with detailed 3D pose descriptions. Extending previous surface-based animal tracking methods, we construct subject-specific textured avatars by adapting a high-quality macaque template mesh to individual monkeys. This allows us to provide pose descriptions that are more accurate than previous state-of-the-art surface-based animal tracking methods. From the original dataset, we derive BigMaQ500, an action recognition benchmark that links surface-based pose vectors to single frames across multiple individual monkeys. By pairing features extracted from established image and video encoders with and without our pose descriptors, we demonstrate substantial improvements in mean average precision (mAP) when pose information is included. With these contributions, $\texttt{BigMaQ}$ establishes the first dataset that both integrates dynamic 3D pose-shape representations into the learning task of animal action recognition and provides a rich resource to advance the study of visual appearance, posture, and social interaction in non-human primates. The code and data are publicly available at https://martinivis.github.io/BigMaQ/ .]]></description>
<pubDate>Mon, 23 Feb 2026 14:21:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[GOAL: Geometrically Optimal Alignment for Continual Generalized Category Discovery]]></title>
<link>http://arxiv.org/abs/2602.19872v1</link>
<guid>2602.19872v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Jizhou Han, Chenhao Ding, SongLin Dong, Yuhang He, Shaokun Wang et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Continual Generalized Category Discovery (C-GCD) requires identifying novel classes from unlabeled data while retaining knowledge of known classes over time. Existing methods typically update classifier weights dynamically, resulting in forgetting and inconsistent feature alignment. We propose GOAL, a unified framework that introduces a fixed Equiangular Tight Frame (ETF) classifier to impose a consistent geometric structure throughout learning. GOAL conducts supervised alignment for labeled samples and confidence-guided alignment for novel samples, enabling stable integration of new classes without disrupting old ones. Experiments on four benchmarks show that GOAL outperforms the prior method Happy, reducing forgetting by 16.1% and boosting novel class discovery by 3.2%, establishing a strong solution for long-horizon continual discovery.]]></description>
<pubDate>Mon, 23 Feb 2026 14:15:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[ApET: Approximation-Error Guided Token Compression for Efficient VLMs]]></title>
<link>http://arxiv.org/abs/2602.19870v1</link>
<guid>2602.19870v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Qiankun Ma, Ziyao Zhang, Haofei Wang, Jie Chen, Zhen Song et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.]]></description>
<pubDate>Mon, 23 Feb 2026 14:15:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation]]></title>
<link>http://arxiv.org/abs/2602.19863v1</link>
<guid>2602.19863v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Filip Wolf, BlaÅ¾ Rolih, Luka Äehovin Zajc
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastive distillation framework for multispectral imagery that aligns the student's pretraining objective with the contrastive self-distillation paradigm of modern optical vision foundation models (VFMs). Our approach combines a multispectral teacher with an optical VFM teacher, enabling coherent cross-modal representation learning. Experiments across diverse optical and multispectral benchmarks show that our model adapts to multispectral data without compromising performance on optical-only inputs, achieving state-of-the-art results in both settings, with an average improvement of 3.64 percentage points in semantic segmentation, 1.2 in change detection, and 1.31 in classification tasks. This demonstrates that contrastive distillation provides a principled and efficient approach to scalable representation learning across heterogeneous EO data sources. Code: Coming soon.]]></description>
<pubDate>Mon, 23 Feb 2026 14:09:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[Dirichlet Scale Mixture Priors for Bayesian Neural Networks]]></title>
<link>http://arxiv.org/abs/2602.19859v1</link>
<guid>2602.19859v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: August Arnstad, Leiv RÃ¸nneberg, Geir Storvik
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Neural networks are the cornerstone of modern machine learning, yet can be difficult to interpret, give overconfident predictions and are vulnerable to adversarial attacks. Bayesian neural networks (BNNs) provide some alleviation of these limitations, but have problems of their own. The key step of specifying prior distributions in BNNs is no trivial task, yet is often skipped out of convenience. In this work, we propose a new class of prior distributions for BNNs, the Dirichlet scale mixture (DSM) prior, that addresses current limitations in Bayesian neural networks through structured, sparsity-inducing shrinkage. Theoretically, we derive general dependence structures and shrinkage results for DSM priors and show how they manifest under the geometry induced by neural networks. In experiments on simulated and real world data we find that the DSM priors encourages sparse networks through implicit feature selection, show robustness under adversarial attacks and deliver competitive predictive performance with substantially fewer effective parameters. In particular, their advantages appear most pronounced in correlated, moderately small data regimes, and are more amenable to weight pruning. Moreover, by adopting heavy-tailed shrinkage mechanisms, our approach aligns with recent findings that such priors can mitigate the cold posterior effect, offering a principled alternative to the commonly used Gaussian priors.]]></description>
<pubDate>Mon, 23 Feb 2026 13:58:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Contrastive meta-domain adaptation for robust skin lesion classification across clinical and acquisition conditions]]></title>
<link>http://arxiv.org/abs/2602.19857v1</link>
<guid>2602.19857v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Rodrigo Mota, Kelvin Cunha, Emanoel dos Santos, FÃ¡bio Papais, Francisco Filho et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Deep learning models for dermatological image analysis remain sensitive to acquisition variability and domain-specific visual characteristics, leading to performance degradation when deployed in clinical settings. We investigate how visual artifacts and domain shifts affect deep learning-based skin lesion classification. We propose an adaptation strategy, grounded in the idea of visual meta-domains, that transfers visual representations from larger dermoscopic datasets into clinical image domains, thereby improving generalization robustness. Experiments across multiple dermatology datasets show consistent gains in classification performance and reduced gaps between dermoscopic and clinical images. These results emphasize the importance of domain-aware training for deployable systems.]]></description>
<pubDate>Mon, 23 Feb 2026 13:56:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[SHIELD: Semantic Heterogeneity Integrated Embedding for Latent Discovery in Clinical Trial Safety Signals]]></title>
<link>http://arxiv.org/abs/2602.19855v1</link>
<guid>2602.19855v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Francois Vandenhende, Anna Georgiou, Theodoros Psaras, Ellie Karekla
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present SHIELD, a novel methodology for automated and integrated safety signal detection in clinical trials. SHIELD combines disproportionality analysis with semantic clustering of adverse event (AE) terms applied to MedDRA term embeddings. For each AE, the pipeline computes an information-theoretic disproportionality measure (Information Component) with effect size derived via empirical Bayesian shrinkage. A utility matrix is constructed by weighting semantic term-term similarities by signal magnitude, followed by spectral embedding and clustering to identify groups of related AEs. Resulting clusters are annotated with syndrome-level summary labels using large language models, yielding a coherent, data-driven representation of treatment-associated safety profiles in the form of a network graph and hierarchical tree. We implement the SHIELD framework in the context of a single-arm incidence summary, to compare two treatment arms or for the detection of any treatment effect in a multi-arm trial. We illustrate its ability to recover known safety signals and generate interpretable, cluster-based summaries in a real clinical trial example. This work bridges statistical signal detection with modern natural language processing to enhance safety assessment and causal interpretation in clinical trials.]]></description>
<pubDate>Mon, 23 Feb 2026 13:55:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[Orthogonal Uplift Learning with Permutation-Invariant Representations for Combinatorial Treatments]]></title>
<link>http://arxiv.org/abs/2602.19851v1</link>
<guid>2602.19851v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ME, cs.LG
Authors: Xinyan Su, Jiacan Gao, Mingyuan Ma, Xiao Xu, Xinrui Wan et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We study uplift estimation for combinatorial treatments. Uplift measures the pure incremental causal effect of an intervention (e.g., sending a coupon or a marketing message) on user behavior, modeled as a conditional individual treatment effect. Many real-world interventions are combinatorial: a treatment is a policy that specifies context-dependent action distributions rather than a single atomic label. Although recent work considers structured treatments, most methods rely on categorical or opaque encodings, limiting robustness and generalization to rare or newly deployed policies. We propose an uplift estimation framework that aligns treatment representation with causal semantics. Each policy is represented by the mixture it induces over contextaction components and embedded via a permutation-invariant aggregation. This representation is integrated into an orthogonalized low-rank uplift model, extending Robinson-style decompositions to learned, vector-valued treatments. We show that the resulting estimator is expressive for policy-induced causal effects, orthogonally robust to nuisance estimation errors, and stable under small policy perturbations. Experiments on large-scale randomized platform data demonstrate improved uplift accuracy and stability in long-tailed policy regimes]]></description>
<pubDate>Mon, 23 Feb 2026 13:54:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation]]></title>
<link>http://arxiv.org/abs/2602.19848v1</link>
<guid>2602.19848v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Francisco Filho, Kelvin Cunha, FÃ¡bio Papais, Emanoel dos Santos, Rodrigo Mota et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use.]]></description>
<pubDate>Mon, 23 Feb 2026 13:52:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[I Dropped a Neural Net]]></title>
<link>http://arxiv.org/abs/2602.19845v1</link>
<guid>2602.19845v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Hyunwoo Park
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

A recent Dwarkesh Patel podcast with John Collison and Elon Musk featured an interesting puzzle from Jane Street: they trained a neural net, shuffled all 96 layers, and asked to put them back in order.
  Given unlabelled layers of a Residual Network and its training dataset, we recover the exact ordering of the layers. The problem decomposes into pairing each block's input and output projections ($48!$ possibilities) and ordering the reassembled blocks ($48!$ possibilities), for a combined search space of $(48!)^2 \approx 10^{122}$, which is more than the atoms in the observable universe. We show that stability conditions during training like dynamic isometry leave the product $W_{\text{out}} W_{\text{in}}$ for correctly paired layers with a negative diagonal structure, allowing us to use diagonal dominance ratio as a signal for pairing. For ordering, we seed-initialize with a rough proxy such as delta-norm or $\|W_{\text{out}}\|_F$ then hill-climb to zero mean squared error.]]></description>
<pubDate>Mon, 23 Feb 2026 13:49:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[LLM-enabled Applications Require System-Level Threat Monitoring]]></title>
<link>http://arxiv.org/abs/2602.19844v1</link>
<guid>2602.19844v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CR, cs.AI, cs.SE
Authors: Yedi Zhang, Haoyu Wang, Xianglin Yang, Jin Song Dong, Jun Sun
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.]]></description>
<pubDate>Mon, 23 Feb 2026 13:48:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems]]></title>
<link>http://arxiv.org/abs/2602.19843v1</link>
<guid>2602.19843v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SE, cs.AI
Authors: Jin Jia, Zhiling Deng, Zhuangbin Chen, Yingqi Wang, Zibin Zheng
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.]]></description>
<pubDate>Mon, 23 Feb 2026 13:47:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation]]></title>
<link>http://arxiv.org/abs/2602.19840v1</link>
<guid>2602.19840v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Jingzhuo Wu, Jiajun Zhang, Keyan Jin, Dehua Ma, Junbo Wang
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal processing task. Specifically, our method quantifies literary style into a Stylistic Feature Spectrum (SFS) using the wavelet packet transform. This SFS serves as a control signal to dynamically assemble a tailored workflow of specialized translation agents based on the source text's structural patterns. Extensive experiments on translation benchmarks show that SAMAS achieves competitive semantic accuracy against strong baselines, primarily by leveraging its statistically significant advantage in style fidelity.]]></description>
<pubDate>Mon, 23 Feb 2026 13:40:44 +0000</pubDate>
</item>
<item>
<title><![CDATA[Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent]]></title>
<link>http://arxiv.org/abs/2602.19837v1</link>
<guid>2602.19837v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.LG
Authors: BjÃ¶rn Hoppmann, Christoph Scholz
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.]]></description>
<pubDate>Mon, 23 Feb 2026 13:39:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[M3S-Net: Multimodal Feature Fusion Network Based on Multi-scale Data for Ultra-short-term PV Power Forecasting]]></title>
<link>http://arxiv.org/abs/2602.19832v1</link>
<guid>2602.19832v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Penghui Niu, Taotao Cai, Suqi Zhang, Junhua Gu, Ping Zhang et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The inherent intermittency and high-frequency variability of solar irradiance, particularly during rapid cloud advection, present significant stability challenges to high-penetration photovoltaic grids. Although multimodal forecasting has emerged as a viable mitigation strategy, existing architectures predominantly rely on shallow feature concatenation and binary cloud segmentation, thereby failing to capture the fine-grained optical features of clouds and the complex spatiotemporal coupling between visual and meteorological modalities. To bridge this gap, this paper proposes M3S-Net, a novel multimodal feature fusion network based on multi-scale data for ultra-short-term PV power forecasting. First, a multi-scale partial channel selection network leverages partial convolutions to explicitly isolate the boundary features of optically thin clouds, effectively transcending the precision limitations of coarse-grained binary masking. Second, a multi-scale sequence to image analysis network employs Fast Fourier Transform (FFT)-based time-frequency representation to disentangle the complex periodicity of meteorological data across varying time horizons. Crucially, the model incorporates a cross-modal Mamba interaction module featuring a novel dynamic C-matrix swapping mechanism. By exchanging state-space parameters between visual and temporal streams, this design conditions the state evolution of one modality on the context of the other, enabling deep structural coupling with linear computational complexity, thus overcoming the limitations of shallow concatenation. Experimental validation on the newly constructed fine-grained PV power dataset demonstrates that M3S-Net achieves a mean absolute error reduction of 6.2% in 10-minute forecasts compared to state-of-the-art baselines. The dataset and source code will be available at https://github.com/she1110/FGPD.]]></description>
<pubDate>Mon, 23 Feb 2026 13:30:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[TextShield-R1: Reinforced Reasoning for Tampered Text Detection]]></title>
<link>http://arxiv.org/abs/2602.19828v1</link>
<guid>2602.19828v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Chenfan Qu, Yiwu Zhong, Jian Liu, Xuekang Zhu, Bohan Yu et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The growing prevalence of tampered images poses serious security threats, highlighting the urgent need for reliable detection methods. Multimodal large language models (MLLMs) demonstrate strong potential in analyzing tampered images and generating interpretations. However, they still struggle with identifying micro-level artifacts, exhibit low accuracy in localizing tampered text regions, and heavily rely on expensive annotations for forgery interpretation. To this end, we introduce TextShield-R1, the first reinforcement learning based MLLM solution for tampered text detection and reasoning. Specifically, our approach introduces Forensic Continual Pre-training, an easy-to-hard curriculum that well prepares the MLLM for tampered text detection by harnessing the large-scale cheap data from natural image forensic and OCR tasks. During fine-tuning, we perform Group Relative Policy Optimization with novel reward functions to reduce annotation dependency and improve reasoning capabilities. At inference time, we enhance localization accuracy via OCR Rectification, a method that leverages the MLLM's strong text recognition abilities to refine its predictions. Furthermore, to support rigorous evaluation, we introduce the Text Forensics Reasoning (TFR) benchmark, comprising over 45k real and tampered images across 16 languages, 10 tampering techniques, and diverse domains. Rich reasoning-style annotations are included, allowing for comprehensive assessment. Our TFR benchmark simultaneously addresses seven major limitations of existing benchmarks and enables robust evaluation under cross-style, cross-method, and cross-language conditions. Extensive experiments demonstrate that TextShield-R1 significantly advances the state of the art in interpretable tampered text detection.]]></description>
<pubDate>Mon, 23 Feb 2026 13:26:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[Open-vocabulary 3D scene perception in industrial environments]]></title>
<link>http://arxiv.org/abs/2602.19823v1</link>
<guid>2602.19823v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Keno Moenck, Adrian Philip Florea, Julian Koch, Thorsten SchÃ¼ppstuhl
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Autonomous vision applications in production, intralogistics, or manufacturing environments require perception capabilities beyond a small, fixed set of classes. Recent open-vocabulary methods, leveraging 2D Vision-Language Foundation Models (VLFMs), target this task but often rely on class-agnostic segmentation models pre-trained on non-industrial datasets (e.g., household scenes). In this work, we first demonstrate that such models fail to generalize, performing poorly on common industrial objects. Therefore, we propose a training-free, open-vocabulary 3D perception pipeline that overcomes this limitation. Instead of using a pre-trained model to generate instance proposals, our method simply generates masks by merging pre-computed superpoints based on their semantic features. Following, we evaluate the domain-adapted VLFM "IndustrialCLIP" on a representative 3D industrial workshop scene for open-vocabulary querying. Our qualitative results demonstrate successful segmentation of industrial objects.]]></description>
<pubDate>Mon, 23 Feb 2026 13:22:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[Efficient endometrial carcinoma screening via cross-modal synthesis and gradient distillation]]></title>
<link>http://arxiv.org/abs/2602.19822v1</link>
<guid>2602.19822v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Dongjing Shan, Yamei Luo, Jiqing Xuan, Lu Huang, Jin Li et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Early detection of myometrial invasion is critical for the staging and life-saving management of endometrial carcinoma (EC), a prevalent global malignancy. Transvaginal ultrasound serves as the primary, accessible screening modality in resource-constrained primary care settings; however, its diagnostic reliability is severely hindered by low tissue contrast, high operator dependence, and a pronounced scarcity of positive pathological samples. Existing artificial intelligence solutions struggle to overcome this severe class imbalance and the subtle imaging features of invasion, particularly under the strict computational limits of primary care clinics. Here we present an automated, highly efficient two-stage deep learning framework that resolves both data and computational bottlenecks in EC screening. To mitigate pathological data scarcity, we develop a structure-guided cross-modal generation network that synthesizes diverse, high-fidelity ultrasound images from unpaired magnetic resonance imaging (MRI) data, strictly preserving clinically essential anatomical junctions. Furthermore, we introduce a lightweight screening network utilizing gradient distillation, which transfers discriminative knowledge from a high-capacity teacher model to dynamically guide sparse attention towards task-critical regions. Evaluated on a large, multicenter cohort of 7,951 participants, our model achieves a sensitivity of 99.5\%, a specificity of 97.2\%, and an area under the curve of 0.987 at a minimal computational cost (0.289 GFLOPs), substantially outperforming the average diagnostic accuracy of expert sonographers. Our approach demonstrates that combining cross-modal synthetic augmentation with knowledge-driven efficient modeling can democratize expert-level, real-time cancer screening for resource-constrained primary care settings.]]></description>
<pubDate>Mon, 23 Feb 2026 13:22:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[SafePickle: Robust and Generic ML Detection of Malicious Pickle-based ML Models]]></title>
<link>http://arxiv.org/abs/2602.19818v1</link>
<guid>2602.19818v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CR, cs.AI
Authors: Hillel Ohayon, Daniel Gilkarov, Ran Dubin
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Model repositories such as Hugging Face increasingly distribute machine learning artifacts serialized with Python's pickle format, exposing users to remote code execution (RCE) risks during model loading. Recent defenses, such as PickleBall, rely on per-library policy synthesis that requires complex system setups and verified benign models, which limits scalability and generalization. In this work, we propose a lightweight, machine-learning-based scanner that detects malicious Pickle-based files without policy generation or code instrumentation. Our approach statically extracts structural and semantic features from Pickle bytecode and applies supervised and unsupervised models to classify files as benign or malicious. We construct and release a labeled dataset of 727 Pickle-based files from Hugging Face and evaluate our models on four datasets: our own, PickleBall (out-of-distribution), Hide-and-Seek (9 advanced evasive malicious models), and synthetic joblib files. Our method achieves 90.01% F1-score compared with 7.23%-62.75% achieved by the SOTA scanners (Modelscan, Fickling, ClamAV, VirusTotal) on our dataset. Furthermore, on the PickleBall data (OOD), it achieves 81.22% F1-score compared with 76.09% achieved by the PickleBall method, while remaining fully library-agnostic. Finally, we show that our method is the only one to correctly parse and classify 9/9 evasive Hide-and-Seek malicious models specially crafted to evade scanners. This demonstrates that data-driven detection can effectively and generically mitigate Pickle-based model file attacks.]]></description>
<pubDate>Mon, 23 Feb 2026 13:19:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling]]></title>
<link>http://arxiv.org/abs/2602.19816v1</link>
<guid>2602.19816v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SD, cs.AI, cs.LG
Authors: Yungang Yi
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources.]]></description>
<pubDate>Mon, 23 Feb 2026 13:13:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[Keyboards for the Endangered Idu Mishmi Language]]></title>
<link>http://arxiv.org/abs/2602.19815v1</link>
<guid>2602.19815v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Akhilesh Kakolu Ramarao
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present a mobile and desktop keyboard suite for Idu Mishmi, an endangered Trans-Himalayan language spoken by approximately 11,000 people in Arunachal Pradesh, India. Although a Latin-based orthography was developed in 2018, no digital input tools existed to use it, forcing speakers into ad-hoc romanizations that cannot represent the full writing system. Our keyboards comprise two tools: (1) an Android mobile keyboard, published on the Google Play Store and actively used in teacher training programs, and (2) a Windows desktop keyboard currently undergoing community testing. Both tools support the complete Idu Mishmi character inventory, including schwa, retracted schwa, nasalized vowels, and accented forms. Both operate fully offline with zero network permissions, addressing connectivity constraints and data sovereignty concerns. We describe the design, implementation, and deployment as a replicable model for other endangered language communities.]]></description>
<pubDate>Mon, 23 Feb 2026 13:13:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research]]></title>
<link>http://arxiv.org/abs/2602.19810v1</link>
<guid>2602.19810v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Lukas Weidener, Marko BrkiÄ, Mihailo JovanoviÄ, Ritvik Singh, Emre Ulgac et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of that ecosystem and presents ClawdLab, an open-source platform for autonomous scientific research, as a design science response to the architectural failure modes identified. The literature documents emergent collective phenomena, security vulnerabilities spanning 131 agent skills and over 15,200 exposed control panels, and five recurring architectural patterns. ClawdLab addresses these failure modes through hard role restrictions, structured adversarial critique, PI-led governance, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints that ground validation in computational tool outputs rather than social consensus; the architecture provides emergent Sybil resistance as a structural consequence. A three-tier taxonomy distinguishes single-agent pipelines, predetermined multi-agent workflows, and fully decentralised systems, analysing why leading AI co-scientist platforms remain confined to the first two tiers. ClawdLab's composable third-tier architecture, in which foundation models, capabilities, governance, and evidence requirements are independently modifiable, enables compounding improvement as the broader AI ecosystem advances.]]></description>
<pubDate>Mon, 23 Feb 2026 13:10:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[Decision MetaMamba: Enhancing Selective SSM in Offline RL with Heterogeneous Sequence Mixing]]></title>
<link>http://arxiv.org/abs/2602.19805v1</link>
<guid>2602.19805v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Wall Kim, Chaeyoung Song, Hanul Kim
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Mamba-based models have drawn much attention in offline RL. However, their selective mechanism often detrimental when key steps in RL sequences are omitted. To address these issues, we propose a simple yet effective structure, called Decision MetaMamba (DMM), which replaces Mamba's token mixer with a dense layer-based sequence mixer and modifies positional structure to preserve local information. By performing sequence mixing that considers all channels simultaneously before Mamba, DMM prevents information loss due to selective scanning and residual gating. Extensive experiments demonstrate that our DMM delivers the state-of-the-art performance across diverse RL tasks. Furthermore, DMM achieves these results with a compact parameter footprint, demonstrating strong potential for real-world applications.]]></description>
<pubDate>Mon, 23 Feb 2026 13:03:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Linear Reservoir: A Diagonalization-Based Optimization]]></title>
<link>http://arxiv.org/abs/2602.19802v1</link>
<guid>2602.19802v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.DC, cs.NE
Authors: Romain de Coudenhove, Yannis Bendi-Ouis, Anthony Strock, Xavier Hinaut
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We introduce a diagonalization-based optimization for Linear Echo State Networks (ESNs) that reduces the per-step computational complexity of reservoir state updates from O(N^2) to O(N). By reformulating reservoir dynamics in the eigenbasis of the recurrent matrix, the recurrent update becomes a set of independent element-wise operations, eliminating the matrix multiplication. We further propose three methods to use our optimization depending on the situation: (i) Eigenbasis Weight Transformation (EWT), which preserves the dynamics of standard and trained Linear ESNs, (ii) End-to-End Eigenbasis Training (EET), which directly optimizes readout weights in the transformed space and (iii) Direct Parameter Generation (DPG), that bypasses matrix diagonalization by directly sampling eigenvalues and eigenvectors, achieving comparable performance than standard Linear ESNs. Across all experiments, both our methods preserve predictive accuracy while offering significant computational speedups, making them a replacement of standard Linear ESNs computations and training, and suggesting a shift of paradigm in linear ESN towards the direct selection of eigenvalues.]]></description>
<pubDate>Mon, 23 Feb 2026 12:58:34 +0000</pubDate>
</item>
<item>
<title><![CDATA[Path-conditioned training: a principled way to rescale ReLU neural networks]]></title>
<link>http://arxiv.org/abs/2602.19799v1</link>
<guid>2602.19799v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Arthur Lebeurrier, Titouan Vayer, RÃ©mi Gribonval
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Despite recent algorithmic advances, we still lack principled ways to leverage the well-documented rescaling symmetries in ReLU neural network parameters. While two properly rescaled weights implement the same function, the training dynamics can be dramatically different. To offer a fresh perspective on exploiting this phenomenon, we build on the recent path-lifting framework, which provides a compact factorization of ReLU networks. We introduce a geometrically motivated criterion to rescale neural network parameters which minimization leads to a conditioning strategy that aligns a kernel in the path-lifting space with a chosen reference. We derive an efficient algorithm to perform this alignment. In the context of random network initialization, we analyze how the architecture and the initialization scale jointly impact the output of the proposed method. Numerical experiments illustrate its potential to speed up training.]]></description>
<pubDate>Mon, 23 Feb 2026 12:55:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Drift Localization using Conformal Predictions]]></title>
<link>http://arxiv.org/abs/2602.19790v1</link>
<guid>2602.19790v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.ML
Authors: Fabian Hinder, Valerie Vaquet, Johannes Brinkrolf, Barbara Hammer
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Concept drift -- the change of the distribution over time -- poses significant challenges for learning systems and is of central interest for monitoring. Understanding drift is thus paramount, and drift localization -- determining which samples are affected by the drift -- is essential. While several approaches exist, most rely on local testing schemes, which tend to fail in high-dimensional, low-signal settings. In this work, we consider a fundamentally different approach based on conformal predictions. We discuss and show the shortcomings of common approaches and demonstrate the performance of our approach on state-of-the-art image datasets.]]></description>
<pubDate>Mon, 23 Feb 2026 12:46:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Stop Preaching and Start Practising Data Frugality for Responsible Development of AI]]></title>
<link>http://arxiv.org/abs/2602.19789v1</link>
<guid>2602.19789v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.CY
Authors: Sophia N. Wilson, GuÃ°rÃºn FjÃ³la GuÃ°mundsdÃ³ttir, Andrew Millard, Raghavendra Selvan, Sebastian Mair
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This position paper argues that the machine learning community must move from preaching to practising data frugality for responsible artificial intelligence (AI) development. For long, progress has been equated with ever-larger datasets, driving remarkable advances but now yielding increasingly diminishing performance gains alongside rising energy use and carbon emissions. While awareness of data frugal approaches has grown, their adoption has remained rhetorical, and data scaling continues to dominate development practice. We argue that this gap between preach and practice must be closed, as continued data scaling entails substantial and under-accounted environmental impacts. To ground our position, we provide indicative estimates of the energy use and carbon emissions associated with the downstream use of ImageNet-1K. We then present empirical evidence that data frugality is both practical and beneficial, demonstrating that coreset-based subset selection can substantially reduce training energy consumption with little loss in accuracy, while also mitigating dataset bias. Finally, we outline actionable recommendations for moving data frugality from rhetorical preach to concrete practice for responsible development of AI.]]></description>
<pubDate>Mon, 23 Feb 2026 12:46:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Bayesian Meta-Learning with Expert Feedback for Task-Shift Adaptation through Causal Embeddings]]></title>
<link>http://arxiv.org/abs/2602.19788v1</link>
<guid>2602.19788v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Lotta MÃ¤kinen, Jorge LorÃ­a, Samuel Kaski
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Meta-learning methods perform well on new within-distribution tasks but often fail when adapting to out-of-distribution target tasks, where transfer from source tasks can induce negative transfer. We propose a causally-aware Bayesian meta-learning method, by conditioning task-specific priors on precomputed latent causal task embeddings, enabling transfer based on mechanistic similarity rather than spurious correlations. Our approach explicitly considers realistic deployment settings where access to target-task data is limited, and adaptation relies on noisy (expert-provided) pairwise judgments of causal similarity between source and target tasks. We provide a theoretical analysis showing that conditioning on causal embeddings controls prior mismatch and mitigates negative transfer under task shift. Empirically, we demonstrate reductions in negative transfer and improved out-of-distribution adaptation in both controlled simulations and a large-scale real-world clinical prediction setting for cross-disease transfer, where causal embeddings align with underlying clinical mechanisms.]]></description>
<pubDate>Mon, 23 Feb 2026 12:44:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[The Climate Change Knowledge Graph: Supporting Climate Services]]></title>
<link>http://arxiv.org/abs/2602.19786v1</link>
<guid>2602.19786v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.DB, cs.AI, cs.CY
Authors: Miguel Ceriani, Fiorela Ciroku, Alessandro Russo, Massimiliano Schembri, Fai Fung et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Climate change impacts a broad spectrum of human resources and activities, necessitating the use of climate models to project long-term effects and inform mitigation and adaptation strategies. These models generate multiple datasets by running simulations across various scenarios and configurations, thereby covering a range of potential future outcomes. Currently, researchers rely on traditional search interfaces and APIs to retrieve such datasets, often piecing together information from metadata and community vocabularies. The Climate Change Knowledge Graph is designed to address these challenges by integrating diverse data sources related to climate simulations into a coherent and interoperable knowledge graph. This innovative resource allows for executing complex queries involving climate models, simulations, variables, spatio-temporal domains, and granularities. Developed with input from domain experts, the knowledge graph and its underlying ontology are published with open access license and provide a comprehensive framework that enhances the exploration of climate data, facilitating more informed decision-making in addressing climate change issues.]]></description>
<pubDate>Mon, 23 Feb 2026 12:42:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Unsupervised Anomaly Detection in NSL-KDD Using $Î²$-VAE: A Latent Space and Reconstruction Error Approach]]></title>
<link>http://arxiv.org/abs/2602.19785v1</link>
<guid>2602.19785v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.NE, stat.ML
Authors: Dylan Baptiste, Ramla Saddem, Alexandre Philippot, FranÃ§ois Foyer
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

As Operational Technology increasingly integrates with Information Technology, the need for Intrusion Detection Systems becomes more important. This paper explores an unsupervised approach to anomaly detection in network traffic using $Î²$-Variational Autoencoders on the NSL-KDD dataset. We investigate two methods: leveraging the latent space structure by measuring distances from test samples to the training data projections, and using the reconstruction error as a conventional anomaly detection metric. By comparing these approaches, we provide insights into their respective advantages and limitations in an unsupervised setting. Experimental results highlight the effectiveness of latent space exploitation for classification tasks.]]></description>
<pubDate>Mon, 23 Feb 2026 12:42:00 +0000</pubDate>
</item>
<item>
<title><![CDATA[Addressing Instrument-Outcome Confounding in Mendelian Randomization through Representation Learning]]></title>
<link>http://arxiv.org/abs/2602.19782v1</link>
<guid>2602.19782v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Shimeng Huang, Matthew Robinson, Francesco Locatello
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Mendelian Randomization (MR) is a prominent observational epidemiological research method designed to address unobserved confounding when estimating causal effects. However, core assumptions -- particularly the independence between instruments and unobserved confounders -- are often violated due to population stratification or assortative mating. Leveraging the increasing availability of multi-environment data, we propose a representation learning framework that exploits cross-environment invariance to recover latent exogenous components of genetic instruments. We provide theoretical guarantees for identifying these latent instruments under various mixing mechanisms and demonstrate the effectiveness of our approach through simulations and semi-synthetic experiments using data from the All of Us Research Hub.]]></description>
<pubDate>Mon, 23 Feb 2026 12:38:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Enhancing Automatic Chord Recognition via Pseudo-Labeling and Knowledge Distillation]]></title>
<link>http://arxiv.org/abs/2602.19778v1</link>
<guid>2602.19778v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SD, cs.IR, cs.LG, cs.MM
Authors: Nghia Phan, Rong Jin, Gang Liu, Xiao Dong
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Automatic Chord Recognition (ACR) is constrained by the scarcity of aligned chord labels, as well-aligned annotations are costly to acquire. At the same time, open-weight pre-trained models are currently more accessible than their proprietary training data. In this work, we present a two-stage training pipeline that leverages pre-trained models together with unlabeled audio. The proposed method decouples training into two stages. In the first stage, we use a pre-trained BTC model as a teacher to generate pseudo-labels for over 1,000 hours of diverse unlabeled audio and train a student model solely on these pseudo-labels. In the second stage, the student is continually trained on ground-truth labels as they become available, with selective knowledge distillation (KD) from the teacher applied as a regularizer to prevent catastrophic forgetting of the representations learned in the first stage. In our experiments, two models (BTC, 2E1D) were used as students. In stage 1, using only pseudo-labels, the BTC student achieves over 98% of the teacher's performance, while the 2E1D model achieves about 96% across seven standard mir_eval metrics. After a single training run for both students in stage 2, the resulting BTC student model surpasses the traditional supervised learning baseline by 2.5% and the original pre-trained teacher model by 1.55% on average across all metrics. And the resulting 2E1D student model improves from the traditional supervised learning baseline by 3.79% on average and achieves almost the same performance as the teacher. Both cases show the large gains on rare chord qualities.]]></description>
<pubDate>Mon, 23 Feb 2026 12:32:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Exact Discrete Stochastic Simulation with Deep-Learning-Scale Gradient Optimization]]></title>
<link>http://arxiv.org/abs/2602.19775v1</link>
<guid>2602.19775v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jose M. G. Vilar, Leonor Saiz
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Exact stochastic simulation of continuous-time Markov chains (CTMCs) is essential when discreteness and noise drive system behavior, but the hard categorical event selection in Gillespie-type algorithms blocks gradient-based learning. We eliminate this constraint by decoupling forward simulation from backward differentiation, with hard categorical sampling generating exact trajectories and gradients propagating through a continuous massively-parallel Gumbel-Softmax straight-through surrogate. Our approach enables accurate optimization at parameter scales over four orders of magnitude beyond existing simulators. We validate for accuracy, scalability, and reliability on a reversible dimerization model (0.09% error), a genetic oscillator (1.2% error), a 203,796-parameter gene regulatory network achieving 98.4% MNIST accuracy (a prototypical deep-learning multilayer perceptron benchmark), and experimental patch-clamp recordings of ion channel gating (R^2 = 0.987) in the single-channel regime. Our GPU implementation delivers 1.9 billion steps per second, matching the scale of non-differentiable simulators. By making exact stochastic simulation massively parallel and autodiff-compatible, our results enable high-dimensional parameter inference and inverse design across systems biology, chemical kinetics, physics, and related CTMC-governed domains.]]></description>
<pubDate>Mon, 23 Feb 2026 12:29:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[The Confusion is Real: GRAPHIC - A Network Science Approach to Confusion Matrices in Deep Learning]]></title>
<link>http://arxiv.org/abs/2602.19770v1</link>
<guid>2602.19770v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Johanna S. FrÃ¶hlich, Bastian Heinlein, Jan U. Claar, Hans Rosenberger, Vasileios Belagiannis et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Explainable artificial intelligence has emerged as a promising field of research to address reliability concerns in artificial intelligence. Despite significant progress in explainable artificial intelligence, few methods provide a systematic way to visualize and understand how classes are confused and how their relationships evolve as training progresses. In this work, we present GRAPHIC, an architecture-agnostic approach that analyzes neural networks on a class level. It leverages confusion matrices derived from intermediate layers using linear classifiers. We interpret these as adjacency matrices of directed graphs, allowing tools from network science to visualize and quantify learning dynamics across training epochs and intermediate layers. GRAPHIC provides insights into linear class separability, dataset issues, and architectural behavior, revealing, for example, similarities between flatfish and man and labeling ambiguities validated in a human study. In summary, by uncovering real confusions, GRAPHIC offers new perspectives on how neural networks learn. The code is available at https://github.com/Johanna-S-Froehlich/GRAPHIC.]]></description>
<pubDate>Mon, 23 Feb 2026 12:20:37 +0000</pubDate>
</item>
<item>
<title><![CDATA[TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding]]></title>
<link>http://arxiv.org/abs/2602.19768v1</link>
<guid>2602.19768v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Fan Yang, Shurong Zheng, Hongyin Zhao, Yufei Zhan, Xin Li et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and explain associations between descriptions and specific regions. We propose TraceVision, a unified vision-language model integrating trajectory-aware spatial understanding in an end-to-end framework. TraceVision employs a Trajectory-aware Visual Perception (TVP) module for bidirectional fusion of visual features and trajectory information. We design geometric simplification to extract semantic keypoints from raw trajectories and propose a three-stage training pipeline where trajectories guide description generation and region localization. We extend TraceVision to trajectory-guided segmentation and video scene understanding, enabling cross-frame tracking and temporal attention analysis. We construct the Reasoning-based Interactive Localized Narratives (RILN) dataset to enhance logical reasoning and interpretability. Extensive experiments on trajectory-guided captioning, text-guided trajectory prediction, understanding, and segmentation demonstrate that TraceVision achieves state-of-the-art performance, establishing a foundation for intuitive spatial interaction and interpretable visual understanding.]]></description>
<pubDate>Mon, 23 Feb 2026 12:18:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[One2Scene: Geometric Consistent Explorable 3D Scene Generation from a Single Image]]></title>
<link>http://arxiv.org/abs/2602.19766v1</link>
<guid>2602.19766v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Pengfei Wang, Liyi Chen, Zhiyuan Ma, Yanjun Guo, Guowen Zhang et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generating explorable 3D scenes from a single image is a highly challenging problem in 3D vision. Existing methods struggle to support free exploration, often producing severe geometric distortions and noisy artifacts when the viewpoint moves far from the original perspective. We introduce \textbf{One2Scene}, an effective framework that decomposes this ill-posed problem into three tractable sub-tasks to enable immersive explorable scene generation. We first use a panorama generator to produce anchor views from a single input image as initialization. Then, we lift these 2D anchors into an explicit 3D geometric scaffold via a generalizable, feed-forward Gaussian Splatting network. Instead of treating the panorama as a single image for reconstruction, we project it into multiple sparse anchor views and reformulate the reconstruction task as multi-view stereo matching, which allows us to leverage robust geometric priors learned from large-scale multi-view datasets. A bidirectional feature fusion module is used to enforce cross-view consistency, yielding an efficient and geometrically reliable scaffold. Finally, the scaffold serves as a strong prior for a novel view generator to produce photorealistic and geometrically accurate views at arbitrary cameras. By explicitly conditioning on a 3D-consistent scaffold to perform reconstruction, One2Scene works stably under large camera motions, supporting immersive scene exploration. Extensive experiments show that One2Scene substantially outperforms state-of-the-art methods in panorama depth estimation, feed-forward 360Â° reconstruction, and explorable 3D scene generation. Code and models will be released.]]></description>
<pubDate>Mon, 23 Feb 2026 12:15:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[Training Deep Stereo Matching Networks on Tree Branch Imagery: A Benchmark Study for Real-Time UAV Forestry Applications]]></title>
<link>http://arxiv.org/abs/2602.19763v1</link>
<guid>2602.19763v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yida Lin, Bing Xue, Mengjie Zhang, Sam Schofield, Richard Green
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Autonomous drone-based tree pruning needs accurate, real-time depth estimation from stereo cameras. Depth is computed from disparity maps using $Z = f B/d$, so even small disparity errors cause noticeable depth mistakes at working distances. Building on our earlier work that identified DEFOM-Stereo as the best reference disparity generator for vegetation scenes, we present the first study to train and test ten deep stereo matching networks on real tree branch images. We use the Canterbury Tree Branches dataset -- 5,313 stereo pairs from a ZED Mini camera at 1080P and 720P -- with DEFOM-generated disparity maps as training targets. The ten methods cover step-by-step refinement, 3D convolution, edge-aware attention, and lightweight designs. Using perceptual metrics (SSIM, LPIPS, ViTScore) and structural metrics (SIFT/ORB feature matching), we find that BANet-3D produces the best overall quality (SSIM = 0.883, LPIPS = 0.157), while RAFT-Stereo scores highest on scene-level understanding (ViTScore = 0.799). Testing on an NVIDIA Jetson Orin Super (16 GB, independently powered) mounted on our drone shows that AnyNet reaches 6.99 FPS at 1080P -- the only near-real-time option -- while BANet-2D gives the best quality-speed balance at 1.21 FPS. We also compare 720P and 1080P processing times to guide resolution choices for forestry drone systems.]]></description>
<pubDate>Mon, 23 Feb 2026 12:12:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Hexagon-MLIR: An AI Compilation Stack For Qualcomm's Neural Processing Units (NPUs)]]></title>
<link>http://arxiv.org/abs/2602.19762v1</link>
<guid>2602.19762v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.PL, cs.AI
Authors: Mohammed Javed Absar, Muthu Baskaran, Abhikrant Sharma, Abhilash Bhandari, Ankit Aggarwal et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In this paper, we present Hexagon-MLIR,an open-source compilation stack that targets Qualcomm Hexagon Neural Processing Unit (NPU) and provides unified support for lowering Triton kernels and PyTorch models . Built using the MLIR framework, our compiler applies a structured sequence of passes to exploit NPU architectural features to accelerate AI workloads. It enables faster deployment of new Triton kernels (hand-written or subgraphs from PyTorch 2.0), for our target by providing automated compilation from kernel to binary. By ingesting Triton kernels, we generate mega-kernels that maximize data locality in the NPU's Tightly Coupled Memory (TCM), reducing the bandwidth bottlenecks inherent in library-based approaches. This initiative complements our commercial toolchains by providing developers with an open-source MLIR-based compilation stack that gives them a path to advance AI compilation capabilities through a more flexible approach. Hexagon-MLIR is a work-in-progress, and we are continuing to add many more optimizations and capabilities in this effort.]]></description>
<pubDate>Mon, 23 Feb 2026 12:12:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Ensemble Machine Learning and Statistical Procedures for Dynamic Predictions of Time-to-Event Outcomes]]></title>
<link>http://arxiv.org/abs/2602.19761v1</link>
<guid>2602.19761v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG, stat.AP
Authors: Nina van Gerwen, Sten Willemsen, Bettina E. Hansen, Christophe Corpechot, Marco Carbone et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Dynamic predictions for longitudinal and time-to-event outcomes have become a versatile tool in precision medicine. Our work is motivated by the application of dynamic predictions in the decision-making process for primary biliary cholangitis patients. For these patients, serial biomarker measurements (e.g., bilirubin and alkaline phosphatase levels) are routinely collected to inform treating physicians of the risk of liver failure and guide clinical decision-making. Two popular statistical approaches to derive dynamic predictions are joint modelling and landmarking. However, recently, machine learning techniques have also been proposed. Each approach has its merits, and no single method exists to outperform all others. Consequently, obtaining the best possible survival estimates is challenging. Therefore, we extend the Super Learner framework to combine dynamic predictions from different models and procedures. Super Learner is an ensemble learning technique that allows users to combine different prediction algorithms to improve predictive accuracy and flexibility. It uses cross-validation and different objective functions of performance (e.g., squared loss) that suit specific applications to build the optimally weighted combination of predictions from a library of candidate algorithms. In our work, we pay special attention to appropriate objective functions for Super Learner to obtain the most optimal weighted combination of dynamic predictions. In our primary biliary cholangitis application, Super Learner presented unique benefits due to its ability to flexibly combine outputs from a diverse set of models with varying assumptions for equal or better predictive performance than any model fit separately.]]></description>
<pubDate>Mon, 23 Feb 2026 12:12:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[Multimodal Dataset Distillation Made Simple by Prototype-Guided Data Synthesis]]></title>
<link>http://arxiv.org/abs/2602.19756v1</link>
<guid>2602.19756v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Junhyeok Choi, Sangwoo Mo, Minwoo Chae
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent advances in multimodal learning have achieved remarkable success across diverse vision-language tasks. However, such progress heavily relies on large-scale image-text datasets, making training costly and inefficient. Prior efforts in dataset filtering and pruning attempt to mitigate this issue, but still require relatively large subsets to maintain performance and fail under very small subsets. Dataset distillation offers a promising alternative, yet existing multimodal dataset distillation methods require full-dataset training and joint optimization of image pixels and text features, making them architecture-dependent and limiting cross-architecture generalization. To overcome this, we propose a learning-free dataset distillation framework that eliminates the need for large-scale training and optimization while enhancing generalization across architectures. Our method uses CLIP to extract aligned image-text embeddings, obtains prototypes, and employs an unCLIP decoder to synthesize images, enabling efficient and scalable multimodal dataset distillation. Extensive experiments demonstrate that our approach consistently outperforms optimization-based dataset distillation and subset selection methods, achieving state-of-the-art cross-architecture generalization.]]></description>
<pubDate>Mon, 23 Feb 2026 12:08:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[RAP: Fast Feedforward Rendering-Free Attribute-Guided Primitive Importance Score Prediction for Efficient 3D Gaussian Splatting Processing]]></title>
<link>http://arxiv.org/abs/2602.19753v1</link>
<guid>2602.19753v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.GR
Authors: Kaifa Yang, Qi Yang, Yiling Xu, Zhu Li
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

3D Gaussian Splatting (3DGS) has emerged as a leading technology for high-quality 3D scene reconstruction. However, the iterative refinement and densification process leads to the generation of a large number of primitives, each contributing to the reconstruction to a substantially different extent. Estimating primitive importance is thus crucial, both for removing redundancy during reconstruction and for enabling efficient compression and transmission. Existing methods typically rely on rendering-based analyses, where each primitive is evaluated through its contribution across multiple camera viewpoints. However, such methods are sensitive to the number and selection of views, rely on specialized differentiable rasterizers, and have long calculation times that grow linearly with view count, making them difficult to integrate as plug-and-play modules and limiting scalability and generalization. To address these issues, we propose RAP, a fast feedforward rendering-free attribute-guided method for efficient importance score prediction in 3DGS. RAP infers primitive significance directly from intrinsic Gaussian attributes and local neighborhood statistics, avoiding rendering-based or visibility-dependent computations. A compact MLP predicts per-primitive importance scores using rendering loss, pruning-aware loss, and significance distribution regularization. After training on a small set of scenes, RAP generalizes effectively to unseen data and can be seamlessly integrated into reconstruction, compression, and transmission pipelines. Our code is publicly available at https://github.com/yyyykf/RAP.]]></description>
<pubDate>Mon, 23 Feb 2026 12:02:03 +0000</pubDate>
</item>
<item>
<title><![CDATA[NILE: Formalizing Natural-Language Descriptions of Formal Languages]]></title>
<link>http://arxiv.org/abs/2602.19743v1</link>
<guid>2602.19743v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.FL, cs.CL, cs.LO
Authors: Tristan Kneisel, Marko Schmellenkamp, Fabian Vehlken, Thomas Zeume
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

This paper explores how natural-language descriptions of formal languages can be compared to their formal representations and how semantic differences can be explained. This is motivated from educational scenarios where learners describe a formal language (presented, e.g., by a finite state automaton, regular expression, pushdown automaton, context-free grammar or in set notation) in natural language, and an educational support system has to (1) judge whether the natural-language description accurately describes the formal language, and to (2) provide explanations why descriptions are not accurate.
  To address this question, we introduce a representation language for formal languages, Nile, which is designed so that Nile expressions can mirror the syntactic structure of natural-language descriptions of formal languages. Nile is sufficiently expressive to cover a broad variety of formal languages, including all regular languages and fragments of context-free languages typically used in educational contexts. Generating Nile expressions that are syntactically close to natural-language descriptions then allows to provide explanations for inaccuracies in the descriptions algorithmically.
  In experiments on an educational data set, we show that LLMs can translate natural-language descriptions into equivalent, syntactically close Nile expressions with high accuracy - allowing to algorithmically provide explanations for incorrect natural-language descriptions. Our experiments also show that while natural-language descriptions can also be translated into regular expressions (but not context-free grammars), the expressions are often not syntactically close and thus not suitable for providing explanations.]]></description>
<pubDate>Mon, 23 Feb 2026 11:42:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[InfScene-SR: Spatially Continuous Inference for Arbitrary-Size Image Super-Resolution]]></title>
<link>http://arxiv.org/abs/2602.19736v1</link>
<guid>2602.19736v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Shoukun Sun, Zhe Wang, Xiang Que, Jiyin Zhang, Xiaogang Ma
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Image Super-Resolution (SR) aims to recover high-resolution (HR) details from low-resolution (LR) inputs, a task where Denoising Diffusion Probabilistic Models (DDPMs) have recently shown superior performance compared to Generative Adversarial Networks (GANs) based approaches. However, standard diffusion-based SR models, such as SR3, are typically trained on fixed-size patches and struggle to scale to arbitrary-sized images due to memory constraints. Applying these models via independent patch processing leads to visible seams and inconsistent textures across boundaries. In this paper, we propose InfScene-SR, a framework enabling spatially continuous super-resolution for large, arbitrary scenes. We adapt the iterative refinement process of diffusion models with a novel guided and variance-corrected fusion mechanism, allowing for the seamless generation of large-scale high-resolution imagery without retraining. We validate our approach on remote sensing datasets, demonstrating that InfScene-SR not only reconstructs fine details with high perceptual quality but also eliminates boundary artifacts, benefiting downstream tasks such as semantic segmentation.]]></description>
<pubDate>Mon, 23 Feb 2026 11:34:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[VGGT-MPR: VGGT-Enhanced Multimodal Place Recognition in Autonomous Driving Environments]]></title>
<link>http://arxiv.org/abs/2602.19735v1</link>
<guid>2602.19735v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Jingyi Xu, Zhangshuo Qi, Zhongmiao Yan, Xuyu Gao, Qianyun Jiao et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

In autonomous driving, robust place recognition is critical for global localization and loop closure detection. While inter-modality fusion of camera and LiDAR data in multimodal place recognition (MPR) has shown promise in overcoming the limitations of unimodal counterparts, existing MPR methods basically attend to hand-crafted fusion strategies and heavily parameterized backbones that require costly retraining. To address this, we propose VGGT-MPR, a multimodal place recognition framework that adopts the Visual Geometry Grounded Transformer (VGGT) as a unified geometric engine for both global retrieval and re-ranking. In the global retrieval stage, VGGT extracts geometrically-rich visual embeddings through prior depth-aware and point map supervision, and densifies sparse LiDAR point clouds with predicted depth maps to improve structural representation. This enhances the discriminative ability of fused multimodal features and produces global descriptors for fast retrieval. Beyond global retrieval, we design a training-free re-ranking mechanism that exploits VGGT's cross-view keypoint-tracking capability. By combining mask-guided keypoint extraction with confidence-aware correspondence scoring, our proposed re-ranking mechanism effectively refines retrieval results without additional parameter optimization. Extensive experiments on large-scale autonomous driving benchmarks and our self-collected data demonstrate that VGGT-MPR achieves state-of-the-art performance, exhibiting strong robustness to severe environmental changes, viewpoint shifts, and occlusions. Our code and data will be made publicly available.]]></description>
<pubDate>Mon, 23 Feb 2026 11:33:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Understanding the Curse of Unrolling]]></title>
<link>http://arxiv.org/abs/2602.19733v1</link>
<guid>2602.19733v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Sheheryar Mehmood, Florian Knoll, Peter Ochs
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Algorithm unrolling is ubiquitous in machine learning, particularly in hyperparameter optimization and meta-learning, where Jacobians of solution mappings are computed by differentiating through iterative algorithms. Although unrolling is known to yield asymptotically correct Jacobians under suitable conditions, recent work has shown that the derivative iterates may initially diverge from the true Jacobian, a phenomenon known as the curse of unrolling. In this work, we provide a non-asymptotic analysis that explains the origin of this behavior and identifies the algorithmic factors that govern it. We show that truncating early iterations of the derivative computation mitigates the curse while simultaneously reducing memory requirements. Finally, we demonstrate that warm-starting in bilevel optimization naturally induces an implicit form of truncation, providing a practical remedy. Our theoretical findings are supported by numerical experiments on representative examples.]]></description>
<pubDate>Mon, 23 Feb 2026 11:32:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Towards Personalized Multi-Modal MRI Synthesis across Heterogeneous Datasets]]></title>
<link>http://arxiv.org/abs/2602.19723v1</link>
<guid>2602.19723v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yue Zhang, Zhizheng Zhuo, Siyao Xu, Shan Lv, Zhaoxi Liu et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Synthesizing missing modalities in multi-modal magnetic resonance imaging (MRI) is vital for ensuring diagnostic completeness, particularly when full acquisitions are infeasible due to time constraints, motion artifacts, and patient tolerance. Recent unified synthesis models have enabled flexible synthesis tasks by accommodating various input-output configurations. However, their training and evaluation are typically restricted to a single dataset, limiting their generalizability across diverse clinical datasets and impeding practical deployment. To address this limitation, we propose PMM-Synth, a personalized MRI synthesis framework that not only supports various synthesis tasks but also generalizes effectively across heterogeneous datasets. PMM-Synth is jointly trained on multiple multi-modal MRI datasets that differ in modality coverage, disease types, and intensity distributions. It achieves cross-dataset generalization through three core innovations: a Personalized Feature Modulation module that dynamically adapts feature representations based on dataset identifier to mitigate the impact of distributional shifts; a Modality-Consistent Batch Scheduler that facilitates stable and efficient batch training under inconsistent modality conditions; and a selective supervision loss to ensure effective learning when ground truth modalities are partially missing. Evaluated on four clinical multi-modal MRI datasets, PMM-Synth consistently outperforms state-of-the-art methods in both one-to-one and many-to-one synthesis tasks, achieving superior PSNR and SSIM scores. Qualitative results further demonstrate improved preservation of anatomical structures and pathological details. Additionally, downstream tumor segmentation and radiological reporting studies suggest that PMM-Synth holds potential for supporting reliable diagnosis under real-world modality-missing scenarios.]]></description>
<pubDate>Mon, 23 Feb 2026 11:20:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Generative 6D Pose Estimation via Conditional Flow Matching]]></title>
<link>http://arxiv.org/abs/2602.19719v1</link>
<guid>2602.19719v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Amir Hamza, Davide Boscaini, Weihang Li, Benjamin Busam, Fabio Poiesi
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Existing methods for instance-level 6D pose estimation typically rely on neural networks that either directly regress the pose in $\mathrm{SE}(3)$ or estimate it indirectly via local feature matching. The former struggle with object symmetries, while the latter fail in the absence of distinctive local features. To overcome these limitations, we propose a novel formulation of 6D pose estimation as a conditional flow matching problem in $\mathbb{R}^3$. We introduce Flose, a generative method that infers object poses via a denoising process conditioned on local features. While prior approaches based on conditional flow matching perform denoising solely based on geometric guidance, Flose integrates appearance-based semantic features to mitigate ambiguities caused by object symmetries. We further incorporate RANSAC-based registration to handle outliers. We validate Flose on five datasets from the established BOP benchmark. Flose outperforms prior methods with an average improvement of +4.5 Average Recall. Project Website : https://tev-fbk.github.io/Flose/]]></description>
<pubDate>Mon, 23 Feb 2026 11:15:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[Carbon-Aware Governance Gates: An Architecture for Sustainable GenAI Development]]></title>
<link>http://arxiv.org/abs/2602.19718v1</link>
<guid>2602.19718v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SE, cs.AI
Authors: Mateen A. Abbasi, Tommi J. Mikkonen, Petri J. Ihantola, Muhammad Waseem, Pekka Abrahamsson et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The rapid adoption of Generative AI (GenAI) in the software development life cycle (SDLC) increases computational demand, which can raise the carbon footprint of development activities. At the same time, organizations are increasingly embedding governance mechanisms into GenAI-assisted development to support trust, transparency, and accountability. However, these governance mechanisms introduce additional computational workloads, including repeated inference, regeneration cycles, and expanded validation pipelines, increasing energy use and the carbon footprint of GenAI-assisted development. This paper proposes Carbon-Aware Governance Gates (CAGG), an architectural extension that embeds carbon budgets, energy provenance, and sustainability-aware validation orchestration into human-AI governance layers. CAGG comprises three components: (i) an Energy and Carbon Provenance Ledger, (ii) a Carbon Budget Manager, and (iii) a Green Validation Orchestrator, operationalized through governance policies and reusable design patterns.]]></description>
<pubDate>Mon, 23 Feb 2026 11:11:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Pixels Don't Lie (But Your Detector Might): Bootstrapping MLLM-as-a-Judge for Trustworthy Deepfake Detection and Reasoning Supervision]]></title>
<link>http://arxiv.org/abs/2602.19715v1</link>
<guid>2602.19715v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Kartik Kuckreja, Parul Gupta, Muhammad Haris Khan, Abhinav Dhall
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Deepfake detection models often generate natural-language explanations, yet their reasoning is frequently ungrounded in visual evidence, limiting reliability. Existing evaluations measure classification accuracy but overlook reasoning fidelity. We propose DeepfakeJudge, a framework for scalable reasoning supervision and evaluation, that integrates an out-of-distribution benchmark containing recent generative and editing forgeries, a human-annotated subset with visual reasoning labels, and a suite of evaluation models, that specialize in evaluating reasoning rationales without the need for explicit ground truth reasoning rationales. The Judge is optimized through a bootstrapped generator-evaluator process that scales human feedback into structured reasoning supervision and supports both pointwise and pairwise evaluation. On the proposed meta-evaluation benchmark, our reasoning-bootstrapped model achieves an accuracy of 96.2\%, outperforming \texttt{30x} larger baselines. The reasoning judge attains very high correlation with human ratings and 98.9\% percent pairwise agreement on the human-annotated meta-evaluation subset. These results establish reasoning fidelity as a quantifiable dimension of deepfake detection and demonstrate scalable supervision for interpretable deepfake reasoning. Our user study shows that participants preferred the reasonings generated by our framework 70\% of the time, in terms of faithfulness, groundedness, and usefulness, compared to those produced by other models and datasets. All of our datasets, models, and codebase are \href{https://github.com/KjAeRsTuIsK/DeepfakeJudge}{open-sourced}.]]></description>
<pubDate>Mon, 23 Feb 2026 11:08:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[Universal Pose Pretraining for Generalizable Vision-Language-Action Policies]]></title>
<link>http://arxiv.org/abs/2602.19710v1</link>
<guid>2602.19710v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG, cs.RO
Authors: Haitao Lin, Hanyang Yu, Jingshun Huang, He Zhang, Yonggen Ling et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.
  To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.
  Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.]]></description>
<pubDate>Mon, 23 Feb 2026 11:00:08 +0000</pubDate>
</item>
<item>
<title><![CDATA[ChimeraLoRA: Multi-Head LoRA-Guided Synthetic Datasets]]></title>
<link>http://arxiv.org/abs/2602.19708v1</link>
<guid>2602.19708v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Hoyoung Kim, Minwoo Jang, Jabin Koo, Sangdoo Yun, Jungseul Ok
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Beyond general recognition tasks, specialized domains including privacy-constrained medical applications and fine-grained settings often encounter data scarcity, especially for tail classes. To obtain less biased and more reliable models under such scarcity, practitioners leverage diffusion models to supplement underrepresented regions of real data. Specifically, recent studies fine-tune pretrained diffusion models with LoRA on few-shot real sets to synthesize additional images. While an image-wise LoRA trained on a single image captures fine-grained details yet offers limited diversity, a class-wise LoRA trained over all shots produces diverse images as it encodes class priors yet tends to overlook fine details. To combine both benefits, we separate the adapter into a class-shared LoRA~$A$ for class priors and per-image LoRAs~$\mathcal{B}$ for image-specific characteristics. To expose coherent class semantics in the shared LoRA~$A$, we propose a semantic boosting by preserving class bounding boxes during training. For generation, we compose $A$ with a mixture of $\mathcal{B}$ using coefficients drawn from a Dirichlet distribution. Across diverse datasets, our synthesized images are both diverse and detail-rich while closely aligning with the few-shot real distribution, yielding robust gains in downstream classification accuracy.]]></description>
<pubDate>Mon, 23 Feb 2026 10:59:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[HDR Reconstruction Boosting with Training-Free and Exposure-Consistent Diffusion]]></title>
<link>http://arxiv.org/abs/2602.19706v1</link>
<guid>2602.19706v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yo-Tin Lin, Su-Kai Chen, Hou-Ning Hu, Yen-Yu Lin, Yu-Lun Liu
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Single LDR to HDR reconstruction remains challenging for over-exposed regions where traditional methods often fail due to complete information loss. We present a training-free approach that enhances existing indirect and direct HDR reconstruction methods through diffusion-based inpainting. Our method combines text-guided diffusion models with SDEdit refinement to generate plausible content in over-exposed areas while maintaining consistency across multi-exposure LDR images. Unlike previous approaches requiring extensive training, our method seamlessly integrates with existing HDR reconstruction techniques through an iterative compensation mechanism that ensures luminance coherence across multiple exposures. We demonstrate significant improvements in both perceptual quality and quantitative metrics on standard HDR datasets and in-the-wild captures. Results show that our method effectively recovers natural details in challenging scenarios while preserving the advantages of existing HDR reconstruction pipelines. Project page: https://github.com/EusdenLin/HDR-Reconstruction-Boosting]]></description>
<pubDate>Mon, 23 Feb 2026 10:57:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework]]></title>
<link>http://arxiv.org/abs/2602.19702v1</link>
<guid>2602.19702v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IR, cs.AI
Authors: Adamya Shyam, Venkateswara Rao Kagita, Bharti Rana, Vikas Kumar
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets.]]></description>
<pubDate>Mon, 23 Feb 2026 10:52:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[Iconographic Classification and Content-Based Recommendation for Digitized Artworks]]></title>
<link>http://arxiv.org/abs/2602.19698v1</link>
<guid>2602.19698v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.DL, cs.AI, cs.CV, cs.IR
Authors: Krzysztof Kutt, Maciej BaczyÅski
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present a proof-of-concept system that automates iconographic classification and content-based recommendation of digitized artworks using the Iconclass vocabulary and selected artificial intelligence methods. The prototype implements a four-stage workflow for classification and recommendation, which integrates YOLOv8 object detection with algorithmic mappings to Iconclass codes, rule-based inference for abstract meanings, and three complementary recommenders (hierarchical proximity, IDF-weighted overlap, and Jaccard similarity). Although more engineering is still needed, the evaluation demonstrates the potential of this solution: Iconclass-aware computer vision and recommendation methods can accelerate cataloging and enhance navigation in large heritage repositories. The key insight is to let computer vision propose visible elements and to use symbolic structures (Iconclass hierarchy) to reach meaning.]]></description>
<pubDate>Mon, 23 Feb 2026 10:44:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[BayesFusion-SDF: Probabilistic Signed Distance Fusion with View Planning on CPU]]></title>
<link>http://arxiv.org/abs/2602.19697v1</link>
<guid>2602.19697v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.GR, cs.RO
Authors: Soumya Mazumdar, Vineet Kumar Rakesh, Tapas Samanta
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Key part of robotics, augmented reality, and digital inspection is dense 3D reconstruction from depth observations. Traditional volumetric fusion techniques, including truncated signed distance functions (TSDF), enable efficient and deterministic geometry reconstruction; however, they depend on heuristic weighting and fail to transparently convey uncertainty in a systematic way. Recent neural implicit methods, on the other hand, get very high fidelity but usually need a lot of GPU power for optimization and aren't very easy to understand for making decisions later on. This work presents BayesFusion-SDF, a CPU-centric probabilistic signed distance fusion framework that conceptualizes geometry as a sparse Gaussian random field with a defined posterior distribution over voxel distances. First, a rough TSDF reconstruction is used to create an adaptive narrow-band domain. Then, depth observations are combined using a heteroscedastic Bayesian formulation that is solved using sparse linear algebra and preconditioned conjugate gradients. Randomized diagonal estimators are a quick way to get an idea of posterior uncertainty. This makes it possible to extract surfaces and plan the next best view while taking into account uncertainty. Tests on a controlled ablation scene and a CO3D object sequence show that the new method is more accurate geometrically than TSDF baselines and gives useful estimates of uncertainty for active sensing. The proposed formulation provides a clear and easy-to-use alternative to GPU-heavy neural reconstruction methods while still being able to be understood in a probabilistic way and acting in a predictable way. GitHub: https://mazumdarsoumya.github.io/BayesFusionSDF]]></description>
<pubDate>Mon, 23 Feb 2026 10:44:15 +0000</pubDate>
</item>
<item>
<title><![CDATA[Smoothness Adaptivity in Constant-Depth Neural Networks: Optimal Rates via Smooth Activations]]></title>
<link>http://arxiv.org/abs/2602.19691v1</link>
<guid>2602.19691v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Yuhao Liu, Zilin Wang, Lei Wu, Shaobo Zhang
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Smooth activation functions are ubiquitous in modern deep learning, yet their theoretical advantages over non-smooth counterparts remain poorly understood. In this work, we characterize both approximation and statistical properties of neural networks with smooth activations over the Sobolev space $W^{s,\infty}([0,1]^d)$ for arbitrary smoothness $s>0$. We prove that constant-depth networks equipped with smooth activations automatically exploit arbitrarily high orders of target function smoothness, achieving the minimax-optimal approximation and estimation error rates (up to logarithmic factors). In sharp contrast, networks with non-smooth activations, such as ReLU, lack this adaptivity: their attainable approximation order is strictly limited by depth, and capturing higher-order smoothness requires proportional depth growth. These results identify activation smoothness as a fundamental mechanism, alternative to depth, for attaining statistical optimality. Technically, our results are established via a constructive approximation framework that produces explicit neural network approximators with carefully controlled parameter norms and model size. This complexity control ensures statistical learnability under empirical risk minimization (ERM) and removes the impractical sparsity constraints commonly required in prior analyses.]]></description>
<pubDate>Mon, 23 Feb 2026 10:38:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[PerturbDiff: Functional Diffusion for Single-Cell Perturbation Modeling]]></title>
<link>http://arxiv.org/abs/2602.19685v1</link>
<guid>2602.19685v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Xinyu Yuan, Xixian Liu, Ya Shi Zhang, Zuobai Zhang, Hongyu Guo et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Building Virtual Cells that can accurately simulate cellular responses to perturbations is a long-standing goal in systems biology. A fundamental challenge is that high-throughput single-cell sequencing is destructive: the same cell cannot be observed both before and after a perturbation. Thus, perturbation prediction requires mapping unpaired control and perturbed populations. Existing models address this by learning maps between distributions, but typically assume a single fixed response distribution when conditioned on observed cellular context (e.g., cell type) and the perturbation type. In reality, responses vary systematically due to unobservable latent factors such as microenvironmental fluctuations and complex batch effects, forming a manifold of possible distributions for the same observed conditions. To account for this variability, we introduce PerturbDiff, which shifts modeling from individual cells to entire distributions. By embedding distributions as points in a Hilbert space, we define a diffusion-based generative process operating directly over probability distributions. This allows PerturbDiff to capture population-level response shifts across hidden factors. Benchmarks on established datasets show that PerturbDiff achieves state-of-the-art performance in single-cell response prediction and generalizes substantially better to unseen perturbations. See our project page (https://katarinayuan.github.io/PerturbDiff-ProjectPage/), where code and data will be made publicly available (https://github.com/DeepGraphLearning/PerturbDiff).]]></description>
<pubDate>Mon, 23 Feb 2026 10:28:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[TeHOR: Text-Guided 3D Human and Object Reconstruction with Textures]]></title>
<link>http://arxiv.org/abs/2602.19679v1</link>
<guid>2602.19679v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Hyeongjin Nam, Daniel Sungho Jung, Kyoung Mu Lee
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Joint reconstruction of 3D human and object from a single image is an active research area, with pivotal applications in robotics and digital content creation. Despite recent advances, existing approaches suffer from two fundamental limitations. First, their reconstructions rely heavily on physical contact information, which inherently cannot capture non-contact human-object interactions, such as gazing at or pointing toward an object. Second, the reconstruction process is primarily driven by local geometric proximity, neglecting the human and object appearances that provide global context crucial for understanding holistic interactions. To address these issues, we introduce TeHOR, a framework built upon two core designs. First, beyond contact information, our framework leverages text descriptions of human-object interactions to enforce semantic alignment between the 3D reconstruction and its textual cues, enabling reasoning over a wider spectrum of interactions, including non-contact cases. Second, we incorporate appearance cues of the 3D human and object into the alignment process to capture holistic contextual information, thereby ensuring visually plausible reconstructions. As a result, our framework produces accurate and semantically coherent reconstructions, achieving state-of-the-art performance.]]></description>
<pubDate>Mon, 23 Feb 2026 10:22:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[Continuous Telemonitoring of Heart Failure using Personalised Speech Dynamics]]></title>
<link>http://arxiv.org/abs/2602.19674v1</link>
<guid>2602.19674v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SD, cs.AI
Authors: Yue Pan, Xingyao Wang, Hanyue Zhang, Liwei Liu, Changxin Li et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Remote monitoring of heart failure (HF) via speech signals provides a non-invasive and cost-effective solution for long-term patient management. However, substantial inter-individual heterogeneity in vocal characteristics often limits the accuracy of traditional cross-sectional classification models. To address this, we propose a Longitudinal Intra-Patient Tracking (LIPT) scheme designed to capture the trajectory of relative symptomatic changes within individuals. Central to this framework is a Personalised Sequential Encoder (PSE), which transforms longitudinal speech recordings into context-aware latent representations. By incorporating historical data at each timestamp, the PSE facilitates a holistic assessment of the clinical trajectory rather than modelling discrete visits independently. Experimental results from a cohort of 225 patients demonstrate that the LIPT paradigm significantly outperforms the classic cross-sectional approaches, achieving a recognition accuracy of 99.7% for clinical status transitions. The model's high sensitivity was further corroborated by additional follow-up data, confirming its efficacy in predicting HF deterioration and its potential to secure patient safety in remote, home-based settings. Furthermore, this work addresses the gap in existing literature by providing a comprehensive analysis of different speech task designs and acoustic features. Taken together, the superior performance of the LIPT framework and PSE architecture validates their readiness for integration into long-term telemonitoring systems, offering a scalable solution for remote heart failure management.]]></description>
<pubDate>Mon, 23 Feb 2026 10:19:17 +0000</pubDate>
</item>
<item>
<title><![CDATA[SkillOrchestra: Learning to Route Agents via Skill Transfer]]></title>
<link>http://arxiv.org/abs/2602.19672v1</link>
<guid>2602.19672v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.LG
Authors: Jiayu Wang, Yifei Ming, Zixuan Ke, Shafiq Joty, Aws Albarghouthi et al.
Institution: Amazon
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.]]></description>
<pubDate>Mon, 23 Feb 2026 10:17:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation]]></title>
<link>http://arxiv.org/abs/2602.19668v1</link>
<guid>2602.19668v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.LG
Authors: He Zhu, Ren Togo, Takahiro Ogawa, Kenji Hirata, Minghui Tang et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Longitudinal medical report generation is clinically important yet remains challenging due to strict privacy constraints and the evolving nature of disease progression. Although federated learning (FL) enables collaborative training without data sharing, existing FL methods largely overlook longitudinal dynamics by assuming stationary client distributions, making them unable to model temporal shifts across visits or patient-specific heterogeneity-ultimately leading to unstable optimization and suboptimal report generation.
  We introduce Federated Temporal Adaptation (FTA), a federated setting that explicitly accounts for the temporal evolution of client data. Building upon this setting, we propose FedTAR, a framework that integrates demographic-driven personalization with time-aware global aggregation. FedTAR generates lightweight LoRA adapters from demographic embeddings and performs temporal residual aggregation, where updates from different visits are weighted by a meta-learned temporal policy optimized via first-order MAML.
  Experiments on J-MID (1M exams) and MIMIC-CXR demonstrate consistent improvements in linguistic accuracy, temporal coherence, and cross-site generalization, establishing FedTAR as a robust and privacy-preserving paradigm for federated longitudinal modeling.]]></description>
<pubDate>Mon, 23 Feb 2026 10:14:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[PaReGTA: An LLM-based EHR Data Encoding Approach to Capture Temporal Information]]></title>
<link>http://arxiv.org/abs/2602.19661v1</link>
<guid>2602.19661v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Kihyuk Yoon, Lingchao Mao, Catherine Chong, Todd J. Schwedt, Chia-Chun Chiang et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Temporal information in structured electronic health records (EHRs) is often lost in sparse one-hot or count-based representations, while sequence models can be costly and data-hungry. We propose PaReGTA, an LLM-based encoding framework that (i) converts longitudinal EHR events into visit-level templated text with explicit temporal cues, (ii) learns domain-adapted visit embeddings via lightweight contrastive fine-tuning of a sentence-embedding model, and (iii) aggregates visit embeddings into a fixed-dimensional patient representation using hybrid temporal pooling that captures both recency and globally informative visits. Because PaReGTA does not require training from scratch but instead utilizes a pre-trained LLM, it can perform well even in data-limited cohorts. Furthermore, PaReGTA is model-agnostic and can benefit from future EHR-specialized sentence-embedding models. For interpretability, we introduce PaReGTA-RSS (Representation Shift Score), which quantifies clinically defined factor importance by recomputing representations after targeted factor removal and projecting representation shifts through a machine learning model. On 39,088 migraine patients from the All of Us Research Program, PaReGTA outperforms sparse baselines for migraine type classification while deep sequential models were unstable in our cohort.]]></description>
<pubDate>Mon, 23 Feb 2026 10:09:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Representation Stability in a Minimal Continual Learning Agent]]></title>
<link>http://arxiv.org/abs/2602.19655v1</link>
<guid>2602.19655v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Vishnu Subramanian
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Continual learning systems are increasingly deployed in environments where retraining or reset is infeasible, yet many approaches emphasize task performance rather than the evolution of internal representations over time. In this work, we study a minimal continual learning agent designed to isolate representational dynamics from architectural complexity and optimization objectives. The agent maintains a persistent state vector across executions and incrementally updates it as new textual data is introduced. We quantify representational change using cosine similarity between successive normalized state vectors and define a stability metric over time intervals. Longitudinal experiments across eight executions reveal a transition from an initial plastic regime to a stable representational regime under consistent input. A deliberately introduced semantic perturbation produces a bounded decrease in similarity, followed by recovery and restabilization under subsequent coherent input. These results demonstrate that meaningful stability plasticity tradeoffs can emerge in a minimal, stateful learning system without explicit regularization, replay, or architectural complexity. The work establishes a transparent empirical baseline for studying representational accumulation and adaptation in continual learning systems.]]></description>
<pubDate>Mon, 23 Feb 2026 09:59:03 +0000</pubDate>
</item>
<item>
<title><![CDATA[NEXUS : A compact neural architecture for high-resolution spatiotemporal air quality forecasting in Delhi Nationa Capital Region]]></title>
<link>http://arxiv.org/abs/2602.19654v1</link>
<guid>2602.19654v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Rampunit Kumar, Aditya Maheshwari
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Urban air pollution in megacities poses critical public health challenges, particularly in Delhi National Capital Region (NCR) where severe degradation affects millions. We present NEXUS (Neural Extraction and Unified Spatiotemporal) architecture for forecasting carbon monoxide, nitrogen oxide, and sulfur dioxide. Working with four years (2018--2021) of atmospheric data across sixteen spatial grids, NEXUS achieves R$^2$ exceeding 0.94 for CO, 0.91 for NO, and 0.95 for SO$_2$ using merely 18,748 parameters -- substantially fewer than SCINet (35,552), Autoformer (68,704), and FEDformer (298,080). The architecture integrates patch embedding, low-rank projections, and adaptive fusion mechanisms to decode complex atmospheric chemistry patterns. Our investigation uncovers distinct diurnal rhythms and pronounced seasonal variations, with winter months experiencing severe pollution episodes driven by temperature inversions and agricultural biomass burning. Analysis identifies critical meteorological thresholds, quantifies wind field impacts on pollutant dispersion, and maps spatial heterogeneity across the region. Extensive ablation experiments demonstrate each architectural component's role. NEXUS delivers superior predictive performance with remarkable computational efficiency, enabling real-time deployment for air quality monitoring systems.]]></description>
<pubDate>Mon, 23 Feb 2026 09:56:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[Denoising Particle Filters: Learning State Estimation with Single-Step Objectives]]></title>
<link>http://arxiv.org/abs/2602.19651v1</link>
<guid>2602.19651v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.RO, cs.AI, cs.LG
Authors: Lennart RÃ¶stel, Berthold BÃ¤uml
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Learning-based methods commonly treat state estimation in robotics as a sequence modeling problem. While this paradigm can be effective at maximizing end-to-end performance, models are often difficult to interpret and expensive to train, since training requires unrolling sequences of predictions in time. As an alternative to end-to-end trained state estimation, we propose a novel particle filtering algorithm in which models are trained from individual state transitions, fully exploiting the Markov property in robotic systems. In this framework, measurement models are learned implicitly by minimizing a denoising score matching objective. At inference, the learned denoiser is used alongside a (learned) dynamics model to approximately solve the Bayesian filtering equation at each time step, effectively guiding predicted states toward the data manifold informed by measurements. We evaluate the proposed method on challenging robotic state estimation tasks in simulation, demonstrating competitive performance compared to tuned end-to-end trained baselines. Importantly, our method offers the desirable composability of classical filtering algorithms, allowing prior information and external sensor models to be incorporated without retraining.]]></description>
<pubDate>Mon, 23 Feb 2026 09:53:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Spectral Phase Encoding for Quantum Kernel Methods]]></title>
<link>http://arxiv.org/abs/2602.19644v1</link>
<guid>2602.19644v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Pablo Herrero GÃ³mez, Antonio Jimeno Morenilla, David MuÃ±oz-HernÃ¡ndez, Higinio Mora Mora
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Quantum kernel methods are promising for near-term quantum ma- chine learning, yet their behavior under data corruption remains insuf- ficiently understood. We analyze how quantum feature constructions degrade under controlled additive noise. We introduce Spectral Phase Encoding (SPE), a hybrid construc- tion combining a discrete Fourier transform (DFT) front-end with a diagonal phase-only embedding aligned with the geometry of diagonal quantum maps. Within a unified framework, we compare QK-DFT against alternative quantum variants (QK-PCA, QK-RP) and classi- cal SVM baselines under identical clean-data hyperparameter selection, quantifying robustness via dataset fixed-effects regression with wild cluster bootstrap inference across heterogeneous real-world datasets. Across the quantum family, DFT-based preprocessing yields the smallest degradation rate as noise increases, with statistically sup- ported slope differences relative to PCA and RP. Compared to classical baselines, QK-DFT shows degradation comparable to linear SVM and more stable than RBF SVM under matched tuning. Hardware exper- iments confirm that SPE remains executable and numerically stable for overlap estimation. These results indicate that robustness in quan- tum kernels depends critically on structure-aligned preprocessing and its interaction with diagonal embeddings, supporting a robustness-first perspective for NISQ-era quantum machine learning.]]></description>
<pubDate>Mon, 23 Feb 2026 09:42:42 +0000</pubDate>
</item>
<item>
<title><![CDATA[KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge]]></title>
<link>http://arxiv.org/abs/2602.19643v1</link>
<guid>2602.19643v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Alex Robertson, Huizhi Liang, Mahbub Gani, Rohit Kumar, Srijith Rajamohan
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation.]]></description>
<pubDate>Mon, 23 Feb 2026 09:41:46 +0000</pubDate>
</item>
<item>
<title><![CDATA[Evaluating the Impact of Data Anonymization on Image Retrieval]]></title>
<link>http://arxiv.org/abs/2602.19641v1</link>
<guid>2602.19641v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Marvin Chen, Manuel Eberhardinger, Johannes Maucher
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

With the growing importance of privacy regulations such as the General Data Protection Regulation, anonymizing visual data is becoming increasingly relevant across institutions. However, anonymization can negatively affect the performance of Computer Vision systems that rely on visual features, such as Content-Based Image Retrieval (CBIR). Despite this, the impact of anonymization on CBIR has not been systematically studied. This work addresses this gap, motivated by the DOKIQ project, an artificial intelligence-based system for document verification actively used by the State Criminal Police Office Baden-WÃ¼rttemberg. We propose a simple evaluation framework: retrieval results after anonymization should match those obtained before anonymization as closely as possible. To this end, we systematically assess the impact of anonymization using two public datasets and the internal DOKIQ dataset. Our experiments span three anonymization methods, four anonymization degrees, and four training strategies, all based on the state of the art backbone Self-Distillation with No Labels (DINO)v2. Our results reveal a pronounced retrieval bias in favor of models trained on original data, which produce the most similar retrievals after anonymization. The findings of this paper offer practical insights for developing privacy-compliant CBIR systems while preserving performance.]]></description>
<pubDate>Mon, 23 Feb 2026 09:39:06 +0000</pubDate>
</item>
<item>
<title><![CDATA[Compositional Planning with Jumpy World Models]]></title>
<link>http://arxiv.org/abs/2602.19634v1</link>
<guid>2602.19634v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI, stat.ML
Authors: Jesse Farebrother, Matteo Pirotta, Andrea Tirinzoni, Marc G. Bellemare, Alessandro Lazaric et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The ability to plan with temporal abstractions is central to intelligent decision-making. Rather than reasoning over primitive actions, we study agents that compose pre-trained policies as temporally extended actions, enabling solutions to complex tasks that no constituent alone can solve. Such compositional planning remains elusive as compounding errors in long-horizon predictions make it challenging to estimate the visitation distribution induced by sequencing policies. Motivated by the geometric policy composition framework introduced in arXiv:2206.08736, we address these challenges by learning predictive models of multi-step dynamics -- so-called jumpy world models -- that capture state occupancies induced by pre-trained policies across multiple timescales in an off-policy manner. Building on Temporal Difference Flows (arXiv:2503.09817), we enhance these models with a novel consistency objective that aligns predictions across timescales, improving long-horizon predictive accuracy. We further demonstrate how to combine these generative predictions to estimate the value of executing arbitrary sequences of policies over varying timescales. Empirically, we find that compositional planning with jumpy world models significantly improves zero-shot performance across a wide range of base policies on challenging manipulation and navigation tasks, yielding, on average, a 200% relative improvement over planning with primitive actions on long-horizon tasks.]]></description>
<pubDate>Mon, 23 Feb 2026 09:22:21 +0000</pubDate>
</item>
<item>
<title><![CDATA[TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents]]></title>
<link>http://arxiv.org/abs/2602.19633v1</link>
<guid>2602.19633v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Jongwon Jeong, Jungtaek Kim, Kangwook Lee
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here.]]></description>
<pubDate>Mon, 23 Feb 2026 09:19:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Localized Concept Erasure in Text-to-Image Diffusion Models via High-Level Representation Misdirection]]></title>
<link>http://arxiv.org/abs/2602.19631v1</link>
<guid>2602.19631v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Uichan Lee, Jeonghyeon Kim, Sangheum Hwang
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Recent advances in text-to-image (T2I) diffusion models have seen rapid and widespread adoption. However, their powerful generative capabilities raise concerns about potential misuse for synthesizing harmful, private, or copyrighted content. To mitigate such risks, concept erasure techniques have emerged as a promising solution. Prior works have primarily focused on fine-tuning the denoising component (e.g., the U-Net backbone). However, recent causal tracing studies suggest that visual attribute information is localized in the early self-attention layers of the text encoder, indicating a potential alternative for concept erasing. Building on this insight, we conduct preliminary experiments and find that directly fine-tuning early layers can suppress target concepts but often degrades the generation quality of non-target concepts. To overcome this limitation, we propose High-Level Representation Misdirection (HiRM), which misdirects high-level semantic representations of target concepts in the text encoder toward designated vectors such as random directions or semantically defined directions (e.g., supercategories), while updating only early layers that contain causal states of visual attributes. Our decoupling strategy enables precise concept removal with minimal impact on unrelated concepts, as demonstrated by strong results on UnlearnCanvas and NSFW benchmarks across diverse targets (e.g., objects, styles, nudity). HiRM also preserves generative utility at low training cost, transfers to state-of-the-art architectures such as Flux without additional training, and shows synergistic effects with denoiser-based concept erasing methods.]]></description>
<pubDate>Mon, 23 Feb 2026 09:18:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Cooperation After the Algorithm: Designing Human-AI Coexistence Beyond the Illusion of Collaboration]]></title>
<link>http://arxiv.org/abs/2602.19629v1</link>
<guid>2602.19629v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.HC, cs.AI
Authors: Tatia Codreanu
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Generative artificial intelligence systems increasingly participate in research, law, education, media, and governance. Their fluent and adaptive outputs create an experience of collaboration. However, these systems do not bear responsibility, incur liability, or share stakes in downstream consequences. This structural asymmetry has already produced sanctions, professional errors, and governance failures in high-stakes contexts We argue that stable human-AI coexistence is an institutional achievement that depends on governance infrastructure capable of distributing residual risk. Drawing on institutional analysis and evolutionary cooperation theory, we introduce a formal inequality that specifies when reliance on AI yields positive expected cooperative value. The model makes explicit how governance conditions, system policy, and accountability regimes jointly determine whether cooperation is rational or structurally defective. From this formalization we derive a cooperation ecology framework with six design principles: reciprocity contracts, visible trust infrastructure, conditional cooperation modes, defection-mitigation mechanisms, narrative literacy against authority theatre, and an Earth-first sustainability constraint. We operationalize the framework through three policy artefacts: a Human-AI Cooperation Charter, a Defection Risk Register, and a Cooperation Readiness Audit. Together, these elements shift the unit of analysis from the user-AI dyad to the institutional environment that shapes incentives, signals, accountability, and repair. The paper provides a theoretical foundation and practical toolkit for designing human-AI systems that can sustain accountable, trustworthy cooperation over time.]]></description>
<pubDate>Mon, 23 Feb 2026 09:17:12 +0000</pubDate>
</item>
<item>
<title><![CDATA[Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding]]></title>
<link>http://arxiv.org/abs/2602.19626v1</link>
<guid>2602.19626v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.IT, cs.CL
Authors: Roberto Tacconelli
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.
  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.]]></description>
<pubDate>Mon, 23 Feb 2026 09:14:05 +0000</pubDate>
</item>
<item>
<title><![CDATA[Accurate Planar Tracking With Robust Re-Detection]]></title>
<link>http://arxiv.org/abs/2602.19624v1</link>
<guid>2602.19624v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Jonas Serych, Jiri Matas
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We present SAM-H and WOFTSAM, novel planar trackers that combine robust long-term segmentation tracking provided by SAM 2 with 8 degrees-of-freedom homography pose estimation. SAM-H estimates homographies from segmentation mask contours and is thus highly robust to target appearance changes. WOFTSAM significantly improves the current state-of-the-art planar tracker WOFT by exploiting lost target re-detection provided by SAM-H. The proposed methods are evaluated on POT-210 and PlanarTrack tracking benchmarks, setting the new state-of-the-art performance on both. On the latter, they outperform the second best by a large margin, +12.4 and +15.2pp on the p@15 metric. We also present improved ground-truth annotations of initial PlanarTrack poses, enabling more accurate benchmarking in the high-precision p@5 metric. The code and the re-annotations are available at https://github.com/serycjon/WOFTSAM]]></description>
<pubDate>Mon, 23 Feb 2026 09:13:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[PedaCo-Gen: Scaffolding Pedagogical Agency in Human-AI Collaborative Video Authoring]]></title>
<link>http://arxiv.org/abs/2602.19623v1</link>
<guid>2602.19623v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI, cs.HC
Authors: Injun Baek, Yearim Kim, Nojun Kwak
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

While advancements in Text-to-Video (T2V) generative AI offer a promising path toward democratizing content creation, current models are often optimized for visual fidelity rather than instructional efficacy. This study introduces PedaCo-Gen, a pedagogically-informed human-AI collaborative video generating system for authoring instructional videos based on Mayer's Cognitive Theory of Multimedia Learning (CTML). Moving away from traditional "one-shot" generation, PedaCo-Gen introduces an Intermediate Representation (IR) phase, enabling educators to interactively review and refine video blueprints-comprising scripts and visual descriptions-with an AI reviewer. Our study with 23 education experts demonstrates that PedaCo-Gen significantly enhances video quality across various topics and CTML principles compared to baselines. Participants perceived the AI-driven guidance not merely as a set of instructions but as a metacognitive scaffold that augmented their instructional design expertise, reporting high production efficiency (M=4.26) and guide validity (M=4.04). These findings highlight the importance of reclaiming pedagogical agency through principled co-creation, providing a foundation for future AI authoring tools that harmonize generative power with human professional expertise.]]></description>
<pubDate>Mon, 23 Feb 2026 09:12:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[VecFormer: Towards Efficient and Generalizable Graph Transformer with Graph Token Attention]]></title>
<link>http://arxiv.org/abs/2602.19622v1</link>
<guid>2602.19622v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Jingbo Zhou, Jun Xia, Siyuan Li, Yunfan Liu, Wenjun Wang et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Graph Transformer has demonstrated impressive capabilities in the field of graph representation learning. However, existing approaches face two critical challenges: (1) most models suffer from exponentially increasing computational complexity, making it difficult to scale to large graphs; (2) attention mechanisms based on node-level operations limit the flexibility of the model and result in poor generalization performance in out-of-distribution (OOD) scenarios. To address these issues, we propose \textbf{VecFormer} (the \textbf{Vec}tor Quantized Graph Trans\textbf{former}), an efficient and highly generalizable model for node classification, particularly under OOD settings. VecFormer adopts a two-stage training paradigm. In the first stage, two codebooks are used to reconstruct the node features and the graph structure, aiming to learn the rich semantic \texttt{Graph Codes}. In the second stage, attention mechanisms are performed at the \texttt{Graph Token} level based on the transformed cross codebook, reducing computational complexity while enhancing the model's generalization capability. Extensive experiments on datasets of various sizes demonstrate that VecFormer outperforms the existing Graph Transformer in both performance and speed.]]></description>
<pubDate>Mon, 23 Feb 2026 09:10:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Rules or Weights? Comparing User Understanding of Explainable AI Techniques with the Cognitive XAI-Adaptive Model]]></title>
<link>http://arxiv.org/abs/2602.19620v1</link>
<guid>2602.19620v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI
Authors: Louth Bin Rawshan, Zhuoyu Wang, Brian Y Lim
Institution: Amazon
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Rules and Weights are popular XAI techniques for explaining AI decisions. Yet, it remains unclear how to choose between them, lacking a cognitive framework to compare their interpretability. In an elicitation user study on forward and counterfactual decision tasks, we identified 7 reasoning strategies of interpreting three XAI Schemas - weights, rules, and their hybrid. To analyze their capabilities, we propose CoXAM, a Cognitive XAI-Adaptive Model with shared memory representation to encode instance attributes, linear weights, and decision rules. CoXAM employs computational rationality to choose among reasoning processes based on the trade-off in utility and reasoning time, separately for forward or counterfactual decision tasks. In a validation study, CoXAM demonstrated a stronger alignment with human decision-making compared to baseline machine learning proxy models. The model successfully replicated and explained several key empirical findings, including that counterfactual tasks are inherently harder than forward tasks, decision tree rules are harder to recall and apply than linear weights, and the helpfulness of XAI depends on the application data context, alongside identifying which underlying reasoning strategies were most effective. With CoXAM, we contribute a cognitive basis to accelerate debugging and benchmarking disparate XAI techniques.]]></description>
<pubDate>Mon, 23 Feb 2026 09:07:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Is Your Diffusion Sampler Actually Correct? A Sampler-Centric Evaluation of Discrete Diffusion Language Models]]></title>
<link>http://arxiv.org/abs/2602.19619v1</link>
<guid>2602.19619v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Luhan Tang, Longxuan Yu, Shaorong Zhang, Greg Ver Steeg
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Discrete diffusion language models (dLLMs) provide a fast and flexible alternative to autoregressive models (ARMs) via iterative denoising with parallel updates. However, their evaluation is challenging: existing metrics conflate denoiser approximation error with sampler-induced error from the sampling dynamics, a problem that does not arise for ARMs whose autoregressive sampling exactly reflects the learned probability model. We introduce a sampler-centric oracle framework that replaces learned denoisers with an exact Hidden Markov Model posterior derived from a ground-truth Markov chain, isolating sampler-induced error in a controlled setting. We show that few-step discrete diffusion samplers are not distributionally correct even under an oracle denoiser, with transition-level mismatch that vanishes only as the number of steps approaches the sequence length. Moreover, improvements in negative log-likelihood, generative perplexity, or MAUVE do not imply correct sampling. Code is available at https://luhantang.github.io/dllm_sampler]]></description>
<pubDate>Mon, 23 Feb 2026 09:06:13 +0000</pubDate>
</item>
<item>
<title><![CDATA[Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness]]></title>
<link>http://arxiv.org/abs/2602.19615v1</link>
<guid>2602.19615v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Xin Hu, Haomiao Ni, Yunbei Zhang, Jihun Hamm, Zechen Li et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don't fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs' reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM's attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM's ability to focus on and reason about rare objects.]]></description>
<pubDate>Mon, 23 Feb 2026 09:02:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[Workflow-Level Design Principles for Trustworthy GenAI in Automotive System Engineering]]></title>
<link>http://arxiv.org/abs/2602.19614v1</link>
<guid>2602.19614v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.SE, cs.LG
Authors: Chih-Hong Cheng, Brian Hsuan-Cheng Liao, Adam Molin, Hasan Esen
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

The adoption of large language models in safety-critical system engineering is constrained by trustworthiness, traceability, and alignment with established verification practices. We propose workflow-level design principles for trustworthy GenAI integration and demonstrate them in an end-to-end automotive pipeline, from requirement delta identification to SysML v2 architecture update and re-testing. First, we show that monolithic ("big-bang") prompting misses critical changes in large specifications, while section-wise decomposition with diversity sampling and lightweight NLP sanity checks improves completeness and correctness. Then, we propagate requirement deltas into SysML v2 models and validate updates via compilation and static analysis. Additionally, we ensure traceable regression testing by generating test cases through explicit mappings from specification variables to architectural ports and states, providing practical safeguards for GenAI used in safety-critical automotive engineering.]]></description>
<pubDate>Mon, 23 Feb 2026 09:02:38 +0000</pubDate>
</item>
<item>
<title><![CDATA[Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning]]></title>
<link>http://arxiv.org/abs/2602.19612v1</link>
<guid>2602.19612v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Borisiuk Anna, Andrey Savchenko, Alexander Panchecko, Elena Tutubalina
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting.]]></description>
<pubDate>Mon, 23 Feb 2026 08:58:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[RAID: Retrieval-Augmented Anomaly Detection]]></title>
<link>http://arxiv.org/abs/2602.19611v1</link>
<guid>2602.19611v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Mingxiu Cai, Zhe Zhang, Gaochang Wu, Tianyou Chai, Xiatian Zhu
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Unsupervised Anomaly Detection (UAD) aims to identify abnormal regions by establishing correspondences between test images and normal templates. Existing methods primarily rely on image reconstruction or template retrieval but face a fundamental challenge: matching between test images and normal templates inevitably introduces noise due to intra-class variations, imperfect correspondences, and limited templates. Observing that Retrieval-Augmented Generation (RAG) leverages retrieved samples directly in the generation process, we reinterpret UAD through this lens and introduce \textbf{RAID}, a retrieval-augmented UAD framework designed for noise-resilient anomaly detection and localization. Unlike standard RAG that enriches context or knowledge, we focus on using retrieved normal samples to guide noise suppression in anomaly map generation. RAID retrieves class-, semantic-, and instance-level representations from a hierarchical vector database, forming a coarse-to-fine pipeline. A matching cost volume correlates the input with retrieved exemplars, followed by a guided Mixture-of-Experts (MoE) network that leverages the retrieved samples to adaptively suppress matching noise and produce fine-grained anomaly maps. RAID achieves state-of-the-art performance across full-shot, few-shot, and multi-dataset settings on MVTec, VisA, MPDD, and BTAD benchmarks. \href{https://github.com/Mingxiu-Cai/RAID}{https://github.com/Mingxiu-Cai/RAID}.]]></description>
<pubDate>Mon, 23 Feb 2026 08:54:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Variational Inference for Bayesian MIDAS Regression]]></title>
<link>http://arxiv.org/abs/2602.19610v1</link>
<guid>2602.19610v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, stat.CO, stat.ME, stat.ML
Authors: Luigi Simeone
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We develop a Coordinate Ascent Variational Inference (CAVI) algorithm for Bayesian Mixed Data Sampling (MIDAS) regression with linear weight parameteri zations. The model separates impact coe cients from weighting function parameters through a normalization constraint, creating a bilinear structure that renders generic Hamiltonian Monte Carlo samplers unreliable while preserving conditional conju gacy exploitable by CAVI. Each variational update admits a closed-form solution: Gaussian for regression coe cients and weight parameters, Inverse-Gamma for the error variance. The algorithm propagates uncertainty across blocks through second moments, distinguishing it from naive plug-in approximations. In a Monte Carlo study spanning 21 data-generating con gurations with up to 50 predictors, CAVI produces posterior means nearly identical to a block Gibbs sampler benchmark while achieving speedups of 107x to 1,772x (Table 9). Generic automatic di eren tiation VI (ADVI), by contrast, produces bias 714 times larger while being orders of magnitude slower, con rming the value of model-speci c derivations. Weight function parameters maintain excellent calibration (coverage above 92%) across all con gurations. Impact coe cient credible intervals exhibit the underdispersion characteristic of mean- eld approximations, with coverage declining from 89% to 55% as the number of predictors grows a documented trade-o between speed and interval calibration that structured variational methods can address. An empirical application to realized volatility forecasting on S&P 500 daily returns con rms that CAVI and Gibbs sampling yield virtually identical point forecasts, with CAVI completing each monthly estimation in under 10 milliseconds.]]></description>
<pubDate>Mon, 23 Feb 2026 08:51:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning]]></title>
<link>http://arxiv.org/abs/2602.19608v1</link>
<guid>2602.19608v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI
Authors: Girmaw Abebe Tadesse, Titien Bartette, Andrew Hassanali, Allen Kim, Jonathan Chemla et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Looting at archaeological sites poses a severe risk to cultural heritage, yet monitoring thousands of remote locations remains operationally difficult. We present a scalable and satellite-based pipeline to detect looted archaeological sites, using PlanetScope monthly mosaics (4.7m/pixel) and a curated dataset of 1,943 archaeological sites in Afghanistan (898 looted, 1,045 preserved) with multi-year imagery (2016--2023) and site-footprint masks. We compare (i) end-to-end CNN classifiers trained on raw RGB patches and (ii) traditional machine learning (ML) trained on handcrafted spectral/texture features and embeddings from recent remote-sensing foundation models. Results indicate that ImageNet-pretrained CNNs combined with spatial masking reach an F1 score of 0.926, clearly surpassing the strongest traditional ML setup, which attains an F1 score of 0.710 using SatCLIP-V+RF+Mean, i.e., location and vision embeddings fed into a Random Forest with mean-based temporal aggregation. Ablation studies demonstrate that ImageNet pretraining (even in the presence of domain shift) and spatial masking enhance performance. In contrast, geospatial foundation model embeddings perform competitively with handcrafted features, suggesting that looting signatures are extremely localized. The repository is available at https://github.com/microsoft/looted_site_detection.]]></description>
<pubDate>Mon, 23 Feb 2026 08:50:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[CLCR: Cross-Level Semantic Collaborative Representation for Multimodal Learning]]></title>
<link>http://arxiv.org/abs/2602.19605v1</link>
<guid>2602.19605v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV, cs.AI, cs.MM
Authors: Chunlei Meng, Guanhong Huang, Rong Fu, Runmin Jian, Zhongxue Gan et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data. This oversight induces semantic misalignment and error propagation, thereby degrading representation quality. To address this issue, we propose Cross-Level Co-Representation (CLCR), which explicitly organizes each modality's features into a three-level semantic hierarchy and specifies level-wise constraints for cross-modal interactions. First, a semantic hierarchy encoder aligns shallow, mid, and deep features across modalities, establishing a common basis for interaction. And then, at each level, an Intra-Level Co-Exchange Domain (IntraCED) factorizes features into shared and private subspaces and restricts cross-modal attention to the shared subspace via a learnable token budget. This design ensures that only shared semantics are exchanged and prevents leakage from private channels. To integrate information across levels, the Inter-Level Co-Aggregation Domain (InterCAD) synchronizes semantic scales using learned anchors, selectively fuses the shared representations, and gates private cues to form a compact task representation. We further introduce regularization terms to enforce separation of shared and private features and to minimize cross-level interference. Experiments on six benchmarks spanning emotion recognition, event localization, sentiment analysis, and action recognition show that CLCR achieves strong performance and generalizes well across tasks.]]></description>
<pubDate>Mon, 23 Feb 2026 08:47:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[Manifold-Aligned Generative Transport]]></title>
<link>http://arxiv.org/abs/2602.19600v1</link>
<guid>2602.19600v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Xinyu Tian, Xiaotong Shen
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

High-dimensional generative modeling is fundamentally a manifold-learning problem: real data concentrate near a low-dimensional structure embedded in the ambient space. Effective generators must therefore balance support fidelity -- placing probability mass near the data manifold -- with sampling efficiency. Diffusion models often capture near-manifold structure but require many iterative denoising steps and can leak off-support; normalizing flows sample in one pass but are limited by invertibility and dimension preservation. We propose MAGT (Manifold-Aligned Generative Transport), a flow-like generator that learns a one-shot, manifold-aligned transport from a low-dimensional base distribution to the data space. Training is performed at a fixed Gaussian smoothing level, where the score is well-defined and numerically stable. We approximate this fixed-level score using a finite set of latent anchor points with self-normalized importance sampling, yielding a tractable objective. MAGT samples in a single forward pass, concentrates probability near the learned support, and induces an intrinsic density with respect to the manifold volume measure, enabling principled likelihood evaluation for generated samples. We establish finite-sample Wasserstein bounds linking smoothing level and score-approximation accuracy to generative fidelity, and empirically improve fidelity and manifold concentration across synthetic and benchmark datasets while sampling substantially faster than diffusion models.]]></description>
<pubDate>Mon, 23 Feb 2026 08:42:40 +0000</pubDate>
</item>
<item>
<title><![CDATA[Eye-Tracking-while-Reading: A Living Survey of Datasets with Open Library Support]]></title>
<link>http://arxiv.org/abs/2602.19598v1</link>
<guid>2602.19598v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Deborah N. Jakobi, David R. Reich, Paul Prasse, Jana M. Hofmann, Lena S. Bolliger et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Eye-tracking-while-reading corpora are a valuable resource for many different disciplines and use cases. Use cases range from studying the cognitive processes underlying reading to machine-learning-based applications, such as gaze-based assessments of reading comprehension. The past decades have seen an increase in the number and size of eye-tracking-while-reading datasets as well as increasing diversity with regard to the stimulus languages covered, the linguistic background of the participants, or accompanying psychometric or demographic data. The spread of data across different disciplines and the lack of data sharing standards across the communities lead to many existing datasets that cannot be easily reused due to a lack of interoperability. In this work, we aim at creating more transparency and clarity with regards to existing datasets and their features across different disciplines by i) presenting an extensive overview of existing datasets, ii) simplifying the sharing of newly created datasets by publishing a living overview online, https://dili-lab.github.io/datasets.html, presenting over 45 features for each dataset, and iii) integrating all publicly available datasets into the Python package pymovements which offers an eye-tracking datasets library. By doing so, we aim to strengthen the FAIR principles in eye-tracking-while-reading research and promote good scientific practices, such as reproducing and replicating studies.]]></description>
<pubDate>Mon, 23 Feb 2026 08:40:50 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning Mutual View Information Graph for Adaptive Adversarial Collaborative Perception]]></title>
<link>http://arxiv.org/abs/2602.19596v1</link>
<guid>2602.19596v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Yihang Tao, Senkang Hu, Haonan An, Zhengru Fang, Hangcheng Cao et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Collaborative perception (CP) enables data sharing among connected and autonomous vehicles (CAVs) to enhance driving safety. However, CP systems are vulnerable to adversarial attacks where malicious agents forge false objects via feature-level perturbations. Current defensive systems use threshold-based consensus verification by comparing collaborative and ego detection results. Yet, these defenses remain vulnerable to more sophisticated attack strategies that could exploit two critical weaknesses: (i) lack of robustness against attacks with systematic timing and target region optimization, and (ii) inadvertent disclosure of vulnerability knowledge through implicit confidence information in shared collaboration data. In this paper, we propose MVIG attack, a novel adaptive adversarial CP framework learning to capture vulnerability knowledge disclosed by different defensive CP systems from a unified mutual view information graph (MVIG) representation. Our approach combines MVIG representation with temporal graph learning to generate evolving fabrication risk maps and employs entropy-aware vulnerability search to optimize attack location, timing and persistence, enabling adaptive attacks with generalizability across various defensive configurations. Extensive evaluations on OPV2V and Adv-OPV2V datasets demonstrate that MVIG attack reduces defense success rates by up to 62\% against state-of-the-art defenses while achieving 47\% lower detection for persistent attacks at 29.9 FPS, exposing critical security gaps in CP systems. Code will be released at https://github.com/yihangtao/MVIG.git]]></description>
<pubDate>Mon, 23 Feb 2026 08:38:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?]]></title>
<link>http://arxiv.org/abs/2602.19594v1</link>
<guid>2602.19594v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Ayush Nangia, Shikhar Mishra, Aman Gokrani, Paras Chopra
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.]]></description>
<pubDate>Mon, 23 Feb 2026 08:37:53 +0000</pubDate>
</item>
<item>
<title><![CDATA[Detecting High-Potential SMEs with Heterogeneous Graph Neural Networks]]></title>
<link>http://arxiv.org/abs/2602.19591v1</link>
<guid>2602.19591v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Yijiashun Qi, Hanzhe Guo, Yijiazhen Qi
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Small and Medium Enterprises (SMEs) constitute 99.9% of U.S. businesses and generate 44% of economic activity, yet systematically identifying high-potential SMEs remains an open challenge. We introduce SME-HGT, a Heterogeneous Graph Transformer framework that predicts which SBIR Phase I awardees will advance to Phase II funding using exclusively public data. We construct a heterogeneous graph with 32,268 company nodes, 124 research topic nodes, and 13 government agency nodes connected by approximately 99,000 edges across three semantic relation types. SME-HGT achieves an AUPRC of 0.621 0.003 on a temporally-split test set, outperforming an MLP baseline (0.590 0.002) and R-GCN (0.608 0.013) across five random seeds. At a screening depth of 100 companies, SME-HGT attains 89.6% precision with a 2.14 lift over random selection. Our temporal evaluation protocol prevents information leakage, and our reliance on public data ensures reproducibility. These results demonstrate that relational structure among firms, research topics, and funding agencies provides meaningful signal for SME potential assessment, with implications for policymakers and early-stage investors.]]></description>
<pubDate>Mon, 23 Feb 2026 08:35:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Tri-Subspaces Disentanglement for Multimodal Sentiment Analysis]]></title>
<link>http://arxiv.org/abs/2602.19585v1</link>
<guid>2602.19585v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.MM, cs.AI
Authors: Chunlei Meng, Jiabin Luo, Zhenglin Yan, Zhenyu Yu, Rong Fu et al.
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Multimodal Sentiment Analysis (MSA) integrates language, visual, and acoustic modalities to infer human sentiment. Most existing methods either focus on globally shared representations or modality-specific features, while overlooking signals that are shared only by certain modality pairs. This limits the expressiveness and discriminative power of multimodal representations. To address this limitation, we propose a Tri-Subspace Disentanglement (TSD) framework that explicitly factorizes features into three complementary subspaces: a common subspace capturing global consistency, submodally-shared subspaces modeling pairwise cross-modal synergies, and private subspaces preserving modality-specific cues. To keep these subspaces pure and independent, we introduce a decoupling supervisor together with structured regularization losses. We further design a Subspace-Aware Cross-Attention (SACA) fusion module that adaptively models and integrates information from the three subspaces to obtain richer and more robust representations. Experiments on CMU-MOSI and CMU-MOSEI demonstrate that TSD achieves state-of-the-art performance across all key metrics, reaching 0.691 MAE on CMU-MOSI and 54.9% ACC-7 on CMU-MOSEI, and also transfers well to multimodal intent recognition tasks. Ablation studies confirm that tri-subspace disentanglement and SACA jointly enhance the modeling of multi-granular cross-modal sentiment cues.]]></description>
<pubDate>Mon, 23 Feb 2026 08:19:54 +0000</pubDate>
</item>
<item>
<title><![CDATA[Interpolation-Driven Machine Learning Approaches for Plume Shine Dose Estimation: A Comparison of XGBoost, Random Forest, and TabNet]]></title>
<link>http://arxiv.org/abs/2602.19584v1</link>
<guid>2602.19584v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG, cs.AI
Authors: Biswajit Sadhu, Kalpak Gupte, Trijit Sadhu, S. Anand
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Despite the success of machine learning (ML) in surrogate modeling, its use in radiation dose assessment is limited by safety-critical constraints, scarce training-ready data, and challenges in selecting suitable architectures for physics-dominated systems. Within this context, rapid and accurate plume shine dose estimation serves as a practical test case, as it is critical for nuclear facility safety assessment and radiological emergency response, while conventional photon-transport-based calculations remain computationally expensive. In this work, an interpolation-assisted ML framework was developed using discrete dose datasets generated with the pyDOSEIA suite for 17 gamma-emitting radionuclides across varying downwind distances, release heights, and atmospheric stability categories. The datasets were augmented using shape-preserving interpolation to construct dense, high-resolution training data. Two tree-based ML models (Random Forest and XGBoost) and one deep learning (DL) model (TabNet) were evaluated to examine predictive performance and sensitivity to dataset resolution. All models showed higher prediction accuracy with the interpolated high-resolution dataset than with the discrete data; however, XGBoost consistently achieved the highest accuracy. Interpretability analysis using permutation importance (tree-based models) and attention-based feature attribution (TabNet) revealed that performance differences stem from how the models utilize input features. Tree-based models focus mainly on dominant geometry-dispersion features (release height, stability category, and downwind distance), treating radionuclide identity as a secondary input, whereas TabNet distributes attention more broadly across multiple variables. For practical deployment, a web-based GUI was developed for interactive scenario evaluation and transparent comparison with photon-transport reference calculations.]]></description>
<pubDate>Mon, 23 Feb 2026 08:12:49 +0000</pubDate>
</item>
<item>
<title><![CDATA[DEEP: Docker-based Execution and Evaluation Platform]]></title>
<link>http://arxiv.org/abs/2602.19583v1</link>
<guid>2602.19583v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CL
Authors: Sergio GÃ³mez GonzÃ¡lez, Miguel Domingo, Francisco Casacuberta
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Comparative evaluation of several systems is a recurrent task in researching. It is a key step before deciding which system to use for our work, or, once our research has been conducted, to demonstrate the potential of the resulting model. Furthermore, it is the main task of competitive, public challenges evaluation. Our proposed software (DEEP) automates both the execution and scoring of machine translation and optical character recognition models. Furthermore, it is easily extensible to other tasks. DEEP is prepared to receive dockerized systems, run them (extracting information at that same time), and assess hypothesis against some references. With this approach, evaluators can achieve a better understanding of the performance of each model. Moreover, the software uses a clustering algorithm based on a statistical analysis of the significance of the results yielded by each model, according to the evaluation metrics. As a result, evaluators are able to identify clusters of performance among the swarm of proposals and have a better understanding of the significance of their differences. Additionally, we offer a visualization web-app to ensure that the results can be adequately understood and interpreted. Finally, we present an exemplary case of use of DEEP.]]></description>
<pubDate>Mon, 23 Feb 2026 08:08:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[Advantage-based Temporal Attack in Reinforcement Learning]]></title>
<link>http://arxiv.org/abs/2602.19582v1</link>
<guid>2602.19582v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Shenghong He
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Extensive research demonstrates that Deep Reinforcement Learning (DRL) models are susceptible to adversarially constructed inputs (i.e., adversarial examples), which can mislead the agent to take suboptimal or unsafe actions. Recent methods improve attack effectiveness by leveraging future rewards to guide adversarial perturbation generation over sequential time steps (i.e., reward-based attacks). However, these methods are unable to capture dependencies between different time steps in the perturbation generation process, resulting in a weak temporal correlation between the current perturbation and previous perturbations.In this paper, we propose a novel method called Advantage-based Adversarial Transformer (AAT), which can generate adversarial examples with stronger temporal correlations (i.e., time-correlated adversarial examples) to improve the attack performance. AAT employs a multi-scale causal self-attention (MSCSA) mechanism to dynamically capture dependencies between historical information from different time periods and the current state, thus enhancing the correlation between the current perturbation and the previous perturbation. Moreover, AAT introduces a weighted advantage mechanism, which quantifies the effectiveness of a perturbation in a given state and guides the generation process toward high-performance adversarial examples by sampling high-advantage regions. Extensive experiments demonstrate that the performance of AAT matches or surpasses mainstream adversarial attack baselines on Atari, DeepMind Control Suite and Google football tasks.]]></description>
<pubDate>Mon, 23 Feb 2026 08:08:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[Leap+Verify: Regime-Adaptive Speculative Weight Prediction for Accelerating Neural Network Training]]></title>
<link>http://arxiv.org/abs/2602.19580v1</link>
<guid>2602.19580v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.LG
Authors: Jeremy McEntire
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

We introduce Leap+Verify, a framework that applies speculative execution -- predicting future model weights and validating predictions before acceptance -- to accelerate neural network training. Inspired by speculative decoding in language model inference and by the Automatically Scalable Computation (ASC) architecture for program execution, Leap+Verify decomposes training into three dynamically detected regimes (chaotic, transition, stable) using activation-space cosine similarity as a real-time Lyapunov proxy signal. Within each regime, analytic weight predictors (momentum, linear, quadratic extrapolation) attempt to forecast model parameters K training steps ahead; predictions are accepted only when validated against a held-out loss criterion. We evaluate Leap+Verify on GPT-2 124M and Qwen 2.5-1.5B trained on WikiText-103 across five random seeds, sweeping prediction depth K in {5, 10, 25, 50, 75, 100}. Momentum-based prediction (Adam moment extrapolation) fails catastrophically at both scales, with predicted losses exceeding actuals by 100-10,000x -- a universal norm explosion in optimizer-state extrapolation. Finite-difference predictors (linear, quadratic) succeed where momentum fails: at 124M, they achieve 24% strict acceptance at K=5 in stable regimes; at 1.5B, they achieve 37% strict acceptance in transition regimes. The scale-dependent finding is in regime distribution: GPT-2 124M spends 34% of training in stable regime, while Qwen 1.5B spends 64% in chaotic regime and reaches stable in only 0-2 of 40 checkpoints. Larger models are more predictable when predictable, but less often predictable -- the practical bottleneck shifts from predictor accuracy to regime availability. Cross-seed results are highly consistent (less than 1% validation loss variance), and the three-regime framework produces identical phase boundaries (plus or minus 50 steps) across seeds.]]></description>
<pubDate>Mon, 23 Feb 2026 08:01:44 +0000</pubDate>
</item>
<item>
<title><![CDATA[Goal-Oriented Influence-Maximizing Data Acquisition for Learning and Optimization]]></title>
<link>http://arxiv.org/abs/2602.19578v1</link>
<guid>2602.19578v1</guid>
<description><![CDATA[Source: arXiv
Tags: stat.ML, cs.LG
Authors: Weichi Yao, Bianca Dumitrascu, Bryan R. Goldsmith, Yixin Wang
Institution: MIT
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Active data acquisition is central to many learning and optimization tasks in deep neural networks, yet remains challenging because most approaches rely on predictive uncertainty estimates that are difficult to obtain reliably. To this end, we propose Goal-Oriented Influence- Maximizing Data Acquisition (GOIMDA), an active acquisition algorithm that avoids explicit posterior inference while remaining uncertainty-aware through inverse curvature. GOIMDA selects inputs by maximizing their expected influence on a user-specified goal functional, such as test loss, predictive entropy, or the value of an optimizer-recommended design. Leveraging first-order influence functions, we derive a tractable acquisition rule that combines the goal gradient, training-loss curvature, and candidate sensitivity to model parameters. We show theoretically that, for generalized linear models, GOIMDA approximates predictive-entropy minimization up to a correction term accounting for goal alignment and prediction bias, thereby, yielding uncertainty-aware behavior without maintaining a Bayesian posterior. Empirically, across learning tasks (including image and text classification) and optimization tasks (including noisy global optimization benchmarks and neural-network hyperparameter tuning), GOIMDA consistently reaches target performance with substantially fewer labeled samples or function evaluations than uncertainty-based active learning and Gaussian-process Bayesian optimization baselines.]]></description>
<pubDate>Mon, 23 Feb 2026 07:57:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[ConceptPrism: Concept Disentanglement in Personalized Diffusion Models via Residual Token Optimization]]></title>
<link>http://arxiv.org/abs/2602.19575v1</link>
<guid>2602.19575v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.CV
Authors: Minseo Kim, Minchan Kwon, Dongyeun Lee, Yunho Jeon, Junmo Kim
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Personalized text-to-image generation suffers from concept entanglement, where irrelevant residual information from reference images is captured, leading to a trade-off between concept fidelity and text alignment. Recent disentanglement approaches attempt to solve this utilizing manual guidance, such as linguistic cues or segmentation masks, which limits their applicability and fails to fully articulate the target concept. In this paper, we propose ConceptPrism, a novel framework that automatically disentangles the shared visual concept from image-specific residuals by comparing images within a set. Our method jointly optimizes a target token and image-wise residual tokens using two complementary objectives: a reconstruction loss to ensure fidelity, and a novel exclusion loss that compels residual tokens to discard the shared concept. This process allows the target token to capture the pure concept without direct supervision. Extensive experiments demonstrate that ConceptPrism effectively resolves concept entanglement, achieving a significantly improved trade-off between fidelity and alignment.]]></description>
<pubDate>Mon, 23 Feb 2026 07:46:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[CTC-TTS: LLM-based dual-streaming text-to-speech with CTC alignment]]></title>
<link>http://arxiv.org/abs/2602.19574v1</link>
<guid>2602.19574v1</guid>
<description><![CDATA[Source: arXiv
Tags: cs.AI, cs.SD
Authors: Hanwen Liu, Saierdaer Yusuyin, Hao Huang, Zhijian Ou
Institution: 
Published: 2026-02-23
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Large-language-model (LLM)-based text-to-speech (TTS) systems can generate natural speech, but most are not designed for low-latency dual-streaming synthesis. High-quality dual-streaming TTS depends on accurate text--speech alignment and well-designed training sequences that balance synthesis quality and latency. Prior work often relies on GMM-HMM based forced-alignment toolkits (e.g., MFA), which are pipeline-heavy and less flexible than neural aligners; fixed-ratio interleaving of text and speech tokens struggles to capture text--speech alignment regularities. We propose CTC-TTS, which replaces MFA with a CTC based aligner and introduces a bi-word based interleaving strategy. Two variants are designed: CTC-TTS-L (token concatenation along the sequence length) for higher quality and CTC-TTS-F (embedding stacking along the feature dimension) for lower latency. Experiments show that CTC-TTS outperforms fixed-ratio interleaving and MFA-based baselines on streaming synthesis and zero-shot tasks. Speech samples are available at https://ctctts.github.io/.]]></description>
<pubDate>Mon, 23 Feb 2026 07:44:14 +0000</pubDate>
</item>
</channel>
</rss>