<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>AI Papers - 2026-02-10</title>
<link>https://arxiv.org</link>
<description>AI papers as of 2026-02-10 - 50 papers</description>
<lastBuildDate>Tue, 10 Feb 2026 04:14:24 +0000</lastBuildDate>
<item>
<title><![CDATA[DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos]]></title>
<link>https://huggingface.co/papers/2602.06949</link>
<guid>2602.06949</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Shenyuan Gao, William Liang, Kaiyuan Zheng, Ayaan Malik, Seonghyeon Ye
Institution: 
Published: 2026-02-06
Score: 9/10
Citations: 0
Upvotes: 24
GitHub: 
Stars: 0

Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.]]></description>
<pubDate>Fri, 06 Feb 2026 18:49:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions]]></title>
<link>https://huggingface.co/papers/2602.05843</link>
<guid>2602.05843</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Fangzhi Xu, Hang Yan, Qiushi Sun, Jinyang Wu, Zixian Huang
Institution: 
Published: 2026-02-05
Score: 8/10
Citations: 0
Upvotes: 52
GitHub: 
Stars: 0

The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena]]></description>
<pubDate>Thu, 05 Feb 2026 16:31:43 +0000</pubDate>
</item>
<item>
<title><![CDATA[Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities]]></title>
<link>https://huggingface.co/papers/2602.05281</link>
<guid>2602.05281</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Pengyi Li, Elizaveta Goncharova, Andrey Kuznetsov, Ivan Oseledets
Institution: 
Published: 2026-02-05
Score: 8/10
Citations: 0
Upvotes: 13
GitHub: 
Stars: 0

Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.]]></description>
<pubDate>Thu, 05 Feb 2026 04:06:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[Uncovering Cross-Objective Interference in Multi-Objective Alignment]]></title>
<link>https://huggingface.co/papers/2602.06869</link>
<guid>2602.06869</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yining Lu, Meng Jiang
Institution: 
Published: 2026-02-06
Score: 8/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.
  To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.]]></description>
<pubDate>Fri, 06 Feb 2026 16:55:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing]]></title>
<link>https://huggingface.co/papers/2602.04837</link>
<guid>2602.04837</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zhaotian Weng, Antonis Antoniades, Deepak Nathani, Zhen Zhang, Xiao Pu
Institution: 
Published: 2026-02-04
Score: 8/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.]]></description>
<pubDate>Wed, 04 Feb 2026 18:29:36 +0000</pubDate>
</item>
<item>
<title><![CDATA[OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale]]></title>
<link>https://huggingface.co/papers/2602.05711</link>
<guid>2602.05711</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jingze Shi, Zhangyang Peng, Yizhang Zhu, Yifan Wu, Guang Liu
Institution: 
Published: 2026-02-05
Score: 8/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.]]></description>
<pubDate>Thu, 05 Feb 2026 14:37:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models]]></title>
<link>https://huggingface.co/papers/2602.04649</link>
<guid>2602.04649</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Binghai Wang, Yantao Liu, Yuxuan Liu, Tianyi Tang, Shenzhi Wang
Institution: 
Published: 2026-02-04
Score: 8/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.]]></description>
<pubDate>Wed, 04 Feb 2026 15:24:52 +0000</pubDate>
</item>
<item>
<title><![CDATA[ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution]]></title>
<link>https://huggingface.co/papers/2602.03075</link>
<guid>2602.03075</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Junjie Huang, Jiarui Qin, Di Yin, Weiwen Liu, Yong Yu
Institution: 
Published: 2026-02-03
Score: 8/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.]]></description>
<pubDate>Tue, 03 Feb 2026 04:04:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees]]></title>
<link>https://huggingface.co/papers/2602.06554</link>
<guid>2602.06554</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Tianyi Hu, Qingxu Fu, Yanxi Chen, Zhaoyang Liu, Bolin Ding
Institution: 
Published: 2026-02-06
Score: 8/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.]]></description>
<pubDate>Fri, 06 Feb 2026 09:57:23 +0000</pubDate>
</item>
<item>
<title><![CDATA[NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.06694</link>
<guid>2602.06694</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Hyochan Chong, Dongkyu Kim, Changdong Kim, Minseop Choi
Institution: 
Published: 2026-02-06
Score: 8/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.]]></description>
<pubDate>Fri, 06 Feb 2026 13:26:44 +0000</pubDate>
</item>
<item>
<title><![CDATA[When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning]]></title>
<link>https://huggingface.co/papers/2602.08236</link>
<guid>2602.08236</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao
Institution: 
Published: 2026-02-09
Score: 8/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.]]></description>
<pubDate>Mon, 09 Feb 2026 03:21:48 +0000</pubDate>
</item>
<item>
<title><![CDATA[Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making]]></title>
<link>https://huggingface.co/papers/2602.06570</link>
<guid>2602.06570</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Baichuan-M3 Team, Chengfeng Dou, Fan Yang, Fei Li, Jiyuan Jia
Institution: 
Published: 2026-02-06
Score: 7/10
Citations: 0
Upvotes: 56
GitHub: 
Stars: 0

We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.]]></description>
<pubDate>Fri, 06 Feb 2026 10:08:59 +0000</pubDate>
</item>
<item>
<title><![CDATA[MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration]]></title>
<link>https://huggingface.co/papers/2602.01734</link>
<guid>2602.01734</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Lianhai Ren, Yucheng Ding, Xiao Liu, Qianxiao Li, Peng Cheng
Institution: 
Published: 2026-02-02
Score: 7/10
Citations: 0
Upvotes: 29
GitHub: 
Stars: 0

Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via μP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.]]></description>
<pubDate>Mon, 02 Feb 2026 07:18:45 +0000</pubDate>
</item>
<item>
<title><![CDATA[Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math]]></title>
<link>https://huggingface.co/papers/2602.06291</link>
<guid>2602.06291</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Guijin Son, Donghun Yang, Hitesh Laxmichand Patel, Hyunwoo Ko, Amit Agarwal
Institution: MIT
Published: 2026-02-06
Score: 7/10
Citations: 0
Upvotes: 16
GitHub: 
Stars: 0

Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.]]></description>
<pubDate>Fri, 06 Feb 2026 01:10:28 +0000</pubDate>
</item>
<item>
<title><![CDATA[Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training]]></title>
<link>https://huggingface.co/papers/2602.05940</link>
<guid>2602.05940</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Junxiao Liu, Zhijun Wang, Yixiao Li, Zhejian Lai, Liqian Huang
Institution: 
Published: 2026-02-05
Score: 7/10
Citations: 0
Upvotes: 16
GitHub: 
Stars: 0

Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.]]></description>
<pubDate>Thu, 05 Feb 2026 17:55:09 +0000</pubDate>
</item>
<item>
<title><![CDATA[OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention]]></title>
<link>https://huggingface.co/papers/2602.05847</link>
<guid>2602.05847</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zhangquan Chen, Jiale Tao, Ruihuang Li, Yihao Hu, Ruitao Chen
Institution: 
Published: 2026-02-05
Score: 7/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to "think with omnimodal cues" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.]]></description>
<pubDate>Thu, 05 Feb 2026 16:35:19 +0000</pubDate>
</item>
<item>
<title><![CDATA[AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research]]></title>
<link>https://huggingface.co/papers/2602.06540</link>
<guid>2602.06540</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yishan Li, Wentong Chen, Yukun Yan, Mingwei Li, Sen Mei
Institution: 
Published: 2026-02-06
Score: 7/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.]]></description>
<pubDate>Fri, 06 Feb 2026 09:45:04 +0000</pubDate>
</item>
<item>
<title><![CDATA[Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control]]></title>
<link>https://huggingface.co/papers/2601.21363</link>
<guid>2601.21363</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su
Institution: 
Published: 2026-01-29
Score: 7/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.]]></description>
<pubDate>Thu, 29 Jan 2026 07:43:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[Vision Transformer Finetuning Benefits from Non-Smooth Components]]></title>
<link>https://huggingface.co/papers/2602.06883</link>
<guid>2602.06883</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ambroise Odonnat, Laetitia Chapel, Romain Tavenard, Ievgen Redko
Institution: 
Published: 2026-02-06
Score: 7/10
Citations: 0
Upvotes: 3
GitHub: 
Stars: 0

The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.]]></description>
<pubDate>Fri, 06 Feb 2026 17:12:22 +0000</pubDate>
</item>
<item>
<title><![CDATA[Uncertainty Drives Social Bias Changes in Quantized Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.06181</link>
<guid>2602.06181</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Stanley Z. Hua, Sanae Lotfi, Irene Y. Chen
Institution: 
Published: 2026-02-05
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.]]></description>
<pubDate>Thu, 05 Feb 2026 20:37:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction]]></title>
<link>https://huggingface.co/papers/2602.06129</link>
<guid>2602.06129</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz
Institution: 
Published: 2026-02-05
Score: 7/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.]]></description>
<pubDate>Thu, 05 Feb 2026 19:01:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.07026</link>
<guid>2602.07026</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Xiaomin Yu, Yi Xin, Wenjie Zhang, Chonghan Liu, Hanzhen Zhao
Institution: 
Published: 2026-02-02
Score: 6/10
Citations: 0
Upvotes: 31
GitHub: 
Stars: 0

Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.]]></description>
<pubDate>Mon, 02 Feb 2026 13:59:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Pisets: A Robust Speech Recognition System for Lectures and Interviews]]></title>
<link>https://huggingface.co/papers/2601.18415</link>
<guid>2601.18415</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin, Mikhail Klementev, Roman Derunets
Institution: 
Published: 2026-01-26
Score: 6/10
Citations: 0
Upvotes: 29
GitHub: 
Stars: 0

This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.]]></description>
<pubDate>Mon, 26 Jan 2026 12:14:51 +0000</pubDate>
</item>
<item>
<title><![CDATA[Self-Improving World Modelling with Latent Actions]]></title>
<link>https://huggingface.co/papers/2602.06130</link>
<guid>2602.06130</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yifu Qiu, Zheng Zhao, Waylon Li, Yftah Ziser, Anna Korhonen
Institution: 
Published: 2026-02-05
Score: 6/10
Citations: 0
Upvotes: 21
GitHub: 
Stars: 0

Internal modelling of the world -- predicting transitions between previous states X and next states Y under actions Z -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) P_θ(Y|X,Z) and an Inverse Dynamics Modelling (IDM) Q_φ(Z|X,Y). SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.]]></description>
<pubDate>Thu, 05 Feb 2026 19:04:41 +0000</pubDate>
</item>
<item>
<title><![CDATA[LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning]]></title>
<link>https://huggingface.co/papers/2602.07075</link>
<guid>2602.07075</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Xinwu Ye, Yicheng Mao, Jia Zhang, Yimeng Liu, Li Hao
Institution: 
Published: 2026-02-06
Score: 6/10
Citations: 0
Upvotes: 10
GitHub: 
Stars: 0

Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.]]></description>
<pubDate>Fri, 06 Feb 2026 01:28:27 +0000</pubDate>
</item>
<item>
<title><![CDATA[EgoAVU: Egocentric Audio-Visual Understanding]]></title>
<link>https://huggingface.co/papers/2602.06139</link>
<guid>2602.06139</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ashish Seth, Xinhao Mei, Changsheng Zhao, Varun Nagaraja, Ernie Chang
Institution: 
Published: 2026-02-05
Score: 6/10
Citations: 0
Upvotes: 10
GitHub: 
Stars: 0

Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.]]></description>
<pubDate>Thu, 05 Feb 2026 19:16:55 +0000</pubDate>
</item>
<item>
<title><![CDATA[RelayGen: Intra-Generation Model Switching for Efficient Reasoning]]></title>
<link>https://huggingface.co/papers/2602.06454</link>
<guid>2602.06454</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jiwon Song, Yoongon Kim, Jae-Joon Kim
Institution: 
Published: 2026-02-06
Score: 6/10
Citations: 0
Upvotes: 8
GitHub: 
Stars: 0

Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.]]></description>
<pubDate>Fri, 06 Feb 2026 07:35:01 +0000</pubDate>
</item>
<item>
<title><![CDATA[RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs]]></title>
<link>https://huggingface.co/papers/2602.05367</link>
<guid>2602.05367</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Youngcheon You, Banseok Lee, Minseop Choi, Seonyoung Kim, Hyochan Chong
Institution: 
Published: 2026-02-05
Score: 6/10
Citations: 0
Upvotes: 7
GitHub: 
Stars: 0

Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (pm1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a 4.49times inference speed-up over full-precision models on an RTX 4090.]]></description>
<pubDate>Thu, 05 Feb 2026 06:41:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs]]></title>
<link>https://huggingface.co/papers/2602.06566</link>
<guid>2602.06566</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Niccolo Avogaro, Nayanika Debnath, Li Mi, Thomas Frick, Junling Wang
Institution: 
Published: 2026-02-06
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the V^* VQA benchmark by 6.7 percentage points, and it surpasses "thinking with images" by 4.6 points on a challenging OOD task despite requiring a 200times lower token budget.]]></description>
<pubDate>Fri, 06 Feb 2026 10:05:25 +0000</pubDate>
</item>
<item>
<title><![CDATA[Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion]]></title>
<link>https://huggingface.co/papers/2602.06724</link>
<guid>2602.06724</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Tian Lan, Felix Henry, Bin Zhu, Qianghuai Jia, Junyang Ren
Institution: 
Published: 2026-02-06
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce Table-as-Search (TaS), a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.]]></description>
<pubDate>Fri, 06 Feb 2026 14:18:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search]]></title>
<link>https://huggingface.co/papers/2602.04454</link>
<guid>2602.04454</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Tianming Liang, Qirui Du, Jian-Fang Hu, Haichao Jiang, Zicheng Lin
Institution: 
Published: 2026-02-04
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.]]></description>
<pubDate>Wed, 04 Feb 2026 11:33:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs]]></title>
<link>https://huggingface.co/papers/2602.01064</link>
<guid>2602.01064</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ruihan Jin, Pengpeng Shao, Zhengqi Wen, Jinyang Wu, Mingkuan Feng
Institution: 
Published: 2026-02-01
Score: 6/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of Knowledge Purification, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.]]></description>
<pubDate>Sun, 01 Feb 2026 07:19:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[Learning a Generative Meta-Model of LLM Activations]]></title>
<link>https://huggingface.co/papers/2602.06964</link>
<guid>2602.06964</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt
Institution: 
Published: 2026-02-06
Score: 6/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating "meta-models" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.]]></description>
<pubDate>Fri, 06 Feb 2026 18:59:56 +0000</pubDate>
</item>
<item>
<title><![CDATA[F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare]]></title>
<link>https://huggingface.co/papers/2602.06717</link>
<guid>2602.06717</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Daniil Plyusov, Alexey Gorbatovski, Boris Shaposhnikov, Viacheslav Sinii, Alexey Malakhov
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 59
GitHub: 
Stars: 0

Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.]]></description>
<pubDate>Fri, 06 Feb 2026 14:07:30 +0000</pubDate>
</item>
<item>
<title><![CDATA[Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO]]></title>
<link>https://huggingface.co/papers/2602.06422</link>
<guid>2602.06422</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yunze Tong, Mushui Liu, Canyu Zhao, Wanggui He, Shiyi Zhang
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 15
GitHub: 
Stars: 0

Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.]]></description>
<pubDate>Fri, 06 Feb 2026 06:37:10 +0000</pubDate>
</item>
<item>
<title><![CDATA[Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers]]></title>
<link>https://huggingface.co/papers/2602.06079</link>
<guid>2602.06079</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Liangyu Wang, Siqi Zhang, Junjie Wang, Yiming Dong, Bo Zheng
Institution: 
Published: 2026-02-04
Score: 5/10
Citations: 0
Upvotes: 15
GitHub: 
Stars: 0

The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.]]></description>
<pubDate>Wed, 04 Feb 2026 07:38:24 +0000</pubDate>
</item>
<item>
<title><![CDATA[POINTS-GUI-G: GUI-Grounding Journey]]></title>
<link>https://huggingface.co/papers/2602.06391</link>
<guid>2602.06391</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Zhongyin Zhao, Yuan Liu, Yikun Liu, Haicheng Wang, Le Tian
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 15
GitHub: 
Stars: 0

The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.]]></description>
<pubDate>Fri, 06 Feb 2026 05:14:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth]]></title>
<link>https://huggingface.co/papers/2602.07962</link>
<guid>2602.07962</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Weihao Zeng, Yuzhen Huang, Junxian He
Institution: 
Published: 2026-02-08
Score: 5/10
Citations: 0
Upvotes: 9
GitHub: 
Stars: 0

Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench]]></description>
<pubDate>Sun, 08 Feb 2026 13:20:39 +0000</pubDate>
</item>
<item>
<title><![CDATA[Large Language Model Reasoning Failures]]></title>
<link>https://huggingface.co/papers/2602.06176</link>
<guid>2602.06176</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Peiyang Song, Pengrui Han, Noah Goodman
Institution: 
Published: 2026-02-05
Score: 5/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.]]></description>
<pubDate>Thu, 05 Feb 2026 20:29:26 +0000</pubDate>
</item>
<item>
<title><![CDATA[PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks]]></title>
<link>https://huggingface.co/papers/2602.06663</link>
<guid>2602.06663</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Junxian Li, Kai Liu, Leyang Chen, Weida Wang, Zhixin Wang
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.]]></description>
<pubDate>Fri, 06 Feb 2026 12:47:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks]]></title>
<link>https://huggingface.co/papers/2602.06854</link>
<guid>2602.06854</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Mingqian Feng, Xiaodong Liu, Weiwei Yang, Jialin Song, Xuekai Zhu
Institution: 
Published: 2026-02-06
Score: 5/10
Citations: 0
Upvotes: 4
GitHub: 
Stars: 0

Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.]]></description>
<pubDate>Fri, 06 Feb 2026 16:44:57 +0000</pubDate>
</item>
<item>
<title><![CDATA[SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue]]></title>
<link>https://huggingface.co/papers/2602.03548</link>
<guid>2602.03548</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yuqin Dai, Ning Gao, Wei Zhang, Jie Wang, Zichen Luo
Institution: 
Published: 2026-02-03
Score: 5/10
Citations: 0
Upvotes: 2
GitHub: 
Stars: 0

Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.]]></description>
<pubDate>Tue, 03 Feb 2026 14:01:11 +0000</pubDate>
</item>
<item>
<title><![CDATA[AtlasPatch: An Efficient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology]]></title>
<link>https://huggingface.co/papers/2602.03998</link>
<guid>2602.03998</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Ahmed Alagha, Christopher Leclerc, Yousef Kotp, Omar Metwally, Calvin Moras
Institution: 
Published: 2026-02-03
Score: 5/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction, is foundational to AI-driven computational pathology workflows. This remains a major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection, or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an efficient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatch's tissue detection module is trained on a heterogeneous and semi-manually annotated dataset of ~30,000 WSI thumbnails, using efficient fine-tuning of the Segment-Anything model. The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly into common image encoders for embedding or store patch images, all efficiently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning, matching state-of-the-art performance while operating at a fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch.]]></description>
<pubDate>Tue, 03 Feb 2026 20:32:07 +0000</pubDate>
</item>
<item>
<title><![CDATA[AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders]]></title>
<link>https://huggingface.co/papers/2602.05027</link>
<guid>2602.05027</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich, Assel Yermekova, Laida Kushnareva
Institution: 
Published: 2026-02-04
Score: 4/10
Citations: 0
Upvotes: 49
GitHub: 
Stars: 0

Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.]]></description>
<pubDate>Wed, 04 Feb 2026 20:29:16 +0000</pubDate>
</item>
<item>
<title><![CDATA[On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models]]></title>
<link>https://huggingface.co/papers/2602.03392</link>
<guid>2602.03392</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Shumin Wang, Yuexiang Xie, Wenhao Zhang, Yuchang Sun, Yanxi Chen
Institution: 
Published: 2026-02-03
Score: 4/10
Citations: 0
Upvotes: 48
GitHub: 
Stars: 0

Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.]]></description>
<pubDate>Tue, 03 Feb 2026 11:14:58 +0000</pubDate>
</item>
<item>
<title><![CDATA[QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals]]></title>
<link>https://huggingface.co/papers/2602.02581</link>
<guid>2602.02581</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Nan Zhang, Eugene Kwek, Yusen Zhang, Muyu Pan, Suhang Wang
Institution: 
Published: 2026-01-31
Score: 4/10
Citations: 0
Upvotes: 6
GitHub: 
Stars: 0

Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.]]></description>
<pubDate>Sat, 31 Jan 2026 16:19:20 +0000</pubDate>
</item>
<item>
<title><![CDATA[compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data]]></title>
<link>https://huggingface.co/papers/2602.06669</link>
<guid>2602.06669</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Lucie Termignon, Simonas Zilinskas, Hadrien Pélissier, Aurélien Barrot, Nicolas Chesnais
Institution: 
Published: 2026-02-06
Score: 4/10
Citations: 0
Upvotes: 5
GitHub: 
Stars: 0

Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.]]></description>
<pubDate>Fri, 06 Feb 2026 12:53:44 +0000</pubDate>
</item>
<item>
<title><![CDATA[SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization]]></title>
<link>https://huggingface.co/papers/2602.04811</link>
<guid>2602.04811</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Jiarui Yuan, Tailin Jin, Weize Chen, Zeyuan Liu, Zhiyuan Liu
Institution: 
Published: 2026-02-04
Score: 4/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.]]></description>
<pubDate>Wed, 04 Feb 2026 17:58:32 +0000</pubDate>
</item>
<item>
<title><![CDATA[Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference]]></title>
<link>https://huggingface.co/papers/2601.23039</link>
<guid>2601.23039</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Yizhi Liu
Institution: 
Published: 2026-01-30
Score: 4/10
Citations: 0
Upvotes: 1
GitHub: 
Stars: 0

Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing εto 0 is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: Premature Mode Collapse. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/ε). To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law.]]></description>
<pubDate>Fri, 30 Jan 2026 14:47:18 +0000</pubDate>
</item>
<item>
<title><![CDATA[ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking]]></title>
<link>https://huggingface.co/papers/2602.06445</link>
<guid>2602.06445</guid>
<description><![CDATA[Source: HuggingFace
Tags: 
Authors: Weidong Huang, Jingwen Zhang, Jiongye Li, Shibowen Zhang, Jiayang Wu
Institution: 
Published: 2026-02-06
Score: 4/10
Citations: 0
Upvotes: 0
GitHub: 
Stars: 0

Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.]]></description>
<pubDate>Fri, 06 Feb 2026 07:14:43 +0000</pubDate>
</item>
</channel>
</rss>