{
  "generated_at": "2026-02-03T14:41:15Z",
  "date": "2026-02-03",
  "total_count": 233,
  "papers": [
    {
      "id": "2602.02276",
      "title": "Kimi K2.5: Visual Agentic Intelligence",
      "link": "https://huggingface.co/papers/2602.02276",
      "pdf_link": "https://arxiv.org/pdf/2602.02276.pdf",
      "authors": "Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai",
      "institution": "",
      "abstract": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "Multimodal Learning",
        "Agentic Intelligence",
        "Parallel Agent Orchestration"
      ],
      "score": 9,
      "score_reason": "Agentic Intelligence",
      "citations": 0,
      "upvotes": 131,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The introduction of Agent Swarm, a self-directed parallel agent orchestration framework, enables dynamic decomposition of complex tasks into heterogeneous sub-problems and concurrent execution, advancing general agentic intelligence.",
      "why_it_matters": "This work matters because it demonstrates state-of-the-art results across multiple domains, including coding, vision, reasoning, and agentic tasks, and has the potential to accelerate research in multimodal and agentic AI applications.",
      "limitations": "The main limitation of this work is that the evaluation of Agent Swarm's performance is based on a limited set of tasks and domains, and its scalability and generalizability to more complex and diverse tasks remain to be thoroughly investigated."
    },
    {
      "id": "2602.02488",
      "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
      "link": "https://huggingface.co/papers/2602.02488",
      "pdf_link": "https://arxiv.org/pdf/2602.02488.pdf",
      "authors": "Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang",
      "institution": "",
      "abstract": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "Reinforcement Learning",
        "Dynamic Environment Adaptation",
        "Reward Model Optimization"
      ],
      "score": 8,
      "score_reason": "Dynamic RL System",
      "citations": 0,
      "upvotes": 23,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, enabling adaptive learning in complex scenarios.",
      "why_it_matters": "This research matters because it has the potential to improve the performance of large language models and agentic systems by amplifying learning signals and strengthening the overall RL system, leading to substantial gains in various tasks.",
      "limitations": "The main limitation of this work is that the effectiveness of the proposed framework relies on the quality of the critic feedback and the consistency of the reward model, which may not always be guaranteed in practice."
    },
    {
      "id": "2602.02214",
      "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation",
      "link": "https://huggingface.co/papers/2602.02214",
      "pdf_link": "https://arxiv.org/pdf/2602.02214.pdf",
      "authors": "Hongzhou Zhu, Min Zhao, Guande He, Hang Su, Chongxuan Li",
      "institution": "",
      "abstract": "To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: https://thu-ml.github.io/CausalForcing.github.io/{https://thu-ml.github.io/CausalForcing.github.io/}",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "autoregressive diffusion models",
        "video generation",
        "knowledge distillation"
      ],
      "score": 8,
      "score_reason": "Real-Time Video",
      "citations": 0,
      "upvotes": 17,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes Causal Forcing, a novel method that uses an autoregressive teacher for ODE initialization to bridge the architectural gap between bidirectional and autoregressive diffusion models for real-time interactive video generation.",
      "why_it_matters": "This work matters because it enables more accurate and efficient distillation of pretrained bidirectional video diffusion models into few-step autoregressive models, which is crucial for real-time interactive video generation applications.",
      "limitations": "The main limitation of this approach is that it relies on the availability of a suitable autoregressive teacher model for ODE initialization, which may not always be feasible or require significant computational resources to train."
    },
    {
      "id": "2602.02486",
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "link": "https://huggingface.co/papers/2602.02486",
      "pdf_link": "https://arxiv.org/pdf/2602.02486.pdf",
      "authors": "Jialiang Zhu, Gongrui Zhang, Xiaolong Ma, Lin Xu, Miaosen Zhang",
      "institution": "",
      "abstract": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "Deep Search Agents",
        "Trajectory Compression",
        "Cross-Trajectory Exploration"
      ],
      "score": 8,
      "score_reason": "Breakthrough Design",
      "citations": 0,
      "upvotes": 11,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes Re-TRAC, a novel agentic framework that enables cross-trajectory exploration and iterative reflection for deep search agents by generating a structured state representation to condition subsequent trajectories.",
      "why_it_matters": "This work matters because it addresses the limitations of linear ReAct framework, which can lead to local optima and inefficient search, and demonstrates improved performance and reduced redundant exploration in empirical results.",
      "limitations": "The main limitation of this work is that the effectiveness of Re-TRAC-aware supervised fine-tuning is only demonstrated for smaller models, and its scalability to larger models is not thoroughly investigated."
    },
    {
      "id": "2602.02343",
      "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
      "link": "https://huggingface.co/papers/2602.02343",
      "pdf_link": "https://arxiv.org/pdf/2602.02343.pdf",
      "authors": "Ziwen Xu, Chenyan Wu, Hengyu Sun, Haiwen Hong, Mengru Wang",
      "institution": "",
      "abstract": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "language model control",
        "parameter dynamics",
        "activation manifold analysis"
      ],
      "score": 8,
      "score_reason": "Unifies language models",
      "citations": 0,
      "upvotes": 10,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces a unified framework for analyzing language model control methods, revealing a trade-off between preference and utility through a preference-utility analysis and an activation manifold perspective.",
      "why_it_matters": "Understanding this trade-off is crucial for developing more effective and efficient language model control methods, as it enables the design of interventions that balance target concept preference with coherent and task-valid generation.",
      "limitations": "The proposed framework and analysis may not generalize to all types of language models or control tasks, and the introduced SPLIT approach requires further evaluation across diverse scenarios to confirm its effectiveness."
    },
    {
      "id": "2602.02227",
      "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation",
      "link": "https://huggingface.co/papers/2602.02227",
      "pdf_link": "https://arxiv.org/pdf/2602.02227.pdf",
      "authors": "Harold Haodong Chen, Xinxiang Yin, Wen-Jie Shu, Hongfei Zhang, Zixin Zhang",
      "institution": "",
      "abstract": "Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by 16% on GenEval and 25% on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by 15% and 11% on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by 44% and token consumption by 51%; and (IV) exhibits 71% cognitive alignment with human intuition on reasoning invocation.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "Text-to-Image Generation",
        "Latent Space Reasoning",
        "Deep Learning"
      ],
      "score": 8,
      "score_reason": "Dynamic reasoning generation",
      "citations": 0,
      "upvotes": 10,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces LatentMorph, a novel framework that integrates implicit latent reasoning into text-to-image generation, allowing for dynamic refinement and self-refinement in continuous latent spaces.",
      "why_it_matters": "This work matters because it addresses the limitations of existing reasoning-augmented paradigms, which rely on explicit thought processes and suffer from inefficiencies, information loss, and cognitive mismatches, thereby enhancing the capabilities of text-to-image generation models.",
      "limitations": "The main limitation of this work is that the evaluation is primarily based on benchmark datasets and tasks, which may not fully capture the complexity and diversity of real-world text-to-image generation scenarios, potentially limiting the generalizability of the results."
    },
    {
      "id": "2602.01675",
      "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios",
      "link": "https://huggingface.co/papers/2602.01675",
      "pdf_link": "https://arxiv.org/pdf/2602.01675.pdf",
      "authors": "Yuanzhe Shen, Zisu Huang, Zhengyuan Wang, Muzhao Tian, Zhengkang Guo",
      "institution": "",
      "abstract": "As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce TRIP-Bench, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose GTPO, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "Reinforcement Learning",
        "Natural Language Processing",
        "Benchmarking for Interactive Agents"
      ],
      "score": 8,
      "score_reason": "New benchmark introduced",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces TRIP-Bench, a novel benchmark for evaluating long-horizon interactive agents in real-world travel-planning scenarios, and proposes GTPO, an online multi-turn reinforcement learning method for robust long-horizon training.",
      "why_it_matters": "This work matters because it addresses the gap in existing benchmarks for complex, real-world interactive agents, and provides a framework for evaluating and improving their performance in long, multi-turn interactions.",
      "limitations": "The main limitation of this work is that the proposed GTPO method is only evaluated on a single model, Qwen2.5-32B-Instruct, and its effectiveness on other models and scenarios is not explored."
    },
    {
      "id": "2602.01077",
      "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
      "link": "https://huggingface.co/papers/2602.01077",
      "pdf_link": "https://arxiv.org/pdf/2602.01077.pdf",
      "authors": "Haopeng Li, Shitong Shao, Wenliang Zhong, Zikai Zhou, Lichen Bai",
      "institution": "",
      "abstract": "Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-01",
      "tags": [],
      "topics": [
        "Diffusion Transformers",
        "Sparse Attention Mechanisms",
        "Efficient Computer Vision"
      ],
      "score": 8,
      "score_reason": "Efficient diffusion transformers",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The authors propose PISA, a novel Piecewise Sparse Attention mechanism that efficiently approximates non-critical attention blocks using block-wise Taylor expansion, rather than discarding them, to achieve sub-quadratic complexity.",
      "why_it_matters": "This work matters because it addresses the efficiency bottleneck of Diffusion Transformers, which are crucial for video and image generation tasks, by providing a speedup without compromising quality, making it a significant advancement for computer vision and machine learning applications.",
      "limitations": "The main limitation of this work is that the effectiveness of the proposed PISA mechanism may depend on the specific distributional stability of attention scores in different models and tasks, which may not always be guaranteed."
    },
    {
      "id": "2602.01897",
      "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs",
      "link": "https://huggingface.co/papers/2602.01897",
      "pdf_link": "https://arxiv.org/pdf/2602.01897.pdf",
      "authors": "Sungheon Jeong, Sanggeon Yun, Ryozo Masukawa, Wenjun Haung, Hanning Chen",
      "institution": "",
      "abstract": "Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce internal flow signatures that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact moving readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. Code is available at github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "Large Language Models",
        "Explainability",
        "Self-Checking and Refinement"
      ],
      "score": 8,
      "score_reason": "Internal auditing mechanism",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces internal flow signatures, a novel method for auditing decision formation in large language models (LLMs) by monitoring depthwise dynamics at a fixed inter-block boundary, enabling self-checking and refinement without modifying the base model.",
      "why_it_matters": "This research matters because it provides a lightweight and efficient way to detect and correct errors in LLMs, which is crucial for improving the reliability and trustworthiness of these models in real-world applications.",
      "limitations": "The main limitation of this approach is that it relies on a fixed inter-block monitoring boundary, which may not be optimal for all types of LLMs or tasks, and may require careful tuning for different models and datasets."
    },
    {
      "id": "2602.00919",
      "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
      "link": "https://huggingface.co/papers/2602.00919",
      "pdf_link": "https://arxiv.org/pdf/2602.00919.pdf",
      "authors": "I. Apanasevich, M. Artemyev, R. Babakyan, P. Fedotova, D. Grankin",
      "institution": "",
      "abstract": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-31",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Generalist robot framework",
      "citations": 0,
      "upvotes": 140,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02437",
      "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
      "link": "https://huggingface.co/papers/2602.02437",
      "pdf_link": "https://arxiv.org/pdf/2602.02437.pdf",
      "authors": "Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, Wei Song",
      "institution": "",
      "abstract": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Unified Framework",
      "citations": 0,
      "upvotes": 67,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02383",
      "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization",
      "link": "https://huggingface.co/papers/2602.02383",
      "pdf_link": "https://arxiv.org/pdf/2602.02383.pdf",
      "authors": "Maksim Afanasyev, Illarion Iov",
      "institution": "",
      "abstract": "Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "Large Language Models",
        "Preference Optimization",
        "Reinforcement Learning from Human Feedback"
      ],
      "score": 7,
      "score_reason": "Enhances preference",
      "citations": 0,
      "upvotes": 26,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces SLIME, a novel reference-free alignment objective that decouples preference learning from generation quality by incorporating a three-pronged objective to stabilize likelihood and enforce implicit margin constraints.",
      "why_it_matters": "This work matters because it addresses the critical objective mismatch in direct preference optimization methods, which can lead to 'unlearning' and 'formatting collapse' in Large Language Models, thereby improving the efficiency and stability of aligning LLMs with human preferences.",
      "limitations": "The main limitation of this work is that the dual-margin mechanism and stabilizing penalty may require careful hyperparameter tuning to balance the trade-off between preference learning and generation quality, which can be challenging in practice."
    },
    {
      "id": "2602.01756",
      "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation",
      "link": "https://huggingface.co/papers/2602.01756",
      "pdf_link": "https://arxiv.org/pdf/2602.01756.pdf",
      "authors": "Jun He, Junyan Ye, Zilong Huang, Dongzhi Jiang, Chenjue Zhang",
      "institution": "",
      "abstract": "While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Novel Image Generation",
      "citations": 0,
      "upvotes": 21,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02472",
      "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
      "link": "https://huggingface.co/papers/2602.02472",
      "pdf_link": "https://arxiv.org/pdf/2602.02472.pdf",
      "authors": "Qifan Yu, Xinyu Ma, Zhijian Zhuo, Minrui Wang, Deyi Liu",
      "institution": "",
      "abstract": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under 2times width expansion.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Enhances progressive learning",
      "citations": 0,
      "upvotes": 9,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01322",
      "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding",
      "link": "https://huggingface.co/papers/2602.01322",
      "pdf_link": "https://arxiv.org/pdf/2602.01322.pdf",
      "authors": "Panagiotis Koromilas, Andreas D. Demou, James Oldfield, Yannis Panagakis, Mihalis Nicolaou",
      "institution": "Mila",
      "abstract": "Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether \"Starbucks\" arises from the composition of \"star\" and \"coffee\" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10times larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency (r = 0.06 vs. r = 0.82 for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-01",
      "tags": [],
      "topics": [
        "Sparse Autoencoders",
        "Neural Network Interpretability",
        "Feature Interaction Modeling"
      ],
      "score": 7,
      "score_reason": "Models feature interactions",
      "citations": 0,
      "upvotes": 8,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces PolySAE, a novel extension of sparse autoencoders that models feature interactions via polynomial decoding, enabling the capture of compositional structure in neural network representations.",
      "why_it_matters": "This work matters because it addresses the limitation of traditional sparse autoencoders in modeling complex feature interactions, which is crucial for improving the interpretability and explainability of neural networks, particularly in natural language processing applications.",
      "limitations": "The main limitation of PolySAE is that it only captures pairwise and triple feature interactions, which may not be sufficient to model more complex and higher-order interactions, potentially limiting its applicability to certain domains or tasks."
    },
    {
      "id": "2602.02156",
      "title": "LoopViT: Scaling Visual ARC with Looped Transformers",
      "link": "https://huggingface.co/papers/2602.02156",
      "pdf_link": "https://arxiv.org/pdf/2602.02156.pdf",
      "authors": "Wen-Jie Shu, Xuerui Qiu, Rui-Jie Zhu, Harold Haodong Chen, Yexin Liu",
      "institution": "",
      "abstract": "Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Novel Architecture",
      "citations": 0,
      "upvotes": 8,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01511",
      "title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training",
      "link": "https://huggingface.co/papers/2602.01511",
      "pdf_link": "https://arxiv.org/pdf/2602.01511.pdf",
      "authors": "Ran Xu, Tianci Liu, Zihan Dong, Tony You, Ilgee Hong",
      "institution": "",
      "abstract": "Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Rubric-based reward modeling",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02453",
      "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
      "link": "https://huggingface.co/papers/2602.02453",
      "pdf_link": "https://arxiv.org/pdf/2602.02453.pdf",
      "authors": "Andong Chen, Wenxin Zhu, Qiuyu Ding, Yuchen Song, Muyun Yang",
      "institution": "",
      "abstract": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "Multimodal Reasoning",
        "Visual Storytelling",
        "Chain-of-Thought Reasoning"
      ],
      "score": 7,
      "score_reason": "Enhances multimodal reasoning",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces 'Thinking with Comics', a novel visual reasoning paradigm that leverages comics as a high information-density medium to enhance multimodal reasoning, particularly for temporal and causal reasoning tasks.",
      "why_it_matters": "This research matters because it offers a more efficient and effective alternative to thinking with images or videos, with potential applications in areas such as visual question answering, narrative understanding, and human-computer interaction.",
      "limitations": "The main limitation of this work is that it relies on a specific type of visual representation, namely comics, which may not be universally applicable or adaptable to all types of reasoning tasks or domains."
    },
    {
      "id": "2601.22674",
      "title": "VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration",
      "link": "https://huggingface.co/papers/2601.22674",
      "pdf_link": "https://arxiv.org/pdf/2601.22674.pdf",
      "authors": "Hanxun Yu, Wentong Li, Xuan Qu, Song Wang, Junbo Chen",
      "institution": "",
      "abstract": "Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-30",
      "tags": [],
      "topics": [
        "Multimodal Large Language Models",
        "Token Compression",
        "Training-Free Model Acceleration"
      ],
      "score": 7,
      "score_reason": "Unified token compression",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes a unified framework called VisionTrim, which integrates two novel modules, DVTS and TGVC, to compress visual tokens in multimodal large language models (MLLMs) without requiring retraining.",
      "why_it_matters": "This research matters because it addresses the computational inefficiency of MLLMs in handling high-resolution visual data, thereby facilitating their deployment in real-world applications such as image and video analysis.",
      "limitations": "The main limitation of this work is that it may not generalize well to scenarios where the textual cues are weak or noisy, potentially affecting the performance of the TGVC module and overall token compression quality."
    },
    {
      "id": "2602.01997",
      "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
      "link": "https://huggingface.co/papers/2602.01997",
      "pdf_link": "https://arxiv.org/pdf/2602.01997.pdf",
      "authors": "Safal Shrestha, Anubhav Shrestha, Aadim Nepal, Minwu Kim, Keith Ross",
      "institution": "",
      "abstract": "Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [
        "layer pruning",
        "generative reasoning",
        "large language models"
      ],
      "score": 7,
      "score_reason": "Limits of pruning",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces a systematic study on the limitations of layer pruning for generative reasoning in large language models, revealing significant degradation in multi-step reasoning tasks despite strong performance on classification benchmarks.",
      "why_it_matters": "Understanding the limits of layer pruning is crucial for developing efficient and effective model compression techniques that can be applied to real-world applications with limited computational resources and data availability.",
      "limitations": "The proposed mitigation strategy based on supervised finetuning with Self-Generated Responses still faces fundamental limitations in recovering generative reasoning capabilities, particularly at higher pruning ratios."
    },
    {
      "id": "2602.00130",
      "title": "On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks",
      "link": "https://huggingface.co/papers/2602.00130",
      "pdf_link": "https://arxiv.org/pdf/2602.00130.pdf",
      "authors": "Sumit Yadav",
      "institution": "MIT",
      "abstract": "We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 (p < 10^(-10)) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, p < 10^(-9)), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show |r| > 0.90. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-28",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Geometry Insights",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.00192",
      "title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange",
      "link": "https://huggingface.co/papers/2602.00192",
      "pdf_link": "https://arxiv.org/pdf/2602.00192.pdf",
      "authors": "Elif Nebioglu, Emirhan Bilgi, Adrian Popescu",
      "institution": "",
      "abstract": "Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\\% to 55\\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-30",
      "tags": [],
      "topics": [
        "image manipulation detection",
        "inpainting",
        "adversarial attacks"
      ],
      "score": 7,
      "score_reason": "Exposes detector flaws",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces Inpainting Exchange (INP-X), a novel operation that isolates the effect of global artifacts in inpainted images, revealing that state-of-the-art detectors rely heavily on these artifacts rather than locally synthesized content.",
      "why_it_matters": "This research matters because it highlights the vulnerability of current image manipulation detectors to adversarial attacks and underscores the need for content-aware detection methods that can accurately identify manipulated regions.",
      "limitations": "The main limitation of this study is that it focuses on VAE-based reconstruction methods, and the findings may not generalize to other inpainting techniques or image manipulation algorithms."
    },
    {
      "id": "2602.02185",
      "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models",
      "link": "https://huggingface.co/papers/2602.02185",
      "pdf_link": "https://arxiv.org/pdf/2602.02185.pdf",
      "authors": "Yu Zeng, Wenxuan Huang, Zhen Fang, Shuang Chen, Yufan Shen",
      "institution": "",
      "abstract": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "New Benchmark",
      "citations": 0,
      "upvotes": 103,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02084",
      "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder",
      "link": "https://huggingface.co/papers/2602.02084",
      "pdf_link": "https://arxiv.org/pdf/2602.02084.pdf",
      "authors": "Jane Luo, Chengyu Yin, Xin Zhang, Qingtao Li, Steven Liu",
      "institution": "",
      "abstract": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Unified Repository",
      "citations": 0,
      "upvotes": 68,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01590",
      "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles",
      "link": "https://huggingface.co/papers/2602.01590",
      "pdf_link": "https://arxiv.org/pdf/2602.01590.pdf",
      "authors": "Shaohan Wang, Benfeng Xu, Licheng Zhang, Mingxuan Du, Chiwei Zhu",
      "institution": "",
      "abstract": "Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "New Challenge",
      "citations": 0,
      "upvotes": 29,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02493",
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "link": "https://huggingface.co/papers/2602.02493",
      "pdf_link": "https://arxiv.org/pdf/2602.02493.pdf",
      "authors": "Zehong Ma, Ruihan Xu, Shiliang Zhang",
      "institution": "",
      "abstract": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves Diffusion",
      "citations": 0,
      "upvotes": 24,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01058",
      "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",
      "link": "https://huggingface.co/papers/2602.01058",
      "pdf_link": "https://arxiv.org/pdf/2602.01058.pdf",
      "authors": "Dylan Zhang, Yufeng Xu, Haojin Wang, Qingzhi Chen, Hao Peng",
      "institution": "",
      "abstract": "Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.\n  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.\n  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.\n  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-01",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improved SFT optimization",
      "citations": 0,
      "upvotes": 10,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.20613",
      "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
      "link": "https://huggingface.co/papers/2601.20613",
      "pdf_link": "https://arxiv.org/pdf/2601.20613.pdf",
      "authors": "Kaiyuan Chen, Qimin Wu, Taiyu Hou, Tianhao Tang, Xueyu Hu",
      "institution": "",
      "abstract": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-28",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "New benchmark introduced",
      "citations": 0,
      "upvotes": 7,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01660",
      "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
      "link": "https://huggingface.co/papers/2602.01660",
      "pdf_link": "https://arxiv.org/pdf/2602.01660.pdf",
      "authors": "Zhongyuan Peng, Caijun Xu, Changyi Xiao, Shibo Hong, Eli Zhang",
      "institution": "",
      "abstract": "Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Better Questions",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02110",
      "title": "An Empirical Study of World Model Quantization",
      "link": "https://huggingface.co/papers/2602.02110",
      "pdf_link": "https://arxiv.org/pdf/2602.02110.pdf",
      "authors": "Zhongqian Fu, Tianyi Zhao, Kai Han, Hang Zhou, Xinghao Chen",
      "institution": "",
      "abstract": "World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "World model quantization",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01296",
      "title": "Interacted Planes Reveal 3D Line Mapping",
      "link": "https://huggingface.co/papers/2602.01296",
      "pdf_link": "https://arxiv.org/pdf/2602.01296.pdf",
      "authors": "Zeran Ke, Bin Tan, Gui-Song Xia, Yujun Shen, Nan Xue",
      "institution": "",
      "abstract": "3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-01",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "3D Line Mapping",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.23000",
      "title": "Mano: Restriking Manifold Optimization for LLM Training",
      "link": "https://huggingface.co/papers/2601.23000",
      "pdf_link": "https://arxiv.org/pdf/2601.23000.pdf",
      "authors": "Yufei Gu, Zeke Xie",
      "institution": "",
      "abstract": "While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-30",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Optimizes LLM training",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01983",
      "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
      "link": "https://huggingface.co/papers/2602.01983",
      "pdf_link": "https://arxiv.org/pdf/2602.01983.pdf",
      "authors": "Xintian Shen, Jiawei Chen, Lihao Zheng, Hao Ma, Tao Wei",
      "institution": "",
      "abstract": "Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%uparrow and +23.04%uparrow on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves TIR models",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01842",
      "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models",
      "link": "https://huggingface.co/papers/2602.01842",
      "pdf_link": "https://arxiv.org/pdf/2602.01842.pdf",
      "authors": "Jinbin Bai, Yixuan Li, Yuchen Zhu, Yi Xin, Qingyu Shi",
      "institution": "",
      "abstract": "Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Efficient Scaling",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01801",
      "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention",
      "link": "https://huggingface.co/papers/2602.01801",
      "pdf_link": "https://arxiv.org/pdf/2602.01801.pdf",
      "authors": "Dvir Samuel, Issar Tzachor, Matan Levy, Micahel Green, Gal Chechik",
      "institution": "",
      "abstract": "Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves video diffusion",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01970",
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "link": "https://huggingface.co/papers/2602.01970",
      "pdf_link": "https://arxiv.org/pdf/2602.01970.pdf",
      "authors": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "institution": "",
      "abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Efficient RL training",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.21968",
      "title": "OVD: On-policy Verbal Distillation",
      "link": "https://huggingface.co/papers/2601.21968",
      "pdf_link": "https://arxiv.org/pdf/2601.21968.pdf",
      "authors": "Jing Xiong, Hui Shen, Shansan Gong, Yuxin Cheng, Jianghan Shen",
      "institution": "",
      "abstract": "Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-29",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "On-policy distillation method",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.00521",
      "title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory",
      "link": "https://huggingface.co/papers/2602.00521",
      "pdf_link": "https://arxiv.org/pdf/2602.00521.pdf",
      "authors": "Junhyuk Choi, Sohhyung Park, Chanhee Cho, Hyeonchu Park, Bugeun Kim",
      "institution": "",
      "abstract": "While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-31",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Evaluates LLM reliability",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01984",
      "title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling",
      "link": "https://huggingface.co/papers/2602.01984",
      "pdf_link": "https://arxiv.org/pdf/2602.01984.pdf",
      "authors": "Minyoung Lee, Yeji Park, Dongjun Hwang, Yejin Kim, Seong Joon Oh",
      "institution": "",
      "abstract": "Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves multi-image",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.22060",
      "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
      "link": "https://huggingface.co/papers/2601.22060",
      "pdf_link": "https://arxiv.org/pdf/2601.22060.pdf",
      "authors": "Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao",
      "institution": "",
      "abstract": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-29",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Incremental MLLM",
      "citations": 0,
      "upvotes": 116,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02361",
      "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
      "link": "https://huggingface.co/papers/2602.02361",
      "pdf_link": "https://arxiv.org/pdf/2602.02361.pdf",
      "authors": "Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao",
      "institution": "",
      "abstract": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Scalable SWE framework",
      "citations": 0,
      "upvotes": 31,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01541",
      "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
      "link": "https://huggingface.co/papers/2602.01541",
      "pdf_link": "https://arxiv.org/pdf/2602.01541.pdf",
      "authors": "Boyi Li, Yifan Shen, Yuanzhe Liu, Yifan Xu, Jiateng Liu",
      "institution": "",
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Cognitive MLLM",
      "citations": 0,
      "upvotes": 14,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01395",
      "title": "Rethinking Selective Knowledge Distillation",
      "link": "https://huggingface.co/papers/2602.01395",
      "pdf_link": "https://arxiv.org/pdf/2602.01395.pdf",
      "authors": "Almog Tavor, Itay Ebenspanger, Neil Cnaan, Mor Geva",
      "institution": "",
      "abstract": "Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-01",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Rethinks knowledge distillation",
      "citations": 0,
      "upvotes": 13,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02092",
      "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space",
      "link": "https://huggingface.co/papers/2602.02092",
      "pdf_link": "https://arxiv.org/pdf/2602.02092.pdf",
      "authors": "FSVideo Team, Qingyu Chen, Zhiyuan Fang, Haibin Huang, Xinwei Huang",
      "institution": "",
      "abstract": "We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space (64times64times4 spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Faster Video",
      "citations": 0,
      "upvotes": 13,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01538",
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "link": "https://huggingface.co/papers/2602.01538",
      "pdf_link": "https://arxiv.org/pdf/2602.01538.pdf",
      "authors": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "institution": "",
      "abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Avatar Interaction",
      "citations": 0,
      "upvotes": 13,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.22296",
      "title": "ParalESN: Enabling parallel information processing in Reservoir Computing",
      "link": "https://huggingface.co/papers/2601.22296",
      "pdf_link": "https://arxiv.org/pdf/2601.22296.pdf",
      "authors": "Matteo Pinna, Giacomo Lagomarsini, Andrea Ceni, Claudio Gallicchio",
      "institution": "",
      "abstract": "Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-29",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Parallelizes reservoir computing",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02354",
      "title": "Implicit neural representation of textures",
      "link": "https://huggingface.co/papers/2602.02354",
      "pdf_link": "https://arxiv.org/pdf/2602.02354.pdf",
      "authors": "Albert Kwok, Zheyuan Hu, Dounia Hammou",
      "institution": "",
      "abstract": "Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "New texture method",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.21759",
      "title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers",
      "link": "https://huggingface.co/papers/2601.21759",
      "pdf_link": "https://arxiv.org/pdf/2601.21759.pdf",
      "authors": "Meet Doshi, Vishwajeet Kumar, Yulong Li, Jaydeep Sen",
      "institution": "",
      "abstract": "General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-29",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Domain adaptation method",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01815",
      "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
      "link": "https://huggingface.co/papers/2602.01815",
      "pdf_link": "https://arxiv.org/pdf/2602.01815.pdf",
      "authors": "Yunhui Jang, Seonghyun Park, Jaehyung Kim, Sungsoo Ahn",
      "institution": "",
      "abstract": "Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Multi-agent system",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.01618",
      "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia",
      "link": "https://huggingface.co/papers/2602.01618",
      "pdf_link": "https://arxiv.org/pdf/2602.01618.pdf",
      "authors": "Panuthep Tasawong, Jian Gang Ngui, Alham Fikri Aji, Trevor Cohn, Peerat Limkonchotiwat",
      "institution": "",
      "abstract": "Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-02",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Cultural Safeguard",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02495v1",
      "title": "Reward-free Alignment for Conflicting Objectives",
      "link": "http://arxiv.org/abs/2602.02495v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02495v1",
      "authors": "Peter Chen, Xiaopeng Li, Xi Chen, Tianyi Lin",
      "institution": "",
      "abstract": "Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02494v1",
      "title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training",
      "link": "http://arxiv.org/abs/2602.02494v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02494v1",
      "authors": "Dulhan Jayalath, Oiwi Parker Jones",
      "institution": "",
      "abstract": "Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02491v1",
      "title": "New explanations and inference for least angle regression",
      "link": "http://arxiv.org/abs/2602.02491v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02491v1",
      "authors": "Karl B. Gregory, Daniel J. Nordman",
      "institution": "",
      "abstract": "Efron et al. (2004) introduced least angle regression (LAR) as an algorithm for linear predictions, intended as an alternative to forward selection with connections to penalized regression. However, LAR has remained somewhat of a \"black box,\" where some basic behavioral properties of LAR output are not well understood, including an appropriate termination point for the algorithm. We provide a novel framework for inference with LAR, which also allows LAR to be understood from new perspectives with several newly developed mathematical properties. The LAR algorithm at a data level can viewed as estimating a population counterpart \"path\" that organizes a response mean along regressor variables which are ordered according to a decreasing series of population \"correlation\" parameters; such parameters are shown to have meaningful interpretations for explaining variable contributions whereby zero correlations denote unimportant variables. In the output of LAR, estimates of all non-zero population correlations turn out to have independent normal distributions for use in inference, while estimates of zero-valued population correlations have a certain non-normal joint distribution. These properties help to provide a formal rule for stopping the LAR algorithm. While the standard bootstrap for regression can fail for LAR, a modified bootstrap provides a practical and formally justified tool for interpreting the entrance of variables and quantifying uncertainty in estimation. The LAR inference method is studied through simulation and illustrated with data examples.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02482v1",
      "title": "Expanding the Capabilities of Reinforcement Learning via Text Feedback",
      "link": "http://arxiv.org/abs/2602.02482v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02482v1",
      "authors": "Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak et al.",
      "institution": "",
      "abstract": "The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02481v1",
      "title": "Flow Policy Gradients for Robot Control",
      "link": "http://arxiv.org/abs/2602.02481v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02481v1",
      "authors": "Brent Yi, Hongsuk Choi, Himanshu Gaurav Singh, Xiaoyu Huang, Takara E. Truong et al.",
      "institution": "",
      "abstract": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.RO",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02477v1",
      "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability",
      "link": "http://arxiv.org/abs/2602.02477v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02477v1",
      "authors": "Xiao Liang, Zhong-Zhi Li, Zhenghao Lin, Eric Hancheng Jiang, Hengyuan Zhang et al.",
      "institution": "",
      "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02475v1",
      "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories",
      "link": "http://arxiv.org/abs/2602.02475v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02475v1",
      "authors": "Shraddha Barke, Arnav Goyal, Alind Khare, Avaljot Singh, Suman Nath et al.",
      "institution": "",
      "abstract": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02474v1",
      "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
      "link": "http://arxiv.org/abs/2602.02474v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02474v1",
      "authors": "Haozhen Zhang, Quanyu Long, Jianzhu Bao, Tao Feng, Weizhi Zhang et al.",
      "institution": "",
      "abstract": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02473v1",
      "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos",
      "link": "http://arxiv.org/abs/2602.02473v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02473v1",
      "authors": "Yinhuai Wang, Qihan Zhao, Yuen Fui Lau, Runyi Yu, Hok Wai Tsui et al.",
      "institution": "",
      "abstract": "Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.RO",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02471v1",
      "title": "Multi-head automated segmentation by incorporating detection head into the contextual layer neural network",
      "link": "http://arxiv.org/abs/2602.02471v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02471v1",
      "authors": "Edwin Kys, Febian Febian",
      "institution": "",
      "abstract": "Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \\pm 0.036$ versus $0.732 \\pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02470v1",
      "title": "Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge",
      "link": "http://arxiv.org/abs/2602.02470v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02470v1",
      "authors": "Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi",
      "institution": "",
      "abstract": "Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the \"reversal curse\" -- when trained on forward knowledge data of the form \"$A \\rightarrow B$\" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge \"$B \\leftarrow A$\" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form \"$A \\to A$\" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02469v1",
      "title": "Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation",
      "link": "http://arxiv.org/abs/2602.02469v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02469v1",
      "authors": "Ahmed M. Elshazly, Ahmed Arafa",
      "institution": "",
      "abstract": "We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \\emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \\emph{AgeTop-\\(k\\)}, which first picks the largest-magnitude entries and then chooses the \\(k\\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \\(k\\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\\(k\\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \\(k\\) depends on the channel, with smaller \\(k\\) being better in noisy settings.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.IT",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02468v1",
      "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts",
      "link": "http://arxiv.org/abs/2602.02468v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02468v1",
      "authors": "Aiden Yiliu Li, Xinyue Hao, Shilong Liu, Mengdi Wang",
      "institution": "",
      "abstract": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02467v1",
      "title": "Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models",
      "link": "http://arxiv.org/abs/2602.02467v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02467v1",
      "authors": "Noam Steinmetz Yalon, Ariel Goldstein, Liad Mudrik, Mor Geva",
      "institution": "",
      "abstract": "Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02465v1",
      "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
      "link": "http://arxiv.org/abs/2602.02465v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02465v1",
      "authors": "Jana Zeller, Thaddus Wiedemer, Fanfei Li, Thomas Klein, Prasanna Mayilvahanan et al.",
      "institution": "",
      "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02464v1",
      "title": "From Directions to Regions: Decomposing Activations in Language Models via Local Geometry",
      "link": "http://arxiv.org/abs/2602.02464v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02464v1",
      "authors": "Or Shafran, Shaked Ronen, Omri Fahn, Shauli Ravfogel, Atticus Geiger et al.",
      "institution": "",
      "abstract": "Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02462v1",
      "title": "Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models",
      "link": "http://arxiv.org/abs/2602.02462v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02462v1",
      "authors": "Gabriele Maraia, Marco Valentino, Fabio Massimo Zanzotto, Leonardo Ranaldi",
      "institution": "",
      "abstract": "Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02458v1",
      "title": "Conflict-Aware Client Selection for Multi-Server Federated Learning",
      "link": "http://arxiv.org/abs/2602.02458v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02458v1",
      "authors": "Mingwei Hong, Zheng Lin, Zehang Lin, Lin Li, Miao Yang et al.",
      "institution": "",
      "abstract": "Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.NI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02455v1",
      "title": "Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction",
      "link": "http://arxiv.org/abs/2602.02455v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02455v1",
      "authors": "Han Bao, Zheyuan Zhang, Pengcheng Jing, Zhengqing Yuan, Kaiwen Shi et al.",
      "institution": "",
      "abstract": "As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \\textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \\textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \\textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \\MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02454v1",
      "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
      "link": "http://arxiv.org/abs/2602.02454v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02454v1",
      "authors": "Ansh Kumar Sharma, Yixiang Sun, Ninghao Lu, Yunzhe Zhang, Jiarao Liu et al.",
      "institution": "",
      "abstract": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.RO",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02451v1",
      "title": "Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization",
      "link": "http://arxiv.org/abs/2602.02451v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02451v1",
      "authors": "Patrick Cooper, Alvaro Velasquez",
      "institution": "",
      "abstract": "Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02445v1",
      "title": "Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation",
      "link": "http://arxiv.org/abs/2602.02445v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02445v1",
      "authors": "Seo Taek Kong, R. Srikant",
      "institution": "",
      "abstract": "This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.\n  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $_n^{1/6}$, where $_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02444v1",
      "title": "RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval",
      "link": "http://arxiv.org/abs/2602.02444v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02444v1",
      "authors": "Tyler Skow, Alexander Martin, Benjamin Van Durme, Rama Chellappa, Reno Kriz",
      "institution": "",
      "abstract": "Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.IR",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02443v1",
      "title": "Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE",
      "link": "http://arxiv.org/abs/2602.02443v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02443v1",
      "authors": "Yuanteng Chen, Peisong Wang, Nanxin Zeng, Yuantian Shao, Gang Li et al.",
      "institution": "",
      "abstract": "Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02440v1",
      "title": "Large Language Models for Mental Health: A Multilingual Evaluation",
      "link": "http://arxiv.org/abs/2602.02440v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02440v1",
      "authors": "Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Ana-Maria Bucur, Stevie Chancellor, Marcos Zampieri",
      "institution": "Max Planck",
      "abstract": "Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02439v1",
      "title": "Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization",
      "link": "http://arxiv.org/abs/2602.02439v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02439v1",
      "authors": "Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz, Duygu Erisken, Rana Irem Turhan",
      "institution": "",
      "abstract": "Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SNN models with hardware-aware optimization for edge deployment. NeuEdge uses a temporal coding scheme that blends rate and spike-timing patterns to reduce spike activity while preserving accuracy, and a hardware-aware training procedure that co-optimizes network structure and on-chip placement to improve utilization on neuromorphic processors. An adaptive threshold mechanism adjusts neuron excitability from input statistics, reducing energy consumption without degrading performance. Across standard vision and audio benchmarks, NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency on edge hardware and an estimated 847 GOp/s/W energy efficiency. A case study on an autonomous-drone workload shows up to 312x energy savings relative to conventional deep neural networks while maintaining real-time operation.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.NE",
        "cs.ET",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02432v1",
      "title": "Maximizing Reliability with Bayesian Optimization",
      "link": "http://arxiv.org/abs/2602.02432v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02432v1",
      "authors": "Jack M. Buckingham, Ivo Couckuyt, Juergen Branke",
      "institution": "",
      "abstract": "Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02431v1",
      "title": "Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning",
      "link": "http://arxiv.org/abs/2602.02431v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02431v1",
      "authors": "Filip Kovaevi, Hong Chang Ji, Denny Wu, Mahdi Soltanolkotabi, Marco Mondelli",
      "institution": "",
      "abstract": "It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\\gtrsim d\\log d$ samples to achieve weak recovery. We first show that this $\\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \\simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \\gtrsim d$ samples and $T \\gtrsim\\log d$ gradient steps suffice to achieve strong (exact) recovery.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "stat.ML",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02427v1",
      "title": "Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning",
      "link": "http://arxiv.org/abs/2602.02427v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02427v1",
      "authors": "Qihao Wen, Jiahao Wang, Yang Nan, Pengfei He, Ravi Tandon et al.",
      "institution": "",
      "abstract": "Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02426v1",
      "title": "SelvaMask: Segmenting Trees in Tropical Forests and Beyond",
      "link": "http://arxiv.org/abs/2602.02426v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02426v1",
      "authors": "Simon-Olivier Duguay, Hugo Baudchon, Etienne Lalibert, Helene Muller-Landau, Gonzalo Rivas-Torres et al.",
      "institution": "",
      "abstract": "Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02425v1",
      "title": "Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization",
      "link": "http://arxiv.org/abs/2602.02425v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02425v1",
      "authors": "Amaru Caceres Arroyo, Lea Bogensperger, Ahmed Allam, Michael Krauthammer, Konrad Schindler et al.",
      "institution": "",
      "abstract": "Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02422v1",
      "title": "Poly-attention: a general scheme for higher-order self-attention",
      "link": "http://arxiv.org/abs/2602.02422v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02422v1",
      "authors": "Sayak Chakrabarti, Toniann Pitassi, Josh Alman",
      "institution": "",
      "abstract": "The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.\n  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.\n  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02419v1",
      "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
      "link": "http://arxiv.org/abs/2602.02419v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02419v1",
      "authors": "Qingni Wang, Yue Fan, Xin Eric Wang",
      "institution": "",
      "abstract": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.SE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02417v1",
      "title": "Trust Region Continual Learning as an Implicit Meta-Learner",
      "link": "http://arxiv.org/abs/2602.02417v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02417v1",
      "authors": "Zekun Wang, Anant Gupta, Christopher J. MacLellan",
      "institution": "",
      "abstract": "Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \\emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \\emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02416v1",
      "title": "Structure Enables Effective Self-Localization of Errors in LLMs",
      "link": "http://arxiv.org/abs/2602.02416v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02416v1",
      "authors": "Ankur Samanta, Akshayaa Magesh, Ayush Jain, Kavosh Asadi, Youliang Yu et al.",
      "institution": "",
      "abstract": "Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02415v1",
      "title": "Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models",
      "link": "http://arxiv.org/abs/2602.02415v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02415v1",
      "authors": "Vivienne Pelletier, Daniel J. Rivera, Obinna Nwokonkwo, Steven A. Wilson, Christopher L. Muhich",
      "institution": "",
      "abstract": "Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02414v1",
      "title": "Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank",
      "link": "http://arxiv.org/abs/2602.02414v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02414v1",
      "authors": "Joshua Mitton, Prarthana Bhattacharyya, Digory Smith, Thomas Christie, Ralph Abboud et al.",
      "institution": "MIT",
      "abstract": "Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02413v1",
      "title": "Masked Autoencoders as Universal Speech Enhancer",
      "link": "http://arxiv.org/abs/2602.02413v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02413v1",
      "authors": "Rajalaxmi Rajagopalan, Ritwik Giri, Zhiqiang Tang, Kyu Han",
      "institution": "",
      "abstract": "Supervised speech enhancement methods have been very successful. However, in practical scenarios, there is a lack of clean speech, and self-supervised learning-based (SSL) speech enhancement methods that offer comparable enhancement performance and can be applied to other speech-related downstream applications are desired. In this work, we develop a masked autoencoder based universal speech enhancer that is agnostic to the type of distortion affecting speech, can handle multiple distortions simultaneously, and is trained in a self-supervised manner. An augmentation stack adds further distortions to the noisy input data. The masked autoencoder model learns to remove the added distortions along with reconstructing the masked regions of the spectrogram during pre-training. The pre-trained embeddings are then used by fine-tuning models trained on a small amount of paired data for specific downstream tasks. We evaluate the pre-trained features for denoising and dereverberation downstream tasks. We explore different augmentations (like single or multi-speaker) in the pre-training augmentation stack and the effect of different noisy input feature representations (like $log1p$ compression) on pre-trained embeddings and downstream fine-tuning enhancement performance. We show that the proposed method not only outperforms the baseline but also achieves state-of-the-art performance for both in-domain and out-of-domain evaluation datasets.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.SD",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02409v1",
      "title": "Catalyst: Out-of-Distribution Detection via Elastic Scaling",
      "link": "http://arxiv.org/abs/2602.02409v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02409v1",
      "authors": "Abid Hassan, Tuan Ngo, Saad Shafiq, Nenad Medvidovic",
      "institution": "",
      "abstract": "Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02408v1",
      "title": "ReasonEdit: Editing Vision-Language Models using Human Reasoning",
      "link": "http://arxiv.org/abs/2602.02408v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02408v1",
      "authors": "Jiaxing Qiu, Kaihua Hou, Roxana Daneshjou, Ahmed Alaa, Thomas Hartvigsen",
      "institution": "",
      "abstract": "Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02406v1",
      "title": "Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function",
      "link": "http://arxiv.org/abs/2602.02406v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02406v1",
      "authors": "Tung Quoc Le, Anh Tuan Nguyen, Viet Anh Nguyen",
      "institution": "",
      "abstract": "Data-driven algorithm design automates hyperparameter tuning, but its statistical foundations remain limited because model performance can depend on hyperparameters in implicit and highly non-smooth ways. Existing guarantees focus on the simple case of a one-dimensional (scalar) hyperparameter. This leaves the practically important, multi-dimensional hyperparameter tuning setting unresolved. We address this open question by establishing the first general framework for establishing generalization guarantees for tuning multi-dimensional hyperparameters in data-driven settings. Our approach strengthens the generalization guarantee framework for semi-algebraic function classes by exploiting tools from real algebraic geometry, yielding sharper, more broadly applicable guarantees. We then extend the analysis to hyperparameter tuning using the validation loss under minimal assumptions, and derive improved bounds when additional structure is available. Finally, we demonstrate the scope of the framework with new learnability results, including data-driven weighted group lasso and weighted fused lasso.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "stat.ML",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02405v1",
      "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
      "link": "http://arxiv.org/abs/2602.02405v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02405v1",
      "authors": "Ethan Mendes, Jungsoo Park, Alan Ritter",
      "institution": "",
      "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02402v1",
      "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
      "link": "http://arxiv.org/abs/2602.02402v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02402v1",
      "authors": "Mu Huang, Hui Wang, Kerui Ren, Linning Xu, Yunsong Zhou et al.",
      "institution": "",
      "abstract": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02401v1",
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "link": "http://arxiv.org/abs/2602.02401v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02401v1",
      "authors": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng et al.",
      "institution": "",
      "abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02400v1",
      "title": "An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence",
      "link": "http://arxiv.org/abs/2602.02400v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02400v1",
      "authors": "Qizhen Zhang, Ankush Garg, Jakob Foerster, Niladri Chatterji, Kshitiz Malik et al.",
      "institution": "",
      "abstract": "Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02396v1",
      "title": "PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning",
      "link": "http://arxiv.org/abs/2602.02396v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02396v1",
      "authors": "Amisha Bhaskar, Pratap Tokekar, Stefano Di Cairano, Alexander Schperberg",
      "institution": "",
      "abstract": "Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.RO",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02395v1",
      "title": "David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning",
      "link": "http://arxiv.org/abs/2602.02395v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02395v1",
      "authors": "Samuel Nellessen, Tal Kachman",
      "institution": "",
      "abstract": "The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary \"tags along\" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.MA"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02393v1",
      "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory",
      "link": "http://arxiv.org/abs/2602.02393v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02393v1",
      "authors": "Ruiqi Wu, Xuanhua He, Meng Cheng, Tianyu Yang, Yong Zhang et al.",
      "institution": "",
      "abstract": "We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02388v1",
      "title": "Personalized Image Generation via Human-in-the-loop Bayesian Optimization",
      "link": "http://arxiv.org/abs/2602.02388v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02388v1",
      "authors": "Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury",
      "institution": "MIT",
      "abstract": "Imagine Alice has a specific image $x^\\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\\ast$, even though the generative model has no information about $x^\\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02386v1",
      "title": "Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing",
      "link": "http://arxiv.org/abs/2602.02386v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02386v1",
      "authors": "Mika Okamoto, Ansel Kaplan Erol, Glenn Matlin",
      "institution": "",
      "abstract": "How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02385v1",
      "title": "Transformers learn factored representations",
      "link": "http://arxiv.org/abs/2602.02385v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02385v1",
      "authors": "Adam Shai, Loren Amdahl-Culleton, Casper L. Christensen, Henry R. Bigelow, Fernando E. Rosas et al.",
      "institution": "",
      "abstract": "Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02382v1",
      "title": "ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs",
      "link": "http://arxiv.org/abs/2602.02382v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02382v1",
      "authors": "Ziyan Zhang, Chao Wang, Zhuo Chen, Chiyi Li, Kai Song",
      "institution": "",
      "abstract": "Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02381v1",
      "title": "Self-Supervised Learning from Structural Invariance",
      "link": "http://arxiv.org/abs/2602.02381v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02381v1",
      "authors": "Yipeng Zhang, Hafez Ghaemi, Jungyoon Lee, Shahab Bakhtiari, Eilif B. Muller et al.",
      "institution": "",
      "abstract": "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02380v1",
      "title": "Unified Personalized Reward Model for Vision Generation",
      "link": "http://arxiv.org/abs/2602.02380v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02380v1",
      "authors": "Yibin Wang, Yuhang Zang, Feng Han, Jiazi Bu, Yujie Zhou et al.",
      "institution": "",
      "abstract": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02378v1",
      "title": "From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making",
      "link": "http://arxiv.org/abs/2602.02378v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02378v1",
      "authors": "Raunak Jain, Mudita Khurana, John Stephens, Srinivas Dharmasanam, Shankar Venkataraman",
      "institution": "",
      "abstract": "As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02377v1",
      "title": "Proof-RM: A Scalable and Generalizable Reward Model for Math Proof",
      "link": "http://arxiv.org/abs/2602.02377v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02377v1",
      "authors": "Haotong Yang, Zitong Wang, Shijia Kang, Siqi Yang, Wenkai Yu et al.",
      "institution": "",
      "abstract": "While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality \"**question-proof-check**\" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02371v1",
      "title": "C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference",
      "link": "http://arxiv.org/abs/2602.02371v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02371v1",
      "authors": "Jing Wang, Jie Shen, Qiaomin Xie, Jeremy C Weiss",
      "institution": "",
      "abstract": "Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \\emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.\n  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.\n  Evaluated on a real-world Long COVID cohort with 13,511 participants, \\emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02370v1",
      "title": "Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes",
      "link": "http://arxiv.org/abs/2602.02370v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02370v1",
      "authors": "Uma Meleti, Jeffrey J. Nirschl",
      "institution": "",
      "abstract": "Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02369v1",
      "title": "Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback",
      "link": "http://arxiv.org/abs/2602.02369v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02369v1",
      "authors": "Yaolun Zhang, Yiran Wu, Yijiong Yu, Qingyun Wu, Huazheng Wang",
      "institution": "",
      "abstract": "Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \\emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \\textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \\textsc{Live-Evo} decouples \\emph{what happened} from \\emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \\textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \\textit{Prophet Arena} benchmark over a 10-week horizon, \\textsc{Live-Evo} improves Brier score by 20.8\\% and increases market returns by 12.9\\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02366v1",
      "title": "ReasonCACHE: Teaching LLMs To Reason Without Weight Updates",
      "link": "http://arxiv.org/abs/2602.02366v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02366v1",
      "authors": "Sharut Gupta, Phillip Isola, Stefanie Jegelka, David Lopez-Paz, Kartik Ahuja et al.",
      "institution": "",
      "abstract": "Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02360v1",
      "title": "Automated Multiple Mini Interview (MMI) Scoring",
      "link": "http://arxiv.org/abs/2602.02360v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02360v1",
      "authors": "Ryan Huynh, Frank Guerin, Alison Callwood",
      "institution": "",
      "abstract": "Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02358v1",
      "title": "Transfer Learning Through Conditional Quantile Matching",
      "link": "http://arxiv.org/abs/2602.02358v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02358v1",
      "authors": "Yikun Zhang, Steven Wilkins-Reeves, Wesley Lee, Aude Hofleitner",
      "institution": "",
      "abstract": "We introduce a transfer learning framework for regression that leverages heterogeneous source domains to improve predictive performance in a data-scarce target domain. Our approach learns a conditional generative model separately for each source domain and calibrates the generated responses to the target domain via conditional quantile matching. This distributional alignment step corrects general discrepancies between source and target domains without imposing restrictive assumptions such as covariate or label shift. The resulting framework provides a principled and flexible approach to high-quality data augmentation for downstream learning tasks in the target domain. From a theoretical perspective, we show that an empirical risk minimizer (ERM) trained on the augmented dataset achieves a tighter excess risk bound than the target-only ERM under mild conditions. In particular, we establish new convergence rates for the quantile matching estimator that governs the transfer bias-variance tradeoff. From a practical perspective, extensive simulations and real data applications demonstrate that the proposed method consistently improves prediction accuracy over target-only learning and competing transfer learning methods.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "stat.ML",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02356v1",
      "title": "NAB: Neural Adaptive Binning for Sparse-View CT reconstruction",
      "link": "http://arxiv.org/abs/2602.02356v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02356v1",
      "authors": "Wangduo Xie, Matthew B. Blaschko",
      "institution": "",
      "abstract": "Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \\textbf{N}eural \\textbf{A}daptive \\textbf{B}inning (\\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02355v1",
      "title": "Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach",
      "link": "http://arxiv.org/abs/2602.02355v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02355v1",
      "authors": "Amirreza Kazemi, Seyed Mohammad Azimi-Abarghouyi, Gabor Fodor, Carlo Fischione",
      "institution": "",
      "abstract": "Hierarchical federated learning (HFL) has emerged as a key architecture for large-scale wireless and Internet of Things systems, where devices communicate with nearby edge servers before reaching the cloud. In these environments, uplink bandwidth and latency impose strict communication limits, thereby making aggressive gradient compression essential. One-bit methods such as sign-based stochastic gradient descent (SignSGD) offer an attractive solution in flat federated settings, but existing theory and algorithms do not naturally extend to hierarchical settings. In particular, the interaction between majority-vote aggregation at the edge layer and model aggregation at the cloud layer, and its impact on end-to-end performance, remains unknown. To bridge this gap, we propose a highly communication-efficient sign-based HFL framework and develop its corresponding formulation for nonconvex learning, where devices send only signed stochastic gradients, edge servers combine them through majority-vote, and the cloud periodically averages the obtained edge models, while utilizing downlink quantization to broadcast the global model. We introduce the resulting scalable HFL algorithm, HierSignSGD, and provide the convergence analysis for SignSGD in a hierarchical setting. Our core technical contribution is a characterization of how biased sign compression, two-level aggregation intervals, and inter-cluster heterogeneity collectively affect convergence. Numerical experiments under homogeneous and heterogeneous data splits show that HierSignSGD, despite employing extreme compression, achieves accuracy comparable to or better than full-precision stochastic gradient descent while reducing communication cost in the process, and remains robust under aggressive downlink sparsification.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.DC",
        "cs.IT",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02351v1",
      "title": "Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data",
      "link": "http://arxiv.org/abs/2602.02351v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02351v1",
      "authors": "Veronica Sanz",
      "institution": "",
      "abstract": "Symmetries play a central role in physics, organizing dynamics, constraining interactions, and determining the effective number of physical degrees of freedom. In parallel, modern artificial intelligence methods have demonstrated a remarkable ability to extract low-dimensional structure from high-dimensional data through representation learning. This review examines the interplay between these two perspectives, focusing on the extent to which symmetry-induced constraints can be identified, encoded, or diagnosed using machine learning techniques.\n  Rather than emphasizing architectures that enforce known symmetries by construction, we concentrate on data-driven approaches and latent representation learning, with particular attention to variational autoencoders. We discuss how symmetries and conservation laws reduce the intrinsic dimensionality of physical datasets, and how this reduction may manifest itself through self-organization of latent spaces in generative models trained to balance reconstruction and compression. We review recent results, including case studies from simple geometric systems and particle physics processes, and analyze the theoretical and practical limitations of inferring symmetry structure without explicit inductive bias.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02350v1",
      "title": "Context Learning for Multi-Agent Discussion",
      "link": "http://arxiv.org/abs/2602.02350v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02350v1",
      "authors": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang et al.",
      "institution": "",
      "abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02341v1",
      "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization",
      "link": "http://arxiv.org/abs/2602.02341v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02341v1",
      "authors": "Zhenpeng Huang, Jiaqi Li, Zihan Jia, Xinhao Li, Desen Meng et al.",
      "institution": "",
      "abstract": "We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02338v1",
      "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs",
      "link": "http://arxiv.org/abs/2602.02338v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02338v1",
      "authors": "Yu Liang, Zhongjin Zhang, Yuxuan Zhu, Kerui Zhang, Zhiluohan Guo et al.",
      "institution": "",
      "abstract": "Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.IR",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02335v1",
      "title": "Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents",
      "link": "http://arxiv.org/abs/2602.02335v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02335v1",
      "authors": "Weiming Sheng, Jinlang Wang, Manuel Barros, Aldrin Montana, Jacopo Tagliabue et al.",
      "institution": "",
      "abstract": "Lakehouses are the default cloud platform for analytics and AI, but they become unsafe when untrusted actors concurrently operate on production data: upstream-downstream mismatches surface only at runtime, and multi-table pipelines can leak partial effects. Inspired by software engineering, we design Bauplan, a code-first lakehouse that aims to make (most) illegal states unrepresentable using familiar abstractions. Bauplan acts along three axes: typed table contracts to make pipeline boundaries checkable, Git-like data versioning for review and reproducibility, and transactional runs that guarantee pipeline-level atomicity. We report early results from a lightweight formal transaction model and discuss future work motivated by counterexamples.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.DC",
        "cs.AI",
        "cs.DB"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02334v1",
      "title": "VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations",
      "link": "http://arxiv.org/abs/2602.02334v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02334v1",
      "authors": "Fatemeh Zargarbashi, Dhruv Agrawal, Jakob Buhmann, Martin Guay, Stelian Coros et al.",
      "institution": "",
      "abstract": "Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02331v1",
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "link": "http://arxiv.org/abs/2602.02331v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02331v1",
      "authors": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang et al.",
      "institution": "",
      "abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.RO",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02326v1",
      "title": "Language Steering for Multilingual In-Context Learning",
      "link": "http://arxiv.org/abs/2602.02326v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02326v1",
      "authors": "Neeraja Kirtane, Kuan-Hao Huang",
      "institution": "",
      "abstract": "While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02320v1",
      "title": "A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method",
      "link": "http://arxiv.org/abs/2602.02320v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02320v1",
      "authors": "Feiyang Cai, Guijuan He, Yi Hu, Jingjing Wang, Joshua Luo et al.",
      "institution": "",
      "abstract": "Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02318v1",
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "link": "http://arxiv.org/abs/2602.02318v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02318v1",
      "authors": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang et al.",
      "institution": "",
      "abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS. With depth integration, DiScene attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02315v1",
      "title": "The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors",
      "link": "http://arxiv.org/abs/2602.02315v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02315v1",
      "authors": "Raphal Sarfati, Eric Bigelow, Daniel Wurgaft, Jack Merullo, Atticus Geiger et al.",
      "institution": "",
      "abstract": "Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved \"belief manifolds\" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02313v1",
      "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient",
      "link": "http://arxiv.org/abs/2602.02313v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02313v1",
      "authors": "Changming Li, Kaixing Zhang, Haoyun Xu, Yingdong Shi, Zheng Zhang et al.",
      "institution": "",
      "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02311v1",
      "title": "Introns and Templates Matter: Rethinking Linkage in GP-GOMEA",
      "link": "http://arxiv.org/abs/2602.02311v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02311v1",
      "authors": "Johannes Koch, Tanja Alderliesten, Peter A. N. Bosman",
      "institution": "",
      "abstract": "GP-GOMEA is among the state-of-the-art for symbolic regression, especially when it comes to finding small and potentially interpretable solutions. A key mechanism employed in any GOMEA variant is the exploitation of linkage, the dependencies between variables, to ensure efficient evolution. In GP-GOMEA, mutual information between node positions in GP trees has so far been used to learn linkage. For this, a fixed expression template is used. This however leads to introns for expressions smaller than the full template. As introns have no impact on fitness, their occurrences are not directly linked to selection. Consequently, introns can adversely affect the extent to which mutual information captures dependencies between tree nodes. To overcome this, we propose two new measures for linkage learning, one that explicitly considers introns in mutual information estimates, and one that revisits linkage learning in GP-GOMEA from a grey-box perspective, yielding a measure that needs not to be learned from the population but is derived directly from the template. Across five standard symbolic regression problems, GP-GOMEA achieves substantial improvements using both measures. We also find that the newly learned linkage structure closely reflects the template linkage structure, and that explicitly using the template structure yields the best performance overall.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.NE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02310v1",
      "title": "FragmentFlow: Scalable Transition State Generation for Large Molecules",
      "link": "http://arxiv.org/abs/2602.02310v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02310v1",
      "authors": "Ron Shprints, Peter Holderrieth, Juno Nam, Rafael Gmez-Bombarelli, Tommi Jaakkola",
      "institution": "",
      "abstract": "Transition states (TSs) are central to understanding and quantitatively predicting chemical reactivity and reaction mechanisms. Although traditional TS generation methods are computationally expensive, recent generative modeling approaches have enabled chemically meaningful TS prediction for relatively small molecules. However, these methods fail to generalize to practically relevant reaction substrates because of distribution shifts induced by increasing molecular sizes. Furthermore, TS geometries for larger molecules are not available at scale, making it infeasible to train generative models from scratch on such molecules. To address these challenges, we introduce FragmentFlow: a divide-and-conquer approach that trains a generative model to predict TS geometries for the reactive core atoms, which define the reaction mechanism. The full TS structure is then reconstructed by re-attaching substituent fragments to the predicted core. By operating on reactive cores, whose size and composition remain relatively invariant across molecular contexts, FragmentFlow mitigates distribution shifts in generative modeling. Evaluated on a new curated dataset of reactions involving reactants with up to 33 heavy atoms, FragmentFlow correctly identifies 90% of TSs while requiring 30% fewer saddle-point optimization steps than classical initialization schemes. These results point toward scalable TS generation for high-throughput reactivity studies.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02306v1",
      "title": "Spark: Modular Spiking Neural Networks",
      "link": "http://arxiv.org/abs/2602.02306v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02306v1",
      "authors": "Mario Franco, Carlos Gershenson",
      "institution": "",
      "abstract": "Nowadays, neural networks act as a synonym for artificial intelligence. Present neural network models, although remarkably powerful, are inefficient both in terms of data and energy. Several alternative forms of neural networks have been proposed to address some of these problems. Specifically, spiking neural networks are suitable for efficient hardware implementations. However, effective learning algorithms for spiking networks remain elusive, although it is suspected that effective plasticity mechanisms could alleviate the problem of data efficiency. Here, we present a new framework for spiking neural networks - Spark - built upon the idea of modular design, from simple components to entire models. The aim of this framework is to provide an efficient and streamlined pipeline for spiking neural networks. We showcase this framework by solving the sparse-reward cartpole problem with simple plasticity mechanisms. We hope that a framework compatible with traditional ML pipelines may accelerate research in the area, specifically for continuous and unbatched learning, akin to the one animals exhibit.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02304v1",
      "title": "Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach",
      "link": "http://arxiv.org/abs/2602.02304v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02304v1",
      "authors": "Martino Ciaperoni, Marzio Di Vece, Luca Pappalardo, Fosca Giannotti, Francesco Giannini",
      "institution": "",
      "abstract": "Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $$-XAI experiment.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02301v1",
      "title": "Advancing General-Purpose Reasoning Models with Modular Gradient Surgery",
      "link": "http://arxiv.org/abs/2602.02301v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02301v1",
      "authors": "Min Cai, Yu Liang, Longzheng Wang, Yan Wang, Yueyang Zhang et al.",
      "institution": "",
      "abstract": "Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\\%) and 4.5 (11.1\\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02296v1",
      "title": "Decoupling Generalizability and Membership Privacy Risks in Neural Networks",
      "link": "http://arxiv.org/abs/2602.02296v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02296v1",
      "authors": "Xingli Fang, Jung-Eun Kim",
      "institution": "",
      "abstract": "A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02295v1",
      "title": "EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models",
      "link": "http://arxiv.org/abs/2602.02295v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02295v1",
      "authors": "Shaima Ahmad Freja, Ferhat Ozgur Catak, Betul Yurdem, Chunming Rong",
      "institution": "",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02290v1",
      "title": "Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?",
      "link": "http://arxiv.org/abs/2602.02290v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02290v1",
      "authors": "Alex Argese, Pasquale Lisena, Raphal Troncy",
      "institution": "",
      "abstract": "Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02288v1",
      "title": "An Optimization Method for Autoregressive Time Series Forecasting",
      "link": "http://arxiv.org/abs/2602.02288v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02288v1",
      "authors": "Zheng Li, Jerry Cheng, Huanying Gu",
      "institution": "",
      "abstract": "Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02287v1",
      "title": "Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages",
      "link": "http://arxiv.org/abs/2602.02287v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02287v1",
      "authors": "Isaac Chung, Linda Freienthal",
      "institution": "",
      "abstract": "Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.\n  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02286v1",
      "title": "DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild",
      "link": "http://arxiv.org/abs/2602.02286v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02286v1",
      "authors": "Arnab Das, Yassine El Kheir, Enes Erdem Erdogan, Feidi Kallel, Tim Polzehl et al.",
      "institution": "",
      "abstract": "This paper presents the DFKI-Speech system developed for the WildSpoof Challenge under the Spoofing aware Automatic Speaker Verification (SASV) track. We propose a robust SASV framework in which a spoofing detector and a speaker verification (SV) network operate in tandem. The spoofing detector employs a self-supervised speech embedding extractor as the frontend, combined with a state-of-the-art graph neural network backend. In addition, a top-3 layer based mixture-of-experts (MoE) is used to fuse high-level and low-level features for effective spoofed utterance detection. For speaker verification, we adapt a low-complexity convolutional neural network that fuses 2D and 1D features at multiple scales, trained with the SphereFace loss. Additionally, contrastive circle loss is applied to adaptively weight positive and negative pairs within each training batch, enabling the network to better distinguish between hard and easy sample pairs. Finally, fixed imposter cohort based AS Norm score normalization and model ensembling are used to further enhance the discriminative capability of the speaker verification system.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02285v1",
      "title": "Statistical Learning Theory in Lean 4: Empirical Processes from Scratch",
      "link": "http://arxiv.org/abs/2602.02285v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02285v1",
      "authors": "Yuanhe Zhang, Jason D. Lee, Fanghui Liu",
      "institution": "",
      "abstract": "We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02283v1",
      "title": "Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management",
      "link": "http://arxiv.org/abs/2602.02283v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02283v1",
      "authors": "Owen Shen, Patrick Jaillet",
      "institution": "",
      "abstract": "We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \\emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\\varepsilon/(1-))$ neighborhood of the optimal Q-function, where $\\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02282v1",
      "title": "MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology",
      "link": "http://arxiv.org/abs/2602.02282v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02282v1",
      "authors": "Susu Hu, Stefanie Speidel",
      "institution": "",
      "abstract": "Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02281v1",
      "title": "Backpropagation as Physical Relaxation: Exact Gradients in Finite Time",
      "link": "http://arxiv.org/abs/2602.02281v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02281v1",
      "authors": "Antonino Emanuele Scurria",
      "institution": "",
      "abstract": "Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02280v1",
      "title": "RACA: Representation-Aware Coverage Criteria for LLM Safety Testing",
      "link": "http://arxiv.org/abs/2602.02280v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02280v1",
      "authors": "Zeming Wei, Zhixin Zhang, Chengcan Wu, Yihao Zhang, Xiaokun Luan et al.",
      "institution": "",
      "abstract": "Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02270v1",
      "title": "dziribot: rag based intelligent conversational agent for algerian arabic dialect",
      "link": "http://arxiv.org/abs/2602.02270v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02270v1",
      "authors": "El Batoul Bechiri, Dihia Lanasri",
      "institution": "",
      "abstract": "The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02269v1",
      "title": "Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems",
      "link": "http://arxiv.org/abs/2602.02269v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02269v1",
      "authors": "Jon kerlj, Seongjin Bien, Abdeldjallil Naceri, Sami Haddadin",
      "institution": "",
      "abstract": "We present $multipanda\\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.RO",
        "cs.AI",
        "cs.SE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02268v1",
      "title": "HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control",
      "link": "http://arxiv.org/abs/2602.02268v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02268v1",
      "authors": "Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Sungheon Jeong, Mohsen Imani",
      "institution": "",
      "abstract": "Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02266v1",
      "title": "OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data",
      "link": "http://arxiv.org/abs/2602.02266v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02266v1",
      "authors": "Tan Sang Nguyen, Muhammad Reza Qorib, Hwee Tou Ng",
      "institution": "",
      "abstract": "Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02264v1",
      "title": "Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training",
      "link": "http://arxiv.org/abs/2602.02264v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02264v1",
      "authors": "Paolo Marcandelli, Natansh Mathur, Stefano Markidis, Martina Siena, Stefano Mariani",
      "institution": "",
      "abstract": "Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02262v1",
      "title": "OmniCode: A Benchmark for Evaluating Software Engineering Agents",
      "link": "http://arxiv.org/abs/2602.02262v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02262v1",
      "authors": "Atharv Sonwane, Eng-Shen Tu, Wei-Chung Lu, Claas Beger, Carter Larsen et al.",
      "institution": "",
      "abstract": "LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02261v1",
      "title": "Unlocking the Duality between Flow and Field Matching",
      "link": "http://arxiv.org/abs/2602.02261v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02261v1",
      "authors": "Daniil Shlenskii, Alexander Varlamov, Nazar Buzun, Alexander Korotin",
      "institution": "",
      "abstract": "Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02260v1",
      "title": "Learning Markov Decision Processes under Fully Bandit Feedback",
      "link": "http://arxiv.org/abs/2602.02260v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02260v1",
      "authors": "Zhengjia Zhuo, Anupam Gupta, Viswanath Nagarajan",
      "institution": "",
      "abstract": "A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $(\\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \\emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\\widetilde{O}(\\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02259v1",
      "title": "Segment to Focus: Guiding Latent Action Models in the Presence of Distractors",
      "link": "http://arxiv.org/abs/2602.02259v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02259v1",
      "authors": "Hamza Adnan, Matthew T. Jackson, Alexey Zakharov",
      "institution": "",
      "abstract": "Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02258v1",
      "title": "Alignment-Aware Model Adaptation via Feedback-Guided Optimization",
      "link": "http://arxiv.org/abs/2602.02258v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02258v1",
      "authors": "Gaurav Bhatt, Aditya Chinchure, Jiawei Zhou, Leonid Sigal",
      "institution": "",
      "abstract": "Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02250v1",
      "title": "Well-Posed KL-Regularized Control via Wasserstein and Kalman-Wasserstein KL Divergences",
      "link": "http://arxiv.org/abs/2602.02250v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02250v1",
      "authors": "Viktor Stein, Adwait Datar, Nihat Ay",
      "institution": "",
      "abstract": "Kullback-Leibler divergence (KL) regularization is widely used in reinforcement learning, but it becomes infinite under support mismatch and can degenerate in low-noise limits. Utilizing a unified information-geometric framework, we introduce (Kalman)-Wasserstein-based KL analogues by replacing the Fisher-Rao geometry in the dynamical formulation of the KL with transport-based geometries, and we derive closed-form values for common distribution families. These divergences remain finite under support mismatch and yield a geometric interpretation of regularization heuristics used in Kalman ensemble methods. We demonstrate the utility of these divergences in KL-regularized optimal control. In the fully tractable setting of linear time-invariant systems with Gaussian process noise, the classical KL reduces to a quadratic control penalty that becomes singular as process noise vanishes. Our variants remove this singularity, yielding well-posed problems. On a double integrator and a cart-pole example, the resulting controls outperform KL-based regularization.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02244v1",
      "title": "Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models",
      "link": "http://arxiv.org/abs/2602.02244v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02244v1",
      "authors": "Hao Wang, Hao Gu, Hongming Piao, Kaixiong Gong, Yuxiao Ye et al.",
      "institution": "",
      "abstract": "The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02241v1",
      "title": "Variational Entropic Optimal Transport",
      "link": "http://arxiv.org/abs/2602.02241v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02241v1",
      "authors": "Roman Dyachenko, Nikita Gushchin, Kirill Sokolov, Petr Mokrov, Evgeny Burnaev et al.",
      "institution": "",
      "abstract": "Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\\log \\mathbb{E}[\\exp(\\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02240v1",
      "title": "Causal Inference for Preprocessed Outcomes with an Application to Functional Connectivity",
      "link": "http://arxiv.org/abs/2602.02240v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02240v1",
      "authors": "Zihang Wang, Razieh Nabi, Benjamin B. Risk",
      "institution": "",
      "abstract": "In biomedical research, repeated measurements within each subject are often processed to remove artifacts and unwanted sources of variation. The resulting data are used to construct derived outcomes that act as proxies for scientific outcomes that are not directly observable. Although intra-subject processing is widely used, its impact on inter-subject statistical inference has not been systematically studied, and a principled framework for causal analysis in this setting is lacking. In this article, we propose a semiparametric framework for causal inference with derived outcomes obtained after intra-subject processing. This framework applies to settings with a modular structure, where intra-subject analyses are conducted independently across subjects and are followed by inter-subject analyses based on parameters from the intra-subject stage. We develop multiply robust estimators of causal parameters under rate conditions on both intra-subject and inter-subject models, which allows the use of flexible machine learning. We specialize the framework to a mediation setting and focus on the natural direct effect. For high dimensional inference, we employ a step-down procedure that controls the exceedance rate of the false discovery proportion. Simulation studies demonstrate the superior performance of the proposed approach. We apply our method to estimate the impact of stimulant medication on brain connectivity in children with autism spectrum disorder.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "stat.ME",
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02239v1",
      "title": "Interpretability in Deep Time Series Models Demands Semantic Alignment",
      "link": "http://arxiv.org/abs/2602.02239v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02239v1",
      "authors": "Giovanni De Felice, Riccardo D'Elia, Alberto Termine, Pietro Barbiero, Giuseppe Marra et al.",
      "institution": "",
      "abstract": "Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02238v1",
      "title": "Geometry- and Relation-Aware Diffusion for EEG Super-Resolution",
      "link": "http://arxiv.org/abs/2602.02238v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02238v1",
      "authors": "Laura Yao, Gengwei Zhang, Moajjem Chowdhury, Yunmei Liu, Tianlong Chen",
      "institution": "",
      "abstract": "Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02236v1",
      "title": "Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL",
      "link": "http://arxiv.org/abs/2602.02236v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02236v1",
      "authors": "Julian Lemmel, Felix Resch, Mnika Farsang, Ramin Hasani, Daniela Rus et al.",
      "institution": "",
      "abstract": "Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.RO",
        "cs.LG",
        "cs.NE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02232v1",
      "title": "LiFlow: Flow Matching for 3D LiDAR Scene Completion",
      "link": "http://arxiv.org/abs/2602.02232v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02232v1",
      "authors": "Andrea Matteazzi, Dietmar Tutsch",
      "institution": "",
      "abstract": "In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02230v1",
      "title": "SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting",
      "link": "http://arxiv.org/abs/2602.02230v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02230v1",
      "authors": "Ziyu Zhou, Yuchen Fang, Weilin Ruan, Shiyu Wang, James Kwok et al.",
      "institution": "",
      "abstract": "Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02229v1",
      "title": "Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts",
      "link": "http://arxiv.org/abs/2602.02229v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02229v1",
      "authors": "Guangyi Zhang, Yunlong Cai, Guanding Yu, Osvaldo Simeone",
      "institution": "",
      "abstract": "We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02223v1",
      "title": "Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type",
      "link": "http://arxiv.org/abs/2602.02223v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02223v1",
      "authors": "Junchi Feng, Nikhil Ballem, Mahya Beheshti, Giles Hamilton-Fletcher, Todd Hudson et al.",
      "institution": "",
      "abstract": "Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02224v1",
      "title": "Spectral Superposition: A Theory of Feature Geometry",
      "link": "http://arxiv.org/abs/2602.02224v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02224v1",
      "authors": "Georgi Ivanov, Narmeen Oozeer, Shivam Raval, Tasana Pejovic, Shriyash Upadhyay et al.",
      "institution": "",
      "abstract": "Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02222v1",
      "title": "MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection",
      "link": "http://arxiv.org/abs/2602.02222v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02222v1",
      "authors": "Ruiqi Liu, Manni Cui, Ziheng Qin, Zhiyuan Yan, Ruoxin Chen et al.",
      "institution": "",
      "abstract": "High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the \"superhuman crossover\" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.CR"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02221v1",
      "title": "Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation",
      "link": "http://arxiv.org/abs/2602.02221v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02221v1",
      "authors": "Frederic Blum, Johann-Mattis List",
      "institution": "",
      "abstract": "Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02220v1",
      "title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation",
      "link": "http://arxiv.org/abs/2602.02220v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02220v1",
      "authors": "Bo Miao, Weijia Liu, Jun Luo, Lachlan Shinnick, Jian Liu et al.",
      "institution": "",
      "abstract": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.RO"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02219v1",
      "title": "Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge",
      "link": "http://arxiv.org/abs/2602.02219v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02219v1",
      "authors": "Yuzheng Xu, Tosho Hirasawa, Tadashi Kozuno, Yoshitaka Ushiku",
      "institution": "",
      "abstract": "Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02215v1",
      "title": "Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism",
      "link": "http://arxiv.org/abs/2602.02215v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02215v1",
      "authors": "Sebastian Mller, Vanessa Toborek, Eike Stadtlnder, Tams Horvth, Brendan Balcerak Jackson et al.",
      "institution": "",
      "abstract": "Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02213v1",
      "title": "Generating Physically Sound Designs from Text and a Set of Physical Constraints",
      "link": "http://arxiv.org/abs/2602.02213v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02213v1",
      "authors": "Gregory Barber, Todd C. Henry, Mulugeta A. Haile",
      "institution": "",
      "abstract": "We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02212v1",
      "title": "MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models",
      "link": "http://arxiv.org/abs/2602.02212v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02212v1",
      "authors": "Zheyuan Zhou, Liang Du, Zixun Sun, Xiaoyu Zhou, Ruimin Ye et al.",
      "institution": "",
      "abstract": "Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02208v1",
      "title": "Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study",
      "link": "http://arxiv.org/abs/2602.02208v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02208v1",
      "authors": "Md. Toufique Hasan, Ayman Asad Khan, Mika Saari, Vaishnavi Bankhele, Pekka Abrahamsson",
      "institution": "",
      "abstract": "Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.SE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02207v1",
      "title": "Sinhala Physical Common Sense Reasoning Dataset for Global PIQA",
      "link": "http://arxiv.org/abs/2602.02207v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02207v1",
      "authors": "Nisansa de Silva, Surangika Ranathunga",
      "institution": "",
      "abstract": "This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02206v1",
      "title": "Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning",
      "link": "http://arxiv.org/abs/2602.02206v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02206v1",
      "authors": "Tong Yang, Yemin Wang, Chaoning Zhang, Aming Wu",
      "institution": "",
      "abstract": "The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02201v1",
      "title": "Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction",
      "link": "http://arxiv.org/abs/2602.02201v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02201v1",
      "authors": "Abhijit Gupta",
      "institution": "",
      "abstract": "Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02199v1",
      "title": "More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression",
      "link": "http://arxiv.org/abs/2602.02199v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02199v1",
      "authors": "Aryan Sood, Tanvi Sharma, Vansh Agrawal",
      "institution": "",
      "abstract": "While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02197v1",
      "title": "Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models",
      "link": "http://arxiv.org/abs/2602.02197v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02197v1",
      "authors": "Xindian Ma, Yidi Lu, Peng Zhang, Jing Zhang",
      "institution": "",
      "abstract": "The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\\% with minimal accuracy loss (0.3\\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02196v1",
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "link": "http://arxiv.org/abs/2602.02196v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02196v1",
      "authors": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding et al.",
      "institution": "",
      "abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02195v1",
      "title": "State Rank Dynamics in Linear Attention LLMs",
      "link": "http://arxiv.org/abs/2602.02195v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02195v1",
      "authors": "Ao Sun, Hongtao Zhang, Heng Zhou, Yixuan Ma, Yiran Qin et al.",
      "institution": "",
      "abstract": "Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\\% reduction in KV-cache overhead while largely maintaining model accuracy.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02193v1",
      "title": "SSI-DM: Singularity Skipping Inversion of Diffusion Models",
      "link": "http://arxiv.org/abs/2602.02193v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02193v1",
      "authors": "Chen Min, Enze Jiang, Jishen Peng, Zheng Ma",
      "institution": "",
      "abstract": "Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02192v1",
      "title": "ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning",
      "link": "http://arxiv.org/abs/2602.02192v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02192v1",
      "authors": "Jie Xiao, Meng Chen, Qingnan Ren, Song Jingwei, Jiaqi Huang et al.",
      "institution": "",
      "abstract": "Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.DC"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02190v1",
      "title": "PCA of probability measures: Sparse and Dense sampling regimes",
      "link": "http://arxiv.org/abs/2602.02190v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02190v1",
      "authors": "Gachon Erell, Jrmie Bigot, Elsa Cazelles",
      "institution": "",
      "abstract": "A common approach to perform PCA on probability measures is to embed them into a Hilbert space where standard functional PCA techniques apply. While convergence rates for estimating the embedding of a single measure from $m$ samples are well understood, the literature has not addressed the setting involving multiple measures. In this paper, we study PCA in a double asymptotic regime where $n$ probability measures are observed, each through $m$ samples. We derive convergence rates of the form $n^{-1/2} + m^{-}$ for the empirical covariance operator and the PCA excess risk, where $>0$ depends on the chosen embedding. This characterizes the relationship between the number $n$ of measures and the number $m$ of samples per measure, revealing a sparse (small $m$) to dense (large $m$) transition in the convergence behavior. Moreover, we prove that the dense-regime rate is minimax optimal for the empirical covariance error. Our numerical experiments validate these theoretical rates and demonstrate that appropriate subsampling preserves PCA accuracy while reducing computational cost.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "stat.ML",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02188v1",
      "title": "Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization",
      "link": "http://arxiv.org/abs/2602.02188v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02188v1",
      "authors": "Xia Jiang, Jing Chen, Cong Zhang, Jie Gao, Chengpeng Hu et al.",
      "institution": "",
      "abstract": "While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \\textbf{N}atural \\textbf{L}anguage \\textbf{C}ombinatorial \\textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02186v1",
      "title": "Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision",
      "link": "http://arxiv.org/abs/2602.02186v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02186v1",
      "authors": "Ziqiao Weng, Jiancheng Yang, Kangxian Xie, Bo Zhou, Weidong Cai",
      "institution": "",
      "abstract": "Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \\textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02184v1",
      "title": "Malware Detection Through Memory Analysis",
      "link": "http://arxiv.org/abs/2602.02184v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02184v1",
      "authors": "Sarah Nassar",
      "institution": "",
      "abstract": "This paper summarizes the research conducted for a malware detection project using the Canadian Institute for Cybersecurity's MalMemAnalysis-2022 dataset. The purpose of the project was to explore the effectiveness and efficiency of machine learning techniques for the task of binary classification (i.e., benign or malicious) as well as multi-class classification to further include three malware sub-types (i.e., benign, ransomware, spyware, or Trojan horse). The XGBoost model type was the final model selected for both tasks due to the trade-off between strong detection capability and fast inference speed. The binary classifier achieved a testing subset accuracy and F1 score of 99.98\\%, while the multi-class version reached an accuracy of 87.54\\% and an F1 score of 81.26\\%, with an average F1 score over the malware sub-types of 75.03\\%. In addition to the high modelling performance, XGBoost is also efficient in terms of classification speed. It takes about 37.3 milliseconds to classify 50 samples in sequential order in the binary setting and about 43.2 milliseconds in the multi-class setting. The results from this research project help advance the efforts made towards developing accurate and real-time obfuscated malware detectors for the goal of improving online privacy and safety. *This project was completed as part of ELEC 877 (AI for Cybersecurity) in the Winter 2024 term.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CR",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02182v1",
      "title": "Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages",
      "link": "http://arxiv.org/abs/2602.02182v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02182v1",
      "authors": "Tjaa Aron, Matej Klemen, Marko Robnik-ikonja, Kaja Dobrovoljc",
      "institution": "",
      "abstract": "Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02180v1",
      "title": "STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs",
      "link": "http://arxiv.org/abs/2602.02180v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02180v1",
      "authors": "Weikang Meng, Liangyu Huo, Yadan Luo, Jiawen Guan, Jingyi Zhang et al.",
      "institution": "",
      "abstract": "Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02179v1",
      "title": "SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks",
      "link": "http://arxiv.org/abs/2602.02179v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02179v1",
      "authors": "Marina Mastroleo, Alberto Archetti, Federico Mastroleo, Matteo Matteucci",
      "institution": "",
      "abstract": "Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02178v1",
      "title": "AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?",
      "link": "http://arxiv.org/abs/2602.02178v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02178v1",
      "authors": "Liang Lin, Feng Xiong, Zengbin Wang, Kun Wang, Junhao Dong et al.",
      "institution": "",
      "abstract": "Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02175v1",
      "title": "CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization",
      "link": "http://arxiv.org/abs/2602.02175v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02175v1",
      "authors": "Xinquan Yu, Wei Lu, Xiangyang Luo",
      "institution": "",
      "abstract": "To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02173v1",
      "title": "Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach",
      "link": "http://arxiv.org/abs/2602.02173v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02173v1",
      "authors": "Jiancheng Tu, Wenqi Fan, Zhibin Wu",
      "institution": "",
      "abstract": "Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02171v1",
      "title": "Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks",
      "link": "http://arxiv.org/abs/2602.02171v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02171v1",
      "authors": "Lu Cao, Xiquan He, Junying Zeng, Chaoyun Mai, Min Luo",
      "institution": "",
      "abstract": "The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02170v1",
      "title": "Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study",
      "link": "http://arxiv.org/abs/2602.02170v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02170v1",
      "authors": "Jose Manuel de la Chica Rodriguez, Juan Manuel Vera Daz",
      "institution": "",
      "abstract": "Contemporary multi-agent systems increasingly rely on internal coordination mechanisms to combine, arbitrate, or constrain the outputs of heterogeneous components. In safety-critical and regulated domains such as finance, these mechanisms must satisfy strict formal requirements, remain auditable, and operate within explicitly bounded limits. Coordination logic therefore functions as a governance layer rather than an optimization heuristic.\n  This paper presents an exploratory systems feasibility study of Self-Evolving Coordination Protocols (SECP): coordination protocols that permit limited, externally validated self-modification while preserving fixed formal invariants. We study a controlled proof-of-concept setting in which six fixed Byzantine consensus protocol proposals are evaluated by six specialized decision modules. All coordination regimes operate under identical hard constraints, including Byzantine fault tolerance (f < n/3), O(n2) message complexity, complete non-statistical safety and liveness arguments, and bounded explainability.\n  Four coordination regimes are compared in a single-shot design: unanimous hard veto, weighted scalar aggregation, SECP v1.0 (an agent-designed non-scalar protocol), and SECP v2.0 (the result of one governed modification). Outcomes are evaluated using a single metric, proposal coverage, defined as the number of proposals accepted. A single recursive modification increased coverage from two to three accepted proposals while preserving all declared invariants.\n  The study makes no claims regarding statistical significance, optimality, convergence, or learning. Its contribution is architectural: it demonstrates that bounded self-modification of coordination protocols is technically implementable, auditable, and analyzable under explicit formal constraints, establishing a foundation for governed multi-agent systems.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.MA",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02167v1",
      "title": "Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding",
      "link": "http://arxiv.org/abs/2602.02167v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02167v1",
      "authors": "Soheil Behnam Roudsari, Alexandre S. Brando, Felipe N. Martins",
      "institution": "",
      "abstract": "Indoor service robots need perception that is robust, more privacy-friendly than RGB video, and feasible on embedded hardware. We present a camera-free 2D LiDAR object detection pipeline that encodes short-term temporal context by stacking three consecutive scans as RGB channels, yielding a compact YOLOv8n input without occupancy-grid construction while preserving angular structure and motion cues. Evaluated in Webots across 160 randomized indoor scenarios with strict scenario-level holdout, the method achieves 98.4% mAP@0.5 (0.778 mAP@0.5:0.95) with 94.9% precision and 94.7% recall on four object classes. On a Raspberry Pi 5, it runs in real time with a mean post-warm-up end-to-end latency of 47.8ms per frame, including scan encoding and postprocessing. Relative to a closely related occupancy-grid LiDAR-YOLO pipeline reported on the same platform, the proposed representation is associated with substantially lower reported end-to-end latency. Although results are simulation-based, they suggest that lightweight temporal encoding can enable accurate and real-time LiDAR-only detection for embedded indoor robotics without capturing RGB appearance.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02164v1",
      "title": "Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents",
      "link": "http://arxiv.org/abs/2602.02164v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02164v1",
      "authors": "Pengfei He, Ash Fox, Lesly Miculicich, Stefan Friedli, Daniel Fabian et al.",
      "institution": "",
      "abstract": "Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.CR"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02163v1",
      "title": "Reg4Pru: Regularisation Through Random Token Routing for Token Pruning",
      "link": "http://arxiv.org/abs/2602.02163v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02163v1",
      "authors": "Julian Wyatt, Ronald Clark, Irina Voiculescu",
      "institution": "",
      "abstract": "Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02162v1",
      "title": "Interpretable Tabular Foundation Models via In-Context Kernel Regression",
      "link": "http://arxiv.org/abs/2602.02162v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02162v1",
      "authors": "Ratmir Miftachov, Bruno Charron, Simon Valentin",
      "institution": "",
      "abstract": "Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02161v1",
      "title": "Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction",
      "link": "http://arxiv.org/abs/2602.02161v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02161v1",
      "authors": "Aniq Ur Rahman, Justin P. Coon",
      "institution": "",
      "abstract": "Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02160v1",
      "title": "D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use",
      "link": "http://arxiv.org/abs/2602.02160v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02160v1",
      "authors": "Bowen Xu, Shaoyu Wu, Hao Jiang, Kai Liu, Xin Chen et al.",
      "institution": "",
      "abstract": "Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\\underline{\\textbf{D}}ecomposing tasks and \\underline{\\textbf{Co}}mposing \\underline{\\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\\% accuracy, surpassing the best-performing 8B model by 5.7\\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\\%, outperforming 70B models despite being 5$\\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02159v1",
      "title": "Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing",
      "link": "http://arxiv.org/abs/2602.02159v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02159v1",
      "authors": "Lingkun Long, Yushi Huang, Shihao Bai, Ruihao Gong, Jun Zhang et al.",
      "institution": "",
      "abstract": "Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02158v1",
      "title": "Traffic-Aware Navigation in Road Networks",
      "link": "http://arxiv.org/abs/2602.02158v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02158v1",
      "authors": "Sarah Nassar",
      "institution": "",
      "abstract": "This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02157v1",
      "title": "Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing",
      "link": "http://arxiv.org/abs/2602.02157v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02157v1",
      "authors": "Egor Serov, Ilya Kuleshov, Alexey Zaytsev",
      "institution": "",
      "abstract": "Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02154v1",
      "title": "Deep learning enables urban change profiling through alignment of historical maps",
      "link": "http://arxiv.org/abs/2602.02154v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02154v1",
      "authors": "Sidi Wu, Yizi Chen, Maurizio Gribaudi, Konrad Schindler, Clment Mallet et al.",
      "institution": "",
      "abstract": "Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.IR"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02153v1",
      "title": "Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model",
      "link": "http://arxiv.org/abs/2602.02153v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02153v1",
      "authors": "Onat Ure, Samet Demir, Zafer Dogan",
      "institution": "",
      "abstract": "We study the effect of high-order statistics of data on the learning dynamics of neural networks (NNs) by using a moment-controllable non-Gaussian data model. Considering the expressivity of two-layer neural networks, we first construct the data model as a generative two-layer NN where the activation function is expanded by using Hermite polynomials. This allows us to achieve interpretable control over high-order cumulants such as skewness and kurtosis through the Hermite coefficients while keeping the data model realistic. Using samples generated from the data model, we perform controlled online learning experiments with a two-layer NN. Our results reveal a moment-wise progression in training: networks first capture low-order statistics such as mean and covariance, and progressively learn high-order cumulants. Finally, we pretrain the generative model on the Fashion-MNIST dataset and leverage the generated samples for further experiments. The results of these additional experiments confirm our conclusions and show the utility of the data model in a real-world scenario. Overall, our proposed approach bridges simplified data assumptions and practical data complexity, which offers a principled framework for investigating distributional effects in machine learning and signal processing.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "stat.ML",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02151v1",
      "title": "Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization",
      "link": "http://arxiv.org/abs/2602.02151v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02151v1",
      "authors": "Yuli Zhou, Qingxuan Chen, Luca Benini, Guolei Sun, Yawei Li",
      "institution": "",
      "abstract": "Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02150v1",
      "title": "ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning",
      "link": "http://arxiv.org/abs/2602.02150v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02150v1",
      "authors": "Chu Zhao, Enneng Yang, Yuting Liu, Jianzhe Zhao, Guibing Guo",
      "institution": "",
      "abstract": "Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02146v1",
      "title": "Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting",
      "link": "http://arxiv.org/abs/2602.02146v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02146v1",
      "authors": "Sunho Kim, Susik Yoon",
      "institution": "",
      "abstract": "Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02143v1",
      "title": "Learning Generative Selection for Best-of-N",
      "link": "http://arxiv.org/abs/2602.02143v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02143v1",
      "authors": "Shubham Toshniwal, Aleksander Ficek, Siddhartha Jain, Wei Du, Vahid Noroozi et al.",
      "institution": "",
      "abstract": "Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02142v1",
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "link": "http://arxiv.org/abs/2602.02142v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02142v1",
      "authors": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay et al.",
      "institution": "",
      "abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.RO",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02140v1",
      "title": "Quantifying the Gap between Understanding and Generation within Unified Multimodal Models",
      "link": "http://arxiv.org/abs/2602.02140v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02140v1",
      "authors": "Chenlong Wang, Yuhang Chen, Zhihan Hu, Dongping Chen, Wenhu Chen et al.",
      "institution": "",
      "abstract": "Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two \"unified\" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02139v1",
      "title": "EvoMU: Evolutionary Machine Unlearning",
      "link": "http://arxiv.org/abs/2602.02139v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02139v1",
      "authors": "Pawel Batorski, Paul Swoboda",
      "institution": "",
      "abstract": "Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02138v1",
      "title": "CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems",
      "link": "http://arxiv.org/abs/2602.02138v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02138v1",
      "authors": "Lyu Zongyi, Ji Zhenlan, Chen Songqiang, Wang Liwen, Huang Yuheng et al.",
      "institution": "",
      "abstract": "Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \\textbf{C}ausality-based \\textbf{A}nalysis framework for \\textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.\n  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.SE",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02137v1",
      "title": "DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations",
      "link": "http://arxiv.org/abs/2602.02137v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02137v1",
      "authors": "Minghao Li, Ruihang Wang, Rui Tan, Yonggang Wen",
      "institution": "",
      "abstract": "Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02136v1",
      "title": "Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models",
      "link": "http://arxiv.org/abs/2602.02136v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02136v1",
      "authors": "Yingsha Xie, Tiansheng Huang, Enneng Yang, Rui Min, Wenjie Lu et al.",
      "institution": "",
      "abstract": "Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \\textbf{+30.2\\%} on DirectRefusal and \\textbf{+21.2\\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \\textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02133v1",
      "title": "Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics",
      "link": "http://arxiv.org/abs/2602.02133v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02133v1",
      "authors": "Sangwoo Shin, BumJun Kim, Kyelim Lee, Moongyu Jeon, Albert No",
      "institution": "",
      "abstract": "Autoregressive language models (ARMs) suffer from the reversal curse: after learning that \"$A$ is $B$\", they often fail on the reverse query \"$B$ is $A$\". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing \"[MASK] is $B$\" during training does not necessarily teach the model to handle the reverse prompt \"$B$ is [MASK]\". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02132v1",
      "title": "There Is More to Refusal in Large Language Models than a Single Direction",
      "link": "http://arxiv.org/abs/2602.02132v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02132v1",
      "authors": "Faaiz Joad, Majd Hawasly, Sabri Boughorbel, Nadir Durrani, Husrev Taha Sencar",
      "institution": "",
      "abstract": "Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02130v1",
      "title": "Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework",
      "link": "http://arxiv.org/abs/2602.02130v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02130v1",
      "authors": "Lukas Zimmermann, Michael Rauter, Maximilian Schmid, Dietmar Georg, Barbara Knusl",
      "institution": "",
      "abstract": "Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02128v1",
      "title": "Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics",
      "link": "http://arxiv.org/abs/2602.02128v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02128v1",
      "authors": "Nima Shoghi, Yuxuan Liu, Yuning Shen, Rob Brekelmans, Pan Li et al.",
      "institution": "",
      "abstract": "Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02126v1",
      "title": "Two-Stage Grid Optimization for Group-wise Quantization of LLMs",
      "link": "http://arxiv.org/abs/2602.02126v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02126v1",
      "authors": "Junhan Kim, Gukryeol Lee, Seungwoo Son, Jeewook Kim, Yongkweon Jeon",
      "institution": "",
      "abstract": "Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02124v1",
      "title": "Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies",
      "link": "http://arxiv.org/abs/2602.02124v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02124v1",
      "authors": "Olga Graf, Dhrupal Patel, Peter Gro, Charlotte Lempp, Matthias Hein et al.",
      "institution": "",
      "abstract": "Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\\% of pathological tissue classified as healthy and 0.35\\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02123v1",
      "title": "MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos",
      "link": "http://arxiv.org/abs/2602.02123v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02123v1",
      "authors": "Yangyi Cao, Yuanhang Li, Lan Chen, Qi Mao",
      "institution": "",
      "abstract": "We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02117v1",
      "title": "The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning",
      "link": "http://arxiv.org/abs/2602.02117v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02117v1",
      "authors": "Youqi Wu, Farzan Farnia",
      "institution": "",
      "abstract": "Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grnwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.IT"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02114v1",
      "title": "Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training",
      "link": "http://arxiv.org/abs/2602.02114v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02114v1",
      "authors": "Xin Ding, Yun Chen, Sen Zhang, Kao Zhang, Nenglun Chen et al.",
      "institution": "",
      "abstract": "Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \\textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\\times64$ to $256\\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02113v1",
      "title": "Training-free score-based diffusion for parameter-dependent stochastic dynamical systems",
      "link": "http://arxiv.org/abs/2602.02113v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02113v1",
      "authors": "Minglei Yang, Sicheng He",
      "institution": "",
      "abstract": "Simulating parameter-dependent stochastic differential equations (SDEs) presents significant computational challenges, as separate high-fidelity simulations are typically required for each parameter value of interest. Despite the success of machine learning methods in learning SDE dynamics, existing approaches either require expensive neural network training for score function estimation or lack the ability to handle continuous parameter dependence. We present a training-free conditional diffusion model framework for learning stochastic flow maps of parameter-dependent SDEs, where both drift and diffusion coefficients depend on physical parameters. The key technical innovation is a joint kernel-weighted Monte Carlo estimator that approximates the conditional score function using trajectory data sampled at discrete parameter values, enabling interpolation across both state space and the continuous parameter domain. Once trained, the resulting generative model produces sample trajectories for any parameter value within the training range without retraining, significantly accelerating parameter studies, uncertainty quantification, and real-time filtering applications. The performance of the proposed approach is demonstrated via three numerical examples of increasing complexity, showing accurate approximation of conditional distributions across varying parameter values.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "stat.ML",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02112v1",
      "title": "Unifying Masked Diffusion Models with Various Generation Orders and Beyond",
      "link": "http://arxiv.org/abs/2602.02112v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02112v1",
      "authors": "Chunsan Hong, Sanghyun Lee, Jong Chul Ye",
      "institution": "",
      "abstract": "Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02108v1",
      "title": "Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts",
      "link": "http://arxiv.org/abs/2602.02108v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02108v1",
      "authors": "Wenhao Li, Daohai Yu, Gen Luo, Yuxin Zhang, Fei Chao et al.",
      "institution": "",
      "abstract": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02107v1",
      "title": "Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model",
      "link": "http://arxiv.org/abs/2602.02107v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02107v1",
      "authors": "Yu Wang, Chuanguang Yang, Zhulin An, Weilun Feng, Jiarui Zhao et al.",
      "institution": "",
      "abstract": "Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02104v1",
      "title": "Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs",
      "link": "http://arxiv.org/abs/2602.02104v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02104v1",
      "authors": "Shaltiel Shmidman, Avi Shmidman, Amir DN Cohen, Moshe Koppel",
      "institution": "",
      "abstract": "Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02103v1",
      "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs",
      "link": "http://arxiv.org/abs/2602.02103v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02103v1",
      "authors": "Liyan Xu, Mo Yu, Fandong Meng, Jie Zhou",
      "institution": "",
      "abstract": "This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02100v1",
      "title": "The Verification Crisis: Expert Perceptions of GenAI Disinformation and the Case for Reproducible Provenance",
      "link": "http://arxiv.org/abs/2602.02100v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02100v1",
      "authors": "Alexander Loth, Martin Kappes, Marc-Oliver Pahl",
      "institution": "",
      "abstract": "The growth of Generative Artificial Intelligence (GenAI) has shifted disinformation production from manual fabrication to automated, large-scale manipulation. This article presents findings from the first wave of a longitudinal expert perception survey (N=21) involving AI researchers, policymakers, and disinformation specialists. It examines the perceived severity of multimodal threats -- text, image, audio, and video -- and evaluates current mitigation strategies.\n  Results indicate that while deepfake video presents immediate \"shock\" value, large-scale text generation poses a systemic risk of \"epistemic fragmentation\" and \"synthetic consensus,\" particularly in the political domain. The survey reveals skepticism about technical detection tools, with experts favoring provenance standards and regulatory frameworks despite implementation barriers.\n  GenAI disinformation research requires reproducible methods. The current challenge is measurement: without standardized benchmarks and reproducibility checklists, tracking or countering synthetic media remains difficult. We propose treating information integrity as an infrastructure with rigor in data provenance and methodological reproducibility.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CY",
        "cs.AI",
        "cs.SI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02099v1",
      "title": "Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning",
      "link": "http://arxiv.org/abs/2602.02099v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02099v1",
      "authors": "Keqin Peng, Yuanxin Ouyang, Xuebo Liu, Zhiliang Tian, Ruijian Han et al.",
      "institution": "",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.CL",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02098v1",
      "title": "Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning",
      "link": "http://arxiv.org/abs/2602.02098v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02098v1",
      "authors": "Yannik Schnitzer, Mathias Jackermeier, Alessandro Abate, David Parker",
      "institution": "",
      "abstract": "Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.02096v1",
      "title": "WADEPre: A Wavelet-based Decomposition Model for Extreme Precipitation Nowcasting with Multi-Scale Learning",
      "link": "http://arxiv.org/abs/2602.02096v1",
      "pdf_link": "https://arxiv.org/pdf/2602.02096v1",
      "authors": "Baitian Liu, Haiping Zhang, Huiling Yuan, Dongjing Wang, Ying Li et al.",
      "institution": "",
      "abstract": "The heavy-tailed nature of precipitation intensity impedes precise precipitation nowcasting. Standard models that optimize pixel-wise losses are prone to regression-to-the-mean bias, which blurs extreme values. Existing Fourier-based methods also lack the spatial localization needed to resolve transient convective cells. To overcome these intrinsic limitations, we propose WADEPre, a wavelet-based decomposition model for extreme precipitation that transitions the modeling into the wavelet domain. By leveraging the Discrete Wavelet Transform for explicit decomposition, WADEPre employs a dual-branch architecture: an Approximation Network to model stable, low-frequency advection, isolating deterministic trends from statistical bias, and a spatially localized Detail Network to capture high-frequency stochastic convection, resolving transient singularities and preserving sharp boundaries. A subsequent Refiner module then dynamically reconstructs these decoupled multi-scale components into the final high-fidelity forecast. To address optimization instability, we introduce a multi-scale curriculum learning strategy that progressively shifts supervision from coarse scales to fine-grained details. Extensive experiments on the SEVIR and Shanghai Radar datasets demonstrate that WADEPre achieves state-of-the-art performance, yielding significant improvements in capturing extreme thresholds and maintaining structural fidelity. Our code is available at https://github.com/sonderlau/WADEPre.",
      "source": "arXiv",
      "pubDateISO": "2026-02-02",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    }
  ]
}