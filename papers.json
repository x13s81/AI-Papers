{
  "generated_at": "2026-02-20T03:55:51Z",
  "date": "2026-02-20",
  "total_count": 50,
  "papers": [
    {
      "id": "2602.15327",
      "title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
      "link": "https://huggingface.co/papers/2602.15327",
      "pdf_link": "https://arxiv.org/pdf/2602.15327.pdf",
      "authors": "Hanlin Zhang, Jikai Jin, Vasilis Syrgkanis, Sham Kakade",
      "institution": "",
      "abstract": "For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [
        "Language Models",
        "Scaling Laws",
        "Quantile Regression"
      ],
      "score": 9,
      "score_reason": "Prescriptive scaling laws",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces a prescriptive scaling methodology using smoothed quantile regression with a monotone, saturating sigmoid parameterization to estimate capability boundaries of language models as a function of log pre-training FLOPs.",
      "why_it_matters": "The proposed approach enables practitioners to translate compute budgets into reliable performance expectations and monitor shifts in capability boundaries over time, which is crucial for deploying foundation models in real-world applications.",
      "limitations": "The methodology relies on large-scale observational evaluations and may not generalize well to tasks or models that are significantly different from those in the evaluated dataset, such as those with distinct architectural designs or training objectives."
    },
    {
      "id": "2602.14979",
      "title": "RynnBrain: Open Embodied Foundation Models",
      "link": "https://huggingface.co/papers/2602.14979",
      "pdf_link": "https://arxiv.org/pdf/2602.14979.pdf",
      "authors": "Ronghao Dang, Jiayan Guo, Bohan Hou, Sicong Leng, Kehan Li",
      "institution": "",
      "abstract": "Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [
        "embodied intelligence",
        "multimodal foundation models",
        "spatiotemporal reasoning and planning"
      ],
      "score": 8,
      "score_reason": "Unified Model",
      "citations": 0,
      "upvotes": 27,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The authors introduce RynnBrain, a unified, open-source, spatiotemporal foundation model that integrates perception, reasoning, and planning for embodied intelligence, outperforming existing models on 20 embodied benchmarks and 8 general vision understanding benchmarks.",
      "why_it_matters": "This work matters because it provides a physically grounded foundation model that can be efficiently adapted to diverse embodied tasks, enabling advancements in areas such as robotics, autonomous systems, and human-computer interaction.",
      "limitations": "The main limitation of this work is that the evaluation of RynnBrain's performance is limited to a set of predefined benchmarks, and its generalizability to real-world, dynamic, and partially observable environments remains to be fully explored."
    },
    {
      "id": "2602.14296",
      "title": "AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines",
      "link": "https://huggingface.co/papers/2602.14296",
      "pdf_link": "https://arxiv.org/pdf/2602.14296.pdf",
      "authors": "Yifan Wu, Yiran Peng, Yiyu Chen, Jianhao Ruan, Zijie Zhuang",
      "institution": "",
      "abstract": "The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [
        "Autonomous Web GUI Agents",
        "Finite State Machines",
        "Synthetic Data Generation"
      ],
      "score": 8,
      "score_reason": "Novel Web Synthesis",
      "citations": 0,
      "upvotes": 20,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments using Finite State Machines (FSMs) to generate interactive websites with explicit state transitions and programmatic verification.",
      "why_it_matters": "This research matters because it addresses the bottleneck of collecting and verifying interaction trajectories from real-world websites, enabling the generation of large amounts of high-quality training data for autonomous Web GUI agents at a significantly reduced cost.",
      "limitations": "The main limitation of this approach is that the synthesized web environments may not fully capture the complexity and variability of real-world websites, potentially limiting the generalizability of the trained agents to novel or unseen environments."
    },
    {
      "id": "2602.12279",
      "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
      "link": "https://huggingface.co/papers/2602.12279",
      "pdf_link": "https://arxiv.org/pdf/2602.12279.pdf",
      "authors": "Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha",
      "institution": "",
      "abstract": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-12",
      "tags": [],
      "topics": [
        "Multimodal Learning",
        "Test-Time Scaling",
        "Chain-of-Thought Reasoning"
      ],
      "score": 8,
      "score_reason": "Unified multimodal chain",
      "citations": 0,
      "upvotes": 19,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces UniT, a framework for multimodal chain-of-thought test-time scaling that enables unified models to reason, verify, and refine across multiple rounds through agentic data synthesis, unified model training, and flexible test-time inference.",
      "why_it_matters": "This work matters because it demonstrates the effectiveness of multimodal test-time scaling in advancing both generation and visual reasoning capabilities of unified models, with potential applications in complex multimodal tasks.",
      "limitations": "The main limitation of this work is that it relies on sequential chain-of-thought reasoning, which may not be suitable for tasks requiring parallel or simultaneous processing of multiple modalities."
    },
    {
      "id": "2602.15112",
      "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
      "link": "https://huggingface.co/papers/2602.15112",
      "pdf_link": "https://arxiv.org/pdf/2602.15112.pdf",
      "authors": "Aniketh Garikaparthi, Manasi Patwardhan, Arman Cohan",
      "institution": "",
      "abstract": "We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [
        "Autonomous Research",
        "Language Model Evaluation",
        "Artificial Intelligence for Scientific Discovery"
      ],
      "score": 8,
      "score_reason": "Research benchmark platform",
      "citations": 0,
      "upvotes": 17,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces ResearchGym, a novel benchmark and execution environment for evaluating language model agents on end-to-end research tasks, providing a systematic framework for assessing autonomous research capabilities.",
      "why_it_matters": "This work matters because it enables the evaluation and improvement of AI agents' ability to perform real-world research, which has significant implications for accelerating scientific progress and automating knowledge discovery.",
      "limitations": "The main limitation of this study is the observed capability-reliability gap, where state-of-the-art language models exhibit inconsistent performance and struggle with long-horizon tasks, such as managing time and resources, and coordinating parallel experiments."
    },
    {
      "id": "2602.16008",
      "title": "MAEB: Massive Audio Embedding Benchmark",
      "link": "https://huggingface.co/papers/2602.16008",
      "pdf_link": "https://arxiv.org/pdf/2602.16008.pdf",
      "authors": "Adnan El Assadi, Isaac Chung, Chenghao Xiao, Roman Solomatin, Animesh Jha",
      "institution": "",
      "abstract": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [
        "audio embedding",
        "multimodal learning",
        "benchmarking"
      ],
      "score": 8,
      "score_reason": "Audio Benchmark",
      "citations": 0,
      "upvotes": 13,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The authors introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across various audio domains and languages, providing a comprehensive evaluation framework for audio embedding models.",
      "why_it_matters": "This benchmark matters because it enables researchers to assess the performance of audio embedding models across a wide range of tasks, facilitating the development of more robust and generalizable models for audio understanding.",
      "limitations": "A main limitation of the benchmark is that clustering remains a challenging task for all evaluated models, with even the best-performing models achieving only modest results, indicating a need for further research in this area."
    },
    {
      "id": "2602.16666",
      "title": "Towards a Science of AI Agent Reliability",
      "link": "https://huggingface.co/papers/2602.16666",
      "pdf_link": "https://arxiv.org/pdf/2602.16666.pdf",
      "authors": "Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala",
      "institution": "",
      "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [
        "AI reliability",
        "agent evaluation",
        "safety-critical systems"
      ],
      "score": 8,
      "score_reason": "Agent Reliability",
      "citations": 0,
      "upvotes": 11,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes a set of twelve concrete metrics to decompose AI agent reliability into four key dimensions: consistency, robustness, predictability, and safety, providing a holistic performance profile beyond traditional success metrics.",
      "why_it_matters": "This work matters because it addresses the discrepancy between rising accuracy scores and persistent failures in practice, offering a more nuanced evaluation framework for AI agents in safety-critical applications.",
      "limitations": "The main limitation of this study is that it only evaluates 14 agentic models across two benchmarks, which may not be representative of the broader range of AI agents and tasks, potentially limiting the generalizability of the findings."
    },
    {
      "id": "2602.15449",
      "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
      "link": "https://huggingface.co/papers/2602.15449",
      "pdf_link": "https://arxiv.org/pdf/2602.15449.pdf",
      "authors": "Chansung Park, Juyong Jiang, Fan Wang, Sayak Paul, Jiasi Shen",
      "institution": "",
      "abstract": "Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [
        "Reinforcement Learning",
        "Curriculum Learning",
        "Large Language Models"
      ],
      "score": 8,
      "score_reason": "Code generation",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces TAROT, a novel curriculum reinforcement fine-tuning approach that decouples curriculum progression from raw reward scores and adapts to a model's inherent capability, enabling more efficient and stable optimization of large language models for code generation.",
      "why_it_matters": "This research matters because it addresses the critical challenge of synthesizing robust and sophisticated code with large language models, which has significant implications for the development of more efficient and effective coding paradigms.",
      "limitations": "The main limitation of this work is that the optimal curriculum design may be highly dependent on the specific model architecture and task, requiring further experimentation to generalize the findings to other domains and models."
    },
    {
      "id": "2602.10210",
      "title": "How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge",
      "link": "https://huggingface.co/papers/2602.10210",
      "pdf_link": "https://arxiv.org/pdf/2602.10210.pdf",
      "authors": "Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund",
      "institution": "",
      "abstract": "Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-10",
      "tags": [],
      "topics": [
        "Natural Language Processing",
        "Knowledge Graphs",
        "Multi-Hop Reasoning"
      ],
      "score": 8,
      "score_reason": "Advances LLMs",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces HybridRAG-Bench, a novel benchmarking framework for evaluating retrieval-intensive, multi-hop reasoning over hybrid knowledge, which automatically generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths.",
      "why_it_matters": "This framework matters because it enables researchers to distinguish genuine retrieval and reasoning from parametric recall in large language models, allowing for more accurate evaluation of their capabilities.",
      "limitations": "The main limitation of this work is that the framework's effectiveness relies on the quality and relevance of the underlying scientific literature on arXiv, which may not always reflect the most up-to-date or diverse knowledge."
    },
    {
      "id": "2602.16928",
      "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
      "link": "https://huggingface.co/papers/2602.16928",
      "pdf_link": "https://arxiv.org/pdf/2602.16928.pdf",
      "authors": "Zun Li, John Schultz, Daniel Hennes, Marc Lanctot",
      "institution": "",
      "abstract": "Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [
        "Multi-Agent Reinforcement Learning",
        "Evolutionary Algorithms",
        "Large Language Models"
      ],
      "score": 8,
      "score_reason": "Novel MARL approach",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper proposes the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms, such as Volatility-Adaptive Discounted CFR and Smoothed Hybrid Optimistic Regret PSRO.",
      "why_it_matters": "The automated discovery of novel multiagent learning algorithms has the potential to significantly advance the field of Multi-Agent Reinforcement Learning by reducing the reliance on human intuition and manual refinement of baselines.",
      "limitations": "The approach relies on the quality of the large language model and the evolutionary coding agent, and may not always discover optimal or efficient algorithms, particularly in complex or high-dimensional game-theoretic learning scenarios."
    },
    {
      "id": "2602.12670",
      "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
      "link": "https://huggingface.co/papers/2602.12670",
      "pdf_link": "https://arxiv.org/pdf/2602.12670.pdf",
      "authors": "Xiangyi Li, Wenbo Chen, Yimin Liu, Shenghan Zheng, Xiaokun Chen",
      "institution": "",
      "abstract": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Skills benchmark",
      "citations": 0,
      "upvotes": 45,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16317",
      "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
      "link": "https://huggingface.co/papers/2602.16317",
      "pdf_link": "https://arxiv.org/pdf/2602.16317.pdf",
      "authors": "Maksim Elistratov, Marina Barannikov, Gregory Ivanov, Valentin Khrulkov, Anton Konushin",
      "institution": "",
      "abstract": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "CAD Evolution",
      "citations": 0,
      "upvotes": 19,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14080",
      "title": "Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality",
      "link": "https://huggingface.co/papers/2602.14080",
      "pdf_link": "https://arxiv.org/pdf/2602.14080.pdf",
      "authors": "Nitay Calderon, Eyal Ben-David, Zorik Gekhman, Eran Ofek, Gal Yona",
      "institution": "",
      "abstract": "Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Novel Evaluation",
      "citations": 0,
      "upvotes": 16,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15922",
      "title": "World Action Models are Zero-shot Policies",
      "link": "https://huggingface.co/papers/2602.15922",
      "pdf_link": "https://arxiv.org/pdf/2602.15922.pdf",
      "authors": "Seonghyeon Ye, Yunhao Ge, Kaiyuan Zheng, Shenyuan Gao, Sihyun Yu",
      "institution": "",
      "abstract": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Zero-Shot Policy",
      "citations": 0,
      "upvotes": 9,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14457",
      "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
      "link": "https://huggingface.co/papers/2602.14457",
      "pdf_link": "https://arxiv.org/pdf/2602.14457.pdf",
      "authors": "Dongrui Liu, Yi Yu, Jie Zhang, Guanxu Chen, Qihao Lin",
      "institution": "",
      "abstract": "To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [
        "Artificial Intelligence",
        "Risk Management",
        "Large Language Models"
      ],
      "score": 7,
      "score_reason": "Comprehensive risk framework",
      "citations": 0,
      "upvotes": 6,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces a comprehensive risk management framework for frontier AI, updating and expanding the assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R&D, and self-replication, with a focus on Large Language Models (LLMs) and agentic AI.",
      "why_it_matters": "This research matters because it provides a preliminary technical and actionable pathway for the secure deployment of frontier AI, addressing emerging threats and urging collective action to mitigate these challenges, which is crucial for ensuring the safe and responsible development of AI.",
      "limitations": "The main limitation of this work is that it relies on a specific set of scenarios and experiments, which may not be exhaustive or generalizable to all possible AI risk scenarios, and the proposed mitigation strategies may require further validation and refinement."
    },
    {
      "id": "2602.15772",
      "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
      "link": "https://huggingface.co/papers/2602.15772",
      "pdf_link": "https://arxiv.org/pdf/2602.15772.pdf",
      "authors": "Sen Ye, Mengde Xu, Shuyang Gu, Di He, Liwei Wang",
      "institution": "",
      "abstract": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Multimodal trade-off",
      "citations": 0,
      "upvotes": 6,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16682",
      "title": "Learning Situated Awareness in the Real World",
      "link": "https://huggingface.co/papers/2602.16682",
      "pdf_link": "https://arxiv.org/pdf/2602.16682.pdf",
      "authors": "Chuhan Li, Ruilin Han, Joy Hsu, Yongyuan Liang, Rajiv Dhawan",
      "institution": "",
      "abstract": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Situated Awareness",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.11389",
      "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
      "link": "https://huggingface.co/papers/2602.11389",
      "pdf_link": "https://arxiv.org/pdf/2602.11389.pdf",
      "authors": "Heejeong Nam, Quentin Le Lidec, Lucas Maes, Yann LeCun, Randall Balestriero",
      "institution": "",
      "abstract": "World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-11",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Novel world model",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15382",
      "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems",
      "link": "https://huggingface.co/papers/2602.15382",
      "pdf_link": "https://arxiv.org/pdf/2602.15382.pdf",
      "authors": "Xiaoze Liu, Ruowang Zhang, Weichen Yu, Siheng Xiong, Liu He",
      "institution": "",
      "abstract": "Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Efficient MAS",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16968",
      "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
      "link": "https://huggingface.co/papers/2602.16968",
      "pdf_link": "https://arxiv.org/pdf/2602.16968.pdf",
      "authors": "Dahye Kim, Deepti Ghadiyaram, Raghudeep Gadde",
      "institution": "",
      "abstract": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-19",
      "tags": [],
      "topics": [
        "Diffusion Transformers",
        "Dynamic Tokenization",
        "Efficient Inference"
      ],
      "score": 7,
      "score_reason": "Efficient diffusion transformers",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes a dynamic patch scheduling strategy for Diffusion Transformers, which adaptively adjusts patch sizes during the denoising phase based on content complexity and timestep.",
      "why_it_matters": "This approach has significant practical implications as it achieves substantial speedups in image and video generation tasks without compromising generation quality, making Diffusion Transformers more efficient and scalable for real-world applications.",
      "limitations": "The dynamic patch scheduling strategy may not be effective for all types of input data or generation tasks, and the optimal patch size allocation may require careful tuning for specific use cases."
    },
    {
      "id": "2602.14498",
      "title": "Uncertainty-Aware Vision-Language Segmentation for Medical Imaging",
      "link": "https://huggingface.co/papers/2602.14498",
      "pdf_link": "https://arxiv.org/pdf/2602.14498.pdf",
      "authors": "Aryan Das, Tanishq Rachamalla, Koushik Biswas, Swalpa Kumar Roy, Vinay Kumar Verma",
      "institution": "",
      "abstract": "We introduce a novel uncertainty-aware multimodal segmentation framework that leverages both radiological images and associated clinical text for precise medical diagnosis. We propose a Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) to enable efficient cross-modal fusion and long-range dependency modelling. To guide learning under ambiguity, we propose the Spectral-Entropic Uncertainty (SEU) Loss, which jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective. In complex clinical circumstances with poor image quality, this formulation improves model reliability. Extensive experiments on various publicly available medical datasets, QATA-COVID19, MosMed++, and Kvasir-SEG, demonstrate that our method achieves superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art (SoTA) approaches. Our results highlight the importance of incorporating uncertainty modelling and structured modality alignment in vision-language medical segmentation tasks. Code: https://github.com/arya-domain/UA-VLS",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [
        "Multimodal Fusion",
        "Uncertainty Modelling",
        "Medical Image Segmentation"
      ],
      "score": 7,
      "score_reason": "Medical Imaging",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces a novel Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) and a Spectral-Entropic Uncertainty (SEU) Loss for uncertainty-aware vision-language segmentation in medical imaging.",
      "why_it_matters": "This work matters because it enables more accurate and reliable medical diagnosis by effectively leveraging both radiological images and associated clinical text, particularly in complex clinical circumstances with poor image quality.",
      "limitations": "The main limitation of this approach is that its performance may be sensitive to the quality and availability of clinical text data, which can be noisy, incomplete, or inconsistent in real-world medical scenarios."
    },
    {
      "id": "2602.13579",
      "title": "TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment",
      "link": "https://huggingface.co/papers/2602.13579",
      "pdf_link": "https://arxiv.org/pdf/2602.13579.pdf",
      "authors": "Youngsun Wi, Jessica Yin, Elvis Xiang, Akash Sharma, Jitendra Malik",
      "institution": "",
      "abstract": "Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-14",
      "tags": [],
      "topics": [
        "human-robot interaction",
        "tactile sensing",
        "policy transfer learning"
      ],
      "score": 7,
      "score_reason": "Tactile policy transfer",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes TactAlign, a novel cross-embodiment tactile alignment method that enables human-to-robot policy transfer without requiring paired datasets, manual labels, or identical tactile sensors.",
      "why_it_matters": "This research matters because it addresses the long-standing challenge of transferring human-collected tactile signals to robots with different embodiment, thereby enhancing the scalability and generality of human-to-robot policy transfer approaches.",
      "limitations": "The main limitation of this work is that it relies on the quality and availability of human-collected tactile data, which may be noisy, incomplete, or difficult to obtain for certain tasks or environments."
    },
    {
      "id": "2602.14602",
      "title": "OPBench: A Graph Benchmark to Combat the Opioid Crisis",
      "link": "https://huggingface.co/papers/2602.14602",
      "pdf_link": "https://arxiv.org/pdf/2602.14602.pdf",
      "authors": "Tianyi Ma, Yiyang Li, Yiyue Qian, Zheyuan Zhang, Zehong Wang",
      "institution": "",
      "abstract": "The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [
        "Graph Learning",
        "Benchmarking",
        "Opioid Crisis Modeling"
      ],
      "score": 7,
      "score_reason": "Opioid Crisis",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces OPBench, a comprehensive graph benchmark for evaluating graph learning methods on real-world opioid crisis scenarios, incorporating diverse graph structures and datasets across three critical application domains.",
      "why_it_matters": "This benchmark enables systematic evaluation and comparison of graph learning methods for combating the opioid crisis, facilitating the development of more effective computational solutions for this pressing public health issue.",
      "limitations": "The benchmark's effectiveness relies on the quality and availability of annotated datasets, which may be limited by data scarcity, privacy concerns, and the need for domain expert collaboration."
    },
    {
      "id": "2602.14111",
      "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?",
      "link": "https://huggingface.co/papers/2602.14111",
      "pdf_link": "https://arxiv.org/pdf/2602.14111.pdf",
      "authors": "Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Rogov, Ivan Oseledets",
      "institution": "",
      "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Useful sanity checks",
      "citations": 0,
      "upvotes": 55,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15547",
      "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation",
      "link": "https://huggingface.co/papers/2602.15547",
      "pdf_link": "https://arxiv.org/pdf/2602.15547.pdf",
      "authors": "Mohammad Kalim Akram, Saba Sturua, Nastia Havriushenko, Quentin Herreros, Michael Gnther",
      "institution": "",
      "abstract": "Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Embedding distillation",
      "citations": 0,
      "upvotes": 15,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16301",
      "title": "Multi-agent cooperation through in-context co-player inference",
      "link": "https://huggingface.co/papers/2602.16301",
      "pdf_link": "https://arxiv.org/pdf/2602.16301.pdf",
      "authors": "Marissa A. Weis, Maciej Woczyk, Rajai Nasser, Rif A. Saurous, Blaise Agera y Arcas",
      "institution": "",
      "abstract": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Cooperative Learning",
      "citations": 0,
      "upvotes": 10,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14486",
      "title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View",
      "link": "https://huggingface.co/papers/2602.14486",
      "pdf_link": "https://arxiv.org/pdf/2602.14486.pdf",
      "authors": "Fabian Grger, Shuo Wen, Maria Brbi",
      "institution": "",
      "abstract": "The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Revisiting hypothesis",
      "citations": 0,
      "upvotes": 9,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15989",
      "title": "SAM 3D Body: Robust Full-Body Human Mesh Recovery",
      "link": "https://huggingface.co/papers/2602.15989",
      "pdf_link": "https://arxiv.org/pdf/2602.15989.pdf",
      "authors": "Xitong Yang, Devansh Kukreja, Don Pinkus, Anushka Sagar, Taosha Fan",
      "institution": "",
      "abstract": "We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves HMR",
      "citations": 4,
      "upvotes": 8,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15156",
      "title": "Panini: Continual Learning in Token Space via Structured Memory",
      "link": "https://huggingface.co/papers/2602.15156",
      "pdf_link": "https://arxiv.org/pdf/2602.15156.pdf",
      "authors": "Shreyas Rajesh, Pavan Holur, Mehmet Yigit Turali, Chenda Duan, Vwani Roychowdhury",
      "institution": "",
      "abstract": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Continual learning",
      "citations": 0,
      "upvotes": 6,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.13515",
      "title": "SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning",
      "link": "https://huggingface.co/papers/2602.13515",
      "pdf_link": "https://arxiv.org/pdf/2602.13515.pdf",
      "authors": "Jintao Zhang, Kai Jiang, Chendong Xiang, Weiqi Feng, Yuezhou Hu",
      "institution": "",
      "abstract": "Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves sparse attention",
      "citations": 1,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16173",
      "title": "Learning Personalized Agents from Human Feedback",
      "link": "https://huggingface.co/papers/2602.16173",
      "pdf_link": "https://arxiv.org/pdf/2602.16173.pdf",
      "authors": "Kaiqu Liang, Julia Kruk, Shengyi Qian, Xianjun Yang, Shengjie Bi",
      "institution": "",
      "abstract": "Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Personalized Agents",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16493",
      "title": "MMA: Multimodal Memory Agent",
      "link": "https://huggingface.co/papers/2602.16493",
      "pdf_link": "https://arxiv.org/pdf/2602.16493.pdf",
      "authors": "Yihao Lu, Wanru Cheng, Zeyu Zhang, Hao Tang",
      "institution": "",
      "abstract": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Memory Agent",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.17270",
      "title": "Unified Latents (UL): How to train your latents",
      "link": "https://huggingface.co/papers/2602.17270",
      "pdf_link": "https://arxiv.org/pdf/2602.17270.pdf",
      "authors": "Jonathan Heek, Emiel Hoogeboom, Thomas Mensink, Tim Salimans",
      "institution": "",
      "abstract": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-19",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Unified latent framework",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15927",
      "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
      "link": "https://huggingface.co/papers/2602.15927",
      "pdf_link": "https://arxiv.org/pdf/2602.15927.pdf",
      "authors": "Christian Schlarmann, Matthias Hein",
      "institution": "",
      "abstract": "Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Security Attack",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.08392",
      "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models",
      "link": "https://huggingface.co/papers/2602.08392",
      "pdf_link": "https://arxiv.org/pdf/2602.08392.pdf",
      "authors": "Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin",
      "institution": "",
      "abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-09",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "New Benchmark",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14514",
      "title": "Efficient Text-Guided Convolutional Adapter for the Diffusion Model",
      "link": "https://huggingface.co/papers/2602.14514",
      "pdf_link": "https://arxiv.org/pdf/2602.14514.pdf",
      "authors": "Aryan Das, Koushik Biswas, Swalpa Kumar Roy, Badri Narayana Patro, Vinay Kumar Verma",
      "institution": "",
      "abstract": "We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning. Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter. Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter, which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Efficient Adapter",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.17363",
      "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
      "link": "https://huggingface.co/papers/2602.17363",
      "pdf_link": "https://arxiv.org/pdf/2602.17363.pdf",
      "authors": "Gabriel Mongaras, Eric C. Larson",
      "institution": "",
      "abstract": "Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-19",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Linear attention improvement",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.12675",
      "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT",
      "link": "https://huggingface.co/papers/2602.12675",
      "pdf_link": "https://arxiv.org/pdf/2602.12675.pdf",
      "authors": "Jintao Zhang, Haoxu Wang, Kai Jiang, Kaiwen Zheng, Youhe Jiang",
      "institution": "",
      "abstract": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Attention Optimization",
      "citations": 1,
      "upvotes": 43,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16705",
      "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
      "link": "https://huggingface.co/papers/2602.16705",
      "pdf_link": "https://arxiv.org/pdf/2602.16705.pdf",
      "authors": "Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta",
      "institution": "",
      "abstract": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "End-Effector Control",
      "citations": 0,
      "upvotes": 25,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16704",
      "title": "Reinforced Fast Weights with Next-Sequence Prediction",
      "link": "https://huggingface.co/papers/2602.16704",
      "pdf_link": "https://arxiv.org/pdf/2602.16704.pdf",
      "authors": "Hee Seung Hwang, Xindi Wu, Sanghyuk Chun, Olga Russakovsky",
      "institution": "",
      "abstract": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Fast Weights",
      "citations": 0,
      "upvotes": 9,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15200",
      "title": "COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression",
      "link": "https://huggingface.co/papers/2602.15200",
      "pdf_link": "https://arxiv.org/pdf/2602.15200.pdf",
      "authors": "Denis Makhov, Dmitriy Shopkhoev, Magauiya Zhussip, Ammar Ali, Baher Mohammad",
      "institution": "MIT",
      "abstract": "Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Transformer compression",
      "citations": 0,
      "upvotes": 6,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.07345",
      "title": "Optimizing Few-Step Generation with Adaptive Matching Distillation",
      "link": "https://huggingface.co/papers/2602.07345",
      "pdf_link": "https://arxiv.org/pdf/2602.07345.pdf",
      "authors": "Lichen Bai, Zikai Zhou, Shitong Shao, Wenliang Zhong, Shuo Yang",
      "institution": "",
      "abstract": "Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-07",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Distillation Method",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16855",
      "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents",
      "link": "https://huggingface.co/papers/2602.16855",
      "pdf_link": "https://arxiv.org/pdf/2602.16855.pdf",
      "authors": "Haiyang Xu, Xi Zhang, Haowei Liu, Junyang Wang, Zhaozai Zhu",
      "institution": "",
      "abstract": "The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "GUI agent update",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.13964",
      "title": "HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam",
      "link": "https://huggingface.co/papers/2602.13964",
      "pdf_link": "https://arxiv.org/pdf/2602.13964.pdf",
      "authors": "Weiqi Zhai, Zhihai Wang, Jinghang Wang, Boyu Yang, Xiaogang Li",
      "institution": "",
      "abstract": "Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Benchmark refinement",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.09653",
      "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference",
      "link": "https://huggingface.co/papers/2602.09653",
      "pdf_link": "https://arxiv.org/pdf/2602.09653.pdf",
      "authors": "Shiwei Lyu, Xidong Wang, Lei Liu, Hao Zhu, Chaohe Zhang",
      "institution": "",
      "abstract": "Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-10",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Clinical alignment method",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.12235",
      "title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation",
      "link": "https://huggingface.co/papers/2602.12235",
      "pdf_link": "https://arxiv.org/pdf/2602.12235.pdf",
      "authors": "Julia Belikova, Danila Rozhevskii, Dennis Svirin, Konstantin Polev, Alexander Panchenko",
      "institution": "",
      "abstract": "Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-12",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Efficient processing",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.17004",
      "title": "Arcee Trinity Large Technical Report",
      "link": "https://huggingface.co/papers/2602.17004",
      "pdf_link": "https://arxiv.org/pdf/2602.17004.pdf",
      "authors": "Varun Singh, Lucas Krauss, Sami Jaghouar, Matej Sirovatka, Charles Goddard",
      "institution": "",
      "abstract": "We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-19",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Model size update",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14364",
      "title": "A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)",
      "link": "https://huggingface.co/papers/2602.14364",
      "pdf_link": "https://arxiv.org/pdf/2602.14364.pdf",
      "authors": "Tianyu Chen, Dongrui Liu, Xia Hu, Jingyi Yu, Wenjie Wang",
      "institution": "",
      "abstract": "Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge (AgentDoG-Qwen3-4B) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Safety audit",
      "citations": 0,
      "upvotes": 17,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.12978",
      "title": "Learning Native Continuation for Action Chunking Flow Policies",
      "link": "https://huggingface.co/papers/2602.12978",
      "pdf_link": "https://arxiv.org/pdf/2602.12978.pdf",
      "authors": "Yufeng Liu, Hang Yu, Juntu Zhao, Bocheng Li, Di Zhang",
      "institution": "",
      "abstract": "Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Minor action chunking",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.17365",
      "title": "Computer-Using World Model",
      "link": "https://huggingface.co/papers/2602.17365",
      "pdf_link": "https://arxiv.org/pdf/2602.17365.pdf",
      "authors": "Yiming Guan, Rui Yu, John Zhang, Lu Wang, Chaoyun Zhang",
      "institution": "",
      "abstract": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-19",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Conceptual world model",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    }
  ]
}