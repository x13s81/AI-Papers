{
  "generated_at": "2026-02-19T04:00:51Z",
  "date": "2026-02-19",
  "total_count": 50,
  "papers": [
    {
      "id": "2602.16666",
      "title": "Towards a Science of AI Agent Reliability",
      "link": "https://huggingface.co/papers/2602.16666",
      "pdf_link": "https://arxiv.org/pdf/2602.16666.pdf",
      "authors": "Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala",
      "institution": "",
      "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [
        "AI reliability",
        "agent evaluation",
        "safety-critical systems"
      ],
      "score": 9,
      "score_reason": "New science",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper proposes a set of twelve concrete metrics to decompose AI agent reliability along four key dimensions: consistency, robustness, predictability, and safety, providing a more nuanced evaluation framework beyond traditional success metrics.",
      "why_it_matters": "The proposed metrics have significant practical implications for ensuring the reliable deployment of AI agents in safety-critical applications, where failures can have severe consequences.",
      "limitations": "The evaluation is limited to 14 agentic models across two complementary benchmarks, which may not be representative of the broader range of AI agents and applications, potentially restricting the generalizability of the findings."
    },
    {
      "id": "2602.13367",
      "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts",
      "link": "https://huggingface.co/papers/2602.13367",
      "pdf_link": "https://arxiv.org/pdf/2602.13367.pdf",
      "authors": "Chen Yang, Guangyue Peng, Jiaying Zhu, Ran Le, Ruixiang Feng",
      "institution": "",
      "abstract": "We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [
        "language modeling",
        "reinforcement learning",
        "generalist models"
      ],
      "score": 8,
      "score_reason": "Small yet versatile model",
      "citations": 0,
      "upvotes": 18,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces Nanbeige4.1-3B, a 3B parameter unified generalist language model that achieves strong agentic behavior, code generation, and general reasoning, outperforming larger models through novel combinations of point-wise and pair-wise reward modeling and complexity-aware rewards.",
      "why_it_matters": "This research matters because it demonstrates that small models can achieve both broad competence and strong specialization, redefining the potential of 3B parameter models and challenging the conventional wisdom that larger models are always necessary for state-of-the-art performance.",
      "limitations": "The main limitation of this work is that the evaluation of Nanbeige4.1-3B's performance is primarily based on its ability to execute up to 600 tool-call turns, which may not generalize to all types of complex problem-solving tasks or real-world applications."
    },
    {
      "id": "2602.15112",
      "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
      "link": "https://huggingface.co/papers/2602.15112",
      "pdf_link": "https://arxiv.org/pdf/2602.15112.pdf",
      "authors": "Aniketh Garikaparthi, Manasi Patwardhan, Arman Cohan",
      "institution": "",
      "abstract": "We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [
        "Autonomous Research",
        "Language Model Evaluation",
        "AI Benchmarking"
      ],
      "score": 8,
      "score_reason": "Evaluates AI research",
      "citations": 0,
      "upvotes": 14,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The authors introduce ResearchGym, a novel benchmark and execution environment for evaluating language model agents on end-to-end research tasks, providing a systematic framework for assessing autonomous research capabilities.",
      "why_it_matters": "This work matters because it provides a rigorous evaluation framework for assessing the capabilities and limitations of AI agents in performing real-world research tasks, which is crucial for advancing autonomous research and understanding the reliability of AI systems.",
      "limitations": "The main limitation of this work is that the evaluated agents, including those powered by GPT-5, exhibit a significant capability-reliability gap, completing only a fraction of sub-tasks and rarely surpassing human baselines, highlighting the need for further improvements in AI research capabilities."
    },
    {
      "id": "2602.16705",
      "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
      "link": "https://huggingface.co/papers/2602.16705",
      "pdf_link": "https://arxiv.org/pdf/2602.16705.pdf",
      "authors": "Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta",
      "institution": "",
      "abstract": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [
        "humanoid robotics",
        "visual loco-manipulation",
        "end-effector control"
      ],
      "score": 8,
      "score_reason": "Humanoid control",
      "citations": 0,
      "upvotes": 10,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces HERO, a novel paradigm that combines large vision models with a residual-aware end-effector tracking policy, leveraging inverse kinematics, learned neural forward models, goal adjustment, and replanning to achieve accurate end-effector control.",
      "why_it_matters": "This research enables humanoid robots to perform open-vocabulary visual loco-manipulation tasks in diverse real-world environments, advancing the field of robotics and facilitating more effective human-robot interaction.",
      "limitations": "The approach relies on simulated training and may not generalize to scenarios with significant differences between the simulated and real-world environments, potentially limiting its robustness and adaptability."
    },
    {
      "id": "2602.15322",
      "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers",
      "link": "https://huggingface.co/papers/2602.15322",
      "pdf_link": "https://arxiv.org/pdf/2602.15322.pdf",
      "authors": "Taejong Joo, Wenhan Xia, Cheolmin Kim, Ming Zhang, Eugene Ie",
      "institution": "",
      "abstract": "Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [
        "optimization algorithms",
        "large language models",
        "deep learning"
      ],
      "score": 8,
      "score_reason": "Surprising effectiveness",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces Momentum-aligned gradient masking (Magma), a novel optimization technique that leverages random masking of parameter updates and momentum-gradient alignment to improve the training of large language models.",
      "why_it_matters": "This research matters because it challenges the dominance of dense adaptive optimizers in large language model training and offers a simple, computationally efficient alternative that achieves state-of-the-art results.",
      "limitations": "The main limitation of this work is that the effectiveness of Magma is primarily demonstrated on large language model pre-training tasks, and its applicability to other domains and model architectures remains to be explored."
    },
    {
      "id": "2602.15449",
      "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
      "link": "https://huggingface.co/papers/2602.15449",
      "pdf_link": "https://arxiv.org/pdf/2602.15449.pdf",
      "authors": "Chansung Park, Juyong Jiang, Fan Wang, Sayak Paul, Jiasi Shen",
      "institution": "",
      "abstract": "Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [
        "Reinforcement Learning",
        "Curriculum Learning",
        "Large Language Models"
      ],
      "score": 8,
      "score_reason": "Advances code gen",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The authors propose TAROT, a novel curriculum reinforcement fine-tuning approach that decouples curriculum progression from raw reward scores and adapts to a model's inherent capability, enabling more efficient and stable optimization of large language models for code generation.",
      "why_it_matters": "This research matters because it addresses the critical challenge of synthesizing robust and algorithmically sophisticated code with large language models, which has significant implications for the development of more efficient and effective coding paradigms.",
      "limitations": "The main limitation of this work is that the optimal curriculum design may be highly dependent on the specific large language model architecture and task, requiring further experimentation to generalize the findings to other settings."
    },
    {
      "id": "2602.15327",
      "title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
      "link": "https://huggingface.co/papers/2602.15327",
      "pdf_link": "https://arxiv.org/pdf/2602.15327.pdf",
      "authors": "Hanlin Zhang, Jikai Jin, Vasilis Syrgkanis, Sham Kakade",
      "institution": "",
      "abstract": "For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [
        "language models",
        "scaling laws",
        "quantile regression"
      ],
      "score": 8,
      "score_reason": "Prescriptive scaling laws",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces a prescriptive scaling methodology using smoothed quantile regression with a monotone, saturating sigmoid parameterization to estimate capability boundaries of language models as a function of log pre-training FLOPs.",
      "why_it_matters": "This work enables practitioners to translate compute budgets into reliable performance expectations and monitor shifts in capability boundaries over time, which is crucial for deploying foundation models effectively.",
      "limitations": "The approach relies on large-scale observational evaluations and may not generalize well to tasks or models that are significantly different from those in the Proteus 2k dataset."
    },
    {
      "id": "2602.10210",
      "title": "How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge",
      "link": "https://huggingface.co/papers/2602.10210",
      "pdf_link": "https://arxiv.org/pdf/2602.10210.pdf",
      "authors": "Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund",
      "institution": "",
      "abstract": "Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-10",
      "tags": [],
      "topics": [
        "Natural Language Processing",
        "Knowledge Graphs",
        "Multi-Hop Reasoning"
      ],
      "score": 8,
      "score_reason": "Benchmark framework",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces HybridRAG-Bench, a novel benchmarking framework for evaluating retrieval-intensive, multi-hop reasoning over hybrid knowledge, which automatically generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths.",
      "why_it_matters": "This framework matters because it enables reliable evaluation of retrieval-augmented models' capabilities, distinguishing genuine retrieval and reasoning from parametric recall, which is crucial for advancing hybrid knowledge-augmented reasoning systems.",
      "limitations": "The main limitation of this work is that the framework's effectiveness relies on the quality and coverage of the underlying knowledge graphs and unstructured text representations, which may not always be comprehensive or up-to-date."
    },
    {
      "id": "2602.12670",
      "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
      "link": "https://huggingface.co/papers/2602.12670",
      "pdf_link": "https://arxiv.org/pdf/2602.12670.pdf",
      "authors": "Xiangyi Li, Wenbo Chen, Yimin Liu, Shenghan Zheng, Xiaokun Chen",
      "institution": "",
      "abstract": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [
        "Natural Language Processing",
        "Large Language Models",
        "Agent Skills and Knowledge Representation"
      ],
      "score": 7,
      "score_reason": "New benchmark",
      "citations": 0,
      "upvotes": 40,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces SkillsBench, a benchmark for evaluating the effectiveness of agent skills across diverse tasks, providing a standardized framework for measuring their impact on large language model (LLM) agents.",
      "why_it_matters": "This research matters because it enables the systematic evaluation and improvement of agent skills, which can significantly enhance the performance of LLMs in various domains, with potential applications in areas like software engineering and healthcare.",
      "limitations": "The main limitation of this work is that self-generated skills are found to provide no benefit on average, suggesting that current models lack the ability to reliably author the procedural knowledge they can leverage when consuming curated skills."
    },
    {
      "id": "2602.15763",
      "title": "GLM-5: from Vibe Coding to Agentic Engineering",
      "link": "https://huggingface.co/papers/2602.15763",
      "pdf_link": "https://arxiv.org/pdf/2602.15763.pdf",
      "authors": "GLM-5 Team, Aohan Zeng, Xin Lv, Zhenyu Hou, Zhengxiao Du",
      "institution": "",
      "abstract": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Next-gen model",
      "citations": 0,
      "upvotes": 39,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.12279",
      "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
      "link": "https://huggingface.co/papers/2602.12279",
      "pdf_link": "https://arxiv.org/pdf/2602.12279.pdf",
      "authors": "Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha",
      "institution": "",
      "abstract": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-12",
      "tags": [],
      "topics": [
        "Multimodal Learning",
        "Test-Time Scaling",
        "Chain-of-Thought Reasoning"
      ],
      "score": 7,
      "score_reason": "Unified multimodal model",
      "citations": 0,
      "upvotes": 18,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds, extending test-time scaling to unified multimodal models.",
      "why_it_matters": "This work matters because it demonstrates the effectiveness of multimodal test-time scaling in advancing both generation and visual reasoning capabilities, with potential applications in complex spatial composition and multi-object interaction tasks.",
      "limitations": "The main limitation of this work is that it relies on sequential chain-of-thought reasoning, which may not be feasible for very long inference chains or real-time applications due to computational efficiency concerns."
    },
    {
      "id": "2602.07824",
      "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training",
      "link": "https://huggingface.co/papers/2602.07824",
      "pdf_link": "https://arxiv.org/pdf/2602.07824.pdf",
      "authors": "Yiwei Qin, Zhen Huang, Tiantian Mi, Weiye Si, Chenyang Zhou",
      "institution": "",
      "abstract": "Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.\n  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-08",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Improves data processing",
      "citations": 0,
      "upvotes": 14,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14486",
      "title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View",
      "link": "https://huggingface.co/papers/2602.14486",
      "pdf_link": "https://arxiv.org/pdf/2602.14486.pdf",
      "authors": "Fabian Gröger, Shuo Wen, Maria Brbić",
      "institution": "",
      "abstract": "The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [
        "neural network representations",
        "representation similarity metrics",
        "multi-task learning"
      ],
      "score": 7,
      "score_reason": "Revisits hypothesis",
      "citations": 0,
      "upvotes": 7,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The authors introduce a permutation-based null-calibration framework to correct the confounding effects of network scale on representational similarity metrics, providing a more accurate assessment of the Platonic Representation Hypothesis.",
      "why_it_matters": "This research has significant implications for understanding the generalizability and transferability of neural network representations across different models and tasks, which is crucial for advancing areas like multi-task learning and domain adaptation.",
      "limitations": "The proposed calibration framework relies on a permutation-based approach, which may not be scalable or efficient for very large neural networks, limiting its applicability to certain domains or models."
    },
    {
      "id": "2602.16682",
      "title": "Learning Situated Awareness in the Real World",
      "link": "https://huggingface.co/papers/2602.16682",
      "pdf_link": "https://arxiv.org/pdf/2602.16682.pdf",
      "authors": "Chuhan Li, Ruilin Han, Joy Hsu, Yongyuan Liang, Rajiv Dhawan",
      "institution": "",
      "abstract": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [
        "Multimodal Foundation Models",
        "Egocentric Vision",
        "Situated Awareness"
      ],
      "score": 7,
      "score_reason": "New awareness model",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces SAW-Bench, a novel benchmark for evaluating egocentric situated awareness in real-world videos, focusing on observer-centric relationships and reasoning relative to the agent's viewpoint, pose, and motion.",
      "why_it_matters": "This research matters because it addresses a significant gap in existing multimodal foundation models, which predominantly emphasize environment-centric spatial relations, and has implications for developing more human-like spatial intelligence in AI systems.",
      "limitations": "The main limitation of this work is that the best-performing model, Gemini 3 Flash, still exhibits a significant human-model performance gap of 37.66%, indicating that substantial challenges remain in achieving robust situated awareness capabilities."
    },
    {
      "id": "2602.15620",
      "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
      "link": "https://huggingface.co/papers/2602.15620",
      "pdf_link": "https://arxiv.org/pdf/2602.15620.pdf",
      "authors": "Shiqi Liu, Zeyu He, Guojian Zhan, Letian Tao, Zhilong Zheng",
      "institution": "",
      "abstract": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term spurious tokens. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO, 20-Entropy and JustRL.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [
        "Reinforcement Learning",
        "Large Language Models",
        "Stability and Robustness"
      ],
      "score": 7,
      "score_reason": "Stabilizes RL",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The authors propose Spurious-Token-Aware Policy Optimization (STAPO), a novel method that stabilizes reinforcement learning for large language models by selectively masking and renormalizing gradient updates for rare spurious tokens.",
      "why_it_matters": "STAPO improves the stability and performance of large language models, which is crucial for reliable and efficient reasoning in various applications, such as natural language processing and decision-making tasks.",
      "limitations": "The method's effectiveness is primarily evaluated on mathematical reasoning benchmarks, and its generalizability to other tasks and domains remains to be explored."
    },
    {
      "id": "2602.15382",
      "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems",
      "link": "https://huggingface.co/papers/2602.15382",
      "pdf_link": "https://arxiv.org/pdf/2602.15382.pdf",
      "authors": "Xiaoze Liu, Ruowang Zhang, Weichen Yu, Siheng Xiong, Liu He",
      "institution": "",
      "abstract": "Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [
        "Multi-Agent Systems",
        "Vision-Language Models",
        "Latent Space Communication"
      ],
      "score": 7,
      "score_reason": "Latent communication",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes a novel framework called the Vision Wormhole, which enables model-agnostic, text-free communication in heterogeneous multi-agent systems by repurposing the visual interface of Vision-Language Models (VLMs) to map reasoning traces into a shared continuous latent space.",
      "why_it_matters": "This work matters because it addresses the inefficiency of discrete text communication in multi-agent systems, potentially unlocking faster and more scalable collaborative reasoning capabilities in applications such as distributed AI and human-computer interaction.",
      "limitations": "The main limitation of this approach is that it relies on the availability of a visual interface in the Vision-Language Models, which may not be applicable to all types of models or domains, and may require significant modifications to existing architectures."
    },
    {
      "id": "2602.14060",
      "title": "LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts",
      "link": "https://huggingface.co/papers/2602.14060",
      "pdf_link": "https://arxiv.org/pdf/2602.14060.pdf",
      "authors": "Yang Liu, Jiaye Yang, Weikang Li, Jiahui Liang, Yang Li",
      "institution": "",
      "abstract": "We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Advances definition modeling",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15922",
      "title": "World Action Models are Zero-shot Policies",
      "link": "https://huggingface.co/papers/2602.15922",
      "pdf_link": "https://arxiv.org/pdf/2602.15922.pdf",
      "authors": "Seonghyeon Ye, Yunhao Ge, Kaiyuan Zheng, Shenyuan Gao, Sihyun Yu",
      "institution": "",
      "abstract": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [
        "Vision-Language-Action models",
        "World Action Models",
        "Autoregressive video diffusion models"
      ],
      "score": 7,
      "score_reason": "Zero-shot policy",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces DreamZero, a World Action Model (WAM) that jointly models video and action to learn physical dynamics and achieve zero-shot generalization to novel environments and tasks.",
      "why_it_matters": "This work matters because it enables robots to learn diverse skills from heterogeneous data and adapt to new environments and embodiments with minimal additional training, which is crucial for real-world robotic applications.",
      "limitations": "The main limitation of this approach is that it relies on a large-scale, pre-trained video diffusion backbone and requires significant computational resources to perform real-time closed-loop control."
    },
    {
      "id": "2602.14941",
      "title": "AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories",
      "link": "https://huggingface.co/papers/2602.14941",
      "pdf_link": "https://arxiv.org/pdf/2602.14941.pdf",
      "authors": "Zun Wang, Han Lin, Jaehong Yoon, Jaemin Cho, Yue Zhang",
      "institution": "",
      "abstract": "Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Better generation",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.12586",
      "title": "Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models",
      "link": "https://huggingface.co/papers/2602.12586",
      "pdf_link": "https://arxiv.org/pdf/2602.12586.pdf",
      "authors": "Joshua Ong Jun Leang, Yu Zhao, Mihaela Cătălina Stoian, Wenda Li, Shay B. Cohen",
      "institution": "",
      "abstract": "While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Improves diffusion models",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.10458",
      "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving",
      "link": "https://huggingface.co/papers/2602.10458",
      "pdf_link": "https://arxiv.org/pdf/2602.10458.pdf",
      "authors": "Yansong Qu, Zihao Sheng, Zilin Huang, Jiancong Chen, Yuhao Luo",
      "institution": "",
      "abstract": "Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-11",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Improved RL",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.10809",
      "title": "DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories",
      "link": "https://huggingface.co/papers/2602.10809",
      "pdf_link": "https://arxiv.org/pdf/2602.10809.pdf",
      "authors": "Chenlong Deng, Mengjie Deng, Junjie Wu, Dun Zeng, Teng Wang",
      "institution": "",
      "abstract": "Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-11",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Enhances image retrieval",
      "citations": 0,
      "upvotes": 42,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14265",
      "title": "STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts",
      "link": "https://huggingface.co/papers/2602.14265",
      "pdf_link": "https://arxiv.org/pdf/2602.14265.pdf",
      "authors": "Zachary Bamberger, Till R. Saenger, Gilad Morad, Ofra Amir, Brandon M. Stewart",
      "institution": "",
      "abstract": "Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improved diversity",
      "citations": 0,
      "upvotes": 18,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.11574",
      "title": "Learning to Configure Agentic AI Systems",
      "link": "https://huggingface.co/papers/2602.11574",
      "pdf_link": "https://arxiv.org/pdf/2602.11574.pdf",
      "authors": "Aditya Taparia, Som Sagar, Ransalu Senanayake",
      "institution": "",
      "abstract": "Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to \"one size fits all\" designs.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-12",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Practical solution",
      "citations": 0,
      "upvotes": 13,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.13294",
      "title": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction",
      "link": "https://huggingface.co/papers/2602.13294",
      "pdf_link": "https://arxiv.org/pdf/2602.13294.pdf",
      "authors": "Jiarong Liang, Max Ku, Ka-Hei Hui, Ping Nie, Wenhu Chen",
      "institution": "",
      "abstract": "Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-09",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "New benchmark",
      "citations": 0,
      "upvotes": 12,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15547",
      "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation",
      "link": "https://huggingface.co/papers/2602.15547",
      "pdf_link": "https://arxiv.org/pdf/2602.15547.pdf",
      "authors": "Mohammad Kalim Akram, Saba Sturua, Nastia Havriushenko, Quentin Herreros, Michael Günther",
      "institution": "",
      "abstract": "Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Better embeddings",
      "citations": 0,
      "upvotes": 9,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15156",
      "title": "Panini: Continual Learning in Token Space via Structured Memory",
      "link": "https://huggingface.co/papers/2602.15156",
      "pdf_link": "https://arxiv.org/pdf/2602.15156.pdf",
      "authors": "Shreyas Rajesh, Pavan Holur, Mehmet Yigit Turali, Chenda Duan, Vwani Roychowdhury",
      "institution": "",
      "abstract": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves RAG",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15200",
      "title": "COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression",
      "link": "https://huggingface.co/papers/2602.15200",
      "pdf_link": "https://arxiv.org/pdf/2602.15200.pdf",
      "authors": "Denis Makhov, Dmitriy Shopkhoev, Magauiya Zhussip, Ammar Ali, Baher Mohammad",
      "institution": "MIT",
      "abstract": "Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves compression",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15772",
      "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
      "link": "https://huggingface.co/papers/2602.15772",
      "pdf_link": "https://arxiv.org/pdf/2602.15772.pdf",
      "authors": "Sen Ye, Mengde Xu, Shuyang Gu, Di He, Liwei Wang",
      "institution": "",
      "abstract": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Analyzes trade-off",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.11389",
      "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
      "link": "https://huggingface.co/papers/2602.11389",
      "pdf_link": "https://arxiv.org/pdf/2602.11389.pdf",
      "authors": "Heejeong Nam, Quentin Le Lidec, Lucas Maes, Yann LeCun, Randall Balestriero",
      "institution": "",
      "abstract": "World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-11",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Object-level interventions",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15278",
      "title": "Visual Persuasion: What Influences Decisions of Vision-Language Models?",
      "link": "https://huggingface.co/papers/2602.15278",
      "pdf_link": "https://arxiv.org/pdf/2602.15278.pdf",
      "authors": "Manuel Cherep, Pranav M R, Pattie Maes, Nikhil Singh",
      "institution": "",
      "abstract": "The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "New insights",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.07854",
      "title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model",
      "link": "https://huggingface.co/papers/2602.07854",
      "pdf_link": "https://arxiv.org/pdf/2602.07854.pdf",
      "authors": "Chendong Xiang, Jiajun Liu, Jintao Zhang, Xiao Yang, Zhengwei Fang",
      "institution": "",
      "abstract": "Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce ViewRope, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose Geometry-Aware Frame-Sparse Attention, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present ViewBench, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-08",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves video models",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16301",
      "title": "Multi-agent cooperation through in-context co-player inference",
      "link": "https://huggingface.co/papers/2602.16301",
      "pdf_link": "https://arxiv.org/pdf/2602.16301.pdf",
      "authors": "Marissa A. Weis, Maciej Wołczyk, Rajai Nasser, Rif A. Saurous, Blaise Agüera y Arcas",
      "institution": "",
      "abstract": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Cooperation method",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15989",
      "title": "SAM 3D Body: Robust Full-Body Human Mesh Recovery",
      "link": "https://huggingface.co/papers/2602.15989",
      "pdf_link": "https://arxiv.org/pdf/2602.15989.pdf",
      "authors": "Xitong Yang, Devansh Kukreja, Don Pinkus, Anushka Sagar, Taosha Fan",
      "institution": "",
      "abstract": "We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-17",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Body mesh recovery",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15031",
      "title": "EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing",
      "link": "https://huggingface.co/papers/2602.15031",
      "pdf_link": "https://arxiv.org/pdf/2602.15031.pdf",
      "authors": "Yehonathan Litman, Shikun Liu, Dario Seyb, Nicholas Milef, Yang Zhou",
      "institution": "",
      "abstract": "High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves video editing",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.11968",
      "title": "DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling",
      "link": "https://huggingface.co/papers/2602.11968",
      "pdf_link": "https://arxiv.org/pdf/2602.11968.pdf",
      "authors": "Mariia Fedorova, Andrey Kutuzov, Khonzoda Umarova",
      "institution": "",
      "abstract": "In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-12",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Provides new corpora",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.09319",
      "title": "Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation",
      "link": "https://huggingface.co/papers/2602.09319",
      "pdf_link": "https://arxiv.org/pdf/2602.09319.pdf",
      "authors": "Zhisheng Qi, Utkarsh Sahu, Li Ma, Haoyu Han, Ryan Rossi",
      "institution": "",
      "abstract": "Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-10",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Evaluates knowledge extraction",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.16493",
      "title": "MMA: Multimodal Memory Agent",
      "link": "https://huggingface.co/papers/2602.16493",
      "pdf_link": "https://arxiv.org/pdf/2602.16493.pdf",
      "authors": "Yihao Lu, Wanru Cheng, Zeyu Zhang, Hao Tang",
      "institution": "",
      "abstract": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-18",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves memory",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.12675",
      "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT",
      "link": "https://huggingface.co/papers/2602.12675",
      "pdf_link": "https://arxiv.org/pdf/2602.12675.pdf",
      "authors": "Jintao Zhang, Haoxu Wang, Kai Jiang, Kaiwen Zheng, Youhe Jiang",
      "institution": "",
      "abstract": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Optimizes attention",
      "citations": 0,
      "upvotes": 16,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.13964",
      "title": "HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam",
      "link": "https://huggingface.co/papers/2602.13964",
      "pdf_link": "https://arxiv.org/pdf/2602.13964.pdf",
      "authors": "Weiqi Zhai, Zhihai Wang, Jinghang Wang, Boyu Yang, Xiaogang Li",
      "institution": "",
      "abstract": "Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Cleans HLE data",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.12978",
      "title": "Learning Native Continuation for Action Chunking Flow Policies",
      "link": "https://huggingface.co/papers/2602.12978",
      "pdf_link": "https://arxiv.org/pdf/2602.12978.pdf",
      "authors": "Yufeng Liu, Hang Yu, Juntu Zhao, Bocheng Li, Di Zhang",
      "institution": "",
      "abstract": "Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Real-time chunking",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.09653",
      "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference",
      "link": "https://huggingface.co/papers/2602.09653",
      "pdf_link": "https://arxiv.org/pdf/2602.09653.pdf",
      "authors": "Shiwei Lyu, Xidong Wang, Lei Liu, Hao Zhu, Chaohe Zhang",
      "institution": "",
      "abstract": "Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-10",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Aligns clinician preferences",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.12235",
      "title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation",
      "link": "https://huggingface.co/papers/2602.12235",
      "pdf_link": "https://arxiv.org/pdf/2602.12235.pdf",
      "authors": "Julia Belikova, Danila Rozhevskii, Dennis Svirin, Konstantin Polev, Alexander Panchenko",
      "institution": "",
      "abstract": "Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-12",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Efficient compression",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.13346",
      "title": "CellMaster: Collaborative Cell Type Annotation in Single-Cell Analysis",
      "link": "https://huggingface.co/papers/2602.13346",
      "pdf_link": "https://arxiv.org/pdf/2602.13346.pdf",
      "authors": "Zhen Wang, Yiming Gao, Jieyuan Liu, Enze Ma, Jefferson Chen",
      "institution": "",
      "abstract": "Single-cell RNA-seq (scRNA-seq) enables atlas-scale profiling of complex tissues, revealing rare lineages and transient states. Yet, assigning biologically valid cell identities remains a bottleneck because markers are tissue- and state-dependent, and novel states lack references. We present CellMaster, an AI agent that mimics expert practice for zero-shot cell-type annotation. Unlike existing automated tools, CellMaster leverages LLM-encoded knowledge (e.g., GPT-4o) to perform on-the-fly annotation with interpretable rationales, without pre-training or fixed marker databases. Across 9 datasets spanning 8 tissues, CellMaster improved accuracy by 7.1% over best-performing baselines (including CellTypist and scTab) in automatic mode. With human-in-the-loop refinement, this advantage increased to 18.6%, with a 22.1% gain on subtype populations. The system demonstrates particular strength in rare and novel cell states where baselines often fail. Source code and the web application are available at https://github.com/AnonymousGym/CellMaster{https://github.com/AnonymousGym/CellMaster}.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-12",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Useful tool",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.07673",
      "title": "Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation",
      "link": "https://huggingface.co/papers/2602.07673",
      "pdf_link": "https://arxiv.org/pdf/2602.07673.pdf",
      "authors": "Jiangnan Fang, Cheng-Tse Liu, Hanieh Deilamsalehy, Nesreen K. Ahmed, Puneet Mathur",
      "institution": "",
      "abstract": "Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-07",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Identifies LLM bias",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.15259",
      "title": "Knowing Isn't Understanding: Re-grounding Generative Proactivity with Epistemic and Behavioral Insight",
      "link": "https://huggingface.co/papers/2602.15259",
      "pdf_link": "https://arxiv.org/pdf/2602.15259.pdf",
      "authors": "Kirandeep Kaur, Xingda Lyu, Chirag Shah",
      "institution": "",
      "abstract": "Generative AI agents equate understanding with resolving explicit queries, an assumption that confines interaction to what users can articulate. This assumption breaks down when users themselves lack awareness of what is missing, risky, or worth considering. In such conditions, proactivity is not merely an efficiency enhancement, but an epistemic necessity. We refer to this condition as epistemic incompleteness: where progress depends on engaging with unknown unknowns for effective partnership. Existing approaches to proactivity remain narrowly anticipatory, extrapolating from past behavior and presuming that goals are already well defined, thereby failing to support users meaningfully. However, surfacing possibilities beyond a user's current awareness is not inherently beneficial. Unconstrained proactive interventions can misdirect attention, overwhelm users, or introduce harm. Proactive agents, therefore, require behavioral grounding: principled constraints on when, how, and to what extent an agent should intervene. We advance the position that generative proactivity must be grounded both epistemically and behaviorally. Drawing on the philosophy of ignorance and research on proactive behavior, we argue that these theories offer critical guidance for designing agents that can engage responsibly and foster meaningful partnerships.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Theoretical contribution",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.13516",
      "title": "SPILLage: Agentic Oversharing on the Web",
      "link": "https://huggingface.co/papers/2602.13516",
      "pdf_link": "https://arxiv.org/pdf/2602.13516.pdf",
      "authors": "Jaechul Roh, Eugene Bagdasarian, Hamed Haddadi, Ali Shahin Shamsabadi",
      "institution": "",
      "abstract": "LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act \"in the wild\", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of \"output\" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-13",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Identifies web agent issue",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14299",
      "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
      "link": "https://huggingface.co/papers/2602.14299",
      "pdf_link": "https://arxiv.org/pdf/2602.14299.pdf",
      "authors": "Ming Li, Xirui Li, Tianyi Zhou",
      "institution": "",
      "abstract": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Case study",
      "citations": 0,
      "upvotes": 23,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14364",
      "title": "A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)",
      "link": "https://huggingface.co/papers/2602.14364",
      "pdf_link": "https://arxiv.org/pdf/2602.14364.pdf",
      "authors": "Tianyu Chen, Dongrui Liu, Xia Hu, Jingyi Yu, Wenjie Wang",
      "institution": "",
      "abstract": "Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge (AgentDoG-Qwen3-4B) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-16",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Safety audit",
      "citations": 0,
      "upvotes": 6,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2602.14111",
      "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?",
      "link": "https://huggingface.co/papers/2602.14111",
      "pdf_link": "https://arxiv.org/pdf/2602.14111.pdf",
      "authors": "Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Rogov, Ivan Oseledets",
      "institution": "",
      "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.",
      "source": "HuggingFace",
      "pubDateISO": "2026-02-15",
      "tags": [],
      "topics": [],
      "score": 3,
      "score_reason": "Sanity checks",
      "citations": 0,
      "upvotes": 51,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    }
  ]
}