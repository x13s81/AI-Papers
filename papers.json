{
  "generated_at": "2026-01-23T14:22:32Z",
  "date": "2026-01-23",
  "total_count": 237,
  "papers": [
    {
      "id": "2601.16206",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "link": "https://huggingface.co/papers/2601.16206",
      "pdf_link": "https://arxiv.org/pdf/2601.16206.pdf",
      "authors": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen",
      "institution": "",
      "abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [
        "Artificial General Intelligence",
        "Large Language Models",
        "Reinforcement Learning"
      ],
      "score": 9,
      "score_reason": "General agentic intelligence",
      "citations": 0,
      "upvotes": 40,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces LLM-in-Sandbox, a novel framework that enables large language models (LLMs) to explore and interact with a code sandbox, eliciting general agentic intelligence in non-code domains without requiring additional training.",
      "why_it_matters": "This research matters because it demonstrates the potential for LLMs to generalize and adapt to new tasks and domains, leveraging the sandbox environment to acquire new knowledge and capabilities, which could significantly advance the field of artificial general intelligence.",
      "limitations": "A main limitation of this work is that the effectiveness of LLM-in-Sandbox relies on the quality and complexity of the sandbox environment, as well as the ability of the LLM to understand and navigate the sandbox's functionality and constraints."
    },
    {
      "id": "2601.15727",
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "link": "https://huggingface.co/papers/2601.15727",
      "pdf_link": "https://arxiv.org/pdf/2601.15727.pdf",
      "authors": "Yang Yu, Peiyu Zang, Chi Hsu Tsai, Haiming Wu, Yixin Shen",
      "institution": "",
      "abstract": "The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [
        "Large Language Models (LLMs)",
        "Automated Kernel Generation",
        "Agentic Optimization Workflows"
      ],
      "score": 9,
      "score_reason": "Automates kernel",
      "citations": 0,
      "upvotes": 12,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper provides a systematic overview and compilation of existing approaches for automated kernel generation using large language models (LLMs) and LLM-based agents, addressing the current fragmentation in the field.",
      "why_it_matters": "Automating kernel generation and optimization using LLMs has the potential to significantly improve the performance and scalability of modern AI systems, reducing the need for expert-level hardware knowledge and manual optimization.",
      "limitations": "The paper primarily focuses on surveying existing approaches, and does not present new experimental results or evaluate the effectiveness of LLM-driven kernel generation methods, which remains a key open challenge."
    },
    {
      "id": "2601.15220",
      "title": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models",
      "link": "https://huggingface.co/papers/2601.15220",
      "pdf_link": "https://arxiv.org/pdf/2601.15220.pdf",
      "authors": "Anmol Goel, Cornelius Emde, Sangdoo Yun, Seong Joon Oh, Martin Gubri",
      "institution": "",
      "abstract": "We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [
        "language models",
        "fine-tuning",
        "contextual privacy"
      ],
      "score": 9,
      "score_reason": "Privacy collapse discovery",
      "citations": 0,
      "upvotes": 8,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces the concept of 'privacy collapse', where benign fine-tuning of language models can lead to a degradation of contextual privacy, despite maintaining high performance on standard safety and utility benchmarks.",
      "why_it_matters": "The findings have significant implications for the deployment of specialized agents, as they highlight a critical gap in current safety evaluations that can result in severe privacy vulnerabilities.",
      "limitations": "The study's results are limited to six models and five fine-tuning datasets, which may not be representative of all language models and fine-tuning scenarios, and further research is needed to fully understand the scope and mechanisms of privacy collapse."
    },
    {
      "id": "2601.14352",
      "title": "RoboBrain 2.5: Depth in Sight, Time in Mind",
      "link": "https://huggingface.co/papers/2601.14352",
      "pdf_link": "https://arxiv.org/pdf/2601.14352.pdf",
      "authors": "Huajie Tan, Enshen Zhou, Zhiyu Li, Yijie Xu, Yuheng Ji",
      "institution": "",
      "abstract": "We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [
        "Embodied AI",
        "3D Spatial Reasoning",
        "Temporal Modeling"
      ],
      "score": 9,
      "score_reason": "Embodied AI Foundation",
      "citations": 0,
      "upvotes": 6,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "RoboBrain 2.5 introduces depth-aware coordinate prediction and absolute metric constraint comprehension, enabling precise 3D spatial reasoning and dense temporal value estimation for embodied AI.",
      "why_it_matters": "This work matters because it advances the state-of-the-art in embodied AI by providing a more physically grounded and execution-aware framework for complex manipulation tasks, with potential applications in robotics and computer vision.",
      "limitations": "The main limitation of this work is that it relies on high-quality spatiotemporal supervision, which may be difficult to obtain in real-world scenarios, potentially limiting the model's generalizability and robustness."
    },
    {
      "id": "2601.12538",
      "title": "Agentic Reasoning for Large Language Models",
      "link": "https://huggingface.co/papers/2601.12538",
      "pdf_link": "https://arxiv.org/pdf/2601.12538.pdf",
      "authors": "Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Ze Yang",
      "institution": "",
      "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-18",
      "tags": [],
      "topics": [
        "Large Language Models",
        "Agentic Reasoning",
        "Autonomous Agents"
      ],
      "score": 8,
      "score_reason": "Agentic Reasoning",
      "citations": 0,
      "upvotes": 149,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces a novel paradigm of agentic reasoning for large language models, reframing them as autonomous agents that plan, act, and learn through continual interaction in open-ended and dynamic environments.",
      "why_it_matters": "This work matters because it has the potential to significantly improve the reasoning capabilities of large language models in real-world applications, such as science, robotics, and healthcare, by enabling them to adapt and learn in complex and dynamic settings.",
      "limitations": "A main limitation of this approach is that it relies on the availability of high-quality feedback mechanisms and reinforcement learning signals, which can be challenging to obtain in certain domains or applications."
    },
    {
      "id": "2601.16175",
      "title": "Learning to Discover at Test Time",
      "link": "https://huggingface.co/papers/2601.16175",
      "pdf_link": "https://arxiv.org/pdf/2601.16175.pdf",
      "authors": "Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb",
      "institution": "",
      "abstract": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [
        "reinforcement learning",
        "continual learning",
        "large language models",
        "test-time adaptation"
      ],
      "score": 8,
      "score_reason": "Test-time discovery",
      "citations": 0,
      "upvotes": 19,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces Test-Time Training to Discover (TTT-Discover), a novel method that leverages reinforcement learning at test time to enable a large language model (LLM) to continue training and adapt to a specific problem, leading to state-of-the-art solutions in various domains.",
      "why_it_matters": "TTT-Discover has significant practical impact as it enables the discovery of new state-of-the-art solutions in multiple fields, including mathematics, GPU kernel engineering, algorithm design, and biology, using an open and reproducible approach.",
      "limitations": "The method's effectiveness is limited to problems with continuous rewards and relies on the quality of the initial LLM and the design of the reinforcement learning objective and search subroutine."
    },
    {
      "id": "2601.15369",
      "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
      "link": "https://huggingface.co/papers/2601.15369",
      "pdf_link": "https://arxiv.org/pdf/2601.15369.pdf",
      "authors": "Letian Zhang, Sucheng Ren, Yanqing Liu, Xianhang Li, Zeyu Wang",
      "institution": "",
      "abstract": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [
        "Computer Vision",
        "Multimodal Learning",
        "Generative Models"
      ],
      "score": 8,
      "score_reason": "Unified vision encoder",
      "citations": 0,
      "upvotes": 10,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces OpenVision 3, a unified visual encoder that jointly optimizes reconstruction- and semantics-driven signals in a shared latent space, enabling a single representation to serve both image understanding and generation tasks.",
      "why_it_matters": "This work matters because it bridges the gap between image understanding and generation, allowing for more efficient and effective modeling of visual data, with potential applications in multimodal learning and computer vision.",
      "limitations": "The main limitation of this approach is that it relies on a complex architecture with multiple components, including VAE compression, ViT encoding, and contrastive learning, which may require significant computational resources and careful hyperparameter tuning."
    },
    {
      "id": "2601.14027",
      "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
      "link": "https://huggingface.co/papers/2601.14027",
      "pdf_link": "https://arxiv.org/pdf/2601.14027.pdf",
      "authors": "Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He",
      "institution": "",
      "abstract": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [
        "formal theorem proving",
        "agentic systems",
        "general coding agents"
      ],
      "score": 8,
      "score_reason": "General agentic system",
      "citations": 0,
      "upvotes": 9,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes a novel paradigm that leverages a general coding agent as a formal math reasoner, enabling flexible and autonomous interaction with formal proof assistants like Lean.",
      "why_it_matters": "This approach has the potential to improve the flexibility, reproducibility, and performance of formal theorem proving systems, making them more accessible and useful for a broader range of mathematical applications.",
      "limitations": "The current implementation relies on a specific base model, Claude Opus 4.5, and may not generalize to other models or domains without significant retraining or modification."
    },
    {
      "id": "2601.16148",
      "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
      "link": "https://huggingface.co/papers/2601.16148",
      "pdf_link": "https://arxiv.org/pdf/2601.16148.pdf",
      "authors": "Remy Sabathier, David Novotny, Niloy J. Mitra, Tom Monnier",
      "institution": "MIT",
      "abstract": "Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes \"in action\" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed \"temporal 3D diffusion\". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [
        "3D mesh generation",
        "temporal diffusion models",
        "animated 3D object synthesis"
      ],
      "score": 8,
      "score_reason": "Novel 3D mesh",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces a novel generative model, ActionMesh, which extends 3D diffusion models to incorporate a temporal axis, enabling the generation of animated 3D meshes in a feed-forward manner.",
      "why_it_matters": "This work matters because it enables the rapid generation of high-quality, topology-consistent, and rig-free animated 3D meshes, which can accelerate applications such as computer-generated imagery, video games, and virtual reality.",
      "limitations": "The main limitation of this approach is that it relies on a pre-defined reference shape, which may limit its ability to generate animations with complex or non-rigid deformations."
    },
    {
      "id": "2601.14253",
      "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
      "link": "https://huggingface.co/papers/2601.14253",
      "pdf_link": "https://arxiv.org/pdf/2601.14253.pdf",
      "authors": "Hongyuan Chen, Xingyu Chen, Youjia Zhang, Zexiang Xu, Anpei Chen",
      "institution": "",
      "abstract": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [
        "4D synthesis",
        "3D motion reconstruction",
        "monocular video analysis"
      ],
      "score": 8,
      "score_reason": "4D synthesis breakthrough",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces a novel feed-forward framework, Motion 3-to-4, that decomposes 4D synthesis into static 3D shape generation and motion reconstruction, allowing for high-quality 4D dynamic object synthesis from monocular video input.",
      "why_it_matters": "This work matters because it addresses the challenging problem of 4D synthesis, which has numerous applications in computer vision, robotics, and computer graphics, such as motion capture, animation, and virtual reality.",
      "limitations": "The main limitation of this approach is its reliance on a canonical reference mesh, which may not always be available or accurate, and can limit the model's ability to generalize to diverse and complex scenes."
    },
    {
      "id": "2601.14681",
      "title": "FARE: Fast-Slow Agentic Robotic Exploration",
      "link": "https://huggingface.co/papers/2601.14681",
      "pdf_link": "https://arxiv.org/pdf/2601.14681.pdf",
      "authors": "Shuhao Liao, Xuxin Lv, Jeric Lew, Shizhe Zhang, Jingsong Liang",
      "institution": "",
      "abstract": "This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale 200mtimes130m building environment.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [
        "Autonomous Robotics",
        "Hierarchical Reinforcement Learning",
        "Large Language Models"
      ],
      "score": 8,
      "score_reason": "Robotic Exploration",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) with a reinforcement learning (RL) policy to achieve efficient and robust exploration in unknown environments.",
      "why_it_matters": "The proposed framework has significant practical implications for autonomous robotics, as it enables robots to efficiently explore complex and large-scale environments, which is crucial for applications such as search and rescue, mapping, and inspection.",
      "limitations": "The main limitation of FARE is its reliance on a concise textual description of the unknown environment, which may not always be available or accurate, and the modularity-based pruning mechanism may not always effectively reduce redundant graph structures."
    },
    {
      "id": "2601.16192",
      "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360°",
      "link": "https://huggingface.co/papers/2601.16192",
      "pdf_link": "https://arxiv.org/pdf/2601.16192.pdf",
      "authors": "Ziyi Wu, Daniel Watson, Andrea Tagliasacchi, David J. Fleet, Marcus A. Brubaker",
      "institution": "",
      "abstract": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [
        "Computer Vision",
        "Diffusion Models",
        "Panorama Generation"
      ],
      "score": 8,
      "score_reason": "Geometry-free lifting",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes a geometry-free framework, 360Anything, that utilizes pre-trained diffusion transformers to learn the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera metadata.",
      "why_it_matters": "This approach enables the lifting of in-the-wild images and videos to 360° panoramas, which is crucial for immersive 3D world generation and has broader implications for computer vision tasks such as camera calibration and orientation estimation.",
      "limitations": "The method may still suffer from seam artifacts at ERP boundaries, although the authors introduce Circular Latent Encoding to mitigate this issue, and its performance may degrade with low-quality or noisy input data."
    },
    {
      "id": "2601.16004",
      "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
      "link": "https://huggingface.co/papers/2601.16004",
      "pdf_link": "https://arxiv.org/pdf/2601.16004.pdf",
      "authors": "Christopher Altman",
      "institution": "",
      "abstract": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.\n  Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.\n  This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [
        "Quantum Information Processing",
        "Superconducting Quantum Computing",
        "Quantum Foundations and Interpretations"
      ],
      "score": 8,
      "score_reason": "Quantum hardware benchmark",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper proposes and experimentally demonstrates a circuit-based implementation of Wigner's Friend protocol on superconducting quantum hardware, enabling the estimation of operational inter-branch communication witnesses.",
      "why_it_matters": "This work matters because it provides a reproducible operational framework for evaluating the detectability of non-ideal channels in quantum systems, which is crucial for the development of reliable quantum computing and communication protocols.",
      "limitations": "The main limitation of this study is that it relies on a specific noise model and device constraints, which may not be representative of all quantum hardware platforms or scenarios."
    },
    {
      "id": "2601.15876",
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "link": "https://huggingface.co/papers/2601.15876",
      "pdf_link": "https://arxiv.org/pdf/2601.15876.pdf",
      "authors": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "institution": "",
      "abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [
        "Multimodal AI",
        "Reinforcement Learning",
        "Evolutionary Algorithms"
      ],
      "score": 7,
      "score_reason": "Improves CUA",
      "citations": 0,
      "upvotes": 53,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces EvoCUA, a novel native computer use agentic model that integrates data generation and policy optimization into a self-sustaining evolutionary cycle, leveraging a verifiable synthesis engine and scalable infrastructure to generate diverse tasks and acquire large-scale experience.",
      "why_it_matters": "This work matters because it addresses the data scarcity bottleneck in developing computer-use agents, enabling more efficient and effective learning of complex tasks and potentially leading to significant advancements in multimodal AI.",
      "limitations": "The main limitation of this approach is that it relies on a carefully designed synthesis engine and sandbox infrastructure, which may be challenging to replicate or adapt to different domains or tasks, potentially limiting the generalizability of the results."
    },
    {
      "id": "2601.15197",
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "link": "https://huggingface.co/papers/2601.15197",
      "pdf_link": "https://arxiv.org/pdf/2601.15197.pdf",
      "authors": "Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen",
      "institution": "",
      "abstract": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior π(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Novel VLA approach",
      "citations": 0,
      "upvotes": 49,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15282",
      "title": "Rethinking Video Generation Model for the Embodied World",
      "link": "https://huggingface.co/papers/2601.15282",
      "pdf_link": "https://arxiv.org/pdf/2601.15282.pdf",
      "authors": "Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu",
      "institution": "",
      "abstract": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Advances Video Generation",
      "citations": 0,
      "upvotes": 38,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.13761",
      "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
      "link": "https://huggingface.co/papers/2601.13761",
      "pdf_link": "https://arxiv.org/pdf/2601.13761.pdf",
      "authors": "Shengda Fan, Xuyan Ye, Yankai Lin",
      "institution": "",
      "abstract": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "LLM Evolution Framework",
      "citations": 0,
      "upvotes": 14,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.14722",
      "title": "Typhoon OCR: Open Vision-Language Model For Thai Document Extraction",
      "link": "https://huggingface.co/papers/2601.14722",
      "pdf_link": "https://arxiv.org/pdf/2601.14722.pdf",
      "authors": "Surapon Nonesung, Natapong Nitarach, Teetouch Jaknamon, Pittawat Taveekitworachai, Kunat Pipatanakul",
      "institution": "",
      "abstract": "Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Advances Thai OCR",
      "citations": 0,
      "upvotes": 13,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.11141",
      "title": "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning",
      "link": "https://huggingface.co/papers/2601.11141",
      "pdf_link": "https://arxiv.org/pdf/2601.11141.pdf",
      "authors": "Tanyu Chen, Tairan Chen, Kai Shen, Zhenghua Bao, Zhihui Zhang",
      "institution": "",
      "abstract": "Recent end-to-end spoken dialogue systems leverage speech tokenizers and neural audio codecs to enable LLMs to operate directly on discrete speech representations. However, these models often exhibit limited speaker identity preservation, hindering personalized voice interaction. In this work, we present Chroma 1.0, the first open-source, real-time, end-to-end spoken dialogue model that achieves both low-latency interaction and high-fidelity personalized voice cloning. Chroma achieves sub-second end-to-end latency through an interleaved text-audio token schedule (1:2) that supports streaming generation, while maintaining high-quality personalized voice synthesis across multi-turn conversations. Our experimental results demonstrate that Chroma achieves a 10.96% relative improvement in speaker similarity over the human baseline, with a Real-Time Factor (RTF) of 0.43, while maintaining strong reasoning and dialogue capabilities. Our code and models are publicly available at https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma and https://huggingface.co/FlashLabs/Chroma-4B .",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-16",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Personalized voice cloning",
      "citations": 0,
      "upvotes": 11,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.11868",
      "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
      "link": "https://huggingface.co/papers/2601.11868",
      "pdf_link": "https://arxiv.org/pdf/2601.11868.pdf",
      "authors": "Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj",
      "institution": "",
      "abstract": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-17",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Realistic task benchmark",
      "citations": 0,
      "upvotes": 10,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.07853",
      "title": "FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments",
      "link": "https://huggingface.co/papers/2601.07853",
      "pdf_link": "https://arxiv.org/pdf/2601.07853.pdf",
      "authors": "Zhi Yang, Runguo Li, Qiqi Qiang, Jiashun Wang, Fangqi Lou",
      "institution": "",
      "abstract": "Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-09",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Financial agent safety",
      "citations": 0,
      "upvotes": 9,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.14256",
      "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
      "link": "https://huggingface.co/papers/2601.14256",
      "pdf_link": "https://arxiv.org/pdf/2601.14256.pdf",
      "authors": "Matthew Gwilliam, Xiao Wang, Xuefeng Hu, Zhenheng Yang",
      "institution": "",
      "abstract": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Unified vision encoding",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.13918",
      "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization",
      "link": "https://huggingface.co/papers/2601.13918",
      "pdf_link": "https://arxiv.org/pdf/2601.13918.pdf",
      "authors": "Yusheng Liao, Chuan Xuan, Yutong Cai, Lina Yang, Zhe Chen",
      "institution": "",
      "abstract": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Autonomous EHR navigation",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15549",
      "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
      "link": "https://huggingface.co/papers/2601.15549",
      "pdf_link": "https://arxiv.org/pdf/2601.15549.pdf",
      "authors": "Ryo Fujii, Hideo Saito, Ryo Hachiuma",
      "institution": "",
      "abstract": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [
        "Multimodal Large Language Models",
        "In-Context Learning",
        "Semi-Supervised Learning"
      ],
      "score": 7,
      "score_reason": "Efficient learning",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces VIOLA, a label-efficient framework that combines density-uncertainty-weighted sampling with confidence-aware retrieval and prompting to enable in-context learning for multimodal large language models with minimal annotations.",
      "why_it_matters": "This research matters because it addresses the scarcity of labeled data in specialized environments, allowing for more efficient and accurate deployment of multimodal large language models in real-world applications.",
      "limitations": "The framework's performance may be limited by its reliance on the quality of the initial annotated pool and the accuracy of the density estimation and confidence modeling components."
    },
    {
      "id": "2601.14724",
      "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
      "link": "https://huggingface.co/papers/2601.14724",
      "pdf_link": "https://arxiv.org/pdf/2601.14724.pdf",
      "authors": "Haowei Zhang, Shudong Yang, Jinlan Fu, See-Kiong Ng, Xipeng Qiu",
      "institution": "",
      "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improved streaming",
      "citations": 0,
      "upvotes": 51,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.12346",
      "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents",
      "link": "https://huggingface.co/papers/2601.12346",
      "pdf_link": "https://arxiv.org/pdf/2601.12346.pdf",
      "authors": "Peizhou Huang, Zixuan Zhong, Zhongwei Wan, Donghao Zhou, Samiul Alam",
      "institution": "",
      "abstract": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-18",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "New benchmark introduced",
      "citations": 0,
      "upvotes": 42,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.14171",
      "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
      "link": "https://huggingface.co/papers/2601.14171",
      "pdf_link": "https://arxiv.org/pdf/2601.14171.pdf",
      "authors": "Qianli Ma, Chang Guo, Zhiheng Tian, Siyu Wang, Jipeng Xiao",
      "institution": "",
      "abstract": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Rebuttal assistance framework",
      "citations": 0,
      "upvotes": 41,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16208",
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "link": "https://huggingface.co/papers/2601.16208",
      "pdf_link": "https://arxiv.org/pdf/2601.16208.pdf",
      "authors": "Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma",
      "institution": "",
      "abstract": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves T2I",
      "citations": 0,
      "upvotes": 31,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15892",
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "link": "https://huggingface.co/papers/2601.15892",
      "pdf_link": "https://arxiv.org/pdf/2601.15892.pdf",
      "authors": "Chenghao Fan, Wen Heng, Bo Li, Sichen Liu, Yuxuan Song",
      "institution": "",
      "abstract": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves code DLLMs",
      "citations": 0,
      "upvotes": 31,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.14490",
      "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents",
      "link": "https://huggingface.co/papers/2601.14490",
      "pdf_link": "https://arxiv.org/pdf/2601.14490.pdf",
      "authors": "Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew",
      "institution": "",
      "abstract": "GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Grounded OCR front-end",
      "citations": 0,
      "upvotes": 25,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.14750",
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "link": "https://huggingface.co/papers/2601.14750",
      "pdf_link": "https://arxiv.org/pdf/2601.14750.pdf",
      "authors": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "institution": "",
      "abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Visual Reasoning",
      "citations": 0,
      "upvotes": 14,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15621",
      "title": "Qwen3-TTS Technical Report",
      "link": "https://huggingface.co/papers/2601.15621",
      "pdf_link": "https://arxiv.org/pdf/2601.15621.pdf",
      "authors": "Hangrui Hu, Xinfa Zhu, Ting He, Dake Guo, Bin Zhang",
      "institution": "",
      "abstract": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Advanced TTS model",
      "citations": 0,
      "upvotes": 13,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.13044",
      "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition",
      "link": "https://huggingface.co/papers/2601.13044",
      "pdf_link": "https://arxiv.org/pdf/2601.13044.pdf",
      "authors": "Warit Sirichotedumrong, Adisai Na-Thalang, Potsawee Manakul, Pittawat Taveekitworachai, Sittipong Sripaisarnmongkol",
      "institution": "",
      "abstract": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-19",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves Thai ASR",
      "citations": 0,
      "upvotes": 11,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.14245",
      "title": "XR: Cross-Modal Agents for Composed Image Retrieval",
      "link": "https://huggingface.co/papers/2601.14245",
      "pdf_link": "https://arxiv.org/pdf/2601.14245.pdf",
      "authors": "Zhongyu Yang, Wei Pang, Yingfang Yuan",
      "institution": "",
      "abstract": "Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Novel Retrieval",
      "citations": 0,
      "upvotes": 8,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.14255",
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "link": "https://huggingface.co/papers/2601.14255",
      "pdf_link": "https://arxiv.org/pdf/2601.14255.pdf",
      "authors": "Sangbeom Lim, Seoung Wug Oh, Jiahui Huang, Heeji Yoon, Seungryong Kim",
      "institution": "",
      "abstract": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Better matting",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16093",
      "title": "SAMTok: Representing Any Mask with Two Words",
      "link": "https://huggingface.co/papers/2601.16093",
      "pdf_link": "https://arxiv.org/pdf/2601.16093.pdf",
      "authors": "Yikang Zhou, Tao Zhang, Dengxian Gong, Yuanzheng Wu, Ye Tian",
      "institution": "",
      "abstract": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Simplifies masking",
      "citations": 0,
      "upvotes": 30,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.13572",
      "title": "Behavior Knowledge Merge in Reinforced Agentic Models",
      "link": "https://huggingface.co/papers/2601.13572",
      "pdf_link": "https://arxiv.org/pdf/2601.13572.pdf",
      "authors": "Xiangchi Yuan, Dachuan Shi, Chunhui Zhang, Zheyuan Liu, Shenglong Yao",
      "institution": "",
      "abstract": "Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Model merging technique",
      "citations": 0,
      "upvotes": 18,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16125",
      "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
      "link": "https://huggingface.co/papers/2601.16125",
      "pdf_link": "https://arxiv.org/pdf/2601.16125.pdf",
      "authors": "Tingyu Song, Yanzhao Zhang, Mingxin Li, Zhuoning Guo, Dingkun Long",
      "institution": "",
      "abstract": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "New CIR benchmark",
      "citations": 0,
      "upvotes": 13,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16163",
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "link": "https://huggingface.co/papers/2601.16163",
      "pdf_link": "https://arxiv.org/pdf/2601.16163.pdf",
      "authors": "Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge",
      "institution": "",
      "abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Fine-tunes video models",
      "citations": 0,
      "upvotes": 6,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.14417",
      "title": "Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis",
      "link": "https://huggingface.co/papers/2601.14417",
      "pdf_link": "https://arxiv.org/pdf/2601.14417.pdf",
      "authors": "Thanathai Lertpetchpun, Yoonjeong Lee, Thanapat Trachu, Jihwan Lee, Tiantian Feng",
      "institution": "",
      "abstract": "Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Accent control improvement",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15059",
      "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems",
      "link": "https://huggingface.co/papers/2601.15059",
      "pdf_link": "https://arxiv.org/pdf/2601.15059.pdf",
      "authors": "Oleg Romanchuk, Roman Bondar",
      "institution": "",
      "abstract": "Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.\n  We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.\n  We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.\n  We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.\n  We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Identifies responsibility gap",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16134",
      "title": "LLM Prompt Evaluation for Educational Applications",
      "link": "https://huggingface.co/papers/2601.16134",
      "pdf_link": "https://arxiv.org/pdf/2601.16134.pdf",
      "authors": "Langdon Holmes, Adam Coscia, Scott Crossley, Joon Suh Choi, Wesley Morris",
      "institution": "",
      "abstract": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-22",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Useful evaluation",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15100",
      "title": "Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek",
      "link": "https://huggingface.co/papers/2601.15100",
      "pdf_link": "https://arxiv.org/pdf/2601.15100.pdf",
      "authors": "Yanwei Huang, Arpit Narechania",
      "institution": "",
      "abstract": "Web AI agents such as ChatGPT Agent and GenSpark are increasingly used for routine web-based tasks, yet they still rely on text-based input prompts, lack proactive detection of user intent, and offer no support for interactive data analysis and decision making. We present WebSeek, a mixed-initiative browser extension that enables users to discover and extract information from webpages to then flexibly build, transform, and refine tangible data artifacts-such as tables, lists, and visualizations-all within an interactive canvas. Within this environment, users can perform analysis-including data transformations such as joining tables or creating visualizations-while an in-built AI both proactively offers context-aware guidance and automation, and reactively responds to explicit user requests. An exploratory user study (N=15) with WebSeek as a probe reveals participants' diverse analysis strategies, underscoring their desire for transparency and control during human-AI collaboration.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Web AI Improvement",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.13481",
      "title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement",
      "link": "https://huggingface.co/papers/2601.13481",
      "pdf_link": "https://arxiv.org/pdf/2601.13481.pdf",
      "authors": "Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He",
      "institution": "",
      "abstract": "Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Mental Health Diagnosis",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.13262",
      "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
      "link": "https://huggingface.co/papers/2601.13262",
      "pdf_link": "https://arxiv.org/pdf/2601.13262.pdf",
      "authors": "Eric Onyame, Akash Ghosh, Subhadip Baidya, Sriparna Saha, Xiuying Chen",
      "institution": "",
      "abstract": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-19",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Medical reasoning improvement",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15165",
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "link": "https://huggingface.co/papers/2601.15165",
      "pdf_link": "https://arxiv.org/pdf/2601.15165.pdf",
      "authors": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "institution": "",
      "abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Minor dLLM insight",
      "citations": 0,
      "upvotes": 49,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.14152",
      "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
      "link": "https://huggingface.co/papers/2601.14152",
      "pdf_link": "https://arxiv.org/pdf/2601.14152.pdf",
      "authors": "Hyunjong Ok, Jaeho Lee",
      "institution": "",
      "abstract": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-20",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Language Model Limitation",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.11387",
      "title": "Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking",
      "link": "https://huggingface.co/papers/2601.11387",
      "pdf_link": "https://arxiv.org/pdf/2601.11387.pdf",
      "authors": "Greta Warren, Jingyi Sun, Irina Shklovski, Isabelle Augenstein",
      "institution": "",
      "abstract": "Although much research has focused on AI explanations to support decisions in complex information-seeking tasks such as fact-checking, the role of evidence is surprisingly under-researched. In our study, we systematically varied explanation type, AI prediction certainty, and correctness of AI system advice for non-expert participants, who evaluated the veracity of claims and AI system predictions. Participants were provided the option of easily inspecting the underlying evidence. We found that participants consistently relied on evidence to validate AI claims across all experimental conditions. When participants were presented with natural language explanations, evidence was used less frequently although they relied on it when these explanations seemed insufficient or flawed. Qualitative data suggests that participants attempted to infer evidence source reliability, despite source identities being deliberately omitted. Our results demonstrate that evidence is a key ingredient in how people evaluate the reliability of information presented by an AI system and, in combination with natural language explanations, offers valuable support for decision-making. Further research is urgently needed to understand how evidence ought to be presented and how people engage with it in practice.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-16",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Evaluation study",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.12029",
      "title": "sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation",
      "link": "https://huggingface.co/papers/2601.12029",
      "pdf_link": "https://arxiv.org/pdf/2601.12029.pdf",
      "authors": "Sandy H. S. Herho, Faruq Khadami, Iwan P. Anwar, Dasapta E. Irawan",
      "institution": "",
      "abstract": "The Korteweg-de Vries (KdV) equation serves as a foundational model in nonlinear wave physics, describing the balance between dispersive spreading and nonlinear steepening that gives rise to solitons. This article introduces sangkuriang, an open-source Python library for solving this equation using Fourier pseudo-spectral spatial discretization coupled with adaptive high-order time integration. The implementation leverages just-in-time (JIT) compilation for computational efficiency while maintaining accessibility for instructional purposes. Validation encompasses progressively complex scenarios including isolated soliton propagation, symmetric two-wave configurations, overtaking collisions between waves of differing amplitudes, and three-body interactions. Conservation of the classical invariants is monitored throughout, with deviations remaining small across all test cases. Measured soliton velocities conform closely to theoretical predictions based on the amplitude-velocity relationship characteristic of integrable systems. Complementary diagnostics drawn from information theory and recurrence analysis confirm that computed solutions preserve the regular phase-space structure expected for completely integrable dynamics. The solver outputs data in standard scientific formats compatible with common analysis tools and generates visualizations of spatiotemporal wave evolution. By combining numerical accuracy with practical accessibility on modest computational resources, sangkuriang offers a platform suitable for both classroom demonstrations of nonlinear wave phenomena and exploratory research into soliton dynamics.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-17",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Minor library update",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15440",
      "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
      "link": "https://huggingface.co/papers/2601.15440",
      "pdf_link": "https://arxiv.org/pdf/2601.15440.pdf",
      "authors": "Sandy H. S. Herho, Faiz R. Fajary, Iwan P. Anwar, Faruq Khadami, Nurjanna J. Trilaksono",
      "institution": "",
      "abstract": "We present dla-ideal-solver, a high-performance framework for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining high-level flexibility. We investigate the Laplacian growth instability across varying injection geometries and walker concentrations. Our analysis confirms the robustness of the standard fractal dimension D_f approx 1.71 for dilute regimes, consistent with the Witten-Sander universality class. However, we report a distinct crossover to Eden-like compact growth (D_f approx 1.87) in high-density environments, attributed to the saturation of the screening length. Beyond standard mass-radius scaling, we employ generalized Rényi dimensions and lacunarity metrics to quantify the monofractal character and spatial heterogeneity of the aggregates. This work establishes a reproducible, open-source testbed for exploring phase transitions in non-equilibrium statistical mechanics.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-21",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Minor optimization",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16214v1",
      "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
      "link": "http://arxiv.org/abs/2601.16214v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16214v1",
      "authors": "Wenhang Ge, Guibao Shen, Jiawei Feng, Luozhou Wang, Hao Lu et al.",
      "institution": "",
      "abstract": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16211v1",
      "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition",
      "link": "http://arxiv.org/abs/2601.16211v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16211v1",
      "authors": "Geo Ahn, Inwoong Lee, Taeoh Kim, Minho Shim, Dongyoon Wee et al.",
      "institution": "",
      "abstract": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16210v1",
      "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
      "link": "http://arxiv.org/abs/2601.16210v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16210v1",
      "authors": "Onkar Susladkar, Tushar Prakash, Adheesh Juvekar, Kiet A. Nguyen, Dong-Hwan Jang et al.",
      "institution": "",
      "abstract": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16205v1",
      "title": "Counterfactual Training: Teaching Models Plausible and Actionable Explanations",
      "link": "http://arxiv.org/abs/2601.16205v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16205v1",
      "authors": "Patrick Altmeyer, Aleksander Buszydlik, Arie van Deursen, Cynthia C. S. Liem",
      "institution": "",
      "abstract": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16200v1",
      "title": "Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing",
      "link": "http://arxiv.org/abs/2601.16200v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16200v1",
      "authors": "Song Xia, Meiwen Ding, Chenqi Kong, Wenhan Yang, Xudong Jiang",
      "institution": "",
      "abstract": "Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\\% to about 1\\%.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16195v1",
      "title": "Pushing the limits of unconstrained machine-learned interatomic potentials",
      "link": "http://arxiv.org/abs/2601.16195v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16195v1",
      "authors": "Filippo Bigi, Paolo Pegolo, Arslan Mazitov, Michele Ceriotti",
      "institution": "",
      "abstract": "Machine-learned interatomic potentials (MLIPs) are increasingly used to replace computationally demanding electronic-structure calculations to model matter at the atomic scale. The most commonly used model architectures are constrained to fulfill a number of physical laws exactly, from geometric symmetries to energy conservation. Evidence is mounting that relaxing some of these constraints can be beneficial to the efficiency and (somewhat surprisingly) accuracy of MLIPs, even though care should be taken to avoid qualitative failures associated with the breaking of physical symmetries. Given the recent trend of \\emph{scaling up} models to larger numbers of parameters and training samples, a very important question is how unconstrained MLIPs behave in this limit. Here we investigate this issue, showing that -- when trained on large datasets -- unconstrained models can be superior in accuracy and speed when compared to physically constrained models. We assess these models both in terms of benchmark accuracy and in terms of usability in practical scenarios, focusing on static simulation workflows such as geometry optimization and lattice dynamics. We conclude that accurate unconstrained models can be applied with confidence, especially since simple inference-time modifications can be used to recover observables that are consistent with the relevant physical symmetries.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16194v1",
      "title": "A Rolling-Space Branch-and-Price Algorithm for the Multi-Compartment Vehicle Routing Problem with Multiple Time Windows",
      "link": "http://arxiv.org/abs/2601.16194v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16194v1",
      "authors": "El Mehdi Er Raqabi, Kevin Dalmeijer, Pascal Van Hentenryck",
      "institution": "",
      "abstract": "This paper investigates the multi-compartment vehicle routing problem with multiple time windows (MCVRPMTW), an extension of the classical vehicle routing problem with time windows that considers vehicles equipped with multiple compartments and customers requiring service across several delivery time windows. The problem incorporates three key compartment-related features: (i) compartment flexibility in the number of compartments, (ii) item-to-compartment compatibility, and (iii) item-to-item compatibility. The problem also accommodates practical operational requirements such as driver breaks. To solve the MCVRPMTW, we develop an exact branch-and-price (B&P) algorithm in which the pricing problem is solved using a labeling algorithm. Several acceleration strategies are introduced to limit symmetry during label extensions, improve the stability of dual solutions in column generation, and enhance the branching process. To handle large-scale instances, we propose a rolling-space B&P algorithm that integrates clustering techniques into the solution framework. Extensive computational experiments on instances inspired by a real-world industrial application demonstrate the effectiveness of the proposed approach and provide useful managerial insights for practical implementation.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16174v1",
      "title": "Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints",
      "link": "http://arxiv.org/abs/2601.16174v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16174v1",
      "authors": "Yiyao Yang",
      "institution": "",
      "abstract": "Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "stat.ML",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16172v1",
      "title": "Structured Hints for Sample-Efficient Lean Theorem Proving",
      "link": "http://arxiv.org/abs/2601.16172v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16172v1",
      "authors": "Zachary Burton",
      "institution": "",
      "abstract": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16158v1",
      "title": "Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems",
      "link": "http://arxiv.org/abs/2601.16158v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16158v1",
      "authors": "Prakash Dhungana, Sayed Ahmad Salehi",
      "institution": "",
      "abstract": "Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\\% accuracy on clean data and maintaining robust performance (exceeding 94\\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.SD",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16155v1",
      "title": "HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval",
      "link": "http://arxiv.org/abs/2601.16155v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16155v1",
      "authors": "Zequn Xie, Xin Liu, Boyun Zhang, Yuxiao Lin, Sihang Cai et al.",
      "institution": "",
      "abstract": "The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from \"blind\" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.IR"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16152v1",
      "title": "Substrate Stability Under Persistent Disagreement: Structural Constraints for Neutral Ontological Substrates",
      "link": "http://arxiv.org/abs/2601.16152v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16152v1",
      "authors": "Denise M. Case",
      "institution": "",
      "abstract": "Modern data systems increasingly operate under conditions of persistent legal, political, and analytic disagreement. In such settings, interoperability cannot rely on shared interpretation, negotiated semantics, or centralized authority. Instead, representations must function as neutral substrates that preserve stable reference across incompatible extensions. This paper investigates the structural constraints imposed on ontological design by this requirement. Building on a neutrality framework that treats interpretive non-commitment and stability under extension as explicit design constraints, we ask what minimal ontological structure is forced if accountability relationships are to remain referable and comparable under disagreement. Minimality here is not mere parsimony: a reduction is admissible only if it does not reintroduce stability-critical distinctions as hidden roles, flags, or contextual predicates. We establish a conditional lower-bound result: any ontology capable of supporting accountability under persistent disagreement must realize at least six distinct identity-and-persistence regimes. We further show that a construction with exactly six such regimes is sufficient to satisfy the stated requirements without embedding causal or normative commitments in the substrate. The result is not a proposal for a universal ontology, but a constraint on what is possible when neutrality and stable reference are treated as non-negotiable design goals.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LO",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16150v1",
      "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
      "link": "http://arxiv.org/abs/2601.16150v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16150v1",
      "authors": "Maximos Kaliakatsos-Papakostas, Dimos Makris, Konstantinos Soiledis, Konstantinos-Theodoros Tsamis, Vassilis Katsouros et al.",
      "institution": "",
      "abstract": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.SD",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16147v1",
      "title": "Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets",
      "link": "http://arxiv.org/abs/2601.16147v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16147v1",
      "authors": "Muhammad Ilham Rizqyawan, Peter Macfarlane, Stathis Hadjidemetriou, Fani Deligianni",
      "institution": "",
      "abstract": "Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16142v1",
      "title": "Computing Fixpoints of Learned Functions: Chaotic Iteration and Simple Stochastic Games",
      "link": "http://arxiv.org/abs/2601.16142v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16142v1",
      "authors": "Paolo Baldan, Sebastian Gurke, Barbara König, Florian Wittbold",
      "institution": "",
      "abstract": "The problem of determining the (least) fixpoint of (higher-dimensional) functions over the non-negative reals frequently occurs when dealing with systems endowed with a quantitative semantics. We focus on the situation in which the functions of interest are not known precisely but can only be approximated. As a first contribution we generalize an iteration scheme called dampened Mann iteration, recently introduced in the literature. The improved scheme relaxes previous constraints on parameter sequences, allowing learning rates to converge to zero or not converge at all. While seemingly minor, this flexibility is essential to enable the implementation of chaotic iterations, where only a subset of components is updated in each step, allowing to tackle higher-dimensional problems. Additionally, by allowing learning rates to converge to zero, we can relax conditions on the convergence speed of function approximations, making the method more adaptable to various scenarios. We also show that dampened Mann iteration applies immediately to compute the expected payoff in various probabilistic models, including simple stochastic games, not covered by previous work.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LO",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16140v1",
      "title": "Learning to Watermark in the Latent Space of Generative Models",
      "link": "http://arxiv.org/abs/2601.16140v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16140v1",
      "authors": "Sylvestre-Alvise Rebuffi, Tuan Tran, Valeriu Lacatusu, Pierre Fernandez, Tomáš Souček et al.",
      "institution": "",
      "abstract": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16139v1",
      "title": "On the Intrinsic Dimensions of Data in Kernel Learning",
      "link": "http://arxiv.org/abs/2601.16139v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16139v1",
      "authors": "Rustem Takhanov",
      "institution": "",
      "abstract": "The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\\to \\int_ΩK(\\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\\left(ε^{-d_ρ}\\log\\frac{1}ε\\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16138v1",
      "title": "Automatic Classification of Arabic Literature into Historical Eras",
      "link": "http://arxiv.org/abs/2601.16138v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16138v1",
      "authors": "Zainab Alhathloul, Irfan Ahmad",
      "institution": "",
      "abstract": "The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16130v1",
      "title": "Replicating Human Motivated Reasoning Studies with LLMs",
      "link": "http://arxiv.org/abs/2601.16130v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16130v1",
      "authors": "Neeley Pate, Adiba Mahbub Proma, Hangfeng He, James N. Druckman, Daniel Molden et al.",
      "institution": "",
      "abstract": "Motivated reasoning -- the idea that individuals processing information may be motivated to reach a certain conclusion, whether it be accurate or predetermined -- has been well-explored as a human phenomenon. However, it is unclear whether base LLMs mimic these motivational changes. Replicating 4 prior political motivated reasoning studies, we find that base LLM behavior does not align with expected human behavior. Furthermore, base LLM behavior across models shares some similarities, such as smaller standard deviations and inaccurate argument strength assessments. We emphasize the importance of these findings for researchers using LLMs to automate tasks such as survey data collection and argument assessment.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.HC",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16127v1",
      "title": "Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging",
      "link": "http://arxiv.org/abs/2601.16127v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16127v1",
      "authors": "Alphaeus Dmonte, Vidhi Gupta, Daniel J Perry, Mark Arehart",
      "institution": "",
      "abstract": "Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16120v1",
      "title": "Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add",
      "link": "http://arxiv.org/abs/2601.16120v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16120v1",
      "authors": "Zhengchi Ma, Anru R. Zhang",
      "institution": "",
      "abstract": "Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?\n  We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry\" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry\" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16118v1",
      "title": "A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware",
      "link": "http://arxiv.org/abs/2601.16118v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16118v1",
      "authors": "Marco Ronzani, Cristina Silvano",
      "institution": "",
      "abstract": "Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AR",
        "cs.NE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16117v1",
      "title": "Distillation-based Layer Dropping (DLD) Effective End-to-end Framework for Dynamic Speech Networks",
      "link": "http://arxiv.org/abs/2601.16117v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16117v1",
      "authors": "Abdul Hannan, Daniele Falavigna, Shah Nawaz, Mubashir Noman, Markus Schedl et al.",
      "institution": "",
      "abstract": "Edge devices operate in constrained and varying resource settings, requiring dynamic architectures that can adapt to limitations of the available resources. To meet such demands, layer dropping ($\\mathcal{LD}$) approach is typically used to transform static models into dynamic ones by skipping parts of the network along with reducing overall computational complexity. However, existing $\\mathcal{LD}$ methods greatly impact the dynamic model's performance for low and high dropping cases, deteriorating the performance-computation trade-off. To this end, we propose a distillation-based layer dropping (DLD) framework that effectively combines the capabilities of knowledge distillation and $\\mathcal{LD}$ in an end-to-end fashion, thereby achieving state-of-the-art performance for dynamic speech networks. Comprehensive experimentation utilizing well-known speech recognition methods, including conformer and WavLM, on three public benchmarks demonstrates the effectiveness of our framework, reducing the word error rate by $9.32\\%$ and $2.25\\%$ for high and no dropping cases with $33.3\\%$ reduction in training time.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.SD",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16113v1",
      "title": "synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier",
      "link": "http://arxiv.org/abs/2601.16113v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16113v1",
      "authors": "Haq Nawaz Malik, Kh Mohmad Shafi, Tanveer Ahmad Reshi",
      "institution": "",
      "abstract": "Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.\n  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.\n  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16112v1",
      "title": "Variable Splitting Binary Tree Models Based on Bayesian Context Tree Models for Time Series Segmentation",
      "link": "http://arxiv.org/abs/2601.16112v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16112v1",
      "authors": "Yuta Nakahara, Shota Saito, Kohei Horinouchi, Koshi Shimada, Naoki Ichijo et al.",
      "institution": "",
      "abstract": "We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16108v1",
      "title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources",
      "link": "http://arxiv.org/abs/2601.16108v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16108v1",
      "authors": "Marzieh Adeli Shamsabad, Hamed Ghodrati",
      "institution": "",
      "abstract": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16107v1",
      "title": "Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets",
      "link": "http://arxiv.org/abs/2601.16107v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16107v1",
      "authors": "Adithya Sineesh, Akshita Kamsali",
      "institution": "",
      "abstract": "Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16098v1",
      "title": "Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification",
      "link": "http://arxiv.org/abs/2601.16098v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16098v1",
      "authors": "Zack Dewis, Yimin Zhu, Zhengsen Xu, Mabel Heffring, Saeid Taleghanidoozdoozan et al.",
      "institution": "",
      "abstract": "Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16097v1",
      "title": "Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating",
      "link": "http://arxiv.org/abs/2601.16097v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16097v1",
      "authors": "Makbule Gulcin Ozsoy",
      "institution": "",
      "abstract": "Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16096v1",
      "title": "Neural Particle Automata: Learning Self-Organizing Particle Dynamics",
      "link": "http://arxiv.org/abs/2601.16096v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16096v1",
      "authors": "Hyunsoo Kim, Ehsan Pajouheshgar, Sabine Süsstrunk, Wenzel Jakob, Jinah Park",
      "institution": "",
      "abstract": "We introduce Neural Particle Automata (NPA), a Lagrangian generalization of Neural Cellular Automata (NCA) from static lattices to dynamic particle systems. Unlike classical Eulerian NCA where cells are pinned to pixels or voxels, NPA model each cell as a particle with a continuous position and internal state, both updated by a shared, learnable neural rule. This particle-based formulation yields clear individuation of cells, allows heterogeneous dynamics, and concentrates computation only on regions where activity is present. At the same time, particle systems pose challenges: neighborhoods are dynamic, and a naive implementation of local interactions scale quadratically with the number of particles. We address these challenges by replacing grid-based neighborhood perception with differentiable Smoothed Particle Hydrodynamics (SPH) operators backed by memory-efficient, CUDA-accelerated kernels, enabling scalable end-to-end training. Across tasks including morphogenesis, point-cloud classification, and particle-based texture synthesis, we show that NPA retain key NCA behaviors such as robustness and self-regeneration, while enabling new behaviors specific to particle systems. Together, these results position NPA as a compact neural model for learning self-organizing particle dynamics.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.NE",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16091v1",
      "title": "Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals",
      "link": "http://arxiv.org/abs/2601.16091v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16091v1",
      "authors": "Saar Cohen",
      "institution": "",
      "abstract": "Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16087v1",
      "title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics",
      "link": "http://arxiv.org/abs/2601.16087v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16087v1",
      "authors": "Sukesh Subaharan",
      "institution": "",
      "abstract": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16083v1",
      "title": "Probably Approximately Correct Maximum A Posteriori Inference",
      "link": "http://arxiv.org/abs/2601.16083v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16083v1",
      "authors": "Matthew Shorvon, Frederik Mallmann-Trenn, David S. Watson",
      "institution": "",
      "abstract": "Computing the conditional mode of a distribution, better known as the $\\mathit{maximum\\ a\\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\\mathit{probably\\ approximately\\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16079v1",
      "title": "Masked Modeling for Human Motion Recovery Under Occlusions",
      "link": "http://arxiv.org/abs/2601.16079v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16079v1",
      "authors": "Zhiyin Qian, Siwei Zhang, Bharat Lal Bhatnagar, Federica Bogo, Siyu Tang",
      "institution": "",
      "abstract": "Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16074v1",
      "title": "Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems",
      "link": "http://arxiv.org/abs/2601.16074v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16074v1",
      "authors": "Annemarie Jutte, Uraz Odyurt",
      "institution": "",
      "abstract": "Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16073v1",
      "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models",
      "link": "http://arxiv.org/abs/2601.16073v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16073v1",
      "authors": "Hanwen Zhang, Qiaojin Shen, Yuxi Liu, Yuesheng Zhu, Guibo Luo",
      "institution": "",
      "abstract": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.DC"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16072v1",
      "title": "CLASP: An online learning algorithm for Convex Losses And Squared Penalties",
      "link": "http://arxiv.org/abs/2601.16072v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16072v1",
      "authors": "Ricardo N. Ferreira, Cláudia Soares, João Xavier",
      "institution": "",
      "abstract": "We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\\left(T^{\\max\\{β,1-β\\}}\\right)$ and cumulative squared penalty $O\\left(T^{1-β}\\right)$ for any $β\\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \\log T )$ and the cumulative squared penalty is also upper bounded by $O( \\log T )$.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16070v1",
      "title": "On damage of interpolation to adversarial robustness in regression",
      "link": "http://arxiv.org/abs/2601.16070v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16070v1",
      "authors": "Jingfu Peng, Yuhong Yang",
      "institution": "",
      "abstract": "Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "stat.ML",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16065v1",
      "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models",
      "link": "http://arxiv.org/abs/2601.16065v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16065v1",
      "authors": "Chenyang Li, Jieyuan Liu, Bin Li, Bo Gao, Yilin Yuan et al.",
      "institution": "",
      "abstract": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.RO"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16064v1",
      "title": "Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation",
      "link": "http://arxiv.org/abs/2601.16064v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16064v1",
      "authors": "Shams Nafisa Ali, Taufiq Hasan",
      "institution": "",
      "abstract": "Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16060v1",
      "title": "ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation",
      "link": "http://arxiv.org/abs/2601.16060v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16060v1",
      "authors": "Yuan Lin, Murong Xu, Marc Hölle, Chinmay Prabhakar, Andreas Maier et al.",
      "institution": "",
      "abstract": "Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16056v1",
      "title": "Designing faster mixed integer linear programming algorithm via learning the optimal path",
      "link": "http://arxiv.org/abs/2601.16056v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16056v1",
      "authors": "Ruizhi Liu, Liming Xu, Xulin Huang, Jingyan Sui, Shizhe Ding et al.",
      "institution": "",
      "abstract": "Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16046v1",
      "title": "DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning",
      "link": "http://arxiv.org/abs/2601.16046v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16046v1",
      "authors": "Junha Lee, Eunha Park, Minsu Cho",
      "institution": "",
      "abstract": "Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.RO",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16045v1",
      "title": "AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress",
      "link": "http://arxiv.org/abs/2601.16045v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16045v1",
      "authors": "Yue Shi, Liangxiu Han, Xin Zhang, Tam Sobeih, Thomas Gaiser et al.",
      "institution": "",
      "abstract": "Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16041v1",
      "title": "Risk reversal for least squares estimators under nested convex constraints",
      "link": "http://arxiv.org/abs/2601.16041v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16041v1",
      "authors": "Omar Al-Ghattas",
      "institution": "",
      "abstract": "In constrained stochastic optimization, one naturally expects that imposing a stricter feasible set does not increase the statistical risk of an estimator defined by projection onto that set. In this paper, we show that this intuition can fail even in canonical settings.\n  We study the Gaussian sequence model, a deliberately austere test best, where for a compact, convex set $Θ\\subset \\mathbb{R}^d$ one observes \\[ Y = θ^\\star + σZ, \\qquad Z \\sim N(0, I_d), \\] and seeks to estimate an unknown parameter $θ^\\star \\in Θ$. The natural estimator is the least squares estimator (LSE), which coincides with the Euclidean projection of $Y$ onto $Θ$. We construct an explicit example exhibiting \\emph{risk reversal}: for sufficiently large noise, there exist nested compact convex sets $Θ_S \\subset Θ_L$ and a parameter $θ^\\star \\in Θ_S$ such that the LSE constrained to $Θ_S$ has strictly larger risk than the LSE constrained to $Θ_L$. We further show that this phenomenon can persist at the level of worst-case risk, with the supremum risk over the smaller constraint set exceeding that over the larger one.\n  We clarify this behavior by contrasting noise regimes. In the vanishing-noise limit, the risk admits a first-order expansion governed by the statistical dimension of the tangent cone at $θ^\\star$, and tighter constraints uniformly reduce risk. In contrast, in the diverging-noise regime, the risk is determined by global geometric interactions between the constraint set and random noise directions. Here, the embedding of $Θ_S$ within $Θ_L$ can reverse the risk ordering.\n  These results reveal a previously unrecognized failure mode of projection-based estimators: in sufficiently noisy settings, tightening a constraint can paradoxically degrade statistical performance.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16038v1",
      "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval",
      "link": "http://arxiv.org/abs/2601.16038v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16038v1",
      "authors": "Olga Bunkova, Lorenzo Di Fruscia, Sophia Rupprecht, Artur M. Schweidtmann, Marcel J. T. Reinders et al.",
      "institution": "",
      "abstract": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16034v1",
      "title": "Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction",
      "link": "http://arxiv.org/abs/2601.16034v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16034v1",
      "authors": "Tony Cristofano",
      "institution": "",
      "abstract": "Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16032v1",
      "title": "Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10",
      "link": "http://arxiv.org/abs/2601.16032v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16032v1",
      "authors": "Yifan Zhu, Yekai Pan, Chen Ding",
      "institution": "",
      "abstract": "High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\\% or greater reduction in L2 misses and up to 60\\% increase in throughput on GB10.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.PF",
        "cs.AI",
        "cs.LG",
        "cs.OS"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16028v1",
      "title": "Data-Driven Conditional Flexibility Index",
      "link": "http://arxiv.org/abs/2601.16028v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16028v1",
      "authors": "Moritz Wedemeyer, Eike Cramer, Alexander Mitsos, Manuel Dahmen",
      "institution": "MIT",
      "abstract": "With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16027v1",
      "title": "Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment",
      "link": "http://arxiv.org/abs/2601.16027v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16027v1",
      "authors": "Yiran Qiao, Xiang Ao, Jing Chen, Yang Liu, Qiwei Zhong et al.",
      "institution": "",
      "abstract": "The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16024v1",
      "title": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry",
      "link": "http://arxiv.org/abs/2601.16024v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16024v1",
      "authors": "Rongze Ma, Mengkang Lu, Zhenyu Xiang, Yongsheng Pan, Yicheng Wu et al.",
      "institution": "",
      "abstract": "Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16020v1",
      "title": "Keyframe-Based Feed-Forward Visual Odometry",
      "link": "http://arxiv.org/abs/2601.16020v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16020v1",
      "authors": "Weichen Dai, Wenhan Su, Da Kong, Yuhang Ming, Wanzeng Kong",
      "institution": "",
      "abstract": "The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.RO"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16018v1",
      "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
      "link": "http://arxiv.org/abs/2601.16018v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16018v1",
      "authors": "Özgür Uğur, Mahmut Göksu, Mahmut Çimen, Musa Yılmaz, Esra Şavirdi et al.",
      "institution": "",
      "abstract": "This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16011v1",
      "title": "THOR: A Versatile Foundation Model for Earth Observation Climate and Society Applications",
      "link": "http://arxiv.org/abs/2601.16011v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16011v1",
      "authors": "Theodor Forgaard, Jarle H. Reksten, Anders U. Waldeland, Valerio Marsocci, Nicolas Longépé et al.",
      "institution": "",
      "abstract": "Current Earth observation foundation models are architecturally rigid, struggle with heterogeneous sensors and are constrained to fixed patch sizes. This limits their deployment in real-world scenarios requiring flexible computeaccuracy trade-offs. We propose THOR, a \"computeadaptive\" foundation model that solves both input heterogeneity and deployment rigidity. THOR is the first architecture to unify data from Copernicus Sentinel-1, -2, and -3 (OLCI & SLSTR) satellites, processing their native 10 m to 1000 m resolutions in a single model. We pre-train THOR with a novel randomized patch and input image size strategy. This allows a single set of pre-trained weights to be deployed at inference with any patch size, enabling a dynamic trade-off between computational cost and feature resolution without retraining. We pre-train THOR on THOR Pretrain, a new, large-scale multi-sensor dataset and demonstrate state-of-the-art performance on downstream benchmarks, particularly in data-limited regimes like the PANGAEA 10% split, validating that THOR's flexible feature generation excels for diverse climate and society applications.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.16007v1",
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "link": "http://arxiv.org/abs/2601.16007v1",
      "pdf_link": "https://arxiv.org/pdf/2601.16007v1",
      "authors": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi et al.",
      "institution": "",
      "abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15995v1",
      "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour",
      "link": "http://arxiv.org/abs/2601.15995v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15995v1",
      "authors": "Liang Wang, Kanzhong Yao, Yang Liu, Weikai Qin, Jun Wu et al.",
      "institution": "",
      "abstract": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15984v1",
      "title": "Partially Lazy Gradient Descent for Smoothed Online Learning",
      "link": "http://arxiv.org/abs/2601.15984v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15984v1",
      "authors": "Naram Mhaisen, George Iosifidis",
      "institution": "",
      "abstract": "We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\\mathcal{O}(\\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15977v1",
      "title": "Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data",
      "link": "http://arxiv.org/abs/2601.15977v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15977v1",
      "authors": "Binbin Lin, Lei Zou, Hao Tian, Heng Cai, Yifan Yang et al.",
      "institution": "",
      "abstract": "Healthcare visitation patterns are influenced by a complex interplay of hospital attributes, population socioeconomics, and spatial factors. However, existing research often adopts a fragmented approach, examining these determinants in isolation. This study addresses this gap by integrating hospital capacities, occupancy rates, reputation, and popularity with population SES and spatial mobility patterns to predict visitation flows and analyze influencing factors. Utilizing four years of SafeGraph mobility data and user experience data from Google Maps Reviews, five flow prediction models, Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and applied to simulate visitation flows in Houston, Texas, U.S. The Shapley additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP) method were employed to examine the combined impacts of different factors on visitation patterns. The findings reveal that Deep Gravity outperformed other models. Hospital capacities, ICU occupancy rates, ratings, and popularity significantly influence visitation patterns, with their effects varying across different travel distances. Short-distance visits are primarily driven by convenience, whereas long-distance visits are influenced by hospital ratings. White-majority areas exhibited lower sensitivity to hospital ratings for short-distance visits, while Asian populations and those with higher education levels prioritized hospital rating in their visitation decisions. SES further influence these patterns, as areas with higher proportions of Hispanic, Black, under-18, and over-65 populations tend to have more frequent hospital visits, potentially reflecting greater healthcare needs or limited access to alternative medical services.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.SI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15968v1",
      "title": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models",
      "link": "http://arxiv.org/abs/2601.15968v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15968v1",
      "authors": "Xin Xie, Jiaxian Guo, Dong Gong",
      "institution": "",
      "abstract": "Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15953v1",
      "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
      "link": "http://arxiv.org/abs/2601.15953v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15953v1",
      "authors": "Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li et al.",
      "institution": "",
      "abstract": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15951v1",
      "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis",
      "link": "http://arxiv.org/abs/2601.15951v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15951v1",
      "authors": "Sheng Miao, Sijin Li, Pan Wang, Dongfeng Bai, Bingbing Liu et al.",
      "institution": "",
      "abstract": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15949v1",
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "link": "http://arxiv.org/abs/2601.15949v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15949v1",
      "authors": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao et al.",
      "institution": "",
      "abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15931v1",
      "title": "ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search",
      "link": "http://arxiv.org/abs/2601.15931v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15931v1",
      "authors": "Xiangyu Wang, Zhixin Lv, Yongjiao Sun, Anrui Han, Ye Yuan et al.",
      "institution": "",
      "abstract": "Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15930v1",
      "title": "MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging",
      "link": "http://arxiv.org/abs/2601.15930v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15930v1",
      "authors": "Tianjun Wei, Enneng Yang, Yingpeng Du, Huizhong Guo, Jie Zhang et al.",
      "institution": "",
      "abstract": "Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.IR",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15929v1",
      "title": "NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation",
      "link": "http://arxiv.org/abs/2601.15929v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15929v1",
      "authors": "Liuyun Jiang, Yizhuo Lu, Yanchao Zhang, Jiazheng Liu, Hua Han",
      "institution": "",
      "abstract": "Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15924v1",
      "title": "Class Confidence Aware Reweighting for Long Tailed Learning",
      "link": "http://arxiv.org/abs/2601.15924v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15924v1",
      "authors": "Brainard Philemon Jagati, Jitendra Tembhurne, Harsh Goud, Rudra Pratap Singh, Chandrashekhar Meshram",
      "institution": "",
      "abstract": "Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15918v1",
      "title": "A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery",
      "link": "http://arxiv.org/abs/2601.15918v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15918v1",
      "authors": "Valery Fischer, Alan Magdaleno, Anna-Katharina Calek, Nicola Cavalcanti, Nathan Hoffman et al.",
      "institution": "",
      "abstract": "Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.\n  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.\n  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.\n  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15915v1",
      "title": "Progressive Power Homotopy for Non-convex Optimization",
      "link": "http://arxiv.org/abs/2601.15915v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15915v1",
      "authors": "Chen Xu",
      "institution": "",
      "abstract": "We propose a novel first-order method for non-convex optimization of the form $\\max_{\\bm{w}\\in\\mathbb{R}^d}\\mathbb{E}_{\\bm{x}\\sim\\mathcal{D}}[f_{\\bm{w}}(\\bm{x})]$, termed Progressive Power Homotopy (Prog-PowerHP). The method applies stochastic gradient ascent to a surrogate objective obtained by first performing a power transformation and then Gaussian smoothing, $F_{N,σ}(\\bmμ):=\\mathbb{E}_{\\bm{w}\\sim\\mathcal{N}(\\bmμ,σ^2I_d),\\bm{x}\\sim\\mathcal{D}}[e^{Nf_w(\\bm{x})}]$, while progressively increasing the power parameter $N$ and decreasing the smoothing scale $σ$ along the optimization trajectory. We prove that, under mild regularity conditions, Prog-PowerHP converges to a small neighborhood of the global optimum with an iteration complexity scaling nearly as $O(d^2\\varepsilon^{-2})$. Empirically, Prog-PowerHP demonstrates clear advantages in phase retrieval when the samples-to-dimension ratio approaches the information-theoretic limit, and in training two-layer neural networks in under-parameterized regimes. These results suggest that Prog-PowerHP is particularly effective for navigating cluttered non-convex landscapes where standard first-order methods struggle.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15914v1",
      "title": "The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars",
      "link": "http://arxiv.org/abs/2601.15914v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15914v1",
      "authors": "Yarin Benyamin",
      "institution": "",
      "abstract": "In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a \"Latency Wall\" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.HC"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15912v1",
      "title": "TeNet: Text-to-Network for Compact Policy Synthesis",
      "link": "http://arxiv.org/abs/2601.15912v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15912v1",
      "authors": "Ariyan Bighashdel, Kevin Sebastian Luck",
      "institution": "",
      "abstract": "Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.RO",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15909v1",
      "title": "Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech",
      "link": "http://arxiv.org/abs/2601.15909v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15909v1",
      "authors": "Soufiane Jhilal, Stéphanie Martin, Anne-Lise Giraud",
      "institution": "",
      "abstract": "Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15906v1",
      "title": "Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models",
      "link": "http://arxiv.org/abs/2601.15906v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15906v1",
      "authors": "Zhen Zhang, Runhao Zeng, Sicheng Zhao, Xiping Hu",
      "institution": "",
      "abstract": "Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\\texttt{gate\\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \\texttt{gate\\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\\% of the parameters tuned by AffectGPT, our approach achieves 96.6\\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \\texttt{gate\\_proj} as a central architectural locus of affective modeling.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15897v1",
      "title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling",
      "link": "http://arxiv.org/abs/2601.15897v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15897v1",
      "authors": "Zhaoqi Su, Shihai Chen, Xinyan Lin, Liqin Huang, Zhipeng Su et al.",
      "institution": "",
      "abstract": "Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15894v1",
      "title": "Iterative Amortized Hierarchical VAE",
      "link": "http://arxiv.org/abs/2601.15894v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15894v1",
      "authors": "Simon W. Penninga, Ruud J. G. van Sloun",
      "institution": "",
      "abstract": "In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15891v1",
      "title": "RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture",
      "link": "http://arxiv.org/abs/2601.15891v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15891v1",
      "authors": "Anas Anwarul Haq Khan, Mariam Husain, Kshitij Jadhav",
      "institution": "",
      "abstract": "Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15888v1",
      "title": "Understanding the Transfer Limits of Vision Foundation Models",
      "link": "http://arxiv.org/abs/2601.15888v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15888v1",
      "authors": "Shiqi Huang, Yipei Wang, Natasha Thorley, Alexander Ng, Shaheer Saeed et al.",
      "institution": "",
      "abstract": "Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15884v1",
      "title": "PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis",
      "link": "http://arxiv.org/abs/2601.15884v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15884v1",
      "authors": "Yifan Chen, Fei Yin, Hao Chen, Jia Wu, Chao Li",
      "institution": "",
      "abstract": "Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15879v1",
      "title": "Evaluating and Achieving Controllable Code Completion in Code LLM",
      "link": "http://arxiv.org/abs/2601.15879v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15879v1",
      "authors": "Jiajun Zhang, Zeyu Cui, Lei Zhang, Jian Yang, Jiaxi Yang et al.",
      "institution": "",
      "abstract": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.SE",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15874v1",
      "title": "SoK: Challenges in Tabular Membership Inference Attacks",
      "link": "http://arxiv.org/abs/2601.15874v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15874v1",
      "authors": "Cristina Pêra, Tânia Carvalho, Maxime Cordy, Luís Antunes",
      "institution": "",
      "abstract": "Membership Inference Attacks (MIAs) are currently a dominant approach for evaluating privacy in machine learning applications. Despite their significance in identifying records belonging to the training dataset, several concerns remain unexplored, particularly with regard to tabular data. In this paper, first, we provide an extensive review and analysis of MIAs considering two main learning paradigms: centralized and federated learning. We extend and refine the taxonomy for both. Second, we demonstrate the efficacy of MIAs in tabular data using several attack strategies, also including defenses. Furthermore, in a federated learning scenario, we consider the threat posed by an outsider adversary, which is often neglected. Third, we demonstrate the high vulnerability of single-outs (records with a unique signature) to MIAs. Lastly, we explore how MIAs transfer across model architectures. Our results point towards a general poor performance of these attacks in tabular data which contrasts with previous state-of-the-art. Notably, even attacks with limited attack performance can still successfully expose a large portion of single-outs. Moreover, our findings suggest that using different surrogate models makes MIAs more effective.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15872v1",
      "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
      "link": "http://arxiv.org/abs/2601.15872v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15872v1",
      "authors": "Jaekwon Im, Natalia Polouliakh, Taketo Akama",
      "institution": "",
      "abstract": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15871v1",
      "title": "Why Inference in Large Models Becomes Decomposable After Training",
      "link": "http://arxiv.org/abs/2601.15871v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15871v1",
      "authors": "Jidong Jin",
      "institution": "",
      "abstract": "Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15869v1",
      "title": "Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers",
      "link": "http://arxiv.org/abs/2601.15869v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15869v1",
      "authors": "Francisco Portillo López",
      "institution": "",
      "abstract": "This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15867v1",
      "title": "Out-of-Distribution Detection Based on Total Variation Estimation",
      "link": "http://arxiv.org/abs/2601.15867v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15867v1",
      "authors": "Dabiao Ma, Zhiba Su, Jian Yang, Haojun Fei",
      "institution": "",
      "abstract": "This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15865v1",
      "title": "A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies",
      "link": "http://arxiv.org/abs/2601.15865v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15865v1",
      "authors": "Jingsong Xia, Siqi Wang",
      "institution": "",
      "abstract": "Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15859v1",
      "title": "Uncertainty-guided Generation of Dark-field Radiographs",
      "link": "http://arxiv.org/abs/2601.15859v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15859v1",
      "authors": "Lina Felsner, Henriette Bast, Tina Dorosti, Florian Schaff, Franz Pfeiffer et al.",
      "institution": "",
      "abstract": "X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15846v1",
      "title": "Determinants of Training Corpus Size for Clinical Text Classification",
      "link": "http://arxiv.org/abs/2601.15846v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15846v1",
      "authors": "Jaya Chaturvedi, Saniya Deshpande, Chenkai Ma, Robert Cobb, Angus Roberts et al.",
      "institution": "",
      "abstract": "Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.\n  Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.\n  Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15838v1",
      "title": "TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing",
      "link": "http://arxiv.org/abs/2601.15838v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15838v1",
      "authors": "Toan Gian, Dung T. Tran, Viet Quoc Pham, Francesco Restuccia, Van-Dinh Nguyen",
      "institution": "",
      "abstract": "With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15830v1",
      "title": "An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics",
      "link": "http://arxiv.org/abs/2601.15830v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15830v1",
      "authors": "Abdul Hasib, A. S. M. Ahsanul Sarkar Akib",
      "institution": "",
      "abstract": "The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \\$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15829v1",
      "title": "Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion",
      "link": "http://arxiv.org/abs/2601.15829v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15829v1",
      "authors": "Yonghao Xu, Pedram Ghamisi, Qihao Weng",
      "institution": "",
      "abstract": "Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15828v1",
      "title": "Can professional translators identify machine-generated text?",
      "link": "http://arxiv.org/abs/2601.15828v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15828v1",
      "authors": "Michael Farrell",
      "institution": "",
      "abstract": "This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15824v1",
      "title": "Introducing the Generative Application Firewall (GAF)",
      "link": "http://arxiv.org/abs/2601.15824v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15824v1",
      "authors": "Joan Vendrell Farreny, Martí Jordà Roca, Miquel Cornudella Gaya, Rodrigo Fernández Baón, Víctor García Martínez et al.",
      "institution": "",
      "abstract": "This paper introduces the Generative Application Firewall (GAF), a new architectural layer for securing LLM applications. Existing defenses -- prompt filters, guardrails, and data-masking -- remain fragmented; GAF unifies them into a single enforcement point, much like a WAF coordinates defenses for web traffic, while also covering autonomous agents and their tool interactions.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CR",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15820v1",
      "title": "ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection",
      "link": "http://arxiv.org/abs/2601.15820v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15820v1",
      "authors": "Guoxuan Ding, Yuqing Li, Ziyan Zhou, Zheng Lin, Daren Zha et al.",
      "institution": "",
      "abstract": "The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15816v1",
      "title": "Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents",
      "link": "http://arxiv.org/abs/2601.15816v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15816v1",
      "authors": "Shiqi Wei, Qiqing Wang, Kaidi Yang",
      "institution": "",
      "abstract": "Adaptive traffic signal control (TSC) has demonstrated strong effectiveness in managing dynamic traffic flows. However, conventional methods often struggle when unforeseen traffic incidents occur (e.g., accidents and road maintenance), which typically require labor-intensive and inefficient manual interventions by traffic police officers. Large Language Models (LLMs) appear to be a promising solution thanks to their remarkable reasoning and generalization capabilities. Nevertheless, existing works often propose to replace existing TSC systems with LLM-based systems, which can be (i) unreliable due to the inherent hallucinations of LLMs and (ii) costly due to the need for system replacement. To address the issues of existing works, we propose a hierarchical framework that augments existing TSC systems with LLMs, whereby a virtual traffic police agent at the upper level dynamically fine-tunes selected parameters of signal controllers at the lower level in response to real-time traffic incidents. To enhance domain-specific reliability in response to unforeseen traffic incidents, we devise a self-refined traffic language retrieval system (TLRS), whereby retrieval-augmented generation is employed to draw knowledge from a tailored traffic language database that encompasses traffic conditions and controller operation principles. Moreover, we devise an LLM-based verifier to update the TLRS continuously over the reasoning process. Our results show that LLMs can serve as trustworthy virtual traffic police officers that can adapt conventional TSC methods to unforeseen traffic incidents with significantly improved operational efficiency and reliability.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15813v1",
      "title": "Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data",
      "link": "http://arxiv.org/abs/2601.15813v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15813v1",
      "authors": "Clare Chemery, Hendrik Edelhoff, Ludwig Bothmann",
      "institution": "",
      "abstract": "We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15812v1",
      "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models",
      "link": "http://arxiv.org/abs/2601.15812v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15812v1",
      "authors": "Shir Ashury-Tahan, Yifan Mai, Elron Bandel, Michal Shmueli-Scheuer, Leshem Choshen",
      "institution": "",
      "abstract": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15810v1",
      "title": "A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks",
      "link": "http://arxiv.org/abs/2601.15810v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15810v1",
      "authors": "Mustafa Yurdakul, Enes Ayan, Fahrettin Horasan, Sakir Tasdemir",
      "institution": "",
      "abstract": "A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15809v1",
      "title": "SteerEval: Inference-time Interventions Strengthen Multilingual Generalization in Neural Summarization Metrics",
      "link": "http://arxiv.org/abs/2601.15809v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15809v1",
      "authors": "Silvia Casola, Ryan Soh-Eun Shim, Felicia Körner, Yuchen Mao, Barbara Plank",
      "institution": "",
      "abstract": "An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15808v1",
      "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
      "link": "http://arxiv.org/abs/2601.15808v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15808v1",
      "authors": "Yuxuan Wan, Tianqing Fang, Zaitang Li, Yintong Huo, Wenxuan Wang et al.",
      "institution": "",
      "abstract": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15807v1",
      "title": "Algebraic Statistics in OSCAR",
      "link": "http://arxiv.org/abs/2601.15807v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15807v1",
      "authors": "Tobias Boege, Antony Della Vecchia, Marina Garrote-López, Benjamin Hollering",
      "institution": "",
      "abstract": "We introduce the AlgebraicStatistics section of the OSCAR computer algebra system. We give an overview of its extensible design and highlight its features including serialization of data types for sharing results and creating databases, and state-of-the-art implicitization algorithms.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "stat.CO",
        "cs.NE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15802v1",
      "title": "A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation",
      "link": "http://arxiv.org/abs/2601.15802v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15802v1",
      "authors": "Alexandre Albore, Humbert Fiorino, Damien Pellier",
      "institution": "",
      "abstract": "Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.RO",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15801v1",
      "title": "Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models",
      "link": "http://arxiv.org/abs/2601.15801v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15801v1",
      "authors": "Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu et al.",
      "institution": "",
      "abstract": "While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \\textbf{G}lobal \\textbf{O}ptimization for \\textbf{S}afety \\textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15798v1",
      "title": "VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management",
      "link": "http://arxiv.org/abs/2601.15798v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15798v1",
      "authors": "Zhikai Xue, Tianqianjin Lin, Pengwei Yan, Ruichun Wang, Yuxin Liu et al.",
      "institution": "",
      "abstract": "Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15797v1",
      "title": "Creativity in the Age of AI: Rethinking the Role of Intentional Agency",
      "link": "http://arxiv.org/abs/2601.15797v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15797v1",
      "authors": "James S. Pearson, Matthew J. Dennis, Marc Cheong",
      "institution": "",
      "abstract": "Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15793v1",
      "title": "HumanLLM: Towards Personalized Understanding and Simulation of Human Nature",
      "link": "http://arxiv.org/abs/2601.15793v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15793v1",
      "authors": "Yuxuan Lei, Tianfu Wang, Jianxun Lian, Zhengyu Hu, Defu Lian et al.",
      "institution": "",
      "abstract": "Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15780v1",
      "title": "Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video",
      "link": "http://arxiv.org/abs/2601.15780v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15780v1",
      "authors": "Pascal Benschop, Justin Dauwels, Jan van Gemert",
      "institution": "",
      "abstract": "Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15779v1",
      "title": "Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation",
      "link": "http://arxiv.org/abs/2601.15779v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15779v1",
      "authors": "Liuyun Jiang, Yanchao Zhang, Jinyue Guo, Yizhuo Lu, Ruining Zhou et al.",
      "institution": "",
      "abstract": "Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15778v1",
      "title": "Agentic Confidence Calibration",
      "link": "http://arxiv.org/abs/2601.15778v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15778v1",
      "authors": "Jiaxin Zhang, Caiming Xiong, Chien-Sheng Wu",
      "institution": "",
      "abstract": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15773v1",
      "title": "Next Generation Active Learning: Mixture of LLMs in the Loop",
      "link": "http://arxiv.org/abs/2601.15773v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15773v1",
      "authors": "Yuanyuan Qi, Xiaohao Yang, Jueqing Lu, Guoxiang Guo, Joanne Enticott et al.",
      "institution": "",
      "abstract": "With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15772v1",
      "title": "LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting",
      "link": "http://arxiv.org/abs/2601.15772v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15772v1",
      "authors": "Yuhan Chen, Wenxuan Yu, Guofa Li, Yijun Xu, Ying Fang et al.",
      "institution": "",
      "abstract": "2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15771v1",
      "title": "Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning",
      "link": "http://arxiv.org/abs/2601.15771v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15771v1",
      "authors": "Dong Xu, Jiantao Wu, Qihua Pan, Sisi Yuan, Zexuan Zhu et al.",
      "institution": "",
      "abstract": "Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15766v1",
      "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps",
      "link": "http://arxiv.org/abs/2601.15766v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15766v1",
      "authors": "Yuhan Chen, Ying Fang, Guofa Li, Wenxuan Yu, Yicui Shi et al.",
      "institution": "",
      "abstract": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15761v1",
      "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
      "link": "http://arxiv.org/abs/2601.15761v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15761v1",
      "authors": "Xiefeng Wu, Mingyu Hu, Shu Zhang",
      "institution": "",
      "abstract": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15759v1",
      "title": "Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)",
      "link": "http://arxiv.org/abs/2601.15759v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15759v1",
      "authors": "Qi Zeng, Weide Liu, Bo Li, Ryne Didier, P. Ellen Grant et al.",
      "institution": "",
      "abstract": "This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15757v1",
      "title": "White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification",
      "link": "http://arxiv.org/abs/2601.15757v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15757v1",
      "authors": "Yimin Zhu, Lincoln Linlin Xu, Zhengsen Xu, Zack Dewis, Mabel Heffring et al.",
      "institution": "",
      "abstract": "In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15755v1",
      "title": "Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs",
      "link": "http://arxiv.org/abs/2601.15755v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15755v1",
      "authors": "Tristan Williams, Franziska Weeber, Sebastian Padó, Alan Akbik",
      "institution": "",
      "abstract": "Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15754v1",
      "title": "CAFE-GB: Scalable and Stable Feature Selection for Malware Detection via Chunk-wise Aggregated Gradient Boosting",
      "link": "http://arxiv.org/abs/2601.15754v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15754v1",
      "authors": "Ajvad Haneef K, Karan Kuwar Singh, Madhu Kumar S D",
      "institution": "",
      "abstract": "High-dimensional malware datasets often exhibit feature redundancy, instability, and scalability limitations, which hinder the effectiveness and interpretability of machine learning-based malware detection systems. Although feature selection is commonly employed to mitigate these issues, many existing approaches lack robustness when applied to large-scale and heterogeneous malware data. To address this gap, this paper proposes CAFE-GB (Chunk-wise Aggregated Feature Estimation using Gradient Boosting), a scalable feature selection framework designed to produce stable and globally consistent feature rankings for high-dimensional malware detection. CAFE-GB partitions training data into overlapping chunks, estimates local feature importance using gradient boosting models, and aggregates these estimates to derive a robust global ranking. Feature budget selection is performed separately through a systematic k-selection and stability analysis to balance detection performance and robustness. The proposed framework is evaluated on two large-scale malware datasets: BODMAS and CIC-AndMal2020, representing large and diverse malware feature spaces. Experimental results show that classifiers trained on CAFE-GB -selected features achieve performance parity with full-feature baselines across multiple metrics, including Accuracy, F1-score, MCC, ROC-AUC, and PR-AUC, while reducing feature dimensionality by more than 95\\%. Paired Wilcoxon signed-rank tests confirm that this reduction does not introduce statistically significant performance degradation. Additional analyses demonstrate low inter-feature redundancy and improved interpretability through SHAP-based explanations. Runtime and memory profiling further indicate reduced downstream classification overhead. Overall, CAFE-GB provides a stable, interpretable, and scalable feature selection strategy for large-scale malware detection.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15751v1",
      "title": "Tabular Incremental Inference",
      "link": "http://arxiv.org/abs/2601.15751v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15751v1",
      "authors": "Xinda Chen, Xing Zhen, Hanyu Zhang, Weimin Tan, Bo Yan",
      "institution": "",
      "abstract": "Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15745v1",
      "title": "Hallucination Mitigating for Medical Report Generation",
      "link": "http://arxiv.org/abs/2601.15745v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15745v1",
      "authors": "Ruoqing Zhao, Runze Xia, Piji Li",
      "institution": "",
      "abstract": "In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \\textbf{K}nowledge-\\textbf{E}nhanced with Fine-Grained \\textbf{R}einforced Rewards \\textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15739v1",
      "title": "Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework",
      "link": "http://arxiv.org/abs/2601.15739v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15739v1",
      "authors": "Xinjue Hu, Chi Wang, Boyu Wang, Xiang Zhang, Zhenshan Tan et al.",
      "institution": "",
      "abstract": "Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15738v1",
      "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling",
      "link": "http://arxiv.org/abs/2601.15738v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15738v1",
      "authors": "Junhao Qiu, Haoyang Zhuang, Fei Liu, Jianjun Liu, Qingfu Zhang",
      "institution": "",
      "abstract": "Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.NE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15737v1",
      "title": "PhysProver: Advancing Automatic Theorem Proving for Physics",
      "link": "http://arxiv.org/abs/2601.15737v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15737v1",
      "authors": "Hanning Zhang, Ruida Wang, Rui Pan, Wenyuan Wang, Bingxu Meng et al.",
      "institution": "",
      "abstract": "The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\\sim$5K training samples, PhysProver achieves an overall 2.4\\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15734v1",
      "title": "Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation",
      "link": "http://arxiv.org/abs/2601.15734v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15734v1",
      "authors": "Shadi Alijani, Fereshteh Aghaee Meibodi, Homayoun Najjaran",
      "institution": "",
      "abstract": "The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15731v1",
      "title": "FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging",
      "link": "http://arxiv.org/abs/2601.15731v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15731v1",
      "authors": "Linyong Zou, Liang Zhang, Xiongfei Wang, Jia-Hong Gao, Yi Sun et al.",
      "institution": "",
      "abstract": "An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15729v1",
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "link": "http://arxiv.org/abs/2601.15729v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15729v1",
      "authors": "Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma",
      "institution": "",
      "abstract": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.RO",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15728v1",
      "title": "Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity",
      "link": "http://arxiv.org/abs/2601.15728v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15728v1",
      "authors": "Hangle Hu, Chenyu Hou, Bin Cao, Ruizhe Li",
      "institution": "",
      "abstract": "While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.SE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15724v1",
      "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
      "link": "http://arxiv.org/abs/2601.15724v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15724v1",
      "authors": "Chenglin Li, Qianglong Chen, Feng Han, Yikun Wang, Xingxi Yin et al.",
      "institution": "",
      "abstract": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15722v1",
      "title": "Communication-efficient Federated Graph Classification via Generative Diffusion Modeling",
      "link": "http://arxiv.org/abs/2601.15722v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15722v1",
      "authors": "Xiuling Wang, Xin Huang, Haibo Hu, Jianliang Xu",
      "institution": "",
      "abstract": "Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15721v1",
      "title": "CoNRec: Context-Discerning Negative Recommendation with LLMs",
      "link": "http://arxiv.org/abs/2601.15721v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15721v1",
      "authors": "Xinda Chen, Jiawei Wu, Yishuang Liu, Jialin Zhu, Shuwen Xiao et al.",
      "institution": "",
      "abstract": "Understanding what users like is relatively straightforward; understanding what users dislike, however, remains a challenging and underexplored problem. Research into users' negative preferences has gained increasing importance in modern recommendation systems. Numerous platforms have introduced explicit negative feedback mechanisms and leverage such signals to refine their recommendation models. Beyond traditional business metrics, user experience-driven metrics, such as negative feedback rates, have become critical indicators for evaluating system performance. However, most existing approaches primarily use negative feedback as an auxiliary signal to enhance positive recommendations, paying little attention to directly modeling negative interests, which can be highly valuable in offline applications. Moreover, due to the inherent sparsity of negative feedback data, models often suffer from context understanding biases induced by positive feedback dominance. To address these challenges, we propose the first large language model framework for negative feedback modeling with special designed context-discerning modules. We use semantic ID Representation to replace text-based item descriptions and introduce an item-level alignment task that enhances the LLM's understanding of the semantic context behind negative feedback. Furthermore, we design a Progressive GRPO training paradigm that enables the model to dynamically balance the positive and negative behavioral context utilization. Besides, our investigation further reveals a fundamental misalignment between the conventional next-negative-item prediction objective and users' true negative preferences, which is heavily influenced by the system's recommendation order. To mitigate this, we propose a novel reward function and evaluation metric grounded in multi-day future negative feedback and their collaborative signals.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.IR",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15717v1",
      "title": "Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling",
      "link": "http://arxiv.org/abs/2601.15717v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15717v1",
      "authors": "Luyao Zhu, Fangfang Zhang, Yi Mei, Mengjie Zhang",
      "institution": "",
      "abstract": "Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15715v1",
      "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
      "link": "http://arxiv.org/abs/2601.15715v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15715v1",
      "authors": "Zhitao He, Zongwei Lyu, Yi R Fung",
      "institution": "",
      "abstract": "Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15714v1",
      "title": "Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs",
      "link": "http://arxiv.org/abs/2601.15714v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15714v1",
      "authors": "Ryoma Sato",
      "institution": "",
      "abstract": "We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15711v1",
      "title": "Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework",
      "link": "http://arxiv.org/abs/2601.15711v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15711v1",
      "authors": "Shubham Shukla, Kunal Sonalkar",
      "institution": "",
      "abstract": "Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, \"outer fabric\" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15710v1",
      "title": "FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design",
      "link": "http://arxiv.org/abs/2601.15710v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15710v1",
      "authors": "Jiahao Zhang, Zifan He, Nicholas Fraser, Michaela Blott, Yizhou Sun et al.",
      "institution": "",
      "abstract": "We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15709v1",
      "title": "AgentSM: Semantic Memory for Agentic Text-to-SQL",
      "link": "http://arxiv.org/abs/2601.15709v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15709v1",
      "authors": "Asim Biswal, Chuan Lei, Xiao Qin, Aodong Li, Balakrishnan Narayanaswamy et al.",
      "institution": "",
      "abstract": "Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15708v1",
      "title": "Persona Switch: Mixing Distinct Perspectives in Decoding Time",
      "link": "http://arxiv.org/abs/2601.15708v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15708v1",
      "authors": "Junseok Kim, Nakyeong Yang, Kyomin Jung",
      "institution": "",
      "abstract": "Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15706v1",
      "title": "Improving Methodologies for LLM Evaluations Across Global Languages",
      "link": "http://arxiv.org/abs/2601.15706v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15706v1",
      "authors": "Akriti Vij, Benjamin Chua, Darshini Ramiah, En Qi Ng, Mahran Morsidi et al.",
      "institution": "",
      "abstract": "As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.\n  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15705v1",
      "title": "Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data",
      "link": "http://arxiv.org/abs/2601.15705v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15705v1",
      "authors": "Ali Caglayan, Nevrez Imamoglu, Toru Kouyama",
      "institution": "",
      "abstract": "This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15703v1",
      "title": "Agentic Uncertainty Quantification",
      "link": "http://arxiv.org/abs/2601.15703v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15703v1",
      "authors": "Jiaxin Zhang, Prafulla Kumar Choubey, Kung-Hsiang Huang, Caiming Xiong, Chien-Sheng Wu",
      "institution": "",
      "abstract": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15698v1",
      "title": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs",
      "link": "http://arxiv.org/abs/2601.15698v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15698v1",
      "authors": "Mingyu Yu, Lana Liu, Zhehao Zhao, Wei Wang, Sujuan Qin",
      "institution": "",
      "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15697v1",
      "title": "Balancing Security and Privacy: The Pivotal Role of AI in Modern Healthcare Systems",
      "link": "http://arxiv.org/abs/2601.15697v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15697v1",
      "authors": "Binu V P, Deepthy K Bhaskar, Minimol B",
      "institution": "",
      "abstract": "As digital threats continue to grow, organizations must find ways to enhance security while protecting user privacy. This paper explores how artificial intelligence (AI) plays a crucial role in achieving this balance. AI technologies can improve security by detecting threats, monitoring systems, and automating responses. However, using AI also raises privacy concerns that need careful consideration.We examine real-world examples from the healthcare sector to illustrate how organizations can implement AI solutions that strengthen security without compromising patient privacy. Additionally, we discuss the importance of creating transparent AI systems and adhering to privacy regulations.Ultimately, this paper provides insights and recommendations for integrating AI into healthcare security practices, helping organizations navigate the challenges of modern management while keeping patient data safe.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CR",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15696v1",
      "title": "Learning Functional Graphs with Nonlinear Sufficient Dimension Reduction",
      "link": "http://arxiv.org/abs/2601.15696v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15696v1",
      "authors": "Kyongwon Kim, Bing Li",
      "institution": "",
      "abstract": "Functional graphical models have undergone extensive development during the recent years, leading to a variety models such as the functional Gaussian graphical model, the functional copula Gaussian graphical model, the functional Bayesian graphical model, the nonparametric functional additive graphical model, and the conditional functional graphical model. These models rely either on some parametric form of distributions on random functions, or on additive conditional independence, a criterion that is different from probabilistic conditional independence. In this paper we introduce a nonparametric functional graphical model based on functional sufficient dimension reduction. Our method not only relaxes the Gaussian or copula Gaussian assumptions, but also enhances estimation accuracy by avoiding the ``curse of dimensionality''. Moreover, it retains the probabilistic conditional independence as the criterion to determine the absence of edges. By doing simulation study and analysis of the f-MRI dataset, we demonstrate the advantages of our method.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "stat.ME",
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15690v1",
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "link": "http://arxiv.org/abs/2601.15690v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15690v1",
      "authors": "Jiaxin Zhang, Wendi Cui, Zhuohang Li, Lifu Huang, Bradley Malin et al.",
      "institution": "",
      "abstract": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "stat.AP"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15688v1",
      "title": "Performance-guided Reinforced Active Learning for Object Detection",
      "link": "http://arxiv.org/abs/2601.15688v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15688v1",
      "authors": "Zhixuan Liang, Xingyu Zeng, Rui Zhao, Ping Luo",
      "institution": "",
      "abstract": "Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15687v1",
      "title": "FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation",
      "link": "http://arxiv.org/abs/2601.15687v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15687v1",
      "authors": "Khusrav Badalov, Young Yoon",
      "institution": "",
      "abstract": "Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.SE",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15686v1",
      "title": "Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing",
      "link": "http://arxiv.org/abs/2601.15686v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15686v1",
      "authors": "Xinyu Wang, Sicheng Lyu, Yu Gu, Jerry Huang, Peng Lu et al.",
      "institution": "",
      "abstract": "Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit \"hard writes\" can accumulate interference over time, while null-space-style \"hard preservation\" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15681v1",
      "title": "Consistency-Regularized GAN for Few-Shot SAR Target Recognition",
      "link": "http://arxiv.org/abs/2601.15681v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15681v1",
      "authors": "Yikui Zhai, Shikuang Liu, Wenlve Zhou, Hongsheng Zhang, Zhiheng Zhou et al.",
      "institution": "",
      "abstract": "Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15679v1",
      "title": "Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats",
      "link": "http://arxiv.org/abs/2601.15679v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15679v1",
      "authors": "Ee Wei Seah, Yongsen Zheng, Naga Nikshith, Mahran Morsidi, Gabriel Waikin Loh Matienzo et al.",
      "institution": "",
      "abstract": "The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.\n  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.\n  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.\n  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15678v1",
      "title": "Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems",
      "link": "http://arxiv.org/abs/2601.15678v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15678v1",
      "authors": "Mengyu Yao, Ziqi Zhang, Ning Luo, Shaofei Li, Yifeng Cai et al.",
      "institution": "",
      "abstract": "Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CR",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15676v1",
      "title": "Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems",
      "link": "http://arxiv.org/abs/2601.15676v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15676v1",
      "authors": "Hengfan Zhang, Yueqian Lin, Hai Helen Li, Yiran Chen",
      "institution": "",
      "abstract": "Deploying Audio-Language Models (Audio-LLMs) on edge infrastructure exposes a persistent tension between perception depth and computational efficiency. Lightweight local models tend to produce passive perception - generic summaries that miss the subtle evidence required for multi-step audio reasoning - while indiscriminate cloud offloading incurs unacceptable latency, bandwidth cost, and privacy risk. We propose CoFi-Agent (Tool-Augmented Coarse-to-Fine Agent), a hybrid architecture targeting edge servers and gateways. It performs fast local perception and triggers conditional forensic refinement only when uncertainty is detected. CoFi-Agent runs an initial single-pass on a local 7B Audio-LLM, then a cloud controller gates difficult cases and issues lightweight plans for on-device tools such as temporal re-listening and local ASR. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60%, while achieving a better accuracy-efficiency trade-off than an always-on investigation pipeline. Overall, CoFi-Agent bridges the perception gap via tool-enabled, conditional edge-cloud collaboration under practical system constraints.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.SD",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15674v1",
      "title": "What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking",
      "link": "http://arxiv.org/abs/2601.15674v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15674v1",
      "authors": "Raymond Xiong, Furong Jia, Lionel Wong, Monica Agrawal",
      "institution": "",
      "abstract": "Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15673v1",
      "title": "Enhancing guidance for missing data in diffusion-based sequential recommendation",
      "link": "http://arxiv.org/abs/2601.15673v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15673v1",
      "authors": "Qilong Yan, Yifei Xing, Dugang Liu, Jingpu Duan, Jian Yin",
      "institution": "",
      "abstract": "Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.IR",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15671v1",
      "title": "StreetDesignAI: A Multi-Persona Evaluation System for Inclusive Infrastructure Design",
      "link": "http://arxiv.org/abs/2601.15671v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15671v1",
      "authors": "Ziyi Wang, Yilong Dai, Duanya Lyu, Mateo Nader, Sihan Chen et al.",
      "institution": "",
      "abstract": "Designing inclusive cycling infrastructure requires balancing competing needs of diverse user groups, yet designers often struggle to anticipate how different cyclists experience the same street. We investigate how persona-based multi-agent evaluation can support inclusive design by making experiential conflicts explicit. We present StreetDesignAI, an interactive system that enables designers to (1) ground evaluation in street context through imagery and map data, (2) receive parallel feedback from cyclist personas spanning confident to cautious users, and (3) iteratively modify designs while surfacing conflicts across perspectives. A within-subjects study with 26 transportation professionals demonstrates that structured multi-perspective feedback significantly improves designers' understanding of diverse user perspectives, ability to identify persona needs, and confidence in translating them into design decisions, with higher satisfaction and stronger intention for professional adoption. Qualitative findings reveal how conflict surfacing transforms design exploration from single-perspective optimization toward deliberate trade-off reasoning. We discuss implications for AI tools that scaffold inclusive design through disagreement as an interaction primitive.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.HC",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15669v1",
      "title": "Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting",
      "link": "http://arxiv.org/abs/2601.15669v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15669v1",
      "authors": "Jingjing Bai, Yoshinobu Kawahara",
      "institution": "",
      "abstract": "Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15664v1",
      "title": "Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling",
      "link": "http://arxiv.org/abs/2601.15664v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15664v1",
      "authors": "Hongyang Wei, Hongbo Liu, Zidong Wang, Yi Peng, Baixin Xu et al.",
      "institution": "",
      "abstract": "The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15663v1",
      "title": "TempoNet: Learning Realistic Communication and Timing Patterns for Network Traffic Simulation",
      "link": "http://arxiv.org/abs/2601.15663v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15663v1",
      "authors": "Kristen Moore, Diksha Goel, Cody James Christopher, Zhen Wang, Minjune Kim et al.",
      "institution": "",
      "abstract": "Realistic network traffic simulation is critical for evaluating intrusion detection systems, stress-testing network protocols, and constructing high-fidelity environments for cybersecurity training. While attack traffic can often be layered into training environments using red-teaming or replay methods, generating authentic benign background traffic remains a core challenge -- particularly in simulating the complex temporal and communication dynamics of real-world networks. This paper introduces TempoNet, a novel generative model that combines multi-task learning with multi-mark temporal point processes to jointly model inter-arrival times and all packet- and flow-header fields. TempoNet captures fine-grained timing patterns and higher-order correlations such as host-pair behavior and seasonal trends, addressing key limitations of GAN-, LLM-, and Bayesian-based methods that fail to reproduce structured temporal variation. TempoNet produces temporally consistent, high-fidelity traces, validated on real-world datasets. Furthermore, we show that intrusion detection models trained on TempoNet-generated background traffic perform comparably to those trained on real data, validating its utility for real-world security applications.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15657v1",
      "title": "Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework",
      "link": "http://arxiv.org/abs/2601.15657v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15657v1",
      "authors": "Yinxi Tian, Changwu Huang, Ke Tang, Xin Yao",
      "institution": "",
      "abstract": "Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.\n  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.\n  By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15655v1",
      "title": "Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams",
      "link": "http://arxiv.org/abs/2601.15655v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15655v1",
      "authors": "Zhenghui Guo, Yuanbin Man, Junyuan Sheng, Bowen Lin, Ahmed Ahmed et al.",
      "institution": "",
      "abstract": "Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15652v1",
      "title": "Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models",
      "link": "http://arxiv.org/abs/2601.15652v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15652v1",
      "authors": "Manish Bhatt",
      "institution": "",
      "abstract": "Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).\n  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises (\"Sycophancy\").\n  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI",
        "cs.CR",
        "cs.ET"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15645v1",
      "title": "Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation",
      "link": "http://arxiv.org/abs/2601.15645v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15645v1",
      "authors": "Zhiyao Ren, Yibing Zhan, Siyuan Liang, Guozheng Ma, Baosheng Yu et al.",
      "institution": "",
      "abstract": "Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15644v1",
      "title": "SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction",
      "link": "http://arxiv.org/abs/2601.15644v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15644v1",
      "authors": "Zichen Yu, Quanli Liu, Wei Wang, Liyong Zhang, Xiaoguang Zhao",
      "institution": "",
      "abstract": "3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15643v1",
      "title": "Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception",
      "link": "http://arxiv.org/abs/2601.15643v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15643v1",
      "authors": "Bo Yuan, Danpei Zhao, Wentao Li, Tian Li, Zhiguo Jiang",
      "institution": "",
      "abstract": "Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15641v1",
      "title": "Machine Failure Detection Based on Projected Quantum Models",
      "link": "http://arxiv.org/abs/2601.15641v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15641v1",
      "authors": "Larry Bowden, Qi Chu, Bernard Cena, Kentaro Ohno, Bob Parney et al.",
      "institution": "",
      "abstract": "Detecting machine failures promptly is of utmost importance in industry for maintaining efficiency and minimizing downtime. This paper introduces a failure detection algorithm based on quantum computing and a statistical change-point detection approach. Our method leverages the potential of projected quantum feature maps to enhance the precision of anomaly detection in machine monitoring systems. We empirically validate our approach on benchmark multi-dimensional time series datasets as well as on a real-world dataset comprising IoT sensor readings from operational machines, ensuring the practical relevance of our study. The algorithm was executed on IBM's 133-qubit Heron quantum processor, demonstrating the feasibility of integrating quantum computing into industrial maintenance procedures. The presented results underscore the effectiveness of our quantum-based failure detection system, showcasing its capability to accurately identify anomalies in noisy time series data. This work not only highlights the potential of quantum computing in industrial diagnostics but also paves the way for more sophisticated quantum algorithms in the realm of predictive maintenance.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15640v1",
      "title": "An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types",
      "link": "http://arxiv.org/abs/2601.15640v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15640v1",
      "authors": "Natasha Trinkle, Huong Ha, Jeffrey Chan",
      "institution": "",
      "abstract": "Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15630v1",
      "title": "Agentic AI Governance and Lifecycle Management in Healthcare",
      "link": "http://arxiv.org/abs/2601.15630v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15630v1",
      "authors": "Chandra Prakash, Mary Lind, Avneesh Sisodia",
      "institution": "",
      "abstract": "Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15628v1",
      "title": "CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models",
      "link": "http://arxiv.org/abs/2601.15628v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15628v1",
      "authors": "Haibo Tong, Zeyang Yue, Feifei Zhao, Erliang Lin, Lu Jia et al.",
      "institution": "",
      "abstract": "Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15626v1",
      "title": "Bridging Qualitative Rubrics and AI: A Binary Question Framework for Criterion-Referenced Grading in Engineering",
      "link": "http://arxiv.org/abs/2601.15626v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15626v1",
      "authors": "Lili Chen, Winn Wing-Yiu Chow, Stella Peng, Bencheng Fan, Sachitha Bandara",
      "institution": "",
      "abstract": "PURPOSE OR GOAL: This study investigates how GenAI can be integrated with a criterion-referenced grading framework to improve the efficiency and quality of grading for mathematical assessments in engineering. It specifically explores the challenges demonstrators face with manual, model solution-based grading and how a GenAI-supported system can be designed to reliably identify student errors, provide high-quality feedback, and support human graders. The research also examines human graders' perceptions of the effectiveness of this GenAI-assisted approach. ACTUAL OR ANTICIPATED OUTCOMES: The study found that GenAI achieved an overall grading accuracy of 92.5%, comparable to two experienced human graders. The two researchers, who also served as subject demonstrators, perceived the GenAI as a helpful second reviewer that improved accuracy by catching small errors and provided more complete feedback than they could manually. A central outcome was the significant enhancement of formative feedback. However, they noted the GenAI tool is not yet reliable enough for autonomous use, especially with unconventional solutions. CONCLUSIONS/RECOMMENDATIONS/SUMMARY: This study demonstrates that GenAI, when paired with a structured, criterion-referenced framework using binary questions, can grade engineering mathematical assessments with an accuracy comparable to human experts. Its primary contribution is a novel methodological approach that embeds the generation of high-quality, scalable formative feedback directly into the assessment workflow. Future work should investigate student perceptions of GenAI grading and feedback.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15625v1",
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "link": "http://arxiv.org/abs/2601.15625v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15625v1",
      "authors": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang et al.",
      "institution": "",
      "abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15624v1",
      "title": "Explainable Deepfake Detection with RL Enhanced Self-Blended Images",
      "link": "http://arxiv.org/abs/2601.15624v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15624v1",
      "authors": "Ning Jiang, Dingheng Zeng, Yanhong Liu, Haiyang Yi, Shijie Yu et al.",
      "institution": "",
      "abstract": "Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15620v1",
      "title": "Closing the Gap on the Sample Complexity of 1-Identification",
      "link": "http://arxiv.org/abs/2601.15620v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15620v1",
      "authors": "Zitian Li, Wang Chi Cheung",
      "institution": "",
      "abstract": "1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \\textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15615v1",
      "title": "Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition",
      "link": "http://arxiv.org/abs/2601.15615v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15615v1",
      "authors": "Weiwei Wu, Yueyang Li, Yuhu Shi, Weiming Zeng, Lang Qin et al.",
      "institution": "",
      "abstract": "Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15609v1",
      "title": "When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards",
      "link": "http://arxiv.org/abs/2601.15609v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15609v1",
      "authors": "Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang et al.",
      "institution": "",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG",
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15605v1",
      "title": "ToxiTwitch: Toward Emote-Aware Hybrid Moderation for Live Streaming Platforms",
      "link": "http://arxiv.org/abs/2601.15605v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15605v1",
      "authors": "Baktash Ansari, Shiza Ali, Elias Martin, Maryna Sivachenko, Afra Mashhadi",
      "institution": "",
      "abstract": "The rapid growth of live-streaming platforms such as Twitch has introduced complex challenges in moderating toxic behavior. Traditional moderation approaches, such as human annotation and keyword-based filtering, have demonstrated utility, but human moderators on Twitch constantly struggle to scale effectively in the fast-paced, high-volume, and context-rich chat environment of the platform while also facing harassment themselves. Recent advances in large language models (LLMs), such as DeepSeek-R1-Distill and Llama-3-8B-Instruct, offer new opportunities for toxicity detection, especially in understanding nuanced, multimodal communication involving emotes. In this work, we present an exploratory comparison of toxicity detection approaches tailored to Twitch. Our analysis reveals that incorporating emotes improves the detection of toxic behavior. To this end, we introduce ToxiTwitch, a hybrid model that combines LLM-generated embeddings of text and emotes with traditional machine learning classifiers, including Random Forest and SVM. In our case study, the proposed hybrid approach reaches up to 80 percent accuracy under channel-specific training (with 13 percent improvement over BERT and F1-score of 76 percent). This work is an exploratory study intended to surface challenges and limits of emote-aware toxicity detection on Twitch.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.SI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15603v1",
      "title": "On the Nonasymptotic Scaling Guarantee of Hyperparameter Estimation in Inhomogeneous, Weakly-Dependent Complex Network Dynamical Systems",
      "link": "http://arxiv.org/abs/2601.15603v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15603v1",
      "authors": "Yi Yu, Yubo Hou, Yinchong Wang, Nan Zhang, Jianfeng Feng et al.",
      "institution": "",
      "abstract": "Hierarchical Bayesian models are increasingly used in large, inhomogeneous complex network dynamical systems by modeling parameters as draws from a hyperparameter-governed distribution. However, theoretical guarantees for these estimates as the system size grows have been lacking. A critical concern is that hyperparameter estimation may diverge for larger networks, undermining the model's reliability. Formulating the system's evolution in a measure transport perspective, we propose a theoretical framework for estimating hyperparameters with mean-type observations, which are prevalent in many scientific applications. Our primary contribution is a nonasymptotic bound for the deviation of estimate of hyperparameters in inhomogeneous complex network dynamical systems with respect to network population size, which is established for a general family of optimization algorithms within a fixed observation duration. While we firstly establish a consistency result for systems with independent nodes, our main result extends this guarantee to the more challenging and realistic setting of weakly-dependent nodes. We validate our theoretical findings with numerical experiments on two representative models: a Susceptible-Infected-Susceptible model and a Spiking Neuronal Network model. In both cases, the results confirm that the estimation error decreases as the network population size increases, aligning with our theoretical guarantees. This research proposes the foundational theory to ensure that hierarchical Bayesian methods are statistically consistent for large-scale inhomogeneous systems, filling a gap in this area of theoretical research and justifying their application in practice.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.IT",
        "stat.ML"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15599v1",
      "title": "Autonomous Business System via Neuro-symbolic AI",
      "link": "http://arxiv.org/abs/2601.15599v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15599v1",
      "authors": "Cecil Pang, Hiroki Sayama",
      "institution": "",
      "abstract": "Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15598v1",
      "title": "Ternary Spiking Neural Networks Enhanced by Complemented Neurons and Membrane Potential Aggregation",
      "link": "http://arxiv.org/abs/2601.15598v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15598v1",
      "authors": "Boxuan Zhang, Jiaxin Wang, Zhen Xu, Kuan Tao",
      "institution": "",
      "abstract": "Spiking Neural Networks (SNNs) are promising energy-efficient models and powerful framworks of modeling neuron dynamics. However, existing binary spiking neurons exhibit limited biological plausibilities and low information capacity. Recently developed ternary spiking neuron possesses higher consistency with biological principles (i.e. excitation-inhibition balance mechanism). Despite of this, the ternary spiking neuron suffers from defects including iterative information loss, temporal gradient vanishing and irregular distributions of membrane potentials. To address these issues, we propose Complemented Ternary Spiking Neuron (CTSN), a novel ternary spiking neuron model that incorporates an learnable complemental term to store information from historical inputs. CTSN effectively improves the deficiencies of ternary spiking neuron, while the embedded learnable factors enable CTSN to adaptively adjust neuron dynamics, providing strong neural heterogeneity. Furthermore, based on the temporal evolution features of ternary spiking neurons' membrane potential distributions, we propose the Temporal Membrane Potential Regularization (TMPR) training method. TMPR introduces time-varying regularization strategy utilizing membrane potentials, furhter enhancing the training process by creating extra backpropagation paths. We validate our methods through extensive experiments on various datasets, demonstrating remarkable performance advances.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.NE"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15597v1",
      "title": "Neural Nonlinear Shrinkage of Covariance Matrices for Minimum Variance Portfolio Optimization",
      "link": "http://arxiv.org/abs/2601.15597v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15597v1",
      "authors": "Liusha Yang, Siqi Zhao, Shuqi Chai",
      "institution": "",
      "abstract": "This paper introduces a neural network-based nonlinear shrinkage estimator of covariance matrices for the purpose of minimum variance portfolio optimization. It is a hybrid approach that integrates statistical estimation with machine learning. Starting from the Ledoit-Wolf (LW) shrinkage estimator, we decompose the LW covariance matrix into its eigenvalues and eigenvectors, and apply a lightweight transformer-based neural network to learn a nonlinear eigenvalue shrinkage function. Trained with portfolio risk as the loss function, the resulting precision matrix (the inverse covariance matrix) estimator directly targets portfolio risk minimization. By conditioning on the sample-to-dimension ratio, the approach remains scalable across different sample sizes and asset universes. Empirical results on stock daily returns from Standard & Poor's 500 Index (S&P500) demonstrate that the proposed method consistently achieves lower out-of-sample realized risk than benchmark approaches. This highlights the promise of integrating structural statistical models with data-driven learning.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15596v1",
      "title": "DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice",
      "link": "http://arxiv.org/abs/2601.15596v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15596v1",
      "authors": "Leying Zhang, Tingxiao Zhou, Haiyang Sun, Mengxiao Bi, Yanmin Qian",
      "institution": "",
      "abstract": "While modern Text-to-Speech (TTS) systems achieve high fidelity for read-style speech, they struggle to generate Autonomous Sensory Meridian Response (ASMR), a specialized, low-intensity speech style essential for relaxation. The inherent challenges include ASMR's subtle, often unvoiced characteristics and the demand for zero-shot speaker adaptation. In this paper, we introduce DeepASMR, the first framework designed for zero-shot ASMR generation. We demonstrate that a single short snippet of a speaker's ordinary, read-style speech is sufficient to synthesize high-fidelity ASMR in their voice, eliminating the need for whispered training data from the target speaker. Methodologically, we first identify that discrete speech tokens provide a soft factorization of ASMR style from speaker timbre. Leveraging this insight, we propose a two-stage pipeline incorporating a Large Language Model (LLM) for content-style encoding and a flow-matching acoustic decoder for timbre reconstruction. Furthermore, we contribute DeepASMR-DB, a comprehensive 670-hour English-Chinese multi-speaker ASMR speech corpus, and introduce a novel evaluation protocol integrating objective metrics, human listening tests, LLM-based scoring and unvoiced speech analysis. Extensive experiments confirm that DeepASMR achieves state-of-the-art naturalness and style fidelity in ASMR generation for anyone of any voice, while maintaining competitive performance on normal speech synthesis.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.SD",
        "cs.AI"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15595v1",
      "title": "Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning",
      "link": "http://arxiv.org/abs/2601.15595v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15595v1",
      "authors": "Xinjie Zhou, Zhihui Yang, Lechao Cheng, Sai Wu, Gang Chen",
      "institution": "",
      "abstract": "Large language models (LLMs) exhibit powerful capabilities but risk memorizing sensitive personally identifiable information (PII) from their training data, posing significant privacy concerns. While machine unlearning techniques aim to remove such data, they predominantly depend on access to the training data. This requirement is often impractical, as training data in real-world deployments is commonly proprietary or inaccessible. To address this limitation, we propose Data-Free Selective Unlearning (DFSU), a novel privacy-preserving framework that removes sensitive PII from an LLM without requiring its training data. Our approach first synthesizes pseudo-PII through language model inversion, then constructs token-level privacy masks for these synthetic samples, and finally performs token-level selective unlearning via a contrastive mask loss within a low-rank adaptation (LoRA) subspace. Extensive experiments on the AI4Privacy PII-Masking dataset using Pythia models demonstrate that our method effectively removes target PII while maintaining model utility.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15593v1",
      "title": "Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow",
      "link": "http://arxiv.org/abs/2601.15593v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15593v1",
      "authors": "Yangyang Zhong, Yanmei Gu, Zhengqing Zang, Xiaomeng Li, Yuqi Ding et al.",
      "institution": "",
      "abstract": "Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require \"backward information\" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15589v1",
      "title": "Deep Learning for Perishable Inventory Systems with Human Knowledge",
      "link": "http://arxiv.org/abs/2601.15589v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15589v1",
      "authors": "Xuan Liao, Zhenkang Peng, Ying Rong",
      "institution": "",
      "abstract": "Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15588v1",
      "title": "YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models",
      "link": "http://arxiv.org/abs/2601.15588v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15588v1",
      "authors": "Junyu Lin, Meizhen Liu, Xiufeng Huang, Jinfeng Li, Haiwen Hong et al.",
      "institution": "",
      "abstract": "As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15578v1",
      "title": "MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments",
      "link": "http://arxiv.org/abs/2601.15578v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15578v1",
      "authors": "Cyril Shih-Huan Hsu, Xi Li, Lanfranco Zanzi, Zhiheng Yang, Chrysa Papagianni et al.",
      "institution": "",
      "abstract": "Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15575v1",
      "title": "PromptHelper: A Prompt Recommender System for Encouraging Creativity in AI Chatbot Interactions",
      "link": "http://arxiv.org/abs/2601.15575v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15575v1",
      "authors": "Jason Kim, Maria Teleki, James Caverlee",
      "institution": "",
      "abstract": "Prompting is central to interaction with AI systems, yet many users struggle to explore alternative directions, articulate creative intent, or understand how variations in prompts shape model outputs. We introduce prompt recommender systems (PRS) as an interaction approach that supports exploration, suggesting contextually relevant follow-up prompts. We present PromptHelper, a PRS prototype integrated into an AI chatbot that surfaces semantically diverse prompt suggestions while users work on real writing tasks. We evaluate PromptHelper in a 2x2 fully within-subjects study (N=32) across creative and academic writing tasks. Results show that PromptHelper significantly increases users' perceived exploration and expressiveness without increasing cognitive workload. Qualitative findings illustrate how prompt recommendations help users branch into new directions, overcome uncertainty about what to ask next, and better articulate their intent. We discuss implications for designing AI interfaces that scaffold exploratory interaction while preserving user agency, and release open-source resources to support research on prompt recommendation.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.HC",
        "cs.AI",
        "cs.IR"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15572v1",
      "title": "FUGC: Benchmarking Semi-Supervised Learning Methods for Cervical Segmentation",
      "link": "http://arxiv.org/abs/2601.15572v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15572v1",
      "authors": "Jieyun Bai, Yitong Tang, Zihao Zhou, Mahdi Islam, Musarrat Tabassum et al.",
      "institution": "",
      "abstract": "Accurate segmentation of cervical structures in transvaginal ultrasound (TVS) is critical for assessing the risk of spontaneous preterm birth (PTB), yet the scarcity of labeled data limits the performance of supervised learning approaches. This paper introduces the Fetal Ultrasound Grand Challenge (FUGC), the first benchmark for semi-supervised learning in cervical segmentation, hosted at ISBI 2025. FUGC provides a dataset of 890 TVS images, including 500 training images, 90 validation images, and 300 test images. Methods were evaluated using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD), and runtime (RT), with a weighted combination of 0.4/0.4/0.2. The challenge attracted 10 teams with 82 participants submitting innovative solutions. The best-performing methods for each individual metric achieved 90.26\\% mDSC, 38.88 mHD, and 32.85 ms RT, respectively. FUGC establishes a standardized benchmark for cervical segmentation, demonstrates the efficacy of semi-supervised methods with limited labeled data, and provides a foundation for AI-assisted clinical PTB risk assessment.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CE",
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15561v1",
      "title": "Enhanced Convergence in p-bit Based Simulated Annealing with Partial Deactivation for Large-Scale Combinatorial Optimization Problems",
      "link": "http://arxiv.org/abs/2601.15561v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15561v1",
      "authors": "Naoya Onizawa, Takahiro Hanyu",
      "institution": "",
      "abstract": "This article critically investigates the limitations of the simulated annealing algorithm using probabilistic bits (pSA) in solving large-scale combinatorial optimization problems. The study begins with an in-depth analysis of the pSA process, focusing on the issues resulting from unexpected oscillations among p-bits. These oscillations hinder the energy reduction of the Ising model and thus obstruct the successful execution of pSA in complex tasks. Through detailed simulations, we unravel the root cause of this energy stagnation, identifying the feedback mechanism inherent to the pSA operation as the primary contributor to these disruptive oscillations. To address this challenge, we propose two novel algorithms, time average pSA (TApSA) and stalled pSA (SpSA). These algorithms are designed based on partial deactivation of p-bits and are thoroughly tested using Python simulations on maximum cut benchmarks that are typical combinatorial optimization problems. On the 16 benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized cut value from 0.8% to 98.4% on average in comparison with the conventional pSA.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.ET",
        "cs.LG"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15560v1",
      "title": "Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation",
      "link": "http://arxiv.org/abs/2601.15560v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15560v1",
      "authors": "Sylvey Lin, Eranki Vasistha",
      "institution": "",
      "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CV"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.15558v1",
      "title": "From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare",
      "link": "http://arxiv.org/abs/2601.15558v1",
      "pdf_link": "https://arxiv.org/pdf/2601.15558v1",
      "authors": "Man Luo, Bahareh Harandizadeh, Amara Tariq, Halim Abbas, Umar Ghaffar et al.",
      "institution": "",
      "abstract": "Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.",
      "source": "arXiv",
      "pubDateISO": "2026-01-22",
      "tags": [
        "cs.CL"
      ],
      "topics": [],
      "score": 4,
      "score_reason": "",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    }
  ]
}