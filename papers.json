{
  "generated_at": "2026-01-08T03:15:04Z",
  "date": "2026-01-08",
  "total_count": 50,
  "papers": [
    {
      "id": "2601.03252",
      "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
      "link": "https://huggingface.co/papers/2601.03252",
      "pdf_link": "https://arxiv.org/pdf/2601.03252.pdf",
      "authors": "Hao Yu, Haotong Lin, Jiawei Wang, Jiaxin Li, Yida Wang",
      "institution": "",
      "abstract": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [
        "Neural Implicit Fields",
        "Depth Estimation",
        "Computer Vision"
      ],
      "score": 9,
      "score_reason": "Breakthrough depth estimation",
      "citations": 0,
      "upvotes": 73,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "InfiniDepth introduces a neural implicit field representation for depth estimation, enabling arbitrary-resolution and fine-grained depth queries through a local implicit decoder.",
      "why_it_matters": "This approach has significant implications for computer vision tasks, such as novel view synthesis and 3D reconstruction, by providing high-quality, continuous depth maps that can be queried at any resolution.",
      "limitations": "The method's reliance on a high-quality synthetic benchmark and potential computational costs of querying neural implicit fields may limit its applicability to certain real-world scenarios."
    },
    {
      "id": "2601.02204",
      "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
      "link": "https://huggingface.co/papers/2601.02204",
      "pdf_link": "https://arxiv.org/pdf/2601.02204.pdf",
      "authors": "Huichao Zhang, Liao Qu, Yiheng Liu, Hang Chen, Yangyang Song",
      "institution": "",
      "abstract": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [
        "Multimodal Generation",
        "Autoregressive Transformers",
        "Computer Vision"
      ],
      "score": 9,
      "score_reason": "Unified sequential modeling",
      "citations": 0,
      "upvotes": 50,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces NextFlow, a unified decoder-only autoregressive transformer that leverages a unified vision representation and next-scale prediction for visual generation, enabling fast and high-quality multimodal understanding and generation.",
      "why_it_matters": "This work matters because it achieves state-of-the-art performance in unified models and rivals specialized diffusion baselines in visual quality, making it a significant advancement in multimodal generation and understanding.",
      "limitations": "The main limitation of this work is that it relies on a robust training recipe to address the instabilities of multi-scale generation, which may not be generalizable to all scenarios or datasets."
    },
    {
      "id": "2601.02346",
      "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
      "link": "https://huggingface.co/papers/2601.02346",
      "pdf_link": "https://arxiv.org/pdf/2601.02346.pdf",
      "authors": "Falcon LLM Team, Iheb Chaabane, Puneesh Khanna, Suhail Mohmad, Slim Frikha",
      "institution": "",
      "abstract": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [
        "Efficient Language Modeling",
        "Reasoning Systems",
        "Test-Time Scaling"
      ],
      "score": 9,
      "score_reason": "State-of-art reasoning",
      "citations": 0,
      "upvotes": 16,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces Falcon-H1R, a 7B-parameter hybrid model that achieves competitive reasoning performance with small language models, outperforming larger state-of-the-art models through careful data curation and targeted training strategies.",
      "why_it_matters": "This research matters because it demonstrates that compact models can deliver robust and scalable reasoning performance, making them a practical choice for applications requiring efficient test-time scaling and chain-of-thoughts generation.",
      "limitations": "The main limitation of this work is that it relies on the recently introduced DeepConf approach and efficient SFT and RL scaling methods, which may not be widely applicable or generalizable to other domains or tasks."
    },
    {
      "id": "2601.03256",
      "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
      "link": "https://huggingface.co/papers/2601.03256",
      "pdf_link": "https://arxiv.org/pdf/2601.03256.pdf",
      "authors": "Hexiao Lu, Xiaokun Sun, Zeyu Cai, Hao Guo, Ying Tai",
      "institution": "",
      "abstract": "We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [
        "3D creature generation",
        "training-free methods",
        "graph-constrained reasoning"
      ],
      "score": 9,
      "score_reason": "Training-free generation",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "Muses introduces a training-free method for generating 3D creatures by leveraging a 3D skeleton as a fundamental representation to compose diverse elements through graph-constrained reasoning and voxel-based assembly.",
      "why_it_matters": "This work matters because it enables the creation of realistic and coherent 3D assets without requiring large datasets or manual assembly, which can accelerate applications in fields such as computer-aided design, video games, and special effects.",
      "limitations": "The main limitation of Muses is that its reliance on a structured latent space and skeletal representation may restrict its ability to generate highly complex or abstract creatures that do not conform to biological forms."
    },
    {
      "id": "2601.03233",
      "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
      "link": "https://huggingface.co/papers/2601.03233",
      "pdf_link": "https://arxiv.org/pdf/2601.03233.pdf",
      "authors": "Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko",
      "institution": "",
      "abstract": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [
        "Multimodal Learning",
        "Audiovisual Generation",
        "Transformer Architectures"
      ],
      "score": 8,
      "score_reason": "Joint audio-visual model",
      "citations": 0,
      "upvotes": 48,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The introduction of LTX-2, a unified audiovisual foundation model that leverages an asymmetric dual-stream transformer with bidirectional audio-video cross-attention layers to efficiently generate high-quality, temporally synchronized audiovisual content.",
      "why_it_matters": "This work matters because it enables the generation of rich, coherent audio tracks that align with video content, significantly advancing the state-of-the-art in audiovisual generation and opening up new possibilities for applications such as video production and multimedia analysis.",
      "limitations": "The main limitation of LTX-2 is its reliance on a large-scale dataset and substantial computational resources for training, which may hinder its adoption and fine-tuning for specific downstream tasks or domains with limited data and resources."
    },
    {
      "id": "2512.24601",
      "title": "Recursive Language Models",
      "link": "https://huggingface.co/papers/2512.24601",
      "pdf_link": "https://arxiv.org/pdf/2512.24601.pdf",
      "authors": "Alex L. Zhang, Tim Kraska, Omar Khattab",
      "institution": "",
      "abstract": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.",
      "source": "HuggingFace",
      "pubDateISO": "2025-12-31",
      "tags": [],
      "topics": [
        "Natural Language Processing",
        "Large Language Models",
        "Inference-Time Scaling"
      ],
      "score": 8,
      "score_reason": "Recursive language models",
      "citations": 0,
      "upvotes": 31,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper introduces Recursive Language Models (RLMs), a novel inference strategy that enables large language models to process arbitrarily long prompts by recursively decomposing and examining prompt snippets.",
      "why_it_matters": "RLMs have the potential to significantly improve the performance of large language models on long-context tasks, which is crucial for many natural language processing applications that require handling lengthy inputs or documents.",
      "limitations": "The proposed approach may be limited by the increased computational cost and potential instability of recursive function calls, which could lead to exploding gradients or stack overflows for very deep recursion levels."
    },
    {
      "id": "2512.22334",
      "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
      "link": "https://huggingface.co/papers/2512.22334",
      "pdf_link": "https://arxiv.org/pdf/2512.22334.pdf",
      "authors": "Yiheng Wang, Yixin Chen, Shuo Li, Yifan Zhou, Bo Liu",
      "institution": "",
      "abstract": "We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.",
      "source": "HuggingFace",
      "pubDateISO": "2025-12-26",
      "tags": [],
      "topics": [
        "Scientific General Intelligence",
        "Multimodal Reasoning",
        "Benchmarking and Evaluation"
      ],
      "score": 8,
      "score_reason": "Comprehensive toolkit",
      "citations": 0,
      "upvotes": 28,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces SciEvalKit, a unified benchmarking toolkit that evaluates AI models across multiple scientific disciplines and task capabilities, providing a standardized yet customizable infrastructure for assessing scientific general intelligence.",
      "why_it_matters": "This toolkit matters because it enables researchers to comprehensively evaluate and compare the performance of AI models in various scientific domains, facilitating the development of more advanced and generalizable scientific foundation models.",
      "limitations": "A main limitation of SciEvalKit is that its effectiveness relies on the quality and diversity of the curated datasets and benchmarks, which may not fully capture the complexity and nuances of real-world scientific challenges."
    },
    {
      "id": "2601.03193",
      "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
      "link": "https://huggingface.co/papers/2601.03193",
      "pdf_link": "https://arxiv.org/pdf/2601.03193.pdf",
      "authors": "Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang",
      "institution": "",
      "abstract": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [
        "Multimodal Learning",
        "Self-Supervised Learning",
        "Generative Models"
      ],
      "score": 8,
      "score_reason": "Innovative generation",
      "citations": 0,
      "upvotes": 27,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes UniCorn, a self-improvement framework that enables Unified Multimodal Models (UMMs) to leverage internal knowledge for high-quality generation through self-generated supervision and cognitive pattern reconstruction.",
      "why_it_matters": "This research matters because it addresses the limitation of UMMs in translating cross-modal comprehension into faithful and controllable synthesis, which is crucial for applications such as text-to-image generation and multimodal dialogue systems.",
      "limitations": "The main limitation of this approach is that it relies on the quality of the initial UMM and the effectiveness of the self-play mechanism, which may not always guarantee convergence to optimal solutions or generalize well to out-of-distribution data."
    },
    {
      "id": "2601.02358",
      "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
      "link": "https://huggingface.co/papers/2601.02358",
      "pdf_link": "https://arxiv.org/pdf/2601.02358.pdf",
      "authors": "Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai",
      "institution": "",
      "abstract": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 8,
      "score_reason": "Unified visual generator",
      "citations": 0,
      "upvotes": 23,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02780",
      "title": "MiMo-V2-Flash Technical Report",
      "link": "https://huggingface.co/papers/2601.02780",
      "pdf_link": "https://arxiv.org/pdf/2601.02780.pdf",
      "authors": "Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen",
      "institution": "",
      "abstract": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [
        "Mixture-of-Experts (MoE) models",
        "Hybrid attention architectures",
        "Knowledge distillation"
      ],
      "score": 8,
      "score_reason": "Large MoE model",
      "citations": 0,
      "upvotes": 18,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm, enabling efficient scaling of post-training compute for large language models like MiMo-V2-Flash.",
      "why_it_matters": "This work matters because it demonstrates a parameter-efficient approach to achieving state-of-the-art performance, rivaling top-tier models while using significantly fewer parameters, which can lead to substantial reductions in computational costs and environmental impact.",
      "limitations": "The main limitation of this work is that the MOPD paradigm relies on the availability of high-quality, domain-specialized teacher models, which can be challenging to obtain, particularly for niche or emerging domains."
    },
    {
      "id": "2601.00501",
      "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
      "link": "https://huggingface.co/papers/2601.00501",
      "pdf_link": "https://arxiv.org/pdf/2601.00501.pdf",
      "authors": "Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain",
      "institution": "",
      "abstract": "We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-01",
      "tags": [],
      "topics": [
        "Vision-Language Models",
        "Reinforcement Learning",
        "Multimodal Reasoning"
      ],
      "score": 8,
      "score_reason": "Novel policy optimization",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces CPPO, a novel method that detects perception tokens via entropy shifts in model outputs under perturbed input images and extends the RL objective function with a Contrastive Perception Loss (CPL) to improve vision-language policy optimization.",
      "why_it_matters": "This work matters because it enables more efficient and scalable fine-tuning of vision-language models (VLMs) without requiring extra models or ground-truth data, which can accelerate progress in multimodal reasoning and related applications.",
      "limitations": "The main limitation of CPPO is that it relies on the assumption that perception tokens can be effectively identified through entropy shifts in model outputs, which may not always hold true or be robust across different datasets and scenarios."
    },
    {
      "id": "2601.01836",
      "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "link": "https://huggingface.co/papers/2601.01836",
      "pdf_link": "https://arxiv.org/pdf/2601.01836.pdf",
      "authors": "Dasol Choi, DongGeon Lee, Brigitta Jesica Kartono, Helena Berndt, Taeyoun Kwon",
      "institution": "",
      "abstract": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [
        "Large Language Models",
        "AI Safety",
        "Policy Alignment Evaluation"
      ],
      "score": 8,
      "score_reason": "Novel policy framework",
      "citations": 0,
      "upvotes": 5,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces COMPASS, a systematic framework for evaluating organization-specific policy alignment in large language models (LLMs), addressing a critical gap in existing safety evaluations that focus solely on universal harms.",
      "why_it_matters": "This research has significant practical implications for high-stakes enterprise applications, such as healthcare and finance, where adherence to organization-specific policies is crucial for ensuring safety and preventing potential harms.",
      "limitations": "The framework's effectiveness relies on the quality and coverage of the generated queries, which may not capture all possible edge cases or scenarios, potentially limiting the generalizability of the results."
    },
    {
      "id": "2601.04194",
      "title": "Choreographing a World of Dynamic Objects",
      "link": "https://huggingface.co/papers/2601.04194",
      "pdf_link": "https://arxiv.org/pdf/2601.04194.pdf",
      "authors": "Yanzhe Lyu, Chen Geng, Karthik Dharmarajan, Yunzhi Zhang, Hadi Alzayer",
      "institution": "",
      "abstract": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-07",
      "tags": [],
      "topics": [
        "Computer Vision",
        "Graphics",
        "Robotics",
        "Generative Models"
      ],
      "score": 8,
      "score_reason": "Novel generative pipeline",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "This paper proposes a novel distillation-based pipeline, CHORD, that extracts Lagrangian motion information from Eulerian representations of 2D videos to generate dynamic 4D scenes in a universal, versatile, and category-agnostic manner.",
      "why_it_matters": "This research has significant implications for robotics, computer vision, and graphics, as it enables the generation of diverse 4D dynamics and manipulation policies without requiring large-scale datasets or category-specific heuristics.",
      "limitations": "The approach relies on the quality and availability of 2D video data, which may not always capture the complexity and nuances of real-world 4D scenes, potentially limiting its applicability in certain scenarios."
    },
    {
      "id": "2601.02359",
      "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
      "link": "https://huggingface.co/papers/2601.02359",
      "pdf_link": "https://arxiv.org/pdf/2601.02359.pdf",
      "authors": "Kaede Shiohara, Toshihiko Yamasaki, Vladislav Golyanik",
      "institution": "",
      "abstract": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [
        "deepfake detection",
        "self-supervised learning",
        "diffusion models"
      ],
      "score": 8,
      "score_reason": "Robust forgery detection",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper proposes a fully self-supervised approach, ExposeAnyone, which utilizes a diffusion model to generate expression sequences from audio and detects face forgeries by computing identity distances via diffusion reconstruction errors.",
      "why_it_matters": "This research matters because it addresses the challenging problem of detecting unknown deepfake manipulations, which is crucial for ensuring the authenticity of digital media and preventing potential misuse.",
      "limitations": "The main limitation of this approach is that it requires a reference set of videos for personalization to specific subjects, which may not always be available or feasible in real-world scenarios."
    },
    {
      "id": "2601.00581",
      "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
      "link": "https://huggingface.co/papers/2601.00581",
      "pdf_link": "https://arxiv.org/pdf/2601.00581.pdf",
      "authors": "Stephen E. Farr, Stefan Doerr, Antonio Mirarchi, Francesc Sabanes Zariquiey, Gianni De Fabritiis",
      "institution": "",
      "abstract": "We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-02",
      "tags": [],
      "topics": [
        "Machine Learning Interatomic Potentials",
        "Small Molecule Drug Discovery",
        "Density Functional Theory"
      ],
      "score": 8,
      "score_reason": "State-of-art MLIP",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The authors introduce AceFF, a pre-trained machine learning interatomic potential that achieves DFT-level accuracy for small molecule drug discovery by leveraging a refined TensorNet2 architecture and a comprehensive dataset of drug-like compounds.",
      "why_it_matters": "This work matters because it enables high-throughput and accurate predictions of molecular properties, which can accelerate the discovery and optimization of small molecule drugs by reducing the need for computationally expensive DFT calculations.",
      "limitations": "The main limitation of AceFF is that its generalizability to large molecules or complex chemical systems beyond the training dataset is not explicitly demonstrated, which may restrict its applicability to certain areas of chemistry."
    },
    {
      "id": "2601.02314",
      "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
      "link": "https://huggingface.co/papers/2601.02314",
      "pdf_link": "https://arxiv.org/pdf/2601.02314.pdf",
      "authors": "Sourena Khanzadeh",
      "institution": "",
      "abstract": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (φ) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (ρ) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [
        "Explainable AI (XAI)",
        "Structural Causal Models (SCMs)",
        "Large Language Models (LLMs)"
      ],
      "score": 8,
      "score_reason": "Causal framework introduced",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "The paper introduces Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning in Large Language Model (LLM) agents.",
      "why_it_matters": "This research matters because it reveals a persistent Faithfulness Gap in state-of-the-art LLM agents, where their reasoning traces often function as post-hoc rationalizations rather than faithful generative drivers of their output, posing a critical safety concern for high-stakes autonomous decision-making.",
      "limitations": "The main limitation of this work is that it relies on a specific type of intervention (do-calculus) on intermediate reasoning nodes, which may not be generalizable to all types of LLM agents or decision-making scenarios."
    },
    {
      "id": "2512.24138",
      "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
      "link": "https://huggingface.co/papers/2512.24138",
      "pdf_link": "https://arxiv.org/pdf/2512.24138.pdf",
      "authors": "Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang",
      "institution": "",
      "abstract": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.",
      "source": "HuggingFace",
      "pubDateISO": "2025-12-30",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Improves diffusion models",
      "citations": 0,
      "upvotes": 27,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02281",
      "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
      "link": "https://huggingface.co/papers/2601.02281",
      "pdf_link": "https://arxiv.org/pdf/2601.02281.pdf",
      "authors": "Shuai Yuan, Yantai Yang, Xiaotian Yang, Xupeng Zhang, Zhonghao Zhao",
      "institution": "",
      "abstract": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Visual geometry transformer",
      "citations": 0,
      "upvotes": 23,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02427",
      "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
      "link": "https://huggingface.co/papers/2601.02427",
      "pdf_link": "https://arxiv.org/pdf/2601.02427.pdf",
      "authors": "Loïc Magne, Anas Awadalla, Guanzhi Wang, Yinzhen Xu, Joshua Belofsky",
      "institution": "",
      "abstract": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-04",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Generalist gaming agent",
      "citations": 0,
      "upvotes": 22,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.03044",
      "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
      "link": "https://huggingface.co/papers/2601.03044",
      "pdf_link": "https://arxiv.org/pdf/2601.03044.pdf",
      "authors": "Mingjie Pan, Siyuan Feng, Qinglin Zhang, Xinchen Li, Jianheng Song",
      "institution": "",
      "abstract": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Scalable post-training",
      "citations": 0,
      "upvotes": 20,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.01874",
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "link": "https://huggingface.co/papers/2601.01874",
      "pdf_link": "https://arxiv.org/pdf/2601.01874.pdf",
      "authors": "Shuhang Chen, Yunqiu Xu, Junjie Xie, Aojun Lu, Tao Feng",
      "institution": "",
      "abstract": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Perception improvement",
      "citations": 0,
      "upvotes": 16,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02356",
      "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
      "link": "https://huggingface.co/papers/2601.02356",
      "pdf_link": "https://arxiv.org/pdf/2601.02356.pdf",
      "authors": "Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang",
      "institution": "",
      "abstract": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Text-instructed transformation",
      "citations": 0,
      "upvotes": 12,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2512.23412",
      "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
      "link": "https://huggingface.co/papers/2512.23412",
      "pdf_link": "https://arxiv.org/pdf/2512.23412.pdf",
      "authors": "Jiawei Chen, Xintian Shen, Lihao Zheng, Zhenwei Shao, Hongyuan Zhang",
      "institution": "",
      "abstract": "Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.",
      "source": "HuggingFace",
      "pubDateISO": "2025-12-29",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Multimodal reasoning",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.01592",
      "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
      "link": "https://huggingface.co/papers/2601.01592",
      "pdf_link": "https://arxiv.org/pdf/2601.01592.pdf",
      "authors": "Xin Wang, Yunhao Chen, Juncheng Li, Yixu Wang, Yang Yao",
      "institution": "",
      "abstract": "The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-04",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Useful red-teaming",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02267",
      "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
      "link": "https://huggingface.co/papers/2601.02267",
      "pdf_link": "https://arxiv.org/pdf/2601.02267.pdf",
      "authors": "Renke Wang, Zhenyu Zhang, Ying Tai, Jian Yang",
      "institution": "",
      "abstract": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 7,
      "score_reason": "Advances human mesh recovery",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02439",
      "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
      "link": "https://huggingface.co/papers/2601.02439",
      "pdf_link": "https://arxiv.org/pdf/2601.02439.pdf",
      "authors": "Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead",
      "institution": "",
      "abstract": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Useful dataset",
      "citations": 0,
      "upvotes": 6,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02989",
      "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
      "link": "https://huggingface.co/papers/2601.02989",
      "pdf_link": "https://arxiv.org/pdf/2601.02989.pdf",
      "authors": "Hosein Hasani, Mohammadali Banayeeanzade, Ali Nafisi, Sadegh Mohammadian, Fatemeh Askari",
      "institution": "",
      "abstract": "Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Interpretability insights",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2512.23035",
      "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
      "link": "https://huggingface.co/papers/2512.23035",
      "pdf_link": "https://arxiv.org/pdf/2512.23035.pdf",
      "authors": "Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang",
      "institution": "",
      "abstract": "Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.",
      "source": "HuggingFace",
      "pubDateISO": "2025-12-28",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Stable semi-supervised segmentation",
      "citations": 0,
      "upvotes": 4,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.01576",
      "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
      "link": "https://huggingface.co/papers/2601.01576",
      "pdf_link": "https://arxiv.org/pdf/2601.01576.pdf",
      "authors": "Ming Zhang, Kexin Tan, Yueyuan Huang, Yujiong Shen, Chunchun Ma",
      "institution": "",
      "abstract": "Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, OpenNovelty grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-04",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "LLM application shown",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2512.21472",
      "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
      "link": "https://huggingface.co/papers/2512.21472",
      "pdf_link": "https://arxiv.org/pdf/2512.21472.pdf",
      "authors": "Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh",
      "institution": "",
      "abstract": "Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.",
      "source": "HuggingFace",
      "pubDateISO": "2025-12-25",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "New dataset",
      "citations": 0,
      "upvotes": 2,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02075",
      "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
      "link": "https://huggingface.co/papers/2601.02075",
      "pdf_link": "https://arxiv.org/pdf/2601.02075.pdf",
      "authors": "Zhuofan Shi, Hubao A, Yufei Shao, Mengyan Dai, Yadong Yu",
      "institution": "",
      "abstract": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Practical application",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.03127",
      "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
      "link": "https://huggingface.co/papers/2601.03127",
      "pdf_link": "https://arxiv.org/pdf/2601.03127.pdf",
      "authors": "Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao",
      "institution": "",
      "abstract": "Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Modular core design",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.03227",
      "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
      "link": "https://huggingface.co/papers/2601.03227",
      "pdf_link": "https://arxiv.org/pdf/2601.03227.pdf",
      "authors": "Ruixing Zhang, Zihan Liu, Leilei Sun, Tongyu Zhu, Weifeng Lv",
      "institution": "",
      "abstract": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "New benchmark",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.03194",
      "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
      "link": "https://huggingface.co/papers/2601.03194",
      "pdf_link": "https://arxiv.org/pdf/2601.03194.pdf",
      "authors": "Mohammad Zia Ur Rehman, Sai Kartheek Reddy Kasu, Shashivardhan Reddy Koppula, Sai Rithwik Reddy Chirra, Shwetank Shekhar Singh",
      "institution": "",
      "abstract": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Explainable framework",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.01584",
      "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
      "link": "https://huggingface.co/papers/2601.01584",
      "pdf_link": "https://arxiv.org/pdf/2601.01584.pdf",
      "authors": "Jakub Hoscilowicz",
      "institution": "",
      "abstract": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). A central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents a significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-04",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Theoretical analysis",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02315",
      "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
      "link": "https://huggingface.co/papers/2601.02315",
      "pdf_link": "https://arxiv.org/pdf/2601.02315.pdf",
      "authors": "Saurabh Kaushik, Lalit Maurya, Beth Tellman",
      "institution": "",
      "abstract": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on https://github.com/Sk-2103/Prithvi-CAFE{Prithvi-CAFE Github}",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Improves flood mapping",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2512.22877",
      "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
      "link": "https://huggingface.co/papers/2512.22877",
      "pdf_link": "https://arxiv.org/pdf/2512.22877.pdf",
      "authors": "Ju-Hsuan Weng, Jia-Wei Liao, Cheng-Fu Chou, Jun-Cheng Chen",
      "institution": "",
      "abstract": "Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.",
      "source": "HuggingFace",
      "pubDateISO": "2025-12-28",
      "tags": [],
      "topics": [],
      "score": 6,
      "score_reason": "Comprehensive benchmark",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.01739",
      "title": "K-EXAONE Technical Report",
      "link": "https://huggingface.co/papers/2601.01739",
      "pdf_link": "https://arxiv.org/pdf/2601.01739.pdf",
      "authors": "Eunbi Choi, Kibong Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon",
      "institution": "",
      "abstract": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Large model presented",
      "citations": 0,
      "upvotes": 66,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.01554",
      "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
      "link": "https://huggingface.co/papers/2601.01554",
      "pdf_link": "https://arxiv.org/pdf/2601.01554.pdf",
      "authors": "MOSI. AI, Donghua Yu, Zhengyuan Lin, Chen Yang, Yiyang Zhang",
      "institution": "",
      "abstract": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-04",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Incremental improvement",
      "citations": 0,
      "upvotes": 46,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.01426",
      "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
      "link": "https://huggingface.co/papers/2601.01426",
      "pdf_link": "https://arxiv.org/pdf/2601.01426.pdf",
      "authors": "Chaofan Tao, Jierun Chen, Yuxin Jiang, Kaiqi Kou, Shaowei Wang",
      "institution": "",
      "abstract": "We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-04",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Fine-tuning recipe",
      "citations": 0,
      "upvotes": 17,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02179",
      "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
      "link": "https://huggingface.co/papers/2601.02179",
      "pdf_link": "https://arxiv.org/pdf/2601.02179.pdf",
      "authors": "Caiqi Zhang, Ruihan Yang, Xiaochen Zhu, Chengzu Li, Tiancheng Hu",
      "institution": "",
      "abstract": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Confidence estimation",
      "citations": 0,
      "upvotes": 10,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.01321",
      "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
      "link": "https://huggingface.co/papers/2601.01321",
      "pdf_link": "https://arxiv.org/pdf/2601.01321.pdf",
      "authors": "Rong Zhou, Dongping Chen, Zihan Jia, Yao Su, Yixin Liu",
      "institution": "",
      "abstract": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-04",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Framework presentation",
      "citations": 0,
      "upvotes": 8,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.01720",
      "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
      "link": "https://huggingface.co/papers/2601.01720",
      "pdf_link": "https://arxiv.org/pdf/2601.01720.pdf",
      "authors": "Xijie Huang, Chengming Xu, Donghao Luo, Xiaobin Hu, Peng Tang",
      "institution": "",
      "abstract": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Scaling video editing",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2512.23950",
      "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
      "link": "https://huggingface.co/papers/2512.23950",
      "pdf_link": "https://arxiv.org/pdf/2512.23950.pdf",
      "authors": "Huibin Li, Haoran Liu, Mingzhe Liu, Yulong Xiao, Peng Li",
      "institution": "",
      "abstract": "Image dehazing is a critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with long-range dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an innovative architecture that integrates a U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with a smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN.",
      "source": "HuggingFace",
      "pubDateISO": "2025-12-30",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Incremental dehazing",
      "citations": 1,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.03153",
      "title": "Parallel Latent Reasoning for Sequential Recommendation",
      "link": "https://huggingface.co/papers/2601.03153",
      "pdf_link": "https://arxiv.org/pdf/2601.03153.pdf",
      "authors": "Jiakai Tang, Xu Chen, Wen Chen, Jian Wu, Yuning Jiang",
      "institution": "",
      "abstract": "Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose Parallel Latent Reasoning (PLR), a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Incremental recommendation",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.03926",
      "title": "Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models",
      "link": "https://huggingface.co/papers/2601.03926",
      "pdf_link": "https://arxiv.org/pdf/2601.03926.pdf",
      "authors": "Haeun Jang, Hwan Chang, Hwanhee Lee",
      "institution": "",
      "abstract": "The deployment of Large Vision-Language Models (LVLMs) for real-world document question answering is often constrained by dynamic, user-defined policies that dictate information disclosure based on context. While ensuring adherence to these explicit constraints is critical, existing safety research primarily focuses on implicit social norms or text-only settings, overlooking the complexities of multimodal documents. In this paper, we introduce Doc-PP (Document Policy Preservation Benchmark), a novel benchmark constructed from real-world reports requiring reasoning across heterogeneous visual and textual elements under strict non-disclosure policies. Our evaluation highlights a systemic Reasoning-Induced Safety Gap: models frequently leak sensitive information when answers must be inferred through complex synthesis or aggregated across modalities, effectively circumventing existing safety constraints. Furthermore, we identify that providing extracted text improves perception but inadvertently facilitates leakage. To address these vulnerabilities, we propose DVA (Decompose-Verify-Aggregation), a structural inference framework that decouples reasoning from policy verification. Experimental results demonstrate that DVA significantly outperforms standard prompting defenses, offering a robust baseline for policy-compliant document understanding",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-07",
      "tags": [],
      "topics": [],
      "score": 5,
      "score_reason": "Benchmark introduction",
      "citations": 0,
      "upvotes": 0,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02785",
      "title": "DreamStyle: A Unified Framework for Video Stylization",
      "link": "https://huggingface.co/papers/2601.02785",
      "pdf_link": "https://arxiv.org/pdf/2601.02785.pdf",
      "authors": "Mengtian Li, Jinshu Chen, Songtao Zhao, Wanquan Feng, Pengqi Tu",
      "institution": "",
      "abstract": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Minor contribution",
      "citations": 0,
      "upvotes": 18,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02553",
      "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
      "link": "https://huggingface.co/papers/2601.02553",
      "pdf_link": "https://arxiv.org/pdf/2601.02553.pdf",
      "authors": "Jiaqi Liu, Yaofeng Su, Peng Xia, Siwei Han, Zeyu Zheng",
      "institution": "",
      "abstract": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) Semantic Structured Compression, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) Recursive Memory Consolidation, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) Adaptive Query-Aware Retrieval, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-05",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Minor memory improvement",
      "citations": 0,
      "upvotes": 14,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.02996",
      "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
      "link": "https://huggingface.co/papers/2601.02996",
      "pdf_link": "https://arxiv.org/pdf/2601.02996.pdf",
      "authors": "Yihong Liu, Raoyuan Zhao, Hinrich Schütze, Michael A. Hedderich",
      "institution": "",
      "abstract": "Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.",
      "source": "HuggingFace",
      "pubDateISO": "2026-01-06",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Analysis of limitations",
      "citations": 0,
      "upvotes": 3,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    },
    {
      "id": "2601.00863",
      "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
      "link": "https://huggingface.co/papers/2601.00863",
      "pdf_link": "https://arxiv.org/pdf/2601.00863.pdf",
      "authors": "Markus J. Buehler",
      "institution": "",
      "abstract": "We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
      "source": "HuggingFace",
      "pubDateISO": "2025-12-30",
      "tags": [],
      "topics": [],
      "score": 4,
      "score_reason": "Creative framework",
      "citations": 0,
      "upvotes": 1,
      "github_url": "",
      "github_stars": 0,
      "key_contribution": "",
      "why_it_matters": "",
      "limitations": ""
    }
  ]
}