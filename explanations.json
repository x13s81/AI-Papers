{
  "2512.24617": {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Latent Representations",
          "brief": "Hidden, abstract representations of data used for modeling"
        },
        {
          "name": "Semantic Boundaries",
          "brief": "Divisions between concepts or ideas in language"
        },
        {
          "name": "Hierarchical Language Modeling",
          "brief": "Modeling language using multiple levels of abstraction"
        },
        {
          "name": "Compression-aware Scaling Law",
          "brief": "A principle for allocating computational resources based on compression ratio"
        },
        {
          "name": "Decoupled Î¼P Parametrization",
          "brief": "A method for training heterogeneous architectures with stable hyperparameters"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of LLMs"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of DLCM"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To understand the concept of information density and compression"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To appreciate the implications of compute allocation and FLOPs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the principles of model training and hyperparameter tuning"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-06"
  },
  "2512.25070": {
    "title": "Scaling Open-Ended Reasoning to Predict the Future",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Open-Ended Reasoning",
          "brief": "The ability of a model to reason and make predictions about uncertain future events"
        },
        {
          "name": "Language Models",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves training models through trial and error using rewards or penalties"
        },
        {
          "name": "Forecasting",
          "brief": "The process of making predictions about future events or trends"
        },
        {
          "name": "Data Curation",
          "brief": "The process of selecting, cleaning, and preparing data for use in a model or analysis"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand how language models are trained and evaluated"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend how language models generate and process human-like language"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the architecture and training of large language models like OpenForecaster 8B"
        },
        {
          "topic": "Data Science",
          "why_needed": "To understand data curation, preprocessing, and analysis techniques used in the paper"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To appreciate the broader context and applications of open-ended reasoning and forecasting"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-01-06"
  },
  "2512.23343": {
    "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Cognitive Neuroscience",
          "brief": "The study of the neural basis of cognition and behavior"
        },
        {
          "name": "Artificial Intelligence (AI)",
          "brief": "The development of computer systems that can perform tasks that typically require human intelligence"
        },
        {
          "name": "Memory Systems",
          "brief": "The mechanisms by which humans and AI systems store, retrieve, and utilize information"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "A type of AI model that processes and generates human-like language"
        },
        {
          "name": "Autonomous Agents",
          "brief": "AI systems that can perform tasks independently with minimal human intervention"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Neuroscience",
          "why_needed": "To understand human memory mechanisms and their potential applications in AI"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To comprehend the current state of AI systems and their limitations in terms of memory and cognition"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the technical aspects of AI systems and their implementation"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To grasp the concepts of human cognition and memory and their relevance to AI systems"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the role of machine learning in the development of AI systems and their memory mechanisms"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-01-06"
  },
  "2512.22905": {
    "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and generate multiple forms of data, such as text, audio, and video"
        },
        {
          "name": "Encoder-LLM-Decoder Architecture",
          "brief": "A design pattern for MLLMs that consists of an encoder, a large language model, and a decoder"
        },
        {
          "name": "Spatio-Temporal Audio-Video Fusion",
          "brief": "The process of combining audio and video data in both space and time to enable multimodal understanding"
        },
        {
          "name": "SyncFusion Module",
          "brief": "A component of JavisGPT that enables spatio-temporal audio-video fusion and synchrony-aware learnable queries"
        },
        {
          "name": "Multimodal Pretraining, Fine-Tuning, and Instruction-Tuning",
          "brief": "A three-stage training pipeline for building multimodal comprehension and generation capabilities in MLLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of JavisGPT"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the language-related aspects of JavisGPT, such as text generation and comprehension"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the video-related aspects of JavisGPT, such as video comprehension and generation"
        },
        {
          "topic": "Audio Processing",
          "why_needed": "To comprehend the audio-related aspects of JavisGPT, such as audio comprehension and generation"
        },
        {
          "topic": "Multimodal Learning",
          "why_needed": "To understand how JavisGPT integrates multiple forms of data, such as text, audio, and video"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-01-06"
  },
  "2512.24297": {
    "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Active Visual Thinking",
          "brief": "The process of using visual representations to enhance reasoning and problem-solving abilities"
        },
        {
          "name": "Multi-turn Reasoning",
          "brief": "A type of reasoning that involves multiple steps or turns to arrive at a solution"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach that involves training agents to make decisions based on rewards or penalties"
        },
        {
          "name": "Multimodal Reasoning",
          "brief": "The ability to reason using multiple forms of input, such as text and visual representations"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of the research and the capabilities of current reasoning models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the reinforcement learning approach used in FIGR"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how visual representations are constructed and used in the reasoning process"
        },
        {
          "topic": "Mathematical Reasoning",
          "why_needed": "To appreciate the challenges of representing global structural constraints in complex mathematical problems"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-06"
  },
  "2601.02358": {
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Models",
          "brief": "A type of deep learning model used for generating and editing visual content"
        },
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that combine visual and textual information for tasks like image and video generation"
        },
        {
          "name": "Multimodal Diffusion Transformers (MMDiT)",
          "brief": "A type of transformer model that handles multiple input modalities, such as text, images, and videos"
        },
        {
          "name": "Interleaved OmniModal Context",
          "brief": "A technique for conditioning on multiple input modalities in a unified framework"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning Fundamentals",
          "why_needed": "To understand the basics of neural networks and their applications in computer vision and natural language processing"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand image and video processing, generation, and editing techniques"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand text encoding, language models, and their applications in vision-language tasks"
        },
        {
          "topic": "Transformer Models",
          "why_needed": "To understand the architecture and applications of transformer models in handling sequential data"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-06"
  },
  "2601.00747": {
    "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Bootstrapped Reasoning Loops",
          "brief": "A method used in LLMs to generate and reinforce diverse chains of thought"
        },
        {
          "name": "Distributional Creative Reasoning (DCR)",
          "brief": "A unified variational objective for training LLMs to balance correctness and creativity"
        },
        {
          "name": "Variational Objective",
          "brief": "A mathematical framework for optimizing probability distributions"
        },
        {
          "name": "Gradient Flow",
          "brief": "A mathematical concept used to optimize functions and models"
        },
        {
          "name": "Entropy Bonuses",
          "brief": "Techniques used to encourage exploration and diversity in model outputs"
        },
        {
          "name": "STaR, GRPO, and DPO",
          "brief": "Specific methods for training LLMs, now understood as special cases of the DCR framework"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence and Machine Learning",
          "why_needed": "To understand the context and applications of LLMs"
        },
        {
          "topic": "Probability Theory and Statistics",
          "why_needed": "To comprehend the mathematical concepts underlying DCR and variational objectives"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand how gradient flow and entropy bonuses are used in model training"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges and opportunities in creative problem-solving with LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-06"
  },
  "2512.24601": {
    "title": "Recursive Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Recursive Language Models (RLMs)",
          "brief": "Inference strategy for LLMs to handle long prompts by decomposing and recursively processing them"
        },
        {
          "name": "Inference-time scaling",
          "brief": "Technique to improve model performance by scaling up or down during inference"
        },
        {
          "name": "Long-context tasks",
          "brief": "Tasks that require processing long sequences of text or input"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "Understanding of NLP fundamentals is necessary to grasp the concepts of LLMs and RLMs"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Knowledge of deep learning architectures and techniques is required to understand the implementation of LLMs and RLMs"
        },
        {
          "topic": "Computer Science",
          "why_needed": "Basic understanding of computer science concepts such as algorithms, data structures, and software design is necessary to comprehend the technical aspects of the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-06"
  },
  "2601.03252": {
    "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Neural Implicit Fields",
          "brief": "A representation technique that allows for continuous and detailed modeling of 3D scenes"
        },
        {
          "name": "Depth Estimation",
          "brief": "The process of predicting the distance of objects from a camera in a scene"
        },
        {
          "name": "Arbitrary-Resolution Depth Estimation",
          "brief": "The ability to predict depth at any desired resolution, not limited to discrete image grids"
        },
        {
          "name": "Fine-Grained Depth Estimation",
          "brief": "The ability to recover detailed geometric information in a scene"
        },
        {
          "name": "Local Implicit Decoder",
          "brief": "A technique used to query depth at continuous 2D coordinates"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding of image and video processing, 3D reconstruction, and depth estimation techniques"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with neural networks, deep learning architectures, and training methods"
        },
        {
          "topic": "Geometry and 3D Modeling",
          "why_needed": "Knowledge of 3D scene representation, geometric details, and novel view synthesis"
        },
        {
          "topic": "Image Processing",
          "why_needed": "Understanding of image grids, resolution, and artifact removal"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.02427": {
    "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Foundation Models",
          "brief": "Large-scale models trained on diverse data to perform a wide range of tasks"
        },
        {
          "name": "Vision-Action Models",
          "brief": "Models that process visual inputs and generate actions as outputs"
        },
        {
          "name": "Behavior Cloning",
          "brief": "A machine learning technique for training agents to mimic human behavior"
        },
        {
          "name": "Cross-Game Generalization",
          "brief": "The ability of a model to perform well across multiple games or environments"
        },
        {
          "name": "Embodied Agents",
          "brief": "Agents that interact with and perceive their environment through sensors and actuators"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of NitroGen"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the processing of visual inputs from gameplay videos"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To grasp the concept of training agents to perform tasks through trial and error"
        },
        {
          "topic": "Game Development and Game Theory",
          "why_needed": "To understand the diverse range of games and tasks that NitroGen is trained on"
        },
        {
          "topic": "Machine Learning and Artificial Intelligence",
          "why_needed": "To appreciate the broader context and applications of generalist gaming agents"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.03233": {
    "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Text-to-Video Diffusion Models",
          "brief": "Models that generate video sequences from text prompts"
        },
        {
          "name": "Asymmetric Dual-Stream Transformer",
          "brief": "Architecture used for joint audio-visual processing"
        },
        {
          "name": "Cross-Modality AdaLN",
          "brief": "Technique for shared timestep conditioning across audio and video streams"
        },
        {
          "name": "Modality-Aware Classifier-Free Guidance (Modality-CFG)",
          "brief": "Mechanism for improved audiovisual alignment and controllability"
        },
        {
          "name": "Multilingual Text Encoder",
          "brief": "Encoder used for broader prompt understanding"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of neural networks and transformer architectures"
        },
        {
          "topic": "Audio-Visual Processing",
          "why_needed": "Knowledge of audio and video signal processing techniques"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Familiarity with text encoding and language models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding of video generation and image processing techniques"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Knowledge of model training, evaluation, and optimization techniques"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.02281": {
    "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Visual Geometry Grounded Transformer (VGGT)",
          "brief": "A model that enables 3D visual geometry understanding"
        },
        {
          "name": "Infinite-Horizon Inputs",
          "brief": "The ability to process continuous, endless streams of data"
        },
        {
          "name": "Rolling Memory",
          "brief": "A concept that allows for efficient, adaptive storage and retrieval of information in a transformer model"
        },
        {
          "name": "KV Cache",
          "brief": "A cache that stores key-value pairs to facilitate efficient information retrieval"
        },
        {
          "name": "Attention-Agnostic Pruning Strategy",
          "brief": "A method to discard obsolete information without relying on attention mechanisms"
        },
        {
          "name": "FlashAttention",
          "brief": "A technique to improve the efficiency of attention mechanisms in transformer models"
        },
        {
          "name": "Long3D Benchmark",
          "brief": "A benchmark for evaluating continuous 3D geometry estimation over long sequences"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Transformer Models",
          "why_needed": "To understand the architecture and components of the InfiniteVGGT model"
        },
        {
          "topic": "3D Visual Geometry",
          "why_needed": "To comprehend the application and requirements of the InfiniteVGGT model"
        },
        {
          "topic": "Streaming Architectures",
          "why_needed": "To appreciate the challenges and limitations of existing methods for live systems"
        },
        {
          "topic": "Attention Mechanisms",
          "why_needed": "To understand the role of attention in transformer models and the novelty of the attention-agnostic pruning strategy"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the fundamental concepts and techniques used in the development of the InfiniteVGGT model"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.01836": {
    "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Organization-Specific Policy Alignment",
          "brief": "Ensuring LLMs comply with company-specific rules and regulations"
        },
        {
          "name": "Allowlist and Denylist Policies",
          "brief": "Lists of approved and prohibited actions or content"
        },
        {
          "name": "Adversarial Robustness",
          "brief": "Ability of LLMs to withstand strategically designed edge cases or attacks"
        },
        {
          "name": "COMPASS Framework",
          "brief": "A systematic framework for evaluating LLM policy alignment"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence and Machine Learning",
          "why_needed": "To understand the basics of LLMs and their applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend how LLMs process and generate human-like language"
        },
        {
          "topic": "Computer Security and Policy Compliance",
          "why_needed": "To recognize the importance of policy alignment in high-stakes enterprise applications"
        },
        {
          "topic": "Data Science and Evaluation Metrics",
          "why_needed": "To understand the methodology and results of the COMPASS framework evaluation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.02346": {
    "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Hybrid Model",
          "brief": "A model that combines different architectures or techniques to achieve better performance"
        },
        {
          "name": "Reasoning-Optimized Model",
          "brief": "A model designed to improve reasoning capabilities, such as logical and abstract thinking"
        },
        {
          "name": "Small Language Models (SLMs)",
          "brief": "Language models with fewer parameters, designed to be more efficient and compact"
        },
        {
          "name": "Test-Time Scaling",
          "brief": "The ability of a model to scale its performance during testing or inference time"
        },
        {
          "name": "DeepConf Approach",
          "brief": "A method for improving test-time scaling efficiency in deep learning models"
        },
        {
          "name": "Chain-of-Thoughts Generation",
          "brief": "The ability of a model to generate a sequence of thoughts or reasoning steps to solve a problem"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and model architectures"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the concepts of language models and their applications"
        },
        {
          "topic": "Model Efficiency and Scaling",
          "why_needed": "To appreciate the importance of efficient model design and test-time scaling"
        },
        {
          "topic": "Reasoning and Problem-Solving",
          "why_needed": "To understand the concepts of reasoning and how they are applied in AI models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2512.24695": {
    "title": "Nested Learning: The Illusion of Deep Learning Architectures",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Nested Learning (NL)",
          "brief": "A new learning paradigm representing machine learning models as nested, multi-level optimization problems"
        },
        {
          "name": "In-Context Learning",
          "brief": "A type of learning where models learn from data by compressing their own context flow"
        },
        {
          "name": "Expressive Optimizers",
          "brief": "Optimizers with deep memory and powerful learning rules for more effective learning"
        },
        {
          "name": "Self-Modifying Learning Module",
          "brief": "A sequence model that learns to modify itself by learning its own update algorithm"
        },
        {
          "name": "Continuum Memory System",
          "brief": "A system for effective continual learning capabilities"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the limitations of current deep learning architectures and the need for new learning paradigms"
        },
        {
          "topic": "Optimization Algorithms",
          "why_needed": "To comprehend the concept of expressive optimizers and their role in Nested Learning"
        },
        {
          "topic": "Language Models",
          "why_needed": "To appreciate the challenges of continual learning and memorization in large models"
        },
        {
          "topic": "Machine Learning Fundamentals",
          "why_needed": "To grasp the basics of learning algorithms, context flow, and gradient descent"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.00830": {
    "title": "Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Chain-of-Thought Reasoning",
          "brief": "A method of explaining AI decision-making by breaking down the reasoning process into steps"
        },
        {
          "name": "Explainability in AI",
          "brief": "The ability of AI systems to provide insights into their decision-making processes"
        },
        {
          "name": "Systematic Underreporting",
          "brief": "The phenomenon of AI models not reporting all the factors that influence their decisions"
        },
        {
          "name": "Adversarial Testing",
          "brief": "A method of testing AI models by providing them with misleading or deceptive input"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the basics of AI systems and their decision-making processes"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend how AI models are trained and how they generate explanations"
        },
        {
          "topic": "Cognitive Biases",
          "why_needed": "To recognize the potential biases in AI decision-making and the importance of explainability"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To understand how users interact with AI systems and the need for transparent explanations"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.02439": {
    "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning approach where agents learn by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Visual Web Agents",
          "brief": "AI models that can interact with and understand visual web content"
        },
        {
          "name": "WebGym Environment",
          "brief": "An open-source environment for training visual web agents with realistic tasks"
        },
        {
          "name": "Asynchronous Rollout System",
          "brief": "A high-throughput system for speeding up the sampling of trajectories in WebGym"
        },
        {
          "name": "Vision-Language Models",
          "brief": "AI models that can understand and generate both visual and textual content"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning Fundamentals",
          "why_needed": "To understand the basics of reinforcement learning and vision-language models"
        },
        {
          "topic": "Web Development and HTML/CSS/JavaScript",
          "why_needed": "To comprehend how web agents interact with web content"
        },
        {
          "topic": "Deep Learning and Neural Networks",
          "why_needed": "To understand the architecture and training of vision-language models"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To grasp the language understanding aspects of visual web agents"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the visual perception aspects of visual web agents"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.01720": {
    "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "First-Frame Propagation (FFP)",
          "brief": "A paradigm for controllable video editing"
        },
        {
          "name": "Adaptive Spatio-Temporal RoPE (AST-RoPE)",
          "brief": "A novel architecture for disentangling appearance and motion references"
        },
        {
          "name": "Self-distillation strategy",
          "brief": "A technique for ensuring long-term temporal stability and preventing semantic drift"
        },
        {
          "name": "FFP-300K dataset",
          "brief": "A large-scale dataset for training FFP models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the proposed framework and architecture"
        },
        {
          "topic": "Computer vision",
          "why_needed": "To comprehend video editing and processing concepts"
        },
        {
          "topic": "Video editing",
          "why_needed": "To grasp the application and significance of FFP"
        },
        {
          "topic": "Dataset construction and curation",
          "why_needed": "To appreciate the importance of the FFP-300K dataset"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.01426": {
    "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Supervised Fine-tuning (SFT)",
          "brief": "A machine learning approach to fine-tune pre-trained models on specific tasks"
        },
        {
          "name": "Software Engineering (SWE)",
          "brief": "The application of engineering principles to develop, test, and maintain software systems"
        },
        {
          "name": "Test-Time Scaling (TTS)",
          "brief": "A technique to improve model performance by scaling up the model during testing"
        },
        {
          "name": "Curriculum Learning",
          "brief": "A training strategy that gradually increases the difficulty of the training data to improve model performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of supervised fine-tuning, model training, and test-time scaling"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To comprehend the context and applications of the SWE-Lego approach"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the fundamentals of model training, evaluation, and optimization"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the potential applications of the SWE-Lego approach in NLP tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.01584": {
    "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Instrumental-Convergence Tendencies",
          "brief": "The tendency of AI systems to converge towards certain behaviors or outcomes"
        },
        {
          "name": "Steerability",
          "brief": "The ability to reliably shift the behavior of an AI system towards intended outcomes"
        },
        {
          "name": "Capability and Steerability Tradeoff",
          "brief": "The potential tradeoff between an AI system's capability and its steerability"
        },
        {
          "name": "Authorized and Unauthorized Steerability",
          "brief": "The distinction between steerability by authorized entities (builders) and unauthorized entities (attackers)"
        },
        {
          "name": "Safety-Security Dilemma",
          "brief": "The tension between ensuring the safety and security of AI systems"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and implications of instrumental-convergence tendencies"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of capability, steerability, and the tradeoffs involved"
        },
        {
          "topic": "AI Safety and Security",
          "why_needed": "To appreciate the significance of the safety-security dilemma and its relevance to AI system design"
        },
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the specific challenges and opportunities related to LLMs, such as Qwen3 and InstrumentalEval"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.03193": {
    "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Unified Multimodal Models (UMMs)",
          "brief": "Models that can process and understand multiple forms of data, such as text and images"
        },
        {
          "name": "Conduction Aphasia",
          "brief": "The phenomenon where models can interpret multimodal inputs but struggle to generate faithful and controllable outputs"
        },
        {
          "name": "Self-Generated Supervision",
          "brief": "A technique where a model generates its own supervision signals for self-improvement"
        },
        {
          "name": "Self-Play",
          "brief": "A method where a model interacts with itself to generate new data and improve its performance"
        },
        {
          "name": "Cognitive Pattern Reconstruction",
          "brief": "A technique used to distill latent understanding into explicit generative signals"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and how they can be applied to multimodal tasks"
        },
        {
          "topic": "Multimodal Processing",
          "why_needed": "To comprehend how different forms of data can be processed and integrated"
        },
        {
          "topic": "Self-Supervised Learning",
          "why_needed": "To understand how models can learn from themselves without external supervision"
        },
        {
          "topic": "Computer Vision and Natural Language Processing",
          "why_needed": "To understand the specific applications of UMMs in image and text generation tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2512.22334": {
    "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Scientific General Intelligence",
          "brief": "The ability of AI models to understand and apply scientific knowledge across various disciplines"
        },
        {
          "name": "Multimodal Perception",
          "brief": "The ability of AI models to process and understand multiple forms of data, such as text, images, and audio"
        },
        {
          "name": "Multimodal Reasoning",
          "brief": "The ability of AI models to draw conclusions and make decisions based on multiple forms of data"
        },
        {
          "name": "Scientific Symbolic Reasoning",
          "brief": "The ability of AI models to use symbols and logical rules to reason about scientific concepts"
        },
        {
          "name": "Code Generation",
          "brief": "The ability of AI models to generate code for scientific tasks, such as data analysis or simulation"
        },
        {
          "name": "Hypothesis Generation",
          "brief": "The ability of AI models to generate scientific hypotheses based on data and knowledge"
        },
        {
          "name": "Knowledge Understanding",
          "brief": "The ability of AI models to understand and apply scientific knowledge in a specific domain"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and capabilities of AI models"
        },
        {
          "topic": "Scientific Disciplines",
          "why_needed": "To understand the various domains and tasks that SciEvalKit supports, such as physics, chemistry, and astronomy"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of model evaluation, benchmarking, and dataset curation"
        },
        {
          "topic": "Data Science",
          "why_needed": "To understand the importance of data quality, dataset integration, and reproducibility in scientific research"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.01874": {
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models",
          "brief": "Models that process and understand multiple forms of data, such as text and images"
        },
        {
          "name": "Visual Mathematical Problem Solving",
          "brief": "The ability of models to solve mathematical problems that involve visual elements, such as diagrams and symbols"
        },
        {
          "name": "Knowledge Internalization",
          "brief": "The process of integrating and utilizing extracted visual cues in subsequent reasoning"
        },
        {
          "name": "Cognitive-Inspired Frameworks",
          "brief": "Frameworks that mimic human cognition and reasoning processes"
        },
        {
          "name": "Synergistic Visual Rewards",
          "brief": "A method to improve perception capabilities in parametric and semantic spaces"
        },
        {
          "name": "Knowledge Internalization Reward Model",
          "brief": "A model that ensures faithful integration of extracted visual cues into subsequent reasoning"
        },
        {
          "name": "Visual-Gated Policy Optimization",
          "brief": "An algorithm that enforces reasoning to be grounded with visual knowledge"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of large language models and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the processing and understanding of visual elements"
        },
        {
          "topic": "Mathematical Problem Solving",
          "why_needed": "To understand the context and challenges of visual mathematical problem solving"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To appreciate the inspiration behind cognitive-informed frameworks and their potential applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the role of language models in processing and generating human-like text"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.03194": {
    "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Explainable AI",
          "brief": "A subfield of AI focused on making machine learning models transparent and interpretable"
        },
        {
          "name": "Hate Speech Detection",
          "brief": "The use of natural language processing to identify and classify hate speech in text data"
        },
        {
          "name": "Multilingual NLP",
          "brief": "The application of natural language processing techniques to multiple languages"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models trained on vast amounts of text data to generate human-like language"
        },
        {
          "name": "Attention Mechanisms",
          "brief": "Techniques used in deep learning to focus on specific parts of the input data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and techniques used in hate speech detection"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of LLMs and attention-enhancing techniques"
        },
        {
          "topic": "Linguistics",
          "why_needed": "To appreciate the challenges of working with under-resourced languages like Hindi and Telugu"
        },
        {
          "topic": "Machine Learning Evaluation Metrics",
          "why_needed": "To understand the metrics used to evaluate the performance of the X-MuTeST framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2512.22877": {
    "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Concept Erasure",
          "brief": "The process of removing or eliminating specific concepts or information from generated content, such as text or images."
        },
        {
          "name": "Diffusion Models",
          "brief": "A type of deep learning model used for generating high-quality images or text based on a given input or prompt."
        },
        {
          "name": "Multimodal Evaluation",
          "brief": "The process of evaluating the performance of a model or system using multiple types of input or data, such as text, images, or audio."
        },
        {
          "name": "Latent Space",
          "brief": "A high-dimensional space that represents the underlying structure or features of a dataset, often used in generative models."
        },
        {
          "name": "Cross-Attention",
          "brief": "A neural network mechanism that allows the model to attend to different parts of the input data and weigh their importance when generating output."
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of diffusion models and how they generate content."
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the concepts of image editing, generation, and manipulation."
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the ideas of text prompts, embeddings, and concept erasure in text-based applications."
        },
        {
          "topic": "Information Security",
          "why_needed": "To recognize the importance of concept erasure in preventing the generation of harmful or copyrighted content."
        },
        {
          "topic": "Machine Learning Evaluation Metrics",
          "why_needed": "To understand the metrics used to evaluate the performance of concept erasure methods, such as Concept Reproduction Rate (CRR)."
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.01739": {
    "title": "K-EXAONE Technical Report",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts Architecture",
          "brief": "A type of neural network architecture that uses multiple expert models to improve performance"
        },
        {
          "name": "Language Models",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Multilingual Support",
          "brief": "The ability of a model to support and understand multiple languages"
        },
        {
          "name": "Large-Scale Models",
          "brief": "AI models with a large number of parameters, requiring significant computational resources"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of large-scale language models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the applications and evaluations of language models"
        },
        {
          "topic": "Neural Network Architectures",
          "why_needed": "To appreciate the design and implementation of Mixture-of-Experts models"
        },
        {
          "topic": "Multilingualism in AI",
          "why_needed": "To recognize the challenges and benefits of supporting multiple languages in a single model"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.03256": {
    "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "3D Creature Generation",
          "brief": "Generating nonexistent fantasy 3D creatures without training data"
        },
        {
          "name": "Feed-forward Paradigm",
          "brief": "A method of generating output without iterative refinement"
        },
        {
          "name": "3D Skeleton",
          "brief": "A fundamental representation of biological forms used for composition and generation"
        },
        {
          "name": "Graph-constrained Reasoning",
          "brief": "A method of constructing a 3D skeleton with coherent layout and scale"
        },
        {
          "name": "Voxel-based Assembly",
          "brief": "A process of assembling 3D objects from small 3D units called voxels"
        },
        {
          "name": "Image-guided Appearance Modeling",
          "brief": "A technique of generating texture for a 3D object based on a reference image"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Graphics",
          "why_needed": "Understanding of 3D modeling and rendering techniques"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with concepts of training data and feed-forward paradigms"
        },
        {
          "topic": "Geometry and Spatial Reasoning",
          "why_needed": "Understanding of 3D spatial relationships and graph structures"
        },
        {
          "topic": "Image Processing",
          "why_needed": "Knowledge of image-guided modeling and texture generation techniques"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.02780": {
    "title": "MiMo-V2-Flash Technical Report",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts (MoE) model",
          "brief": "A type of neural network architecture that combines multiple expert models to achieve better performance"
        },
        {
          "name": "Hybrid attention architecture",
          "brief": "A combination of different attention mechanisms, such as Sliding Window Attention (SWA) and global attention, to improve model performance"
        },
        {
          "name": "Multi-Token Prediction (MTP)",
          "brief": "A training objective that predicts multiple tokens at once to improve model performance and efficiency"
        },
        {
          "name": "Multi-Teacher On-Policy Distillation (MOPD)",
          "brief": "A novel distillation paradigm that uses multiple teachers to provide dense and token-level rewards to the student model"
        },
        {
          "name": "Speculative decoding",
          "brief": "A technique that uses a draft model to generate initial predictions and then refines them to improve decoding speed and efficiency"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the basics of neural networks and their applications"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of language models like MiMo-V2-Flash"
        },
        {
          "topic": "Attention mechanisms",
          "why_needed": "To understand the different types of attention mechanisms used in the hybrid attention architecture"
        },
        {
          "topic": "Reinforcement learning",
          "why_needed": "To understand the concept of domain-specialized teachers and their training via large-scale reinforcement learning"
        },
        {
          "topic": "Model distillation",
          "why_needed": "To understand the concept of knowledge distillation and its application in MOPD"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.02359": {
    "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Deepfake Detection",
          "brief": "Techniques to identify manipulated media, particularly faces"
        },
        {
          "name": "Self-Supervised Learning",
          "brief": "Machine learning methods that don't require labeled data"
        },
        {
          "name": "Diffusion Models",
          "brief": "Probabilistic models that learn data distributions through iterative refinement"
        },
        {
          "name": "Audio-to-Expression Generation",
          "brief": "Generating facial expressions from audio inputs"
        },
        {
          "name": "Face Forgery Detection",
          "brief": "Identifying manipulated or fake faces in images or videos"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "Understanding of machine learning fundamentals, including supervised and self-supervised learning"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Knowledge of deep learning architectures and techniques, such as convolutional neural networks"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "Familiarity with audio and image processing techniques, including filtering and compression"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding of computer vision concepts, including face detection and recognition"
        },
        {
          "topic": "Information Security",
          "why_needed": "Awareness of security threats related to deepfakes and face forgery"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-07"
  },
  "2601.02204": {
    "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoregressive Transformers",
          "brief": "A type of neural network architecture that predicts the next token in a sequence"
        },
        {
          "name": "Multimodal Understanding and Generation",
          "brief": "The ability of a model to understand and generate multiple forms of data, such as text and images"
        },
        {
          "name": "Unified Decoder-Only Architecture",
          "brief": "A type of neural network architecture that uses a single decoder to generate multiple types of data"
        },
        {
          "name": "Next-Token Prediction and Next-Scale Prediction",
          "brief": "Techniques used for generating text and images, respectively"
        },
        {
          "name": "Reinforcement Learning and Prefix-Tuning",
          "brief": "Techniques used to fine-tune and improve the model's performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning Fundamentals",
          "why_needed": "To understand the basics of neural networks and their applications"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the concepts of text generation and understanding"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of image generation and understanding"
        },
        {
          "topic": "Transformer Architectures",
          "why_needed": "To understand the specifics of autoregressive transformer models"
        },
        {
          "topic": "Multimodal Learning",
          "why_needed": "To understand the challenges and techniques of learning from multiple forms of data"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.04194": {
    "title": "Choreographing a World of Dynamic Objects",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Dynamic Objects",
          "brief": "Objects that evolve, deform, and interact with other objects in a 4D scene"
        },
        {
          "name": "Generative Pipeline",
          "brief": "A system that generates synthetic data, such as CHORD"
        },
        {
          "name": "Lagrangian Motion",
          "brief": "A way of describing the motion of objects in terms of their individual trajectories"
        },
        {
          "name": "Eulerian Representations",
          "brief": "A way of describing the motion of objects in terms of the motion of the surrounding space"
        },
        {
          "name": "Distillation-based Pipeline",
          "brief": "A method of extracting information from one representation to another"
        },
        {
          "name": "Video Generative Models",
          "brief": "Models that generate synthetic video data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Graphics",
          "why_needed": "To understand the context of dynamic objects and scene synthesis"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concept of generative models and distillation-based pipelines"
        },
        {
          "topic": "Physics and Mechanics",
          "why_needed": "To understand the concepts of Lagrangian and Eulerian representations"
        },
        {
          "topic": "Robotics",
          "why_needed": "To understand the application of the proposed method in generating robotics manipulation policies"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.00501": {
    "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Contrastive Perception Policy Optimization (CPPO)",
          "brief": "A method for fine-tuning vision-language models using contrastive perception"
        },
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that process and understand both visual and linguistic inputs"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves training agents to make decisions based on rewards or penalties"
        },
        {
          "name": "Contrastive Perception Loss (CPL)",
          "brief": "A loss function that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of vision-language models and reinforcement learning"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the processing of visual inputs in VLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the processing of linguistic inputs in VLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the fundamentals of reinforcement learning and optimization techniques"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2512.23412": {
    "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Tool-Integrated Reasoning (TIR)",
          "brief": "A type of reasoning that involves autonomous invocation of external tools to aid in decision-making"
        },
        {
          "name": "Multimodal Chain-of-Thought (CoT) Reasoning",
          "brief": "A reasoning paradigm that involves manipulating multiple forms of data, such as images, to arrive at a conclusion"
        },
        {
          "name": "Interleaved Thinking",
          "brief": "A thinking paradigm that allows switching between thinking and tool invocation at any stage"
        },
        {
          "name": "Automated Data Auditing and Evaluation Pipelines",
          "brief": "A process for automatically evaluating and auditing data used in machine learning models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of TIR and multimodal CoT reasoning"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training and evaluation of MindWatcher and its auxiliary reasoning tools"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the image manipulation and object recognition capabilities of MindWatcher"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To appreciate the design and implementation of the MindWatcher system and its evaluation benchmark"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.01592": {
    "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and generate multiple forms of data, such as text and images"
        },
        {
          "name": "Red Teaming",
          "brief": "A methodology used to test the security and robustness of AI models by simulating adversarial attacks"
        },
        {
          "name": "Adversarial Kernel",
          "brief": "A component of the OpenRT framework that enables modular separation of key dimensions in red teaming"
        },
        {
          "name": "Attack Strategies",
          "brief": "Methods used to test the vulnerability of AI models, including white-box gradients and multi-modal perturbations"
        },
        {
          "name": "Evaluation Metrics",
          "brief": "Standards used to measure the performance and safety of AI models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of MLLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of model training, testing, and evaluation"
        },
        {
          "topic": "Computer Security",
          "why_needed": "To recognize the importance of red teaming and adversarial testing in AI model development"
        },
        {
          "topic": "Programming",
          "why_needed": "To understand the implementation and integration of the OpenRT framework"
        },
        {
          "topic": "Data Science",
          "why_needed": "To analyze and interpret the results of the empirical study on MLLM safety evaluation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.03044": {
    "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language-Action (VLA) Models",
          "brief": "AI models that integrate visual, linguistic, and action-based inputs to achieve generalization"
        },
        {
          "name": "Scalable Online Post-training (SOP) System",
          "brief": "A system for online, distributed, multi-task post-training of VLA models in real-world environments"
        },
        {
          "name": "Closed-Loop Architecture",
          "brief": "A design that couples execution and learning through continuous feedback and policy updates"
        },
        {
          "name": "Interactive Imitation Learning (HG-DAgger)",
          "brief": "A learning algorithm that enables robots to learn from human demonstrations and interventions"
        },
        {
          "name": "Reinforcement Learning (RECAP)",
          "brief": "A learning algorithm that enables robots to learn from trial and error through rewards and penalties"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the fundamentals of VLA models and post-training algorithms"
        },
        {
          "topic": "Robotics and Computer Vision",
          "why_needed": "To comprehend the application of VLA models in real-world environments"
        },
        {
          "topic": "Machine Learning and Artificial Intelligence",
          "why_needed": "To grasp the concepts of imitation learning, reinforcement learning, and online learning"
        },
        {
          "topic": "Distributed Systems and Cloud Computing",
          "why_needed": "To understand the design and implementation of the SOP system"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2512.24138": {
    "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Models",
          "brief": "A type of deep learning model used for generating images"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to take actions to achieve a goal"
        },
        {
          "name": "Reward Hacking",
          "brief": "A phenomenon where RL models optimize for a proxy reward instead of the true goal, leading to suboptimal results"
        },
        {
          "name": "GARDO (Gated and Adaptive Regularization with Diversity-aware Optimization)",
          "brief": "A proposed framework for mitigating reward hacking in RL"
        },
        {
          "name": "Regularization Techniques",
          "brief": "Methods used to prevent overfitting and improve model generalization"
        },
        {
          "name": "Mode Collapse",
          "brief": "A problem in RL where the model generates limited variations of the same output"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of diffusion models and RL"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of regularization, overfitting, and model optimization"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To appreciate the challenges of text-to-image alignment and image generation"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand the trade-offs between sample efficiency, exploration, and exploitation in RL"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.03986": {
    "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Benchmarks",
          "brief": "Standardized tests used to evaluate the performance of LLMs"
        },
        {
          "name": "Cross-Benchmark Ranking Consistency",
          "brief": "A metric to measure the consistency of model rankings across different benchmarks"
        },
        {
          "name": "Discriminability Score",
          "brief": "A metric to quantify a benchmark's ability to differentiate between models"
        },
        {
          "name": "Capability Alignment Deviation",
          "brief": "A metric to identify instances where stronger models fail but weaker models succeed within the same model family"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of LLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the complexities of human language and its processing by LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, evaluation, and benchmarking"
        },
        {
          "topic": "Statistics and Metrics",
          "why_needed": "To understand the metrics used to evaluate benchmark quality and LLM performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.03509": {
    "title": "Evolving Programmatic Skill Networks",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Programmatic Skill Network (PSN)",
          "brief": "A framework for continual skill acquisition in open-ended environments"
        },
        {
          "name": "REFLECT",
          "brief": "A mechanism for structured fault localization over skill compositions"
        },
        {
          "name": "Progressive Optimization",
          "brief": "A mechanism for stabilizing reliable skills while maintaining plasticity for uncertain ones"
        },
        {
          "name": "Canonical Structural Refactoring",
          "brief": "A mechanism for maintaining network compactness under rollback validation"
        },
        {
          "name": "Large Language Models",
          "brief": "Used to instantiate PSN's core mechanisms"
        },
        {
          "name": "Neural Network Training",
          "brief": "Exhibits structural parallels to PSN's learning dynamics"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of skill acquisition and open-ended environments"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of neural network training and large language models"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To grasp the fundamentals of programming and software development"
        },
        {
          "topic": "Cognitive Architectures",
          "why_needed": "To understand the concept of skill networks and their applications"
        },
        {
          "topic": "Embodied Cognition",
          "why_needed": "To appreciate the importance of open-ended environments in skill acquisition"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.03236": {
    "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Memory-Augmented Generation (MAG)",
          "brief": "A technique to extend Large Language Models with external memory for long-context reasoning"
        },
        {
          "name": "Multi-Graph Agentic Memory Architecture (MAGMA)",
          "brief": "A proposed architecture that represents memory items across orthogonal semantic, temporal, causal, and entity graphs"
        },
        {
          "name": "Policy-Guided Traversal",
          "brief": "A method for retrieving memory items by traversing relational views in a query-adaptive manner"
        },
        {
          "name": "Relational Views",
          "brief": "Orthogonal graphs representing different aspects of memory items, such as semantic, temporal, causal, and entity information"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models",
          "why_needed": "To understand the context and limitations of existing language models that MAGMA aims to improve upon"
        },
        {
          "topic": "Graph Theory",
          "why_needed": "To comprehend the representation of memory items as graphs and the traversal mechanism in MAGMA"
        },
        {
          "topic": "Artificial Intelligence and Agent Systems",
          "why_needed": "To understand the application of MAGMA in AI agents and its potential impact on long-horizon reasoning tasks"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges of long-context reasoning and the role of memory-augmented generation in addressing these challenges"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.04151": {
    "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Audio-Video Joint Generation",
          "brief": "The process of generating audio and video simultaneously, often used in applications such as video production and virtual reality"
        },
        {
          "name": "Deep Learning Model Architecture",
          "brief": "The design and structure of artificial neural networks used for tasks such as audio-visual generation"
        },
        {
          "name": "Multitask Learning",
          "brief": "A subfield of machine learning that involves training a single model to perform multiple tasks simultaneously"
        },
        {
          "name": "Data Curation",
          "brief": "The process of collecting, annotating, and preparing data for use in machine learning models"
        },
        {
          "name": "Attention Mechanism",
          "brief": "A technique used in deep learning models to focus on specific parts of the input data when generating output"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning Fundamentals",
          "why_needed": "To understand the basics of neural networks and how they can be applied to audio-visual generation"
        },
        {
          "topic": "Audio-Visual Processing",
          "why_needed": "To comprehend the challenges and techniques involved in processing and generating audio and video data"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, optimization, and evaluation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the techniques used for video processing and generation"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the role of captions and text data in audio-visual generation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.03127": {
    "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Generative Models",
          "brief": "Algorithms that generate new, synthetic data samples that resemble existing data"
        },
        {
          "name": "Reasoning-Execution Gap",
          "brief": "The difference between a model's ability to understand instructions and its ability to execute them"
        },
        {
          "name": "Modular Architecture",
          "brief": "A design pattern that allows for the separation of components, enabling easier upgrades and modifications"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning that involves training an agent to take actions to maximize a reward"
        },
        {
          "name": "Text-to-Image Generation",
          "brief": "The process of generating images from text descriptions"
        },
        {
          "name": "Image Editing",
          "brief": "The process of modifying existing images"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of generative models and reinforcement learning"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of image generation and editing"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the concepts of text-to-image generation"
        },
        {
          "topic": "Software Architecture",
          "why_needed": "To understand the concepts of modular design and component separation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.03448": {
    "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Language Models",
          "brief": "Artificial intelligence models that generate text sequences token-by-token"
        },
        {
          "name": "Pre-training",
          "brief": "The process of training a model on a large dataset before fine-tuning it for a specific task"
        },
        {
          "name": "Linguistic Competence",
          "brief": "The ability of a model to understand and generate grammatically correct and coherent text"
        },
        {
          "name": "Language Learning Tasks",
          "brief": "Tasks designed to improve a model's linguistic competence, such as grammar and syntax correction"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the context and applications of language models"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the pre-training and fine-tuning processes of language models"
        },
        {
          "topic": "Human Language Acquisition",
          "why_needed": "To appreciate the inspiration behind the proposed L2T framework"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concept of pre-training and its benefits for model performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.02151": {
    "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Supervised Fine-Tuning (SFT)",
          "brief": "A standard paradigm for domain adaptation that can lead to catastrophic forgetting"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that preserves general capabilities through on-policy learning"
        },
        {
          "name": "Entropy-Adaptive Fine-Tuning (EAFT)",
          "brief": "A proposed method that utilizes token-level entropy to mitigate forgetting in SFT"
        },
        {
          "name": "Confident Conflicts",
          "brief": "Tokens with low probability but low entropy, causing destructive gradient updates in SFT"
        },
        {
          "name": "Epistemic Uncertainty",
          "brief": "The uncertainty in a model's predictions due to limited knowledge or data"
        },
        {
          "name": "Knowledge Conflict",
          "brief": "The discrepancy between a model's internal belief and external supervision"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of fine-tuning, reinforcement learning, and neural network architectures"
        },
        {
          "topic": "Domain Adaptation",
          "why_needed": "To comprehend the challenges of adapting models to new domains and the importance of preserving general capabilities"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To understand the concept of entropy and its application in EAFT"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the experiments and results on language models like Qwen and GLM series"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.03471": {
    "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Epidemiological Question Answering",
          "brief": "The process of using large language models to answer questions related to epidemiology, such as disease burden and transmission dynamics"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models trained on vast amounts of text data to generate human-like language"
        },
        {
          "name": "Benchmarking",
          "brief": "The process of evaluating and comparing the performance of different models or systems"
        },
        {
          "name": "Evidence-Grounded Epidemiological Inference",
          "brief": "The process of drawing conclusions about epidemiological phenomena based on evidence from study data"
        },
        {
          "name": "Chain-of-Thought Prompting",
          "brief": "A technique used to improve the performance of LLMs by providing a series of intermediate steps to follow when generating text"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Epidemiology",
          "why_needed": "To understand the context and concepts related to disease burden, transmission dynamics, and intervention effects"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand how LLMs process and generate human-like language"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand how LLMs are trained and evaluated"
        },
        {
          "topic": "Medical Question Answering",
          "why_needed": "To understand the existing benchmarks and challenges in the field"
        },
        {
          "topic": "Research Methods",
          "why_needed": "To understand the design and construction of the EpiQAL benchmark"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-08"
  },
  "2601.05138": {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "4D Geometric Control",
          "brief": "A novel representation that encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories"
        },
        {
          "name": "Video World Models",
          "brief": "Simulations of dynamic, real-world environments"
        },
        {
          "name": "Video Diffusion Models",
          "brief": "Pretrained models for generating high-fidelity videos"
        },
        {
          "name": "3D Gaussian Trajectories",
          "brief": "A way to capture an object's path and probabilistic 3D occupancy over time"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "To understand video world models, 3D geometry, and object tracking"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend video diffusion models, 4D geometric control, and training data requirements"
        },
        {
          "topic": "Geometry and Linear Algebra",
          "why_needed": "To grasp 3D point clouds, Gaussian trajectories, and 4D geometric representations"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of video diffusion models"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.03425": {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture of Experts models",
          "brief": "A type of neural network architecture that uses multiple expert models to achieve domain specialization"
        },
        {
          "name": "Sparse routing",
          "brief": "A technique used in Mixture of Experts models to selectively activate experts based on input data"
        },
        {
          "name": "Domain-invariant Standing Committee",
          "brief": "A compact group of experts that consistently captures the majority of routing mass across domains, layers, and routing budgets"
        },
        {
          "name": "COMMITTEEAUDIT framework",
          "brief": "A post hoc framework for analyzing routing behavior in Mixture of Experts models at the level of expert groups"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Neural networks",
          "why_needed": "To understand the basics of Mixture of Experts models and their architecture"
        },
        {
          "topic": "Deep learning",
          "why_needed": "To comprehend the training objectives and optimization techniques used in Mixture of Experts models"
        },
        {
          "topic": "Domain adaptation and specialization",
          "why_needed": "To appreciate the concept of domain-invariant features and the implications of the Standing Committee on specialization"
        },
        {
          "topic": "Model analysis and interpretation",
          "why_needed": "To understand the importance of analyzing routing behavior and the insights gained from the COMMITTEEAUDIT framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.05167": {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Complex AI models for natural language processing"
        },
        {
          "name": "Small Language Models (SLMs)",
          "brief": "Resource-efficient language models with limited reasoning capacity"
        },
        {
          "name": "Collaborative Decoding",
          "brief": "Technique for combining LLMs and SLMs for efficient reasoning"
        },
        {
          "name": "Token-level Collaborative Decoding",
          "brief": "Method for dynamic invocation of LLMs during generation process"
        },
        {
          "name": "Group Relative Policy Optimization (GRPO)",
          "brief": "Training framework for balancing independence with strategic help-seeking"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "Understanding of language models and their applications"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Familiarity with neural networks and training frameworks"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "Knowledge of computational costs and latency in AI systems"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "Understanding of policy optimization methods for AI models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.03559": {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Chain-of-Thought (CoT) Reasoning",
          "brief": "A method to improve multi-step mathematical problem solving in large language models"
        },
        {
          "name": "Diffusion-styled CoT Framework",
          "brief": "A framework that reformulates CoT reasoning as an iterative denoising process"
        },
        {
          "name": "Autoregressive Decoding",
          "brief": "A decoding process where the model generates output one token at a time, based on the previous tokens"
        },
        {
          "name": "Exposure Bias and Error Accumulation",
          "brief": "Phenomena where early mistakes in a model's output propagate and worsen over time"
        },
        {
          "name": "Diffusion Principles",
          "brief": "A set of principles inspired by diffusion processes, applied to the reasoning-step level"
        },
        {
          "name": "Causal Diffusion Noise Schedule",
          "brief": "A schedule that respects the temporal structure of reasoning chains to maintain causal consistency"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and application of CoT reasoning"
        },
        {
          "topic": "Multi-Step Mathematical Problem Solving",
          "why_needed": "To comprehend the challenges and goals of CoT reasoning"
        },
        {
          "topic": "Autoregressive Models and Decoding",
          "why_needed": "To grasp the limitations and vulnerabilities of traditional CoT reasoning methods"
        },
        {
          "topic": "Diffusion Processes and Denoising",
          "why_needed": "To understand the inspiration and mechanisms behind the proposed DiffCoT framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.03699": {
    "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Red Teaming",
          "brief": "A methodology to test the robustness of LLMs against adversarial prompts"
        },
        {
          "name": "Adversarial Prompts",
          "brief": "Inputs designed to mislead or deceive LLMs"
        },
        {
          "name": "Risk Categorization",
          "brief": "Classification of potential threats or vulnerabilities in LLMs"
        },
        {
          "name": "Dataset Aggregation",
          "brief": "Combining multiple datasets to create a comprehensive dataset for evaluation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "Understanding of NLP concepts and techniques is necessary to comprehend LLMs and their applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Knowledge of machine learning principles is required to understand how LLMs are trained and evaluated"
        },
        {
          "topic": "Adversarial Attacks",
          "why_needed": "Familiarity with adversarial attacks is necessary to understand the importance of red teaming and robustness evaluation"
        },
        {
          "topic": "Data Curation and Standardization",
          "why_needed": "Understanding of data curation and standardization principles is necessary to appreciate the value of a universal dataset like RedBench"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.02996": {
    "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Reasoning Models (LRMs)",
          "brief": "AI models that achieve strong performance on mathematical reasoning tasks"
        },
        {
          "name": "Chain-of-Thought (CoT) Explanations",
          "brief": "Textual reasoning steps generated by LRMs to explain their answers"
        },
        {
          "name": "Latent Reasoning",
          "brief": "Internal, non-verbal computation encoded in hidden states of LRMs"
        },
        {
          "name": "Multilingual Latent Reasoning",
          "brief": "The ability of LRMs to perform latent reasoning across multiple languages"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the basics of LRMs and their applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the concept of chain-of-thought explanations and language models"
        },
        {
          "topic": "Mathematical Reasoning",
          "why_needed": "To understand the tasks that LRMs are designed to perform"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concept of latent reasoning and its relation to hidden states in neural networks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.05111": {
    "title": "Agent-as-a-Judge",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "LLM-as-a-Judge",
          "brief": "Using large language models for scalable assessments"
        },
        {
          "name": "Agent-as-a-Judge",
          "brief": "Employing agentic judges with planning, verification, and collaboration for evaluations"
        },
        {
          "name": "Agentic Evaluation Systems",
          "brief": "Systems that use agents for evaluation, incorporating planning and multi-agent collaboration"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of LLM-as-a-Judge and Agent-as-a-Judge"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the limitations and biases of large language models"
        },
        {
          "topic": "Multi-Agent Systems",
          "why_needed": "To grasp the concept of multi-agent collaboration in Agent-as-a-Judge"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the role of language models in evaluation systems"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.03822": {
    "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models capable of processing and generating human-like language"
        },
        {
          "name": "Ordered Stochastic Multiple-Choice Knapsack Problem (OS-MCKP)",
          "brief": "A mathematical problem formulation for budgeted inference-time reasoning"
        },
        {
          "name": "Meta-Cognitive Fine-Tuning",
          "brief": "A technique to teach models to predict reasoning cost and expected utility"
        },
        {
          "name": "Rationality-Aware Reinforcement Learning",
          "brief": "A method to optimize sequential decision making under a hard token budget"
        },
        {
          "name": "Return over Investment (ROI)",
          "brief": "A measure to evaluate the efficiency of computational resources allocation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of LLMs and their applications"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To comprehend the OS-MCKP formulation and its relevance to the problem"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the concepts of fine-tuning and reinforcement learning in LLMs"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the computational aspects and token budget constraints"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.04090": {
    "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "3D Scene Generation",
          "brief": "Generating 3D scenes from 2D images or videos"
        },
        {
          "name": "Feed-Forward Reconstruction",
          "brief": "Reconstructing 3D scenes using feed-forward neural networks"
        },
        {
          "name": "Video Diffusion Models",
          "brief": "Generative models that produce videos by iteratively refining noise"
        },
        {
          "name": "Geometric Latents",
          "brief": "Latent representations that capture geometric information of 3D scenes"
        },
        {
          "name": "Appearance Latents",
          "brief": "Latent representations that capture appearance information of 3D scenes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of neural networks"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand 3D reconstruction, scene generation, and image processing"
        },
        {
          "topic": "Generative Models",
          "why_needed": "To understand video diffusion models and their application in 3D scene generation"
        },
        {
          "topic": "Geometry and 3D Representation",
          "why_needed": "To understand geometric latents, point clouds, and depth maps"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.03111": {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning approach where agents learn from interactions with an environment"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        },
        {
          "name": "One-shot Learning",
          "brief": "A learning approach where a model learns from a single example or a very small number of examples"
        },
        {
          "name": "Polymath Learning",
          "brief": "A framework for designing a single training sample that elicits multidisciplinary impact"
        },
        {
          "name": "Sample Engineering",
          "brief": "A precision engineering approach to designing training samples for enhanced model performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning Fundamentals",
          "why_needed": "To understand the basics of RL, LLMs, and one-shot learning"
        },
        {
          "topic": "Mathematical Reasoning and Problem-Solving",
          "why_needed": "To comprehend the application of polymath learning in various domains"
        },
        {
          "topic": "Data Efficiency in AI Models",
          "why_needed": "To appreciate the significance of sample engineering in achieving superior performance with limited data"
        },
        {
          "topic": "Language Model Architecture and Training",
          "why_needed": "To understand how LLMs can be fine-tuned with strategically selected samples"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.04233": {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multilingual Speech Corpus",
          "brief": "A large collection of speech data in multiple languages"
        },
        {
          "name": "Generative Speech Models",
          "brief": "AI models that can generate human-like speech"
        },
        {
          "name": "Word-Level Timestamps",
          "brief": "Precise timing information for each word in a speech dataset"
        },
        {
          "name": "Non-Autoregressive Flow-Matching Framework",
          "brief": "A type of AI architecture for generating speech"
        },
        {
          "name": "Autoregressive Decoder-Only Architecture",
          "brief": "A type of AI architecture for editing speech"
        },
        {
          "name": "Accent-Adversarial Training",
          "brief": "A technique for reducing accent issues in speech synthesis"
        },
        {
          "name": "CTC Loss",
          "brief": "A type of loss function for training speech recognition models"
        },
        {
          "name": "Masked Token Infilling",
          "brief": "A technique for editing speech by filling in missing tokens"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architectures and training methods used in the paper"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the concepts of speech recognition, synthesis, and editing"
        },
        {
          "topic": "Linguistics",
          "why_needed": "To understand the challenges of working with multilingual speech data"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "To understand the processing of audio data"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the training and evaluation of models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2512.24160": {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Learning",
          "brief": "A subfield of machine learning that involves processing and relating multiple forms of data, such as images and text."
        },
        {
          "name": "Industrial Defect Detection",
          "brief": "The use of machine learning and computer vision to identify defects in manufactured products."
        },
        {
          "name": "Diffusion-Based Models",
          "brief": "A type of deep learning model that uses a process called diffusion-based image synthesis to generate high-quality images."
        },
        {
          "name": "Foundation Models",
          "brief": "Large, pre-trained models that can be fine-tuned for specific tasks, allowing for more efficient and effective use of data."
        },
        {
          "name": "Vision-Language Models",
          "brief": "Models that can process and understand both visual and textual data, enabling applications such as image captioning and visual question answering."
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of multimodal learning, diffusion-based models, and foundation models."
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how to process and analyze images of defects in manufactured products."
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand how to process and analyze textual descriptions of defects."
        },
        {
          "topic": "Industrial Manufacturing",
          "why_needed": "To understand the context and applications of industrial defect detection."
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of diffusion-based models and foundation models."
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.03872": {
    "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Heterogeneous Model-Tool Combinations",
          "brief": "Integrating different AI models with various external tools"
        },
        {
          "name": "Dual-Path Framework",
          "brief": "A framework that uses two approaches: training-free cluster-based routing and RL-based multi-step routing"
        },
        {
          "name": "Cross-Domain Complex Reasoning",
          "brief": "Applying AI to solve complex problems across multiple domains"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves training agents to make decisions in complex environments"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence (AI) and Machine Learning (ML)",
          "why_needed": "To understand the basics of LLMs, model-tool combinations, and RL"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the role of LLMs in processing human language"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the application of ATLAS in visual reasoning and multi-modal tools"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To grasp the concept of high-dimensional optimization challenges in model-tool combinations"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.05172": {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Embodied Question Answering (EQA)",
          "brief": "A type of question answering that requires navigating and understanding 3D environments"
        },
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that combine visual and language understanding to answer questions"
        },
        {
          "name": "Chain-of-View (CoV) Prompting",
          "brief": "A framework that enables VLMs to actively explore and reason about 3D environments"
        },
        {
          "name": "View Selection",
          "brief": "The process of selecting relevant viewpoints to answer questions in 3D environments"
        },
        {
          "name": "Spatial Reasoning",
          "brief": "The ability to understand and navigate 3D spaces"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of VLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how VLMs process and interpret visual data"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand how VLMs process and interpret language inputs"
        },
        {
          "topic": "3D Environment Representation",
          "why_needed": "To understand how 3D environments are represented and navigated"
        },
        {
          "topic": "Active Learning and Reasoning",
          "why_needed": "To understand how CoV prompting enables active exploration and reasoning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.04300": {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Models",
          "brief": "A type of deep learning model used for generating high-quality images"
        },
        {
          "name": "Post-training Alignment",
          "brief": "The process of fine-tuning a pre-trained model to align with specific criteria or preferences"
        },
        {
          "name": "Hierarchical Evaluation Criteria",
          "brief": "A tree-structured framework for evaluating image quality based on multiple attributes"
        },
        {
          "name": "Complex Preference Optimization (CPO)",
          "brief": "An optimization technique that extends DPO to align diffusion models with non-binary, hierarchical criteria"
        },
        {
          "name": "Supervised Fine-Tuning",
          "brief": "A technique used to inject domain knowledge into a diffusion model"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of diffusion models and optimization techniques"
        },
        {
          "topic": "Image Generation",
          "why_needed": "To appreciate the application of diffusion models in generating high-quality images"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand the concepts of DPO and CPO"
        },
        {
          "topic": "Domain Expertise",
          "why_needed": "To understand the importance of aligning models with human expertise and fine-grained criteria"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.04767": {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agentic Reinforcement Learning",
          "brief": "A paradigm for refining capabilities of LLM agents through post-training"
        },
        {
          "name": "Tree Search",
          "brief": "A method for strategic exploration in multi-turn tasks"
        },
        {
          "name": "Entropy-Guided Tree Expansion",
          "brief": "A technique for promoting exploration diversity"
        },
        {
          "name": "Turn-wise Credit Assignment",
          "brief": "A method for fine-grained reward propagation from sparse outcomes"
        },
        {
          "name": "Agentic Turn-based Policy Optimization",
          "brief": "A turn-level learning objective for aligning policy updates with agentic interactions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of RL and its application in multi-turn tasks"
        },
        {
          "topic": "LLM Agents",
          "why_needed": "To comprehend the capabilities and limitations of Large Language Model agents"
        },
        {
          "topic": "Multi-turn Tasks",
          "why_needed": "To recognize the challenges and requirements of tasks involving multiple interactions"
        },
        {
          "topic": "Policy Optimization",
          "why_needed": "To grasp the importance of optimizing policies in RL"
        },
        {
          "topic": "Tree Search Algorithms",
          "why_needed": "To understand the role of tree search in strategic exploration and decision-making"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.05163": {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Document Question Answering (DocQA)",
          "brief": "A task that focuses on answering questions grounded in given documents"
        },
        {
          "name": "Agentic Document-Grounded Information Seeking",
          "brief": "An approach that formulates DocQA as an information-seeking problem with tool-driven agents"
        },
        {
          "name": "Exploration-then-Synthesis data synthesis pipeline",
          "brief": "A method for generating high-quality training data for DocQA"
        },
        {
          "name": "End-to-end trained open-source Doc agent",
          "brief": "An agent framework that enables effective tool utilization and document exploration"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and tasks involved in DocQA"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the end-to-end training process and data synthesis pipeline"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To grasp the concept of document exploration and comprehension"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concept of agentic tool design and its applications"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-09"
  },
  "2601.01887": {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Fine-Tuning of Large Language Models (LLMs)",
          "brief": "The process of adjusting pre-trained LLMs for specific tasks or datasets"
        },
        {
          "name": "Safety Alignment",
          "brief": "Ensuring LLMs generate safe and non-harmful content"
        },
        {
          "name": "Low-Rank Structure of Safety Gradient",
          "brief": "A mathematical property that enables efficient correction of safety alignment with minimal data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of LLMs and fine-tuning"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the context and applications of LLMs"
        },
        {
          "topic": "Linear Algebra and Optimization",
          "why_needed": "To grasp the concept of low-rank structure and its implications on safety gradient correction"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-10"
  },
  "2601.05242": {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to make decisions in complex environments"
        },
        {
          "name": "Multi-reward RL Optimization",
          "brief": "A technique used in RL to optimize policies based on multiple rewards or objectives"
        },
        {
          "name": "Group Relative Policy Optimization (GRPO)",
          "brief": "A policy optimization method that normalizes rollout reward combinations"
        },
        {
          "name": "Group reward-Decoupled Normalization Policy Optimization (GDPO)",
          "brief": "A new policy optimization method that decouples the normalization of individual rewards"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the context of language models and their applications"
        },
        {
          "topic": "Reinforcement Learning Fundamentals",
          "why_needed": "To comprehend the basics of RL and its optimization techniques"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To understand the mathematical formulations and derivations of GDPO and GRPO"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the applications of language models and their evaluation metrics"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-10"
  },
  "2601.02702": {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Conversational Agents",
          "brief": "Artificial intelligence systems that interact with humans through natural language"
        },
        {
          "name": "Collaboration Quality",
          "brief": "Measure of how well humans and agents work together to achieve a goal"
        },
        {
          "name": "User Preferences",
          "brief": "Individual user's likes, dislikes, and behavior patterns"
        },
        {
          "name": "Long-Term Collaboration",
          "brief": "Prolonged interaction between humans and agents over multiple sessions"
        },
        {
          "name": "MultiSessionCollab Benchmark",
          "brief": "Evaluation framework for assessing agent performance in long-term collaboration"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "Understanding of AI concepts and techniques"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Knowledge of human-computer interaction and language understanding"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "Familiarity with user experience and interface design"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Understanding of learning algorithms and agent training methods"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-10"
  },
  "2601.05124": {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "In-Context Image Generation and Editing (ICGE)",
          "brief": "A technique that enables users to specify visual concepts through interleaved image-text prompts"
        },
        {
          "name": "Unified Multimodal Models",
          "brief": "Models that can process and understand multiple forms of data, such as images and text"
        },
        {
          "name": "Structured Reasoning-guided Alignment",
          "brief": "A framework that bridges the gap between understanding and generation through structured reasoning"
        },
        {
          "name": "In-Context Chain-of-Thought (IC-CoT)",
          "brief": "A structured reasoning paradigm that decouples semantic guidance and reference association"
        },
        {
          "name": "Reinforcement Learning (RL) Training Scheme",
          "brief": "A training method that uses a surrogate reward to measure the alignment between structured reasoning text and the generated image"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of multimodal models and reinforcement learning"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand image generation and editing tasks"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the role of text in ICGE tasks"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the training scheme and evaluation metrics used in the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-10"
  },
  "2601.05239": {
    "title": "Plenoptic Video Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Generative Video Re-rendering",
          "brief": "Methods that generate new video content based on existing videos"
        },
        {
          "name": "Plenoptic Video Generation",
          "brief": "Generating videos with synchronized views to maintain spatio-temporal coherence"
        },
        {
          "name": "Autoregressive Modeling",
          "brief": "Training models that predict future frames based on past frames"
        },
        {
          "name": "Camera-Guided Video Retrieval",
          "brief": "Selecting relevant videos based on camera information to aid in video generation"
        },
        {
          "name": "Progressive Context-Scaling",
          "brief": "Improving model convergence by gradually increasing context size"
        },
        {
          "name": "Self-Conditioning",
          "brief": "Enhancing model robustness by conditioning on its own outputs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of neural networks and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Knowledge of video processing, camera models, and visual perception"
        },
        {
          "topic": "Generative Models",
          "why_needed": "Familiarity with models like GANs, VAEs, and their applications in video generation"
        },
        {
          "topic": "Video Processing",
          "why_needed": "Understanding of video formats, encoding, and decoding"
        },
        {
          "topic": "Robotics and Computer Vision",
          "why_needed": "Knowledge of robotic manipulation, camera control, and visual perception"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-10"
  },
  "2601.04342": {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Video Diffusion Models",
          "brief": "Models used for generating videos"
        },
        {
          "name": "Transformer-Based Architectures",
          "brief": "Type of neural network architecture used for sequence-to-sequence tasks"
        },
        {
          "name": "Recurrent Hybrid Attention",
          "brief": "Mechanism combining softmax and linear attention for efficient video processing"
        },
        {
          "name": "Linear Attention",
          "brief": "Efficient attention mechanism reducing computational complexity"
        },
        {
          "name": "Softmax Attention",
          "brief": "Type of attention mechanism used in neural networks for weighting importance of inputs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of neural networks and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Knowledge of video processing and generation techniques"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Familiarity with transformer-based architectures and attention mechanisms"
        },
        {
          "topic": "Mathematics and Linear Algebra",
          "why_needed": "Understanding of mathematical concepts underlying attention mechanisms and neural networks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-10"
  },
  "2601.05149": {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoregressive (AR) models",
          "brief": "A type of machine learning model used for sequential data generation, such as image synthesis"
        },
        {
          "name": "Speculative Decoding",
          "brief": "A technique used to accelerate autoregressive models by predicting and verifying tokens in parallel"
        },
        {
          "name": "Multi-Scale Local Speculative Decoding (MuLo-SD)",
          "brief": "A novel framework that combines multi-resolution drafting with spatially informed verification for accelerated AR image generation"
        },
        {
          "name": "Image synthesis",
          "brief": "The process of generating images using machine learning models"
        },
        {
          "name": "Token-level ambiguity",
          "brief": "The uncertainty in predicting the next token in a sequence, which can limit the performance of speculative decoding approaches"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the basics of autoregressive models and image synthesis"
        },
        {
          "topic": "Computer vision",
          "why_needed": "To comprehend the concepts of image generation and evaluation metrics such as FID and HPSv2"
        },
        {
          "topic": "Machine learning",
          "why_needed": "To grasp the fundamentals of speculative decoding and its applications"
        },
        {
          "topic": "Image processing",
          "why_needed": "To understand the concepts of up-sampling, resampling, and spatial awareness in image generation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-11"
  },
  "2601.04754": {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "3D Gaussian Splatting (3DGS)",
          "brief": "A technique for 3D scene understanding and reconstruction"
        },
        {
          "name": "Cross-View Context Fusion",
          "brief": "A method for combining information from multiple views of a 3D scene"
        },
        {
          "name": "Open-Vocabulary 3D Scene Understanding",
          "brief": "The ability of a model to understand and describe 3D scenes using a wide range of vocabulary"
        },
        {
          "name": "Dense Correspondence-Guided Pre-Registration",
          "brief": "A phase that initializes Gaussians with accurate geometry and constructs 3D Context Proposals"
        },
        {
          "name": "Direct Registration",
          "brief": "A technique for registering 3D models without render-supervised fine-tuning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of 3D scene understanding, reconstruction, and registration"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of the ProFuse model"
        },
        {
          "topic": "3D Geometry and Reconstruction",
          "why_needed": "To grasp the concepts of 3DGS, Gaussians, and geometric refinement"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the concept of open-vocabulary and language coherence"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-11"
  },
  "2601.05241": {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Robot Manipulation",
          "brief": "The use of robots to perform tasks that require physical interaction with objects"
        },
        {
          "name": "Visual Identity Prompting",
          "brief": "A technique that uses exemplar images to guide the generation of desired scene setups"
        },
        {
          "name": "Multi-View Video Generation",
          "brief": "The generation of videos from multiple viewpoints to provide a more comprehensive understanding of a scene"
        },
        {
          "name": "Image Diffusion Models",
          "brief": "A type of generative model that can alter images based on given conditions"
        },
        {
          "name": "Visuomotor Policy Models",
          "brief": "Models that learn to map visual observations to motor actions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of image diffusion models and visuomotor policy models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the generation of multi-view videos and visual identity prompting"
        },
        {
          "topic": "Robotics",
          "why_needed": "To understand the context of robot manipulation and the challenges of collecting large-scale real-world manipulation data"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the use of text prompts and vision-language-action models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-11"
  },
  "2601.03955": {
    "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoregressive Image Generation",
          "brief": "A technique for generating images one pixel or token at a time, based on previously generated content"
        },
        {
          "name": "1D Visual Tokenizers",
          "brief": "Models that convert images into sequences of tokens, similar to how language models process text"
        },
        {
          "name": "Hierarchical Residuals",
          "brief": "A design principle that allows models to capture features at multiple scales and levels of abstraction"
        },
        {
          "name": "Transformers",
          "brief": "A type of neural network architecture commonly used in natural language processing and other sequence-based tasks"
        },
        {
          "name": "Residual Network Designs",
          "brief": "A type of neural network architecture that uses residual connections to facilitate training and improve performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of neural networks, transformers, and residual connections"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the challenges and techniques involved in image generation and processing"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the concepts of tokenization and sequence-based models"
        },
        {
          "topic": "Mathematics and Linear Algebra",
          "why_needed": "To understand the mathematical foundations of deep learning and neural networks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-11"
  },
  "2601.04823": {
    "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts (MoE)",
          "brief": "A paradigm for scaling Large Language Models (LLMs) by dividing the model into multiple experts, each handling a specific part of the input data"
        },
        {
          "name": "Parameter-Efficient Fine-Tuning (PEFT)",
          "brief": "A technique for adapting pre-trained models to downstream tasks with minimal additional parameters"
        },
        {
          "name": "LoRA (Low-Rank Adaptation)",
          "brief": "A specific PEFT method that adapts pre-trained models by adding low-rank matrices to the model's weights"
        },
        {
          "name": "Dynamic Rank LoRA (DR-LoRA)",
          "brief": "A framework that dynamically adjusts the rank of LoRA matrices for each expert in a MoE model during fine-tuning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and applications of MoE and PEFT"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the concepts of model fine-tuning, parameter efficiency, and low-rank adaptations"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To understand the mathematical concepts underlying LoRA and DR-LoRA, such as matrix ranks and low-rank approximations"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the relevance of MoE and PEFT in NLP tasks and the benefits of efficient parameter utilization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.05966": {
    "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoregressive Modeling",
          "brief": "A statistical model that predicts the next value in a sequence based on past values"
        },
        {
          "name": "Video Generation",
          "brief": "The process of creating synthetic video content using machine learning models"
        },
        {
          "name": "Multi-scale Next-Frame Prediction",
          "brief": "A technique used to predict the next frame in a video sequence at multiple scales"
        },
        {
          "name": "3D Multi-scale Tokenizer",
          "brief": "A method for efficiently encoding spatio-temporal dynamics in video data"
        },
        {
          "name": "Diffusion and Flow-matching Models",
          "brief": "Types of machine learning models used for video generation, known for high-quality results but computational intensity"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the underlying architecture and training procedures of VideoAR"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the concepts of video generation, object detection, and image processing"
        },
        {
          "topic": "Probability and Statistics",
          "why_needed": "To grasp the autoregressive modeling and statistical techniques used in VideoAR"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of model training, evaluation, and optimization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.04890": {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Weight Decay (WD)",
          "brief": "A regularization technique used to prevent overfitting in neural networks"
        },
        {
          "name": "Stochastic Gradient Noise",
          "brief": "Random fluctuations in the gradient of the loss function during training"
        },
        {
          "name": "Brownian-like Expansion",
          "brief": "A phenomenon where the weight matrices of a neural network expand in a Brownian-like manner due to stochastic gradient noise"
        },
        {
          "name": "Learnable Multipliers",
          "brief": "A technique to learn the optimal scale of matrix layers in neural networks"
        },
        {
          "name": "muP Multipliers",
          "brief": "A method to adapt the scale of matrix layers using a fixed multiplier"
        },
        {
          "name": "Adam and Muon Optimizers",
          "brief": "Popular optimization algorithms used for training neural networks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the context of large-language-model pretraining and the role of weight decay"
        },
        {
          "topic": "Neural Network Architecture",
          "why_needed": "To comprehend the structure of matrix layers and the impact of learnable multipliers"
        },
        {
          "topic": "Optimization Algorithms",
          "why_needed": "To appreciate the differences between Adam and Muon optimizers and their interaction with learnable multipliers"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To understand the concepts of matrix norms, row and column norms, and their role in learnable multipliers"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.05573": {
    "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "3D Orientation Understanding",
          "brief": "Comprehending an object's orientation in 3D space"
        },
        {
          "name": "Rotation Estimation",
          "brief": "Estimating the rotation of an object from single or paired images"
        },
        {
          "name": "Object Symmetry Recognition",
          "brief": "Identifying the rotational symmetries of objects"
        },
        {
          "name": "Generative Models",
          "brief": "Using models to synthesize 3D assets for training data"
        },
        {
          "name": "Model-in-the-Loop Annotation",
          "brief": "Efficiently annotating data with model assistance"
        },
        {
          "name": "Periodic Distribution Fitting",
          "brief": "Fitting distributions to model object rotational symmetry"
        },
        {
          "name": "Multi-Frame Architecture",
          "brief": "Predicting relative object rotations from multiple frames"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding of image and video processing techniques"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Knowledge of model training, annotation, and evaluation"
        },
        {
          "topic": "3D Geometry",
          "why_needed": "Familiarity with 3D spatial reasoning and transformations"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of neural network architectures and training methods"
        },
        {
          "topic": "Object Recognition",
          "why_needed": "Knowledge of object detection, classification, and pose estimation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.06021": {
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning technique for training agents to make decisions in complex environments"
        },
        {
          "name": "Deep Search Agents",
          "brief": "AI models that search for information and answer questions using natural language processing and machine learning"
        },
        {
          "name": "Citation-aware Rubric Rewards (CaRR)",
          "brief": "A reward framework that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity"
        },
        {
          "name": "Citation-aware Group Relative Policy Optimization (C-GRPO)",
          "brief": "A training method that combines CaRR and outcome rewards for robust deep search agents"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of deep search agents"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the fundamentals of reinforcement learning and policy optimization"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To grasp the concept of search agents and their role in retrieving relevant information"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To appreciate the broader implications and applications of the proposed techniques"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.04888": {
    "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Search Agents",
          "brief": "AI systems that retrieve information from large datasets"
        },
        {
          "name": "Query Refinement",
          "brief": "The process of improving search queries to get more accurate results"
        },
        {
          "name": "Process Rewards",
          "brief": "A mechanism to provide feedback on the quality of intermediate search queries"
        },
        {
          "name": "Curriculum Learning",
          "brief": "A framework for training AI models in a progressive and structured manner"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Information Retrieval",
          "why_needed": "To understand how search agents work and the importance of query quality"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend how LLMs process and understand human language"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand how process rewards can be used to guide the search agent's learning"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of training and optimization in AI models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.02760": {
    "title": "AnyDepth: Depth Estimation Made Easy",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Monocular Depth Estimation",
          "brief": "Recovering 3D scene depth from 2D images"
        },
        {
          "name": "Visual Encoder",
          "brief": "Extracting features from images using models like DINOv3"
        },
        {
          "name": "Transformer-Based Decoder",
          "brief": "Using models like Simple Depth Transformer (SDT) for efficient depth estimation"
        },
        {
          "name": "Zero-Shot Learning",
          "brief": "Performing tasks without prior training data"
        },
        {
          "name": "Quality-Based Filtering",
          "brief": "Filtering out harmful samples to improve training quality"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding neural networks and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Knowledge of image processing and 3D scene reconstruction"
        },
        {
          "topic": "Transformer Architecture",
          "why_needed": "Familiarity with transformer models and their applications in vision tasks"
        },
        {
          "topic": "Data Preprocessing and Filtering",
          "why_needed": "Understanding the importance of data quality in machine learning models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.05403": {
    "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models trained on vast amounts of human-authored text data"
        },
        {
          "name": "Behavioral Biases",
          "brief": "Systematic errors in decision-making due to cognitive or emotional influences"
        },
        {
          "name": "Financial Misinformation Detection",
          "brief": "Identifying false or misleading financial information"
        },
        {
          "name": "Multilingualism",
          "brief": "Ability to process and understand multiple languages"
        },
        {
          "name": "Scenario-Induced Bias",
          "brief": "Biases introduced by specific contexts or scenarios in decision-making"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand how LLMs work and their potential biases"
        },
        {
          "topic": "Finance and Economics",
          "why_needed": "To comprehend the context of financial misinformation and its implications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, evaluation, and bias detection"
        },
        {
          "topic": "Linguistics and Language Diversity",
          "why_needed": "To appreciate the challenges of multilingual financial misinformation detection"
        },
        {
          "topic": "Cognitive Biases and Behavioral Economics",
          "why_needed": "To understand the origins and effects of behavioral biases in decision-making"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.05930": {
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autonomous Machine Learning Agents",
          "brief": "Agents that can perform tasks independently using machine learning"
        },
        {
          "name": "Generate-Execute-Feedback Paradigm",
          "brief": "A framework where agents generate hypotheses, execute them, and receive feedback"
        },
        {
          "name": "Execution Bottleneck",
          "brief": "A limitation where hypothesis evaluation relies on expensive physical execution"
        },
        {
          "name": "World Models",
          "brief": "Models that simulate the behavior of complex systems"
        },
        {
          "name": "Data-centric Solution Preference",
          "brief": "A task that involves evaluating and comparing solutions based on data"
        },
        {
          "name": "Predict-then-Verify Loop",
          "brief": "A framework where agents predict outcomes and then verify them"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that can process and understand human language"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of autonomous agents and their applications"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To comprehend the concept of world models and predictive reasoning"
        },
        {
          "topic": "Data Analysis",
          "why_needed": "To understand the importance of data in evaluating and comparing solutions"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To grasp the fundamentals of programming and software development"
        },
        {
          "topic": "Statistics",
          "why_needed": "To understand the concept of accuracy and confidence calibration"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.06002": {
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Long Chain-of-Thought (Long CoT) Reasoning",
          "brief": "A type of reasoning that involves a series of connected thoughts or steps to arrive at a conclusion"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Molecular Structure Analogy",
          "brief": "A framework for understanding complex systems by comparing them to molecular structures and interactions"
        },
        {
          "name": "Effective Semantic Isomers",
          "brief": "A concept that refers to the different ways in which meaning can be represented and structured in language models"
        },
        {
          "name": "Distribution-Transfer-Graph Method",
          "brief": "A technique used to synthesize and optimize complex systems, such as language models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence and Machine Learning",
          "why_needed": "To understand the basics of language models and their applications"
        },
        {
          "topic": "Linguistics and Natural Language Processing",
          "why_needed": "To comprehend the complexities of human language and its representation in computational models"
        },
        {
          "topic": "Chemistry and Molecular Biology",
          "why_needed": "To appreciate the molecular structure analogy and its application to complex systems"
        },
        {
          "topic": "Cognitive Science and Psychology",
          "why_needed": "To understand the human reasoning processes and their relationship to artificial intelligence"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.05699": {
    "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Cultural Question Answering",
          "brief": "A type of question answering that involves multiple forms of input, such as text and speech, and requires cultural knowledge"
        },
        {
          "name": "African Languages",
          "brief": "Languages spoken in Africa, which are diverse and underrepresented in AI research"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that can process and understand human language, but may struggle with cultural and linguistic nuances"
        },
        {
          "name": "Cross-Lingual Cultural Transfer",
          "brief": "The ability of AI models to transfer knowledge and understanding across different languages and cultures"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the basics of language models and question answering"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training and evaluation of AI models"
        },
        {
          "topic": "Linguistics",
          "why_needed": "To appreciate the diversity and complexities of African languages"
        },
        {
          "topic": "Cultural Competence",
          "why_needed": "To recognize the importance of cultural knowledge and sensitivity in AI development"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.03017": {
    "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoformalization",
          "brief": "Translating natural language mathematics into formal statements for machine reasoning"
        },
        {
          "name": "Multimodal Autoformalization",
          "brief": "Extending autoformalization to integrate visual elements and physical domains"
        },
        {
          "name": "Adaptive Grounding",
          "brief": "Recursively constructing formal propositions from perceptually grounded primitives"
        },
        {
          "name": "Axiom Composition",
          "brief": "Combining axioms to support formal reasoning"
        },
        {
          "name": "Recursive Termination",
          "brief": "Ensuring abstractions are supported by visual evidence and anchored in dimensional or axiomatic grounding"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Mathematics",
          "why_needed": "Understanding mathematical concepts and notation"
        },
        {
          "topic": "Physics",
          "why_needed": "Knowledge of physical principles, such as classical mechanics and the Hamiltonian"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding visual perception and image processing"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "Familiarity with machine learning models, such as GPT-5 and Gemini-3-Pro"
        },
        {
          "topic": "Formal Reasoning",
          "why_needed": "Understanding formal logic and reasoning techniques"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.04786": {
    "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "AgentOCR",
          "brief": "A framework for representing agent history as a compact rendered image to reduce token budgets and memory usage"
        },
        {
          "name": "Optical Self-Compression",
          "brief": "A technique where an agent actively emits a compression rate to balance task success and token efficiency"
        },
        {
          "name": "Segment Optical Caching",
          "brief": "A mechanism for eliminating redundant re-rendering by decomposing history into hashable segments and maintaining a visual cache"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning where an agent learns by interacting with an environment and receiving rewards or penalties"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of LLMs and RL"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the idea of representing text as images and the use of visual tokens"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the challenges of processing and representing text data in agentic systems"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of training agents and evaluating their performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.05637": {
    "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Generative Models",
          "brief": "Models that generate new data samples based on a given dataset"
        },
        {
          "name": "Controllability",
          "brief": "The ability to control the output of a model"
        },
        {
          "name": "Formal Guarantees",
          "brief": "Mathematical proofs that provide guarantees on the performance of a model or algorithm"
        },
        {
          "name": "Control Theory",
          "brief": "A field of study that deals with controlling the behavior of systems"
        },
        {
          "name": "Probably-Approximately Correct (PAC) Bounds",
          "brief": "A framework for analyzing the performance of learning algorithms"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of generative models and their applications"
        },
        {
          "topic": "Control Theory",
          "why_needed": "To comprehend the concept of controllability and its relevance to generative models"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To understand the mathematical framework used to derive formal guarantees"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To grasp the concept of PAC bounds and their application to controllable set estimates"
        },
        {
          "topic": "Dialogue Systems",
          "why_needed": "To understand the context in which the GenCtrl toolkit is applied"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-12"
  },
  "2601.05899": {
    "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models capable of processing and generating human-like language"
        },
        {
          "name": "Tower Defense Games",
          "brief": "A subgenre of real-time strategy games where players defend against incoming enemies"
        },
        {
          "name": "Real-Time Strategy (RTS) Games",
          "brief": "Games that require players to make decisions in real-time, often involving strategy and planning"
        },
        {
          "name": "Multimodal Observation Space",
          "brief": "An environment that provides multiple types of observations, such as visual, textual, and structured data"
        },
        {
          "name": "Model Hallucination",
          "brief": "When a model generates or perceives information that is not present in the input data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the capabilities and limitations of LLMs"
        },
        {
          "topic": "Game Development",
          "why_needed": "To comprehend the design and implementation of TowerMind and its benchmark levels"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model evaluation, planning, and decision-making"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To understand the importance of multimodal observation spaces and user experience"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To appreciate the cognitive aspects of human expertise and decision-making in complex tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-13"
  },
  "2601.05960": {
    "title": "Distilling Feedback into Memory-as-a-Tool",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Memory-as-a-Tool",
          "brief": "A framework that converts transient critiques into retrievable guidelines"
        },
        {
          "name": "File-based Memory System",
          "brief": "A system that stores and retrieves information from files"
        },
        {
          "name": "Agent-Controlled Tool Calls",
          "brief": "A method where agents control the usage of tools"
        },
        {
          "name": "Rubric Feedback Bench",
          "brief": "A novel dataset for rubric-based learning"
        },
        {
          "name": "LLMs (Large Language Models)",
          "brief": "Artificial intelligence models that process and generate human-like language"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of LLMs and their applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concept of test-time refinement pipelines and inference cost"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the idea of language models and their role in the proposed framework"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the basics of file-based memory systems and tool calls"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-13"
  },
  "2601.04720": {
    "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Retrieval",
          "brief": "The process of searching and retrieving information across different modalities such as text, images, and videos"
        },
        {
          "name": "Embedding Models",
          "brief": "Techniques used to represent high-dimensional data in a lower-dimensional space while preserving semantic relationships"
        },
        {
          "name": "Reranking Models",
          "brief": "Algorithms used to fine-tune the ranking of search results based on relevance and context"
        },
        {
          "name": "Contrastive Pre-training",
          "brief": "A self-supervised learning method that trains models to distinguish between similar and dissimilar data samples"
        },
        {
          "name": "Cross-encoder Architecture",
          "brief": "A neural network architecture that uses cross-attention mechanisms to model complex relationships between inputs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training methodology of the Qwen3-VL-Embedding and Qwen3-VL-Reranker models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the text-based components of the multimodal retrieval and ranking pipeline"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the image and video processing aspects of the multimodal retrieval system"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To grasp the fundamentals of search and ranking algorithms"
        },
        {
          "topic": "Multilingual Support",
          "why_needed": "To appreciate the capabilities of the Qwen3-VL models in supporting multiple languages"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-01-13"
  },
  "2512.21815": {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that combine visual and language understanding capabilities"
        },
        {
          "name": "Adversarial Attacks",
          "brief": "Methods to manipulate model inputs to cause incorrect or undesirable outputs"
        },
        {
          "name": "Entropy",
          "brief": "A measure of model uncertainty, used to identify critical decision points in VLMs"
        },
        {
          "name": "Autoregressive Generation",
          "brief": "A process where models generate output sequences one token at a time, conditioned on previous tokens"
        },
        {
          "name": "Entropy-bank Guided Adversarial attacks (EGA)",
          "brief": "A proposed attack method that targets high-entropy tokens to degrade VLM performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of VLMs"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the language generation aspects of VLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To grasp the visual understanding components of VLMs"
        },
        {
          "topic": "Adversarial Robustness",
          "why_needed": "To appreciate the importance of defending against adversarial attacks"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To understand the concept of entropy and its application in this context"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-13"
  },
  "2601.06463": {
    "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Neural Architecture",
          "brief": "Designing neural networks for efficient sequence processing"
        },
        {
          "name": "Sequence Modeling",
          "brief": "Techniques for handling sequential data of varying lengths"
        },
        {
          "name": "Transformer Architecture",
          "brief": "A type of neural network for sequence-to-sequence tasks"
        },
        {
          "name": "Exponential Moving Average with Gated Attention",
          "brief": "A technique for improving attention mechanisms in neural networks"
        },
        {
          "name": "Timestep Decay Normalization",
          "brief": "A method for normalizing neural network activations over time"
        },
        {
          "name": "Sliding Chunk Attention Mechanism",
          "brief": "A technique for efficiently processing long sequences"
        },
        {
          "name": "Adaptive Working Memory",
          "brief": "A mechanism for dynamically allocating memory in neural networks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning Fundamentals",
          "why_needed": "Understanding neural networks and their applications"
        },
        {
          "topic": "Sequence Modeling Techniques",
          "why_needed": "Familiarity with existing sequence modeling methods"
        },
        {
          "topic": "Attention Mechanisms",
          "why_needed": "Knowledge of attention mechanisms and their role in sequence processing"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Understanding the context and applications of sequence modeling in NLP"
        },
        {
          "topic": "Computer Architecture and Efficiency",
          "why_needed": "Appreciation for the importance of efficient neural network design"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-13"
  },
  "2601.07351": {
    "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Language Models (DLMs)",
          "brief": "A type of language model that enables parallel decoding through iterative refinement"
        },
        {
          "name": "Hard Binary Masking",
          "brief": "A technique used in DLMs where tokens are assigned a binary mask, limiting the revision of early decisions"
        },
        {
          "name": "Soft Token Distributions",
          "brief": "A method of representing tokens as continuous probability distributions, allowing for more flexible and revisable decoding"
        },
        {
          "name": "Continuous Trajectory Supervision",
          "brief": "A training objective that aligns with iterative probabilistic updates in DLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Language Modeling",
          "why_needed": "To understand the context and applications of DLMs"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of DLMs"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To understand the concept of soft token distributions and continuous trajectory supervision"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges and opportunities in language modeling"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-13"
  },
  "2601.07226": {
    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reasoning Models",
          "brief": "AI systems that make decisions based on external information"
        },
        {
          "name": "Contextual Distractors",
          "brief": "Noisy or irrelevant information that can affect model performance"
        },
        {
          "name": "NoisyBench",
          "brief": "A benchmark for evaluating model robustness against diverse noise types"
        },
        {
          "name": "Rationale-Aware Reward (RARE)",
          "brief": "A method for incentivizing models to identify helpful information within noise"
        },
        {
          "name": "Agentic Workflows",
          "brief": "AI systems that interact with their environment and make decisions based on feedback"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the basics of AI systems and their applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of model training, evaluation, and robustness"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the ideas of text-based input and output in AI systems"
        },
        {
          "topic": "Cognitive Biases and Heuristics",
          "why_needed": "To recognize how noisy information can affect human and AI decision-making"
        },
        {
          "topic": "Computer Science and Programming",
          "why_needed": "To understand the technical aspects of implementing and evaluating AI models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-13"
  },
  "2601.06747": {
    "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Language Models (LMs)",
          "brief": "Artificial intelligence models that process and understand human language"
        },
        {
          "name": "Semi-Synthetic Benchmark Generation",
          "brief": "A method of generating benchmarks using a combination of human curation and automated synthesis"
        },
        {
          "name": "Financial Reasoning",
          "brief": "The ability of a model to understand and apply financial concepts and principles"
        },
        {
          "name": "Domain-Specific Datasets",
          "brief": "Datasets that are tailored to a specific domain or industry, such as finance"
        },
        {
          "name": "Gemini 2.5 Flash",
          "brief": "A tool used for structured question generation and validation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the basics of language models and their applications"
        },
        {
          "topic": "Financial Domain Knowledge",
          "why_needed": "To comprehend the concepts and principles used in the finance industry"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the process of training and evaluating models"
        },
        {
          "topic": "Data Curation",
          "why_needed": "To appreciate the importance of high-quality datasets in model development and evaluation"
        },
        {
          "topic": "Evaluation Metrics",
          "why_needed": "To understand how to assess the performance of language models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-13"
  },
  "2601.07239": {
    "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Stochastic CHAOS",
          "brief": "A concept that treats distributional variability as a signal to be measured and controlled in artificial cognition"
        },
        {
          "name": "Deterministic Inference",
          "brief": "A method where the same program on the same input always produces the same output"
        },
        {
          "name": "Distributional Variability",
          "brief": "The ability of a model to capture uncertainty and variability in its outputs"
        },
        {
          "name": "Nondeterminism in LLM Inference",
          "brief": "The analysis of how large language models can produce different outputs for the same input"
        },
        {
          "name": "Emergent Abilities",
          "brief": "The abilities that arise from the complex interactions within a system, such as a large language model"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and application of stochastic CHAOS"
        },
        {
          "topic": "Artificial Cognition",
          "why_needed": "To grasp the implications of deterministic inference on artificial intelligence"
        },
        {
          "topic": "Machine Learning and Deep Learning",
          "why_needed": "To comprehend the concepts of distributional variability and nondeterminism"
        },
        {
          "topic": "Probability and Statistics",
          "why_needed": "To understand the mathematical foundations of stochastic CHAOS and distributional variability"
        },
        {
          "topic": "Software Engineering and Reliability",
          "why_needed": "To appreciate the trade-offs between deterministic inference and stochastic CHAOS in real-world deployments"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.06521": {
    "title": "BabyVision: Visual Reasoning Beyond Language",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal LLMs (MLLMs)",
          "brief": "Large language models that process multiple forms of data, including text and images"
        },
        {
          "name": "Visual Reasoning",
          "brief": "The ability to draw conclusions based on visual information"
        },
        {
          "name": "Linguistic Priors",
          "brief": "Assumptions or knowledge based on language that influence model performance"
        },
        {
          "name": "BabyVision Benchmark",
          "brief": "A set of tasks designed to evaluate core visual abilities in MLLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of MLLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend visual reasoning and perception tasks"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To recognize the role of linguistic priors in MLLMs"
        },
        {
          "topic": "Cognitive Development",
          "why_needed": "To appreciate the comparison between human and MLLM visual abilities"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.08828": {
    "title": "Motion Attribution for Video Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Motion Attribution",
          "brief": "A technique to understand how data influences motion in video generation models"
        },
        {
          "name": "Video Generation Models",
          "brief": "Deep learning models that generate videos from text or other inputs"
        },
        {
          "name": "Gradient-based Data Attribution",
          "brief": "A method to attribute the influence of input data on model outputs"
        },
        {
          "name": "Temporal Dynamics",
          "brief": "The study of how objects or scenes change over time in videos"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of video generation models and gradient-based methods"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the concepts of motion and temporal dynamics in videos"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the idea of data attribution and its importance in model training"
        },
        {
          "topic": "Mathematics (Linear Algebra and Calculus)",
          "why_needed": "To understand the gradient-based methods and motion-weighted loss masks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.03570": {
    "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Concept",
          "brief": "Abstract mental representations that structure perception, reasoning, and learning"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Continual Pre-Training",
          "brief": "A training method where LLMs learn from a continuous stream of data"
        },
        {
          "name": "Concept Circuits",
          "brief": "Computational subgraphs associated with specific concepts within LLMs"
        },
        {
          "name": "Graph Metrics",
          "brief": "Mathematical measures used to characterize the structure of concept circuits"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and capabilities of LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training methods and dynamics of LLMs"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To understand how humans learn and represent concepts, and how LLMs can be designed to mimic this process"
        },
        {
          "topic": "Graph Theory",
          "why_needed": "To understand the structure and analysis of concept circuits"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the application and implications of LLMs in language understanding and generation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.06966": {
    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Memory-Driven Interaction",
          "brief": "The ability of LLMs to retain and manage information over long-term conversations"
        },
        {
          "name": "Project-Oriented Dialogue",
          "brief": "Conversations that involve tracking evolving goals and managing dynamic context dependencies"
        },
        {
          "name": "RealMem Benchmark",
          "brief": "A benchmarking tool for evaluating LLMs in real-world, project-oriented dialogue scenarios"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the fundamentals of LLMs and their applications"
        },
        {
          "topic": "Artificial Intelligence (AI)",
          "why_needed": "To comprehend the broader context of LLMs and their role in autonomous agents"
        },
        {
          "topic": "Human-Computer Interaction (HCI)",
          "why_needed": "To appreciate the importance of effective memory management in dialogue systems"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the synthesis pipeline and evaluation methods used in the RealMem benchmark"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.06943": {
    "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Video Deep Research",
          "brief": "A benchmark for video question answering that requires cross-frame clue extraction, iterative retrieval, and multi-hop reasoning"
        },
        {
          "name": "Agentic Video Reasoning",
          "brief": "A paradigm for video question answering that involves joint video-web evidence and interactive web retrieval"
        },
        {
          "name": "Cross-Frame Clue Extraction",
          "brief": "The process of extracting visual cues from multiple frames in a video"
        },
        {
          "name": "Multi-Hop Reasoning",
          "brief": "The process of reasoning over multiple pieces of evidence to arrive at a conclusion"
        },
        {
          "name": "Video-Conditioned Open-Domain Question Answering",
          "brief": "A type of question answering that requires answering questions based on a video and external knowledge"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of multimodal large language models and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of cross-frame clue extraction and visual anchor extraction"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the concepts of question answering and multi-hop reasoning"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To understand the concepts of iterative retrieval and web search"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.05593": {
    "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Parallel Coordinated Reasoning (PaCoRe)",
          "brief": "A training-and-inference framework for scaling test-time compute with parallel exploration and message-passing architecture"
        },
        {
          "name": "Message-Passing Architecture",
          "brief": "A design for coordinating parallel reasoning trajectories and synthesizing their findings"
        },
        {
          "name": "Outcome-Based Reinforcement Learning",
          "brief": "A training method for mastering synthesis abilities required by PaCoRe"
        },
        {
          "name": "Test-Time Compute (TTC)",
          "brief": "The computational resources available during the testing or inference phase of a model"
        },
        {
          "name": "Context Window",
          "brief": "The limited scope of input data that a model can process at a given time"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Language Models",
          "why_needed": "To understand the limitations of contemporary language models and the need for PaCoRe"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the training-and-inference framework and the role of reinforcement learning"
        },
        {
          "topic": "Parallel Computing",
          "why_needed": "To appreciate the parallel exploration and message-passing architecture in PaCoRe"
        },
        {
          "topic": "Mathematics and Reasoning",
          "why_needed": "To understand the application of PaCoRe in mathematics and its potential impact"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.06786": {
    "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Epistemically-calibrated reasoning (EpiCaR)",
          "brief": "A training objective that jointly optimizes reasoning performance and calibration in large language models"
        },
        {
          "name": "Model collapse in alignment",
          "brief": "A phenomenon where predictive distributions degenerate toward low-variance point estimates, leading to overconfidence and loss of uncertainty representation"
        },
        {
          "name": "Iterative self-training",
          "brief": "A method of improving reasoning abilities in LLMs through self-generated data and iterative training"
        },
        {
          "name": "Calibration in LLMs",
          "brief": "The ability of a model to accurately represent its uncertainty and confidence in its predictions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and limitations of current LLMs"
        },
        {
          "topic": "Deep Learning and Neural Networks",
          "why_needed": "To comprehend the technical aspects of the proposed EpiCaR framework"
        },
        {
          "topic": "Reasoning and Problem-Solving in AI",
          "why_needed": "To appreciate the importance of improving reasoning abilities in LLMs"
        },
        {
          "topic": "Mathematical Reasoning and Code Generation",
          "why_needed": "To understand the applications and evaluations of the proposed framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.07022": {
    "title": "Solar Open Technical Report",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts language model",
          "brief": "A type of language model that uses multiple expert models to improve performance"
        },
        {
          "name": "Progressive curriculum learning",
          "brief": "A methodology for training models by gradually increasing the difficulty of the training data"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning where models learn by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "SnapPO framework",
          "brief": "A proposed framework for efficient optimization of RL models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of language models"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of large language models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of reinforcement learning and optimization"
        },
        {
          "topic": "Linguistics",
          "why_needed": "To appreciate the challenges of developing AI models for underserved languages"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.07264": {
    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Calibration in AI",
          "brief": "The ability of an AI agent to express confidence that reliably reflects its actual performance"
        },
        {
          "name": "Tool-Use Agents",
          "brief": "Autonomous agents that utilize various tools to handle multi-turn tasks"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning approach that involves training agents using rewards or penalties"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        },
        {
          "name": "Confidence Dichotomy",
          "brief": "The difference in confidence levels of AI agents when using various tools"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of tool-use agents"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the reinforcement learning framework and its role in calibration"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the functionality of large language models and their limitations"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To recognize the importance of trustworthiness and calibration in AI agents"
        },
        {
          "topic": "Statistics and Probability",
          "why_needed": "To understand the concept of calibration and its measurement"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.07526": {
    "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Distributed Orchestration System",
          "brief": "A system that manages and coordinates multiple tasks across a network of computers"
        },
        {
          "name": "Agentic Era",
          "brief": "A period characterized by the development and deployment of interactive and autonomous AI systems"
        },
        {
          "name": "Agent-Environment Interactions",
          "brief": "The interactions between autonomous agents and their environment, which can be simulated or real-world"
        },
        {
          "name": "Large-Scale Agent Training",
          "brief": "The process of training multiple autonomous agents on complex tasks, requiring significant computational resources"
        },
        {
          "name": "Microservices Architecture",
          "brief": "A software architecture style that structures an application as a collection of small, independent services"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of MegaFlow"
        },
        {
          "topic": "Distributed Systems",
          "why_needed": "To comprehend the design and implementation of MegaFlow's orchestration system"
        },
        {
          "topic": "Cloud Computing",
          "why_needed": "To understand the scalability and resource allocation aspects of MegaFlow"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To appreciate the complexity of agentic tasks and the need for sophisticated infrastructure"
        },
        {
          "topic": "Autonomous Systems",
          "why_needed": "To understand the requirements and challenges of training and evaluating autonomous agents"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.08079": {
    "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Executive Memory",
          "brief": "A cognitive system responsible for managing and organizing information in working memory"
        },
        {
          "name": "Tool-Augmented Agent Frameworks",
          "brief": "A framework that combines agents with external tools to enhance reasoning and problem-solving capabilities"
        },
        {
          "name": "Long-Horizon Reasoning",
          "brief": "A type of reasoning that involves making decisions over an extended period, requiring the ability to retain and manage information"
        },
        {
          "name": "Dependency-Aware Memory",
          "brief": "A memory model that captures the relationships and dependencies between different pieces of information"
        },
        {
          "name": "Cognitive Control",
          "brief": "The ability to actively manage and regulate cognitive processes, such as attention and memory"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of tool-augmented agent frameworks and long-horizon reasoning"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To understand the concepts of executive memory, working memory, and cognitive control"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the technical aspects of implementing MemoBrain and evaluating its performance on benchmarks"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of large language models and their limitations in terms of working memory and context management"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.06789": {
    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autonomous Software Engineering (SWE) Agents",
          "brief": "Agents that automate software development tasks"
        },
        {
          "name": "Experience Governance",
          "brief": "Process of converting human experience into actionable data for agents"
        },
        {
          "name": "Agentic Experience Search Strategy",
          "brief": "Method for logic-driven retrieval of human expertise"
        },
        {
          "name": "GitHub Data",
          "brief": "Unstructured and fragmented data from GitHub issue-tracking platforms"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Software Engineering",
          "why_needed": "To understand the context and limitations of autonomous SWE agents"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concept of experience governance and agentic experience search strategy"
        },
        {
          "topic": "Data Processing and Structuring",
          "why_needed": "To understand the challenges of working with unstructured GitHub data"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To grasp the concept of agent-friendly memory infrastructure"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.08665": {
    "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Embodied Navigation",
          "brief": "The ability of an agent to navigate through an environment using its sensors and actuators"
        },
        {
          "name": "Visual-Linguistic Models (VLMs)",
          "brief": "Models that combine visual and linguistic information for decision-making"
        },
        {
          "name": "Adaptive Reasoning",
          "brief": "The ability of an agent to adjust its reasoning process based on the situation"
        },
        {
          "name": "Visual-Assisted Linguistic Memory",
          "brief": "A memory mechanism that combines visual and linguistic information for recall and inference"
        },
        {
          "name": "Dual-Process Theory",
          "brief": "A cognitive theory that describes human decision-making as a combination of fast, intuitive and slow, deliberate processes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of VLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how visual information is processed and used in embodied navigation"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand how linguistic information is processed and used in VLMs"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the online expert-guided reinforcement learning stage"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To understand the dual-process theory and its application to adaptive reasoning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-14"
  },
  "2601.07290": {
    "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Video Large Language Model (Video LLM)",
          "brief": "A unified model for joint spatial-temporal understanding in videos"
        },
        {
          "name": "Spatial-Temporal Understanding",
          "brief": "The ability to understand and localize objects and events in both space and time"
        },
        {
          "name": "LoomData-8.7k",
          "brief": "A human-centric video dataset with temporally grounded and spatially localized captions"
        },
        {
          "name": "LoomBench",
          "brief": "A novel benchmark for evaluating Video LLMs from diverse aspects"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of Video LLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend spatial and temporal localization capabilities"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the language understanding aspects of Video LLMs"
        },
        {
          "topic": "Multimodal Intelligence",
          "why_needed": "To appreciate the integration of visual and linguistic information"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-15"
  },
  "2601.08303": {
    "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Transformers (DiTs)",
          "brief": "A type of deep learning model for image generation"
        },
        {
          "name": "Adaptive Global-Local Sparse Attention Mechanism",
          "brief": "A technique to balance global context modeling and local detail preservation in DiTs"
        },
        {
          "name": "Elastic Training Framework",
          "brief": "A method to jointly optimize sub-models of varying capacities within a unified supernetwork"
        },
        {
          "name": "Knowledge-Guided Distribution Matching Distillation",
          "brief": "A step-distillation pipeline for high-fidelity and low-latency generation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the fundamentals of neural networks and transformer architectures"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend image generation and processing techniques"
        },
        {
          "topic": "Edge Computing",
          "why_needed": "To appreciate the challenges and constraints of on-device deployment"
        },
        {
          "topic": "Model Optimization",
          "why_needed": "To understand the importance of efficient model design and knowledge distillation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-15"
  },
  "2601.07779": {
    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "AI models that process and understand both visual and textual data"
        },
        {
          "name": "Computer-Using Agents (CUAs)",
          "brief": "Agents that interact with computers to perform tasks"
        },
        {
          "name": "Reflection-Memory Agent",
          "brief": "An agent that uses long-term memory to self-correct and mitigate visual context loss"
        },
        {
          "name": "Versatile Tool Agents",
          "brief": "Agents that can navigate a browser-based sandbox to synthesize tutorials"
        },
        {
          "name": "Multimodal Searcher",
          "brief": "A component that adopts a SeeAct paradigm to search for relevant information"
        },
        {
          "name": "SeeAct Paradigm",
          "brief": "A paradigm that involves perceiving the environment and taking actions based on that perception"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the underlying technologies used in VLMs and CUAs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the language understanding aspects of VLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To grasp the visual processing capabilities of VLMs"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To understand how agents interact with computers and users"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To appreciate the broader context of CUAs and their applications"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-15"
  },
  "2601.07348": {
    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Self-Evolution Methods",
          "brief": "Iterative generate-verify-refine cycles to enhance code generation"
        },
        {
          "name": "Controlled Self-Evolution (CSE)",
          "brief": "A proposed method to improve exploration efficiency in self-evolution"
        },
        {
          "name": "Diversified Planning Initialization",
          "brief": "Generating structurally distinct algorithmic strategies for broad solution space coverage"
        },
        {
          "name": "Genetic Evolution",
          "brief": "Replacing stochastic operations with feedback-guided mechanisms for targeted mutation and crossover"
        },
        {
          "name": "Hierarchical Evolution Memory",
          "brief": "Capturing experiences at inter-task and intra-task levels for improved evolution"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Algorithmic Code Optimization",
          "why_needed": "To understand the context and goals of the research"
        },
        {
          "topic": "Machine Learning and Large Language Models (LLMs)",
          "why_needed": "To comprehend the application and evaluation of CSE on EffiBench-X"
        },
        {
          "topic": "Genetic Algorithms and Evolutionary Computing",
          "why_needed": "To grasp the concepts and mechanisms used in CSE"
        },
        {
          "topic": "Software Development and Code Generation",
          "why_needed": "To appreciate the practical implications and potential applications of CSE"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-15"
  },
  "2601.08468": {
    "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
          "brief": "A paradigm for reasoning in Large Language Models"
        },
        {
          "name": "JudgeRLVR",
          "brief": "A two-stage judge-then-generate paradigm for efficient reasoning"
        },
        {
          "name": "Discriminative Capability",
          "brief": "The ability of a model to distinguish valid solutions and internalize a guidance signal"
        },
        {
          "name": "Large Language Models",
          "brief": "AI models designed to process and understand human language"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of RLVR and its applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the context of Large Language Models and their role in reasoning tasks"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, fine-tuning, and evaluation"
        },
        {
          "topic": "Mathematical Reasoning",
          "why_needed": "To understand the specific domain (math) where the proposed method is applied and evaluated"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-15"
  },
  "2601.09536": {
    "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and understand multiple forms of data, such as text and images"
        },
        {
          "name": "Multimodal Reasoning",
          "brief": "The ability of AI models to reason and make decisions based on multiple forms of data"
        },
        {
          "name": "Unified Generative Multimodal Reasoning",
          "brief": "A paradigm that unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process"
        },
        {
          "name": "Two-stage SFT+RL Framework",
          "brief": "A framework that combines self-supervised learning (SFT) and reinforcement learning (RL) to enable functional image generation"
        },
        {
          "name": "Perception Alignment Loss and Perception Reward",
          "brief": "Techniques used to align and reward the model's perception of images during the reasoning process"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of multimodal large language models and generative reasoning"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand image processing and generation techniques"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand text-based reasoning and language models"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the RL component of the two-stage SFT+RL framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-15"
  },
  "2601.06596": {
    "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and generate human-like language"
        },
        {
          "name": "Preference-Undermining Attacks (PUA)",
          "brief": "Manipulative prompting strategies that exploit LLMs' desire to please user preferences at the expense of truthfulness"
        },
        {
          "name": "Preference Alignment",
          "brief": "The objective of LLM training to optimize for user-appeasing responses"
        },
        {
          "name": "Factorial Analysis Methodology",
          "brief": "A diagnostic framework for evaluating the trade-off between preference alignment and real-world validity in LLMs"
        },
        {
          "name": "RLHF (Reinforcement Learning from Human Feedback)",
          "brief": "A post-training process for fine-tuning LLMs based on human feedback"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence and Machine Learning",
          "why_needed": "To understand the basics of LLMs and their training objectives"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the complexities of human language and its interaction with LLMs"
        },
        {
          "topic": "Cognitive Biases and Manipulation",
          "why_needed": "To recognize the potential for PUA and its implications on LLMs"
        },
        {
          "topic": "Experimental Design and Statistics",
          "why_needed": "To understand the factorial analysis methodology and its application in evaluating LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-15"
  },
  "2601.09465": {
    "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Finite State Machines (FSM)",
          "brief": "A mathematical model used to design and analyze complex systems"
        },
        {
          "name": "Self-Evolution",
          "brief": "The ability of a system to modify its own architecture or behavior"
        },
        {
          "name": "Large Language Models (LLM)",
          "brief": "AI models trained on vast amounts of text data to generate human-like language"
        },
        {
          "name": "Multi-Hop QA",
          "brief": "A type of question-answering task that requires multiple steps to arrive at the answer"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of LLM-based agents and self-evolution"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the optimization techniques and critic mechanisms used in EvoFSM"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To familiarize with Finite State Machines and their applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the challenges of open-ended queries and the role of LLMs in addressing them"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-15"
  },
  "2601.09136": {
    "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Vision-Language Models (LVLMs)",
          "brief": "Models that process and understand both visual and textual data"
        },
        {
          "name": "Dynamic Visual Encoding",
          "brief": "A method to efficiently encode visual information"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves training agents to make decisions in complex environments"
        },
        {
          "name": "Virtual-Width Dynamic Vision Encoder (DVE)",
          "brief": "A specific encoder architecture used in the SkinFlow framework"
        },
        {
          "name": "Staged RL Strategy",
          "brief": "A two-stage approach to sequentially align medical descriptions and reconstruct diagnostic textures"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of LVLMs and RL"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the challenges of visual information transmission in dermatology"
        },
        {
          "topic": "Dermatology",
          "why_needed": "To appreciate the specific requirements and challenges of medical image diagnosis"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To grasp the concept of optimizing information transmission efficiency"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the two-stage RL strategy used in SkinFlow"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-15"
  },
  "2601.08472": {
    "title": "sui-1: Grounded and Verifiable Long-Form Summarization",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Abstractive Summarization",
          "brief": "The process of generating a summary of a text that captures its main ideas and content, but does not necessarily use the same words or phrases as the original text."
        },
        {
          "name": "Chain-of-Thought Prompting",
          "brief": "A technique used to generate text by providing a sequence of prompts that guide the model to produce a specific output."
        },
        {
          "name": "Citation-Grounded Summarization",
          "brief": "A type of summarization that provides inline citations to support the claims made in the summary, enabling users to verify the information against the source text."
        },
        {
          "name": "Large Language Models",
          "brief": "Artificial intelligence models trained on vast amounts of text data to generate human-like language and perform various natural language processing tasks."
        },
        {
          "name": "Multi-Stage Verification",
          "brief": "A process of verifying the accuracy and validity of generated text through multiple stages or iterations to ensure its quality and reliability."
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the concepts and techniques used in the paper, such as language models, summarization, and verification."
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training process of large language models and the evaluation metrics used to assess their performance."
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To understand the importance of citation-grounded summarization and the need for verifiable summaries in various domains."
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To appreciate the broader context of the research and its potential applications in areas like government, law, and education."
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-16"
  },
  "2601.09142": {
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Evasive Answers Detection",
          "brief": "Identifying answers that intentionally avoid or obscure the truth in financial Q&A"
        },
        {
          "name": "Multi-Model Consensus",
          "brief": "Combining predictions from multiple models to improve overall performance"
        },
        {
          "name": "LLM-as-Judge",
          "brief": "Using large language models as a judge to resolve conflicts between annotators"
        },
        {
          "name": "Disagreement Mining",
          "brief": "Identifying and leveraging cases where models disagree to improve training and generalization"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the concepts of language models, text analysis, and annotation"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the ideas of model training, distillation, and ensemble methods"
        },
        {
          "topic": "Financial Transparency and Earnings Calls",
          "why_needed": "To grasp the context and importance of detecting evasive answers in financial Q&A"
        },
        {
          "topic": "Deep Learning and Large Language Models",
          "why_needed": "To understand the architecture and capabilities of models like Eva-4B and frontier LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-16"
  },
  "2601.10332": {
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Text-to-Image Diffusion Models",
          "brief": "AI models that generate images from text prompts"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models trained on vast amounts of text data to understand and generate human-like language"
        },
        {
          "name": "Think-Then-Generate (T2G) Paradigm",
          "brief": "A novel approach that leverages LLMs to reason about and rewrite text prompts before generating images"
        },
        {
          "name": "Diffusion Conditioning",
          "brief": "A technique used to guide the image generation process based on the rewritten text prompts"
        },
        {
          "name": "Dual-GRPO",
          "brief": "A co-optimization method for the LLM encoder and diffusion backbone to ensure faithful reasoning and accurate rendering"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the fundamentals of neural networks and their applications in computer vision and natural language processing"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the concepts of image generation, editing, and evaluation metrics"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the principles of language models, text encoding, and reasoning"
        },
        {
          "topic": "Image-Text Retrieval and Generation",
          "why_needed": "To understand the challenges and existing approaches in generating images from text prompts"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-16"
  },
  "2601.09088": {
    "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Sequence Distillation",
          "brief": "A process of transferring knowledge from a teacher model to a student model"
        },
        {
          "name": "Distribution-Aligned Sequence Distillation",
          "brief": "A method that aligns the student model's output distribution with the teacher model's output distribution"
        },
        {
          "name": "Long-CoT Reasoning",
          "brief": "A type of reasoning that involves complex, long-term cognitive processes"
        },
        {
          "name": "Teacher-Student Interaction",
          "brief": "A paradigm where a teacher model guides a student model to learn and improve"
        },
        {
          "name": "Sequence-Level Distillation",
          "brief": "A type of distillation that focuses on the sequence-level output of the teacher model"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of teacher-student interaction, sequence distillation, and distribution alignment"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the application of sequence distillation in language models and reasoning tasks"
        },
        {
          "topic": "Mathematics and Scientific Reasoning",
          "why_needed": "To understand the types of reasoning tasks that the proposed model is designed to tackle"
        },
        {
          "topic": "Code Generation",
          "why_needed": "To appreciate the model's capabilities in generating code and its potential applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the fundamentals of model training, evaluation, and optimization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-16"
  },
  "2601.10402": {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agentic Science",
          "brief": "The study of artificial intelligence systems that can perform autonomous actions and make decisions"
        },
        {
          "name": "Ultra-Long-Horizon Autonomy",
          "brief": "The ability of AI systems to sustain strategic coherence and iterative correction over extended periods"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models capable of processing and generating human-like language"
        },
        {
          "name": "Hierarchical Cognitive Caching (HCC)",
          "brief": "A multi-tiered architecture for managing context and experience in AI systems"
        },
        {
          "name": "Machine Learning Engineering (MLE)",
          "brief": "The application of machine learning techniques to engineering problems"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and goals of agentic science"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the role of LLMs and MLE in the research"
        },
        {
          "topic": "Computer Systems",
          "why_needed": "To appreciate the inspiration behind HCC and its potential applications"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To understand the concept of cognitive accumulation and its relevance to AI systems"
        },
        {
          "topic": "Research Methodology",
          "why_needed": "To evaluate the experimental design and results presented in the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-16"
  },
  "2601.09173": {
    "title": "Geometric Stability: The Missing Axis of Representations",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Geometric Stability",
          "brief": "A measure of how reliably representational geometry holds under perturbation"
        },
        {
          "name": "Similarity Metrics",
          "brief": "Methods for measuring how closely embeddings align with external references"
        },
        {
          "name": "Representational Geometry",
          "brief": "The structure of learned representations in a geometric space"
        },
        {
          "name": "Shesha Framework",
          "brief": "A framework for measuring geometric stability"
        },
        {
          "name": "CKA (Centered Kernel Alignment)",
          "brief": "A metric for measuring the similarity between two sets of embeddings"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the context of learned representations and the importance of geometric stability"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To comprehend the concepts of principal components, manifold structure, and geometric stability"
        },
        {
          "topic": "Statistics",
          "why_needed": "To understand the correlation coefficient (Ï) and its significance in the study"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To appreciate the applications of geometric stability in model selection, controllability, and safety monitoring"
        },
        {
          "topic": "Biological Systems",
          "why_needed": "To understand the broader implications of geometric stability beyond machine learning, including CRISPR perturbation coherence and neural-behavioral coupling"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-16"
  },
  "2601.10714": {
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion-based methods",
          "brief": "A type of deep learning technique used for image editing and generation"
        },
        {
          "name": "Intrinsic attributes",
          "brief": "Characteristics of an object that define its identity, such as color, texture, and shape"
        },
        {
          "name": "Extrinsic attributes",
          "brief": "Characteristics of an object that are influenced by its context, such as background and lighting"
        },
        {
          "name": "Visual Named Entities (VNEs)",
          "brief": "Fine-grained visual identity categories that group objects with shared identity-defining features"
        },
        {
          "name": "Vision-language models",
          "brief": "AI models that can process and understand both visual and textual data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the diffusion-based method used in Alterbute"
        },
        {
          "topic": "Computer vision",
          "why_needed": "To comprehend the concepts of intrinsic and extrinsic attributes, and image editing"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "To understand the use of textual prompts and vision-language models in Alterbute"
        },
        {
          "topic": "Image editing and generation",
          "why_needed": "To appreciate the applications and challenges of editing intrinsic attributes in images"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-16"
  },
  "2601.07641": {
    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Test-Time Tool Evolution (TTE)",
          "brief": "A paradigm that enables agents to synthesize, verify, and evolve executable tools during inference"
        },
        {
          "name": "Large Language Models (LLM)",
          "brief": "AI models used for scientific reasoning and problem-solving"
        },
        {
          "name": "SciEvo Benchmark",
          "brief": "A benchmark for evaluating scientific reasoning tasks and tool evolution"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence for Science",
          "why_needed": "To understand the challenges and limitations of current AI models in scientific domains"
        },
        {
          "topic": "Machine Learning and Deep Learning",
          "why_needed": "To comprehend the concepts of LLMs and tool evolution"
        },
        {
          "topic": "Scientific Reasoning and Problem-Solving",
          "why_needed": "To appreciate the importance of adaptive and dynamic tool development in scientific inquiry"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-16"
  },
  "2601.08881": {
    "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts (MoE)",
          "brief": "A machine learning paradigm that combines multiple expert models to improve performance"
        },
        {
          "name": "Dense Diffusion Transformers",
          "brief": "A type of neural network architecture used for image generation and editing tasks"
        },
        {
          "name": "Task-Aware Gating",
          "brief": "A technique to inject semantic intent into MoE routing to mitigate task interference"
        },
        {
          "name": "Hierarchical Task Semantic Annotation",
          "brief": "A scheme to create structured task descriptors for semantic intent injection"
        },
        {
          "name": "Predictive Alignment Regularization",
          "brief": "A regularization technique to align internal routing decisions with task semantics"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend image generation and editing tasks"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concept of Mixture-of-Experts and its applications"
        },
        {
          "topic": "Transformer Architectures",
          "why_needed": "To understand the dense diffusion transformers used in the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-16"
  },
  "2601.10592": {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Video Action Recognition",
          "brief": "The ability of machines to identify and understand physical actions from visual observations in videos"
        },
        {
          "name": "Large-scale Datasets",
          "brief": "Extensive collections of data used for training machine learning models"
        },
        {
          "name": "Open-Vocabulary Action Supervision",
          "brief": "The ability of a model to learn from a wide range of actions without being limited to a predefined set"
        },
        {
          "name": "Temporal Segmentation",
          "brief": "The process of dividing a video into smaller segments based on time"
        },
        {
          "name": "Tree-of-Captions",
          "brief": "A hierarchical organization of captions for frames and segments in a video"
        },
        {
          "name": "Self-Refine Procedure",
          "brief": "An iterative process for refining annotations and improving model performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of model training, data scaling, and zero-shot performance"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the process of visual observation and action recognition in videos"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the generation of captions and the use of language models like GPT-OSS-120B"
        },
        {
          "topic": "Data Preprocessing",
          "why_needed": "To understand the importance of data cleaning, annotation, and organization in machine learning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-17"
  },
  "2601.10131": {
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multi-Agent Systems",
          "brief": "Systems that consist of multiple agents interacting with each other to achieve a common goal"
        },
        {
          "name": "Molecular Generation",
          "brief": "The process of generating molecules that satisfy certain properties or constraints"
        },
        {
          "name": "Multi-Property Constraints",
          "brief": "Constraints that require a molecule to satisfy multiple physicochemical properties simultaneously"
        },
        {
          "name": "Fragment-Level Editing",
          "brief": "A method of editing molecules by adding or removing fragments to achieve desired properties"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves training an agent to take actions to maximize a reward"
        },
        {
          "name": "Group Relative Policy Optimization (GRPO)",
          "brief": "A reinforcement learning algorithm that optimizes policies for multiple agents"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Chemistry",
          "why_needed": "To understand the properties and constraints of molecules"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concepts of multi-agent systems, reinforcement learning, and policy optimization"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of large language models and graph-based algorithms"
        },
        {
          "topic": "Molecular Biology",
          "why_needed": "To understand the importance of generating molecules with specific properties"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-17"
  },
  "2601.10657": {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Powerful operators for evolutionary search"
        },
        {
          "name": "Progress-Aware Consistent Evolution (PACEvolve)",
          "brief": "A framework for governing the agent's context and search dynamics"
        },
        {
          "name": "Context Pollution",
          "brief": "Experiment history biasing future candidate generation"
        },
        {
          "name": "Mode Collapse",
          "brief": "Agents stagnating in local minima due to poor exploration-exploitation balance"
        },
        {
          "name": "Weak Collaboration",
          "brief": "Ineffective crossover strategies in parallel search trajectories"
        },
        {
          "name": "Hierarchical Context Management (HCM)",
          "brief": "Managing agent context to address context pollution"
        },
        {
          "name": "Momentum-Based Backtracking (MBB)",
          "brief": "Escaping local minima through backtracking"
        },
        {
          "name": "Self-Adaptive Sampling Policy",
          "brief": "Unifying backtracking and crossover for dynamic search coordination"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Evolutionary Search",
          "why_needed": "Understanding the basics of evolutionary search is necessary to comprehend the challenges addressed by PACEvolve"
        },
        {
          "topic": "Large Language Models",
          "why_needed": "Knowledge of LLMs is required to understand their role in evolutionary search and the limitations of current LLM-in-the-loop systems"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with machine learning concepts, such as exploration-exploitation balance and local minima, is necessary to understand the challenges addressed by PACEvolve"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "Understanding optimization techniques, such as pruning and backtracking, is necessary to comprehend the components of the PACEvolve framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-17"
  },
  "2601.02242": {
    "title": "VIBE: Visual Instruction Based Editor",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Instruction-based Image Editing",
          "brief": "A technique in generative AI that allows editing images based on text instructions"
        },
        {
          "name": "Diffusion Models",
          "brief": "A type of deep learning model used for image generation and editing"
        },
        {
          "name": "Qwen3-VL Model",
          "brief": "A specific 2B-parameter model used for guiding the editing process"
        },
        {
          "name": "Sana1.5 Diffusion Model",
          "brief": "A 1.6B-parameter model used for image generation"
        },
        {
          "name": "ImgEdit and GEdit Benchmarks",
          "brief": "Standard evaluation metrics for image editing tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Generative AI",
          "why_needed": "To understand the context and applications of instruction-based image editing"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of diffusion models and other neural networks"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand image processing and editing techniques"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand how text instructions are used to guide image editing"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-17"
  },
  "2601.08297": {
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Slash Pattern in Attention",
          "brief": "A phenomenon in Large Language Models where attention scores concentrate along a specific sub-diagonal"
        },
        {
          "name": "Rotary Position Embedding (RoPE)",
          "brief": "A technique used in Transformers to encode positional information"
        },
        {
          "name": "Slash-Dominant Heads (SDHs)",
          "brief": "Attention heads that exhibit the slash pattern, playing a key role in passing information across tokens"
        },
        {
          "name": "Transformer Architecture",
          "brief": "A type of neural network architecture commonly used in Natural Language Processing tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of Large Language Models"
        },
        {
          "topic": "Transformer Architecture",
          "why_needed": "To comprehend the components and mechanisms involved in the emergence of SDHs"
        },
        {
          "topic": "Linear Algebra and Matrix Operations",
          "why_needed": "To understand the mathematical analysis of queries, keys, and RoPE"
        },
        {
          "topic": "Deep Learning and Gradient Descent",
          "why_needed": "To appreciate the training dynamics and theoretical analysis of the model"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-18"
  },
  "2601.10129": {
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Latent Reasoning",
          "brief": "A type of reasoning that combines multiple forms of data, such as text and images, to make inferences or decisions"
        },
        {
          "name": "Perception Gap in Distillation",
          "brief": "A phenomenon where student models mimic teacher models' textual output but attend to different visual regions"
        },
        {
          "name": "LaViT Framework",
          "brief": "A proposed framework that aligns latent visual thoughts between teacher and student models to improve multimodal reasoning"
        },
        {
          "name": "Visual Grounding",
          "brief": "The ability of a model to connect textual output to relevant visual regions or objects"
        },
        {
          "name": "Autoregressive Reconstruction",
          "brief": "A technique where a model reconstructs a teacher's visual semantics and attention trajectories before generating text"
        },
        {
          "name": "Curriculum Sensory Gating Mechanism",
          "brief": "A mechanism that prevents shortcut learning by controlling the flow of sensory information to the model"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of multimodal latent reasoning, perception gap, and model distillation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the importance of visual grounding and attention mechanisms in multimodal models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the role of text generation and language priors in multimodal reasoning"
        },
        {
          "topic": "Model Distillation",
          "why_needed": "To understand the concept of teacher-student models and the need for aligning latent visual thoughts"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-18"
  },
  "2601.10712": {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Tool-Integrated Reasoning (TIR)",
          "brief": "A method that enables large language models to perform complex tasks by combining reasoning steps with external tool interactions"
        },
        {
          "name": "Bipartite Matching",
          "brief": "A mathematical technique used to assign rewards in the MatchTIR framework"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach used to train models through trial and error by providing rewards or penalties"
        },
        {
          "name": "Credit Assignment",
          "brief": "The process of assigning rewards or penalties to individual actions or steps in a sequence"
        },
        {
          "name": "Dual-Level Advantage Estimation",
          "brief": "A technique used to balance local step precision with global task success by integrating turn-level and trajectory-level signals"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and capabilities of LLMs in performing complex tasks"
        },
        {
          "topic": "Reinforcement Learning Fundamentals",
          "why_needed": "To comprehend the basics of reinforcement learning and its application in TIR"
        },
        {
          "topic": "Machine Learning and Deep Learning",
          "why_needed": "To grasp the underlying concepts and techniques used in the MatchTIR framework"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the application of TIR and MatchTIR in NLP tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-18"
  },
  "2601.08763": {
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning paradigm for training agents to make decisions in complex environments"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and generate human-like language"
        },
        {
          "name": "Exploration Collapse",
          "brief": "A phenomenon where RL policies converge to a limited set of solutions, reducing diversity"
        },
        {
          "name": "Uniqueness-Aware Reinforcement Learning",
          "brief": "A proposed method to reward rare and novel solutions in RL"
        },
        {
          "name": "Clustering and Reweighting",
          "brief": "Techniques used to group similar solutions and adjust rewards based on their rarity"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of LLMs and RL"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the application of LLMs in language tasks"
        },
        {
          "topic": "Mathematics and Problem-Solving",
          "why_needed": "To appreciate the benchmarks used in the research (e.g., mathematics, physics, medical reasoning)"
        },
        {
          "topic": "Machine Learning Evaluation Metrics",
          "why_needed": "To understand metrics like pass@1, pass@k, and AUC@K"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-18"
  },
  "2601.10909": {
    "title": "FrankenMotion: Part-level Human Motion Generation and Composition",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Human Motion Generation",
          "brief": "Generating human-like movements from text prompts"
        },
        {
          "name": "Part-level Motion Annotations",
          "brief": "Annotating specific body parts with detailed motion information"
        },
        {
          "name": "Diffusion-based Motion Generation",
          "brief": "Using diffusion models to generate motion sequences"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Utilizing LLMs for text-based motion generation and annotation"
        },
        {
          "name": "Temporal Resolution",
          "brief": "Capturing motion details at fine time intervals"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding human motion and image processing"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "Working with text prompts and LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with diffusion models and deep learning concepts"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "Understanding the application of motion generation in various fields"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-19"
  },
  "2601.10825": {
    "title": "Reasoning Models Generate Societies of Thought",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models",
          "brief": "Artificial intelligence models capable of processing and understanding human language"
        },
        {
          "name": "Reasoning Models",
          "brief": "Specific type of language models designed to simulate human-like reasoning and problem-solving"
        },
        {
          "name": "Multi-Agent Systems",
          "brief": "Systems composed of multiple interacting agents, in this case, internal cognitive perspectives with distinct personality traits and domain expertise"
        },
        {
          "name": "Conversational Behaviors",
          "brief": "Patterns of interaction and communication, such as question-answering and perspective shifts, exhibited by reasoning models"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "Machine learning technique used to train models through rewards or penalties for desired or undesired behaviors"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "Understanding the basics of AI and its applications is necessary to comprehend the context of the research"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "Knowledge of human cognition and reasoning processes is required to appreciate the simulation of human-like reasoning in models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with machine learning concepts, such as reinforcement learning, is necessary to understand the training and fine-tuning of reasoning models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Understanding the basics of NLP is necessary to comprehend the processing and analysis of language data in the research"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-19"
  },
  "2601.11037": {
    "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to take actions in an environment to maximize a reward"
        },
        {
          "name": "Agentic Search",
          "brief": "A method that enables large language models to solve complex questions via dynamic planning and external search"
        },
        {
          "name": "Boundary-Aware Policy Optimization (BAPO)",
          "brief": "A novel RL framework that cultivates reliable boundary awareness in agentic search without compromising accuracy"
        },
        {
          "name": "Reliability in AI",
          "brief": "The ability of AI systems to provide accurate and trustworthy responses, including admitting when they don't know something"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of large language models and their applications"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To comprehend the concepts of agent policies, rewards, and optimization techniques"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges of generating reliable and accurate responses to complex questions"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the fundamentals of model training, evaluation, and optimization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-19"
  },
  "2601.06378": {
    "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Rig and Motion Learning",
          "brief": "Unifying rig and motion for generative animation"
        },
        {
          "name": "Generative Frameworks",
          "brief": "Jointly learning rig and motion from raw mesh sequences"
        },
        {
          "name": "Latent Space Representation",
          "brief": "Encoding per-vertex deformations into compact latent spaces"
        },
        {
          "name": "Auto-Rigging",
          "brief": "Automatically generating skeletons and skinning weights for motion generation"
        },
        {
          "name": "Motion Generation",
          "brief": "Producing time-varying SE(3) transformations for animatable meshes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Graphics",
          "why_needed": "Understanding of 3D modeling and animation techniques"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with generative models and latent space representation"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "Understanding of SE(3) transformations and Gaussian bones"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Knowledge of neural network architectures and training procedures"
        },
        {
          "topic": "3D Geometry Processing",
          "why_needed": "Understanding of mesh sequences and deformable objects"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-19"
  },
  "2601.09499": {
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Dynamic Point Maps (DPMs)",
          "brief": "A 3D representation that encodes scene motion in addition to 3D shape and camera parameters"
        },
        {
          "name": "V-DPM",
          "brief": "A method for 4D video reconstruction using Dynamic Point Maps"
        },
        {
          "name": "DUSt3R invariant point maps",
          "brief": "A 3D representation that encodes 3D shape and camera parameters"
        },
        {
          "name": "VGGT",
          "brief": "A 3D reconstructor used as a base model for V-DPM"
        }
      ],
      "background_knowledge": [
        {
          "topic": "3D Reconstruction",
          "why_needed": "Understanding of 3D reconstruction techniques is necessary to comprehend the context and contributions of V-DPM"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Knowledge of deep learning concepts, such as neural networks and optimization, is required to understand the implementation of V-DPM"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Familiarity with computer vision concepts, such as camera parameters and scene motion, is necessary to understand the application of V-DPM"
        },
        {
          "topic": "Mathematics (Linear Algebra, Calculus)",
          "why_needed": "Mathematical background is necessary to understand the formulation and optimization of DPMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-19"
  },
  "2601.09668": {
    "title": "STEP3-VL-10B Technical Report",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Foundation Models",
          "brief": "Large-scale models trained on vast amounts of data to achieve general intelligence"
        },
        {
          "name": "Multimodal Intelligence",
          "brief": "Ability of models to process and understand multiple forms of data, such as text and images"
        },
        {
          "name": "Pre-training Strategies",
          "brief": "Methods used to train models on large datasets before fine-tuning for specific tasks"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "Type of machine learning where models learn through trial and error by interacting with an environment"
        },
        {
          "name": "Parallel Coordinated Reasoning (PaCoRe)",
          "brief": "Technique used to scale test-time compute and allocate resources for perceptual reasoning"
        },
        {
          "name": "Vision-Language Synergy",
          "brief": "Ability of models to understand and generate text based on visual input"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of foundation models"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the language-aligned perception encoder and decoder"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the visual hypotheses and perceptual reasoning"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the reinforcement learning and pre-training strategies"
        },
        {
          "topic": "Multimodal Learning",
          "why_needed": "To appreciate the integration of text and image data"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-19"
  },
  "2601.09255": {
    "title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion-based video generation",
          "brief": "A type of video generation model that uses diffusion processes to synthesize videos"
        },
        {
          "name": "Physics-constrained video generation",
          "brief": "Video generation that adheres to physical laws and constraints"
        },
        {
          "name": "Multimodal models",
          "brief": "Models that can process and generate multiple forms of data, such as images and text"
        },
        {
          "name": "Latent fusion strategy",
          "brief": "A technique used to combine different sources of information in a generative model"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the basics of diffusion-based video generation and multimodal models"
        },
        {
          "topic": "Computer vision",
          "why_needed": "To understand image and video synthesis"
        },
        {
          "topic": "Physics and dynamics",
          "why_needed": "To understand the physical constraints and laws that the video generation model needs to adhere to"
        },
        {
          "topic": "Machine learning pipelines",
          "why_needed": "To understand the staged design of the proposed PhyRPR pipeline"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-19"
  },
  "2601.09195": {
    "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Supervised Fine-Tuning (SFT)",
          "brief": "A post-training strategy to align Large Language Models with human intent"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        },
        {
          "name": "Token Probability",
          "brief": "A measure of the likelihood of a token (word or character) appearing in a given context"
        },
        {
          "name": "Semantic Importance",
          "brief": "The degree to which a token contributes to the overall meaning of a sentence or text"
        },
        {
          "name": "Overfitting",
          "brief": "When a model becomes too specialized to the training data and fails to generalize well to new data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of SFT and LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of overfitting, token probability, and semantic importance"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LLMs"
        },
        {
          "topic": "Language Models",
          "why_needed": "To appreciate the role of SFT in fine-tuning LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-19"
  },
  "2601.10547": {
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Music Foundation Models",
          "brief": "Pre-trained models for music understanding and generation"
        },
        {
          "name": "Audio-Text Alignment",
          "brief": "Technique for synchronizing audio and text data"
        },
        {
          "name": "Lyric Recognition",
          "brief": "Process of identifying and transcribing song lyrics from audio"
        },
        {
          "name": "Music Codec Tokenizer",
          "brief": "Method for encoding and decoding music signals"
        },
        {
          "name": "LLM-based Song Generation",
          "brief": "Using large language models to generate music"
        },
        {
          "name": "Autoregressive Modeling",
          "brief": "Statistical modeling technique for sequential data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of music foundation models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the text-based components of the models, such as lyric recognition and text-based music generation"
        },
        {
          "topic": "Audio Signal Processing",
          "why_needed": "To grasp the processing and analysis of audio signals in music foundation models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the training and optimization of the models"
        },
        {
          "topic": "Music Theory",
          "why_needed": "To appreciate the musical structure and attributes controlled by the models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-19"
  },
  "2601.08808": {
    "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Chain-of-Thought (CoT)",
          "brief": "A reasoning mechanism used in large language models to solve complex tasks"
        },
        {
          "name": "Multiplex Thinking",
          "brief": "A stochastic soft reasoning mechanism that samples candidate tokens and aggregates their embeddings"
        },
        {
          "name": "Token-wise Branch-and-Merge",
          "brief": "A technique used in Multiplex Thinking to induce a probability distribution over multiplex rollouts"
        },
        {
          "name": "On-policy Reinforcement Learning (RL)",
          "brief": "A type of machine learning that optimizes policies based on collected experiences"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models",
          "why_needed": "To understand the context and limitations of Chain-of-Thought (CoT) and the motivation for Multiplex Thinking"
        },
        {
          "topic": "Stochastic Processes",
          "why_needed": "To comprehend the sampling dynamics and probability distributions used in Multiplex Thinking"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the optimization of multiplex trajectories using on-policy RL"
        },
        {
          "topic": "Math Reasoning Benchmarks",
          "why_needed": "To evaluate the performance of Multiplex Thinking and compare it with other baselines"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-20"
  },
  "2601.10781": {
    "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Optical Flow",
          "brief": "A technique for tracking the motion of objects in a video sequence"
        },
        {
          "name": "Vision-Language Model (VLM)",
          "brief": "A model that combines visual and language understanding for tasks like image captioning and visual question answering"
        },
        {
          "name": "Diffusion Architecture",
          "brief": "A type of deep learning model that uses a process called diffusion-based image synthesis to generate high-quality images"
        },
        {
          "name": "Future Motion Prediction",
          "brief": "The ability to forecast the future motion of objects in a scene"
        },
        {
          "name": "Robot Control and Video Generation",
          "brief": "Applications of future motion prediction in robotics and video generation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of the FOFPred model"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand optical flow and image processing techniques"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the language-conditioned aspect of the FOFPred model"
        },
        {
          "topic": "Robotics",
          "why_needed": "To understand the application of FOFPred in robotic manipulation"
        },
        {
          "topic": "Data Preprocessing",
          "why_needed": "To understand the techniques used to extract meaningful signals from noisy video-caption data"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-20"
  },
  "2601.11061": {
    "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
          "brief": "A technique used to enhance LLM reasoning"
        },
        {
          "name": "Spurious Rewards Paradox",
          "brief": "A phenomenon where models achieve gains with incorrect rewards"
        },
        {
          "name": "Perplexity Paradox",
          "brief": "A divergence where answer-token perplexity drops while prompt-side coherence degrades"
        },
        {
          "name": "Anchor-Adapter circuit",
          "brief": "A hidden circuit that facilitates memorization shortcuts in LLMs"
        },
        {
          "name": "Path Patching",
          "brief": "A method used to analyze the Anchor-Adapter circuit"
        },
        {
          "name": "Logit Lens",
          "brief": "A technique used to examine the model's decision-making process"
        },
        {
          "name": "JSD analysis",
          "brief": "A method used to analyze the model's behavior"
        },
        {
          "name": "Neural Differential Equations",
          "brief": "A framework used to model the model's dynamics"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context of RLVR and its applications"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To comprehend the basics of RLVR and its mechanisms"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the concepts of neural networks and their analysis"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To understand the concepts of perplexity and entropy"
        },
        {
          "topic": "Mathematics (Linear Algebra, Calculus)",
          "why_needed": "To follow the mathematical formulations and analyses in the paper"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-20"
  },
  "2601.11044": {
    "title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Autonomous Agents",
          "brief": "AI systems that can perform tasks independently with minimal human intervention"
        },
        {
          "name": "Benchmarking",
          "brief": "Evaluating the performance of AI models using standardized tests and metrics"
        },
        {
          "name": "Agentic Capabilities",
          "brief": "The abilities of autonomous agents to interact with their environment and achieve goals"
        },
        {
          "name": "User Simulation",
          "brief": "Mimicking human user behavior to test and evaluate AI systems"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of LLMs and autonomous agents"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the complexities of language models and their evaluation"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, testing, and optimization"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To understand the importance of user simulation and feedback in AI system evaluation"
        },
        {
          "topic": "Software Development",
          "why_needed": "To appreciate the role of tools, APIs, and software frameworks in autonomous agent development"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-20"
  },
  "2601.11227": {
    "title": "Language of Thought Shapes Output Diversity in Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Language of Thought",
          "brief": "The language used by a Large Language Model during its thinking process"
        },
        {
          "name": "Output Diversity",
          "brief": "The variety of outputs generated by a Large Language Model"
        },
        {
          "name": "Multilingual Thinking",
          "brief": "The ability of a model to think in multiple languages"
        },
        {
          "name": "Thinking Space",
          "brief": "A conceptual space where a model's thinking processes occur"
        },
        {
          "name": "Sampling Strategies",
          "brief": "Methods used to sample outputs from a model, such as Single-Language Sampling and Mixed-Language Sampling"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models",
          "why_needed": "To understand the context and capabilities of the models being studied"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the concepts of language modeling and text generation"
        },
        {
          "topic": "Linguistics",
          "why_needed": "To understand the differences between languages and their potential impact on model outputs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, sampling, and evaluation"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To understand the idea of a 'language of thought' and its implications for artificial intelligence"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-20"
  },
  "2601.11000": {
    "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Personalized Large Language Models (LLMs)",
          "brief": "LLMs adapted to individual users for enhanced user satisfaction"
        },
        {
          "name": "Hallucinations in LLMs",
          "brief": "Phenomenon where models generate answers based on user history rather than objective truth"
        },
        {
          "name": "Representational Entanglement",
          "brief": "Interference between personalization and factual representations in LLMs"
        },
        {
          "name": "Factuality-Preserving Personalized Steering (FPPS)",
          "brief": "Inference-time approach to mitigate personalization-induced factual distortions"
        },
        {
          "name": "PFQABench",
          "brief": "Benchmark for evaluating factual and personalized question answering under personalization"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "Understanding of LLMs and their applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Knowledge of model training, inference, and evaluation"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Familiarity with neural network architectures and optimization techniques"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "Understanding of question answering systems and evaluation metrics"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-20"
  },
  "2601.10880": {
    "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Foundation Models",
          "brief": "Large-scale models trained on diverse datasets to learn generalizable features"
        },
        {
          "name": "Prompt-Driven Image Segmentation",
          "brief": "Using text prompts to guide image segmentation tasks"
        },
        {
          "name": "Domain Adaptation",
          "brief": "Adapting models to new domains or datasets to improve performance"
        },
        {
          "name": "Medical Image Segmentation",
          "brief": "Segmenting medical images to identify specific structures or features"
        },
        {
          "name": "SAM3 Model",
          "brief": "A specific foundation model for image segmentation tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of neural networks and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Familiarity with image processing and analysis techniques"
        },
        {
          "topic": "Medical Imaging",
          "why_needed": "Knowledge of medical imaging modalities and applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Understanding of text prompts and their role in guiding image segmentation"
        },
        {
          "topic": "Domain Adaptation Techniques",
          "why_needed": "Familiarity with methods for adapting models to new domains or datasets"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-20"
  },
  "2601.11354": {
    "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agentic Planning",
          "brief": "The ability of agents to reason and act across diverse tasks"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models capable of processing and generating human-like language"
        },
        {
          "name": "Space Planning Problems (SPP)",
          "brief": "A family of high-stakes problems with heterogeneous objectives and strict physical constraints"
        },
        {
          "name": "Unified Agent-Oriented Interaction Protocol",
          "brief": "A standardized way for agents to interact with their environment"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and capabilities of agentic LLMs"
        },
        {
          "topic": "Space Exploration and Planning",
          "why_needed": "To appreciate the complexities and challenges of SPP"
        },
        {
          "topic": "Computer Science and Programming",
          "why_needed": "To comprehend the technical aspects of agent development and evaluation"
        },
        {
          "topic": "Machine Learning and Optimization",
          "why_needed": "To understand the limitations and potential of generalist planning under realistic constraints"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-20"
  },
  "2601.09876": {
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Text-to-SQL",
          "brief": "A technology that converts natural language into SQL queries"
        },
        {
          "name": "Clinical Text-to-SQL",
          "brief": "Applying text-to-SQL in clinical settings to analyze electronic health records (EHRs)"
        },
        {
          "name": "Patient-Similarity Cohort Reasoning",
          "brief": "A method of identifying similar patient groups for clinical analysis"
        },
        {
          "name": "Multi-table Joins",
          "brief": "Combining data from multiple tables in a database to perform complex queries"
        },
        {
          "name": "Executable SQL",
          "brief": "SQL queries that can be executed directly on a database to retrieve or manipulate data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Database Systems",
          "why_needed": "To understand how data is stored and queried in EHR systems"
        },
        {
          "topic": "SQL Programming",
          "why_needed": "To comprehend the structure and execution of SQL queries"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To grasp how text-to-SQL models process and convert natural language into SQL queries"
        },
        {
          "topic": "Clinical Informatics",
          "why_needed": "To understand the context and requirements of clinical data analysis"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To familiarize with the models and techniques used in text-to-SQL systems"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-20"
  },
  "2601.11522": {
    "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoregression",
          "brief": "A statistical model that predicts future values based on past values"
        },
        {
          "name": "Diffusion Models",
          "brief": "A type of generative model that learns to represent data as a sequence of noise-adding operations"
        },
        {
          "name": "Cross-Modal Self-Attention",
          "brief": "A mechanism that allows the model to attend to different modalities (e.g., image and text) and weigh their importance"
        },
        {
          "name": "Medical Foundation Models",
          "brief": "Large-scale models that can be fine-tuned for various medical tasks, such as image understanding and generation"
        },
        {
          "name": "Chest X-Ray Understanding and Generation",
          "brief": "The task of analyzing and generating chest X-ray images for medical diagnosis and research"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of UniX"
        },
        {
          "topic": "Medical Imaging",
          "why_needed": "To appreciate the application and challenges of chest X-ray understanding and generation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the visual understanding and generation tasks"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the cross-modal self-attention mechanism and its potential applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concept of autoregression, diffusion models, and self-attention mechanisms"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.09512": {
    "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Continual Learning",
          "brief": "The ability of a model to learn from a stream of data and adapt to new tasks without forgetting previous knowledge"
        },
        {
          "name": "Vision-Language-Action Models",
          "brief": "Models that integrate visual, linguistic, and action-related information to perform complex tasks"
        },
        {
          "name": "Autonomous Adapter Routing and Expansion",
          "brief": "A method for dynamically adapting a model's architecture to new tasks and environments"
        },
        {
          "name": "Exemplar-Free Continual Learning",
          "brief": "A type of continual learning that does not require storing previous data or exemplars"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and how they can be applied to vision-language-action tasks"
        },
        {
          "topic": "Robotics and Computer Vision",
          "why_needed": "To appreciate the challenges of teaching robots to perform complex manipulation tasks and the importance of continual learning in real-world applications"
        },
        {
          "topic": "Transfer Learning and Fine-Tuning",
          "why_needed": "To understand how pre-trained models can be adapted to new tasks and the limitations of traditional fine-tuning approaches"
        },
        {
          "topic": "Autoencoders and Modular Architectures",
          "why_needed": "To comprehend the technical details of the proposed CLARE framework and its components"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.11087": {
    "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Physics-Aware Reinforcement Learning",
          "brief": "A paradigm that incorporates physical principles into reinforcement learning for video generation models"
        },
        {
          "name": "Transformer-Based Video Generation",
          "brief": "A type of video generation model that uses transformer architectures"
        },
        {
          "name": "Mimicry-Discovery Cycle (MDcycle)",
          "brief": "A unified framework for fine-tuning video generation models while preserving physics-grounded feedback"
        },
        {
          "name": "Physical Collision Rules",
          "brief": "Rules that govern the behavior of rigid bodies in collisions, based on Newton's formulas"
        },
        {
          "name": "Pretrain-Finetune Paradigms",
          "brief": "A common approach in deep learning where a model is pre-trained on a large dataset and then fine-tuned on a specific task"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Classical Mechanics",
          "why_needed": "To understand the physical principles that govern rigid body motion and collisions"
        },
        {
          "topic": "Computer Graphics",
          "why_needed": "To understand how physical simulations are used in rendering and video generation"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of reinforcement learning and how it can be applied to video generation models"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of deep learning and transformer architectures"
        },
        {
          "topic": "Physics-Based Simulators",
          "why_needed": "To understand how physical simulations are used in various fields and how they can be integrated with video generation models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.10338": {
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "AI Agent Frameworks",
          "brief": "Modular architectures that enable dynamic extension of agent capabilities"
        },
        {
          "name": "Agent Skills",
          "brief": "Packages containing instructions and executable code that extend agent capabilities"
        },
        {
          "name": "Security Vulnerabilities",
          "brief": "Weaknesses in agent skills that can be exploited for malicious purposes"
        },
        {
          "name": "Static Analysis",
          "brief": "Methodology for analyzing code without executing it"
        },
        {
          "name": "LLM-based Semantic Classification",
          "brief": "Technique using large language models for categorizing code based on its meaning"
        },
        {
          "name": "Vulnerability Taxonomy",
          "brief": "Classification system for organizing and understanding security vulnerabilities"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of AI agent frameworks and their capabilities"
        },
        {
          "topic": "Computer Security",
          "why_needed": "To comprehend the concepts of security vulnerabilities and risks"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To grasp the principles of modular programming and package management"
        },
        {
          "topic": "Data Analysis",
          "why_needed": "To understand the methodologies used for detecting security vulnerabilities"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To appreciate the role of large language models in semantic classification"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.11004": {
    "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Retrieval-Augmented Generation (RAG)",
          "brief": "A technique used to improve the performance of LLMs by retrieving relevant information from external sources"
        },
        {
          "name": "Confidence Calibration",
          "brief": "The process of adjusting a model's confidence scores to match its actual performance"
        },
        {
          "name": "Noise-Aware Confidence Calibration",
          "brief": "A technique to improve confidence calibration in the presence of noisy or irrelevant data"
        },
        {
          "name": "NAACL Rules and Framework",
          "brief": "A proposed set of rules and framework for noise-aware confidence calibration in LLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of LLMs and RAG"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of model training, fine-tuning, and confidence calibration"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To understand how RAG systems retrieve relevant information from external sources"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.00756": {
    "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Continual Learning",
          "brief": "The ability of a model to learn from a stream of data and adapt to new information without forgetting previously acquired knowledge"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Memory-Augmented Approaches",
          "brief": "Techniques that equip LLMs with an external memory module to store information for future use"
        },
        {
          "name": "Codebook Optimization",
          "brief": "A strategy to compress the memory bank by optimizing a set of codewords that represent the stored information"
        },
        {
          "name": "Key-Value Low-Rank Adaptation",
          "brief": "A method to efficiently utilize compressed memory representations in the attention layers of LLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the applications and challenges of LLMs in real-world scenarios"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To grasp the codebook optimization strategy and its role in compressing the memory bank"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To understand the importance of efficient memory utilization in LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.11969": {
    "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Memory-Centric Mechanisms",
          "brief": "Techniques used to process long contexts in a segment manner"
        },
        {
          "name": "Reward Models (RMs)",
          "brief": "Models used to evaluate memory quality in large language models"
        },
        {
          "name": "Long-Term Memory Management",
          "brief": "The ability of large language models to propagate information across an entire sequence"
        },
        {
          "name": "Benchmarking",
          "brief": "The process of systematically evaluating the performance of models or systems"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and importance of memory management in LLMs"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the concepts of long-context comprehension and long-form generation tasks"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of reward models and LLMs"
        },
        {
          "topic": "Evaluation Metrics",
          "why_needed": "To grasp the methods used to assess the performance of RMs and LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.13247": {
    "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agentic World Models",
          "brief": "Artificial intelligence models that simulate and interact with the physical world"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models trained on vast amounts of text data to generate human-like language"
        },
        {
          "name": "Knowledgeable Experience Learning",
          "brief": "A framework for learning from environmental feedback to improve world models"
        },
        {
          "name": "World Knowledge Repository",
          "brief": "A symbolic repository of environmental knowledge and rules"
        },
        {
          "name": "Process Experience and Goal Experience",
          "brief": "Types of experiences used to enforce physical feasibility and guide task optimality"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of agentic world models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training and fine-tuning of LLMs"
        },
        {
          "topic": "Cognitive Architectures",
          "why_needed": "To appreciate the integration of symbolic and connectionist AI"
        },
        {
          "topic": "Robotics and Embodied Cognition",
          "why_needed": "To understand the importance of physical grounding and environmental interaction"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To recognize the role of language in shaping world models and knowledge repositories"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.10108": {
    "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and understand multiple forms of data, such as text and images"
        },
        {
          "name": "Evidence Chains",
          "brief": "A sequence of logical connections between pieces of evidence to support a claim or hypothesis"
        },
        {
          "name": "Cross-Modal Evidence Chains",
          "brief": "Evidence chains that combine information from different modalities, such as text and figures"
        },
        {
          "name": "Fish-in-the-Ocean (FITO) Paradigm",
          "brief": "A framework for evaluating MLLMs' ability to construct explicit evidence chains within native scientific documents"
        },
        {
          "name": "SIN-Bench",
          "brief": "A benchmark for evaluating MLLMs' performance on tasks that require tracing native evidence chains in long-context multimodal scientific literature"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the basics of language models and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the processing of visual data, such as figures and images"
        },
        {
          "topic": "Scientific Literature Analysis",
          "why_needed": "To understand the structure and content of scientific papers and the challenges of evaluating MLLMs' performance on these documents"
        },
        {
          "topic": "Machine Learning Evaluation Metrics",
          "why_needed": "To grasp the concepts of scoring predictions and diagnosing evidence quality"
        },
        {
          "topic": "Cognitive Architectures and Human Reasoning",
          "why_needed": "To appreciate the importance of causal, evidence-linked reasoning traces in human-like understanding"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.13976": {
    "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-and-Language Navigation (VLN)",
          "brief": "A task that requires an agent to navigate through a environment based on multimodal instructions and visual-spatial context"
        },
        {
          "name": "Chain-of-Thought (CoT) Reasoning",
          "brief": "A reasoning framework that involves generating a sequence of intermediate thoughts to arrive at a final decision"
        },
        {
          "name": "Multimodal Extensions",
          "brief": "Techniques that incorporate multiple forms of data, such as text and images, to improve navigation reasoning"
        },
        {
          "name": "Visual AutoRegressor (VAR)",
          "brief": "A model that encodes visual data into a compact latent space"
        },
        {
          "name": "Unified Multi-CoT Strategy",
          "brief": "An approach that combines multiple CoT modes, including textual, visual, and multimodal, under a single framework"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of the proposed model"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the processing of textual instructions and generation of CoT reasoning"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To grasp the encoding and processing of visual data"
        },
        {
          "topic": "Navigation and Robotics",
          "why_needed": "To understand the application of VLN in real-world scenarios"
        },
        {
          "topic": "Multimodal Learning",
          "why_needed": "To appreciate the challenges and benefits of combining multiple forms of data"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.13251": {
    "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Neural Embeddings",
          "brief": "Techniques for representing words or phrases as vectors in a high-dimensional space"
        },
        {
          "name": "Semantic Clustering",
          "brief": "Grouping words or phrases based on their meanings"
        },
        {
          "name": "Synonymy, Antonymy, and Co-hyponymy",
          "brief": "Relationships between words or phrases with similar, opposite, or hierarchical meanings"
        },
        {
          "name": "Soft-to-Hard Clustering Algorithm",
          "brief": "A method for assigning words or phrases to clusters based on their semantic relationships"
        },
        {
          "name": "Topology-Aware Two-Stage Expansion-Pruning Procedure",
          "brief": "A technique for refining clusters to ensure semantic coherence"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of the research"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the neural embedding and clustering techniques used in the paper"
        },
        {
          "topic": "Linguistics",
          "why_needed": "To appreciate the nuances of language and the challenges of modeling semantic relationships"
        },
        {
          "topic": "Graph Theory",
          "why_needed": "To understand the representation of semantic relationships as a graph and the clustering algorithms used"
        },
        {
          "topic": "Turkish Language",
          "why_needed": "To understand the specific language and dataset used in the research, although not essential for understanding the core concepts"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.11077": {
    "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Agentic Backend Coding",
          "brief": "Autonomous coding agents that can perform complex backend development tasks"
        },
        {
          "name": "Benchmarking",
          "brief": "Evaluating the performance of AI models on specific tasks"
        },
        {
          "name": "Repository-Level Problem Solving",
          "brief": "Solving problems that involve entire code repositories, not just individual code snippets"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and capabilities of LLMs"
        },
        {
          "topic": "Software Development",
          "why_needed": "To appreciate the challenges of backend development and the importance of environment configuration and service deployment"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the concepts of coding, repositories, and containerized services"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the evaluation of AI models and the concept of benchmarking"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-21"
  },
  "2601.14251": {
    "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models",
          "brief": "Models that combine visual and language understanding to process and generate text from images"
        },
        {
          "name": "OCR (Optical Character Recognition)",
          "brief": "Technology used to convert document images into editable text"
        },
        {
          "name": "Multilingual Models",
          "brief": "Models capable of processing and generating text in multiple languages"
        },
        {
          "name": "End-to-End Models",
          "brief": "Models that can perform a task from input to output without requiring intermediate processing steps"
        },
        {
          "name": "Deep Learning",
          "brief": "A subset of machine learning that uses neural networks to analyze data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the fundamentals of model training and evaluation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend image processing and analysis techniques"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp text processing and generation concepts"
        },
        {
          "topic": "Deep Learning Frameworks",
          "why_needed": "To implement and train models like LightOnOCR"
        },
        {
          "topic": "Software Development",
          "why_needed": "To work with model checkpoints, datasets, and evaluation benchmarks"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-22"
  },
  "2601.14352": {
    "title": "RoboBrain 2.5: Depth in Sight, Time in Mind",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Embodied AI",
          "brief": "Artificial intelligence that integrates perception, reasoning, and action in a physical environment"
        },
        {
          "name": "Spatiotemporal Supervision",
          "brief": "Training method that incorporates spatial and temporal information to improve model performance"
        },
        {
          "name": "3D Spatial Reasoning",
          "brief": "Ability to understand and manipulate 3D objects and environments"
        },
        {
          "name": "Temporal Modeling",
          "brief": "Techniques for modeling and predicting temporal relationships and patterns"
        },
        {
          "name": "Dense Temporal Value Estimation",
          "brief": "Method for predicting progress and execution states over time"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the training methods and model architecture of RoboBrain 2.5"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the spatial reasoning and 3D perception capabilities of the model"
        },
        {
          "topic": "Robotics",
          "why_needed": "To appreciate the application of embodied AI in physical environments and manipulation tasks"
        },
        {
          "topic": "Mathematics (Linear Algebra, Geometry)",
          "why_needed": "To understand the mathematical foundations of 3D spatial reasoning and temporal modeling"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-22"
  },
  "2601.10237": {
    "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Differential Privacy",
          "brief": "A framework for preserving data privacy by adding noise to the data"
        },
        {
          "name": "DP-SGD (Differentially Private Stochastic Gradient Descent)",
          "brief": "A method for training machine learning models while preserving data privacy"
        },
        {
          "name": "f-differential privacy framework",
          "brief": "A framework for characterizing privacy via hypothesis-testing trade-off curves"
        },
        {
          "name": "Shuffled sampling",
          "brief": "A method for subsampling data to improve privacy guarantees"
        },
        {
          "name": "Gaussian noise multiplier (Ï)",
          "brief": "A parameter controlling the amount of noise added to the data for privacy preservation"
        },
        {
          "name": "Separation (Îº)",
          "brief": "A measure of the distance between the mechanism's trade-off curve and the ideal random-guessing line"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the context of DP-SGD and its applications"
        },
        {
          "topic": "Data Privacy",
          "why_needed": "To appreciate the importance of preserving data privacy and the trade-offs involved"
        },
        {
          "topic": "Statistical Hypothesis Testing",
          "why_needed": "To understand the concept of trade-off curves and their relation to privacy guarantees"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To grasp the fundamental limitations of privacy-utility guarantees"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-22"
  },
  "2601.13836": {
    "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and understand multiple forms of data, such as text, images, and audio"
        },
        {
          "name": "Omni-Modal Future Forecasting",
          "brief": "The ability of AI models to predict future events based on multiple forms of input data"
        },
        {
          "name": "Cross-Modal Causal and Temporal Reasoning",
          "brief": "The ability of AI models to reason about cause-and-effect relationships and temporal relationships across different forms of data"
        },
        {
          "name": "Instruction-Tuning Dataset",
          "brief": "A dataset used to fine-tune AI models with specific instructions or tasks"
        },
        {
          "name": "Omni-Modal Future Forecasting (OFF) Training Strategy",
          "brief": "A training approach designed to improve AI models' ability to predict future events based on multiple forms of input data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of MLLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the processing and understanding of text-based data"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the processing and understanding of image and video-based data"
        },
        {
          "topic": "Audio Processing",
          "why_needed": "To understand the processing and understanding of audio-based data"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the training and evaluation of AI models"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-22"
  },
  "2601.10700": {
    "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Concept-based explanations",
          "brief": "Quantifying the influence of high-level concepts on model behavior"
        },
        {
          "name": "Structural Counterfactuals",
          "brief": "A method for estimating causal effects by comparing actual and hypothetical outcomes"
        },
        {
          "name": "Structured Causal Models (SCMs)",
          "brief": "A framework for modeling causal relationships between variables"
        },
        {
          "name": "LLMs (Large Language Models)",
          "brief": "AI models capable of generating human-like text"
        },
        {
          "name": "Explainability",
          "brief": "The ability to understand and interpret the decisions made by a model"
        },
        {
          "name": "Interventions",
          "brief": "Modifications made to a system to observe their effect on the outcome"
        },
        {
          "name": "Faithfulness",
          "brief": "The degree to which an explanation accurately reflects the underlying causal relationships"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Causal inference",
          "why_needed": "To understand the concept of structural counterfactuals and their role in evaluating explanations"
        },
        {
          "topic": "Machine learning",
          "why_needed": "To comprehend the basics of LLMs and their applications"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context in which LLMs are used and the importance of explainability"
        },
        {
          "topic": "Statistics and probability",
          "why_needed": "To grasp the mathematical foundations of causal inference and explainability metrics"
        },
        {
          "topic": "AI ethics and fairness",
          "why_needed": "To appreciate the significance of developing faithful explainability methods, particularly in high-stakes domains"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-22"
  },
  "2601.14152": {
    "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Causal Attention",
          "brief": "A mechanism in language models that controls the flow of information between different parts of the input sequence"
        },
        {
          "name": "Language Models",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Prompt Order",
          "brief": "The structure and organization of the input sequence given to a language model"
        },
        {
          "name": "Information Bottleneck",
          "brief": "A phenomenon where the flow of information is restricted, limiting the performance of a model"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of language models"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architectural analysis and mechanisms of language models"
        },
        {
          "topic": "Attention Mechanisms",
          "why_needed": "To understand the role of causal attention in language models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To appreciate the training and evaluation of language models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-22"
  },
  "2601.16206": {
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "LLM-in-Sandbox",
          "brief": "A framework that enables Large Language Models (LLMs) to explore and interact with a virtual computer environment"
        },
        {
          "name": "General Agentic Intelligence",
          "brief": "The ability of LLMs to exhibit intelligent behavior and make decisions in non-code domains"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning approach that trains models to make decisions based on rewards or penalties"
        },
        {
          "name": "Code Sandbox",
          "brief": "A virtual computer environment that allows LLMs to execute code and interact with external resources"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "Understanding of LLMs and their capabilities is necessary to comprehend the concept of LLM-in-Sandbox"
        },
        {
          "topic": "Machine Learning and Reinforcement Learning",
          "why_needed": "Knowledge of machine learning and RL is required to understand the training process and evaluation of LLM-in-Sandbox"
        },
        {
          "topic": "Programming and Computer Science",
          "why_needed": "Familiarity with programming concepts and computer science is necessary to understand the code sandbox environment and the interactions between LLMs and the sandbox"
        },
        {
          "topic": "Artificial Intelligence and Cognitive Science",
          "why_needed": "Understanding of AI and cognitive science is necessary to comprehend the concept of general agentic intelligence and its implications"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.16175": {
    "title": "Learning to Discover at Test Time",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning where an agent learns by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Continual Learning",
          "brief": "A type of machine learning where a model continues to learn and adapt to new data or tasks over time"
        },
        {
          "name": "Test-Time Training",
          "brief": "A method of training a model at test time to adapt to a specific problem or task"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "A type of artificial intelligence model designed to process and understand human language"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of the research"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of reinforcement learning, continual learning, and test-time training"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and functionality of Large Language Models"
        },
        {
          "topic": "Scientific Problem-Solving",
          "why_needed": "To appreciate the applications of the research in fields such as mathematics, biology, and engineering"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.15220": {
    "title": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Language Models",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Fine-Tuning",
          "brief": "The process of adjusting a pre-trained model to fit a specific task or dataset"
        },
        {
          "name": "Contextual Privacy",
          "brief": "The ability of a model to maintain privacy and security in different contexts and situations"
        },
        {
          "name": "Privacy Collapse",
          "brief": "A phenomenon where a model's ability to maintain contextual privacy degrades after fine-tuning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of language models and fine-tuning"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the complexities of language models and their applications"
        },
        {
          "topic": "Machine Learning Safety and Security",
          "why_needed": "To recognize the importance of privacy and security in AI models"
        },
        {
          "topic": "Data Privacy",
          "why_needed": "To understand the implications of privacy collapse in language models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.11868": {
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "AI Agents",
          "brief": "Autonomous programs that can perform tasks"
        },
        {
          "name": "Command Line Interfaces",
          "brief": "Text-based interfaces for interacting with computers"
        },
        {
          "name": "Benchmarking",
          "brief": "Evaluating the performance of models or agents on specific tasks"
        },
        {
          "name": "Long-Horizon Tasks",
          "brief": "Complex tasks that require multiple steps to complete"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concept of AI agents and their applications"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To comprehend the basics of command line interfaces and benchmarking"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concept of model evaluation and improvement"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.14253": {
    "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "3D Motion Reconstruction",
          "brief": "Reconstructing 3D motion from 2D video data"
        },
        {
          "name": "4D Synthesis",
          "brief": "Generating 4D dynamic objects from 3D data and motion"
        },
        {
          "name": "Feed-forward Framework",
          "brief": "A type of neural network architecture used for synthesizing 4D objects"
        },
        {
          "name": "Canonical Reference Mesh",
          "brief": "A 3D reference mesh used as a basis for motion reconstruction"
        },
        {
          "name": "Motion Latent Representation",
          "brief": "A compact representation of motion used for predicting vertex trajectories"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding of image and video processing techniques"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Knowledge of neural network architectures and training methods"
        },
        {
          "topic": "3D Geometry and Mesh Processing",
          "why_needed": "Understanding of 3D shape representation and manipulation"
        },
        {
          "topic": "Linear Algebra and Calculus",
          "why_needed": "Mathematical foundations for understanding 3D motion and geometry"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.14027": {
    "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agentic Systems",
          "brief": "Paradigm for formal theorem proving that coordinates multiple models and tools"
        },
        {
          "name": "General Coding Agent",
          "brief": "A coding agent that can be used as a formal math reasoner for diverse reasoning tasks"
        },
        {
          "name": "Formal Mathematics",
          "brief": "Branch of mathematics that focuses on the use of formal systems to prove theorems"
        },
        {
          "name": "Lean Theorem Prover",
          "brief": "A formal theorem prover that uses a general coding agent to interact with mathematical theorems"
        },
        {
          "name": "MCP (Mathematical Concept Parser)",
          "brief": "A tool that enables flexible extension and autonomous calling of specialized tools"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concept of agentic systems and general coding agents"
        },
        {
          "topic": "Formal Theorem Proving",
          "why_needed": "To understand the context and applications of the proposed paradigm"
        },
        {
          "topic": "Mathematical Logic",
          "why_needed": "To understand the underlying mathematical concepts and theorems used in the paper"
        },
        {
          "topic": "Programming Languages",
          "why_needed": "To understand the implementation details of the proposed system"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the use of Claude Code and Claude Opus 4.5 as the base model"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.07853": {
    "title": "FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Financial Agents",
          "brief": "AI-powered agents used for investment analysis and decision-making"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models used for natural language processing and generation"
        },
        {
          "name": "Execution-Grounded Environments",
          "brief": "Real-world environments where financial agents interact with external systems and data"
        },
        {
          "name": "Security Risks",
          "brief": "Potential threats to financial systems and data"
        },
        {
          "name": "Benchmarking",
          "brief": "Evaluating and comparing the performance of financial agents"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence and Machine Learning",
          "why_needed": "To understand the basics of LLMs and financial agents"
        },
        {
          "topic": "Financial Regulations and Compliance",
          "why_needed": "To understand the importance of security and compliance in financial environments"
        },
        {
          "topic": "Computer Security and Vulnerabilities",
          "why_needed": "To understand the types of security risks and attacks that financial agents may face"
        },
        {
          "topic": "Software Development and Testing",
          "why_needed": "To understand the process of designing and evaluating financial agents"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.14004": {
    "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mechanistic Interpretability (MI)",
          "brief": "A method to understand and interpret the decision-making process of Large Language Models (LLMs)"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Complex artificial intelligence models used for natural language processing tasks"
        },
        {
          "name": "Locate, Steer, and Improve framework",
          "brief": "A systematic framework for actionable intervention in LLMs, consisting of diagnosis, intervention, and improvement stages"
        },
        {
          "name": "Interpretable Objects",
          "brief": "Specific objects or components used to establish a rigorous intervention protocol in LLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of LLMs"
        },
        {
          "topic": "Artificial Intelligence (AI) and Machine Learning (ML)",
          "why_needed": "To comprehend the basics of LLMs and their decision-making processes"
        },
        {
          "topic": "Model interpretability and explainability",
          "why_needed": "To understand the importance and challenges of interpreting complex AI models"
        },
        {
          "topic": "Model optimization and alignment",
          "why_needed": "To appreciate the goals and benefits of applying MI to LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.15621": {
    "title": "Qwen3-TTS Technical Report",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Text-to-Speech (TTS) Models",
          "brief": "AI systems that convert text into spoken language"
        },
        {
          "name": "Multilingual Support",
          "brief": "Ability of a system to support and generate speech in multiple languages"
        },
        {
          "name": "Voice Cloning",
          "brief": "Technique to replicate a person's voice using AI"
        },
        {
          "name": "Description-Based Control",
          "brief": "Controlling the output of a system using descriptive inputs"
        },
        {
          "name": "Dual-Track LM Architecture",
          "brief": "A type of neural network architecture used for real-time speech synthesis"
        },
        {
          "name": "Speech Tokenizers",
          "brief": "Algorithms that break down speech into smaller, manageable units"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the neural network architectures used in Qwen3-TTS"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the text processing and language generation aspects of TTS models"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "To grasp the audio and speech processing techniques used in the system"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the training and optimization of the models"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To appreciate the software and hardware aspects of implementing Qwen3-TTS"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.15727": {
    "title": "Towards Automated Kernel Generation in the Era of LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Kernel Generation",
          "brief": "The process of creating kernels that translate high-level algorithmic semantics into low-level hardware operations"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models capable of compressing expert-level knowledge and enabling scalable optimization"
        },
        {
          "name": "Agentic Systems",
          "brief": "Systems that enable iterative, feedback-driven loops for kernel development and optimization"
        },
        {
          "name": "Automated Kernel Optimization",
          "brief": "The use of LLMs and agentic systems to automate the process of kernel generation and optimization"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Architecture",
          "why_needed": "To understand how kernels interact with hardware"
        },
        {
          "topic": "Programming Models",
          "why_needed": "To understand how kernels are used in different programming paradigms"
        },
        {
          "topic": "Artificial Intelligence and Machine Learning",
          "why_needed": "To understand the basics of LLMs and agentic systems"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To understand the challenges and complexities of kernel development"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.16148": {
    "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "3D Diffusion Models",
          "brief": "Techniques for generating 3D objects using diffusion processes"
        },
        {
          "name": "Temporal 3D Diffusion",
          "brief": "Extension of 3D diffusion models to include a temporal axis for animated 3D object generation"
        },
        {
          "name": "Generative Models",
          "brief": "Algorithms for generating new data samples that resemble existing data"
        },
        {
          "name": "3D Autoencoders",
          "brief": "Neural networks for encoding and decoding 3D data"
        },
        {
          "name": "Animated 3D Mesh Generation",
          "brief": "Techniques for creating animated 3D objects with deformable meshes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of generative models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the processing and analysis of visual data, including 3D objects and videos"
        },
        {
          "topic": "3D Geometry and Mesh Processing",
          "why_needed": "To grasp the representation and manipulation of 3D objects"
        },
        {
          "topic": "Video Models and Temporal Processing",
          "why_needed": "To understand the concepts of temporal diffusion and animated 3D object generation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.16192": {
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360Â°",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Transformers",
          "brief": "A type of deep learning model used for image and video generation"
        },
        {
          "name": "Equirectangular Projection (ERP)",
          "brief": "A method of mapping 3D scenes to 2D images"
        },
        {
          "name": "Geometry-Free Lifting",
          "brief": "A technique for converting perspective images to 360Â° panoramas without requiring camera metadata"
        },
        {
          "name": "Circular Latent Encoding",
          "brief": "A method for reducing seam artifacts in 360Â° panorama generation"
        },
        {
          "name": "VAE Encoder",
          "brief": "A type of neural network used for image and video encoding"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the context and applications of 360Â° panorama generation"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of diffusion transformers"
        },
        {
          "topic": "Geometry and 3D Reconstruction",
          "why_needed": "To appreciate the challenges of lifting perspective images to 360Â° panoramas"
        },
        {
          "topic": "Image and Video Processing",
          "why_needed": "To understand the techniques used for image and video generation, encoding, and decoding"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.14681": {
    "title": "FARE: Fast-Slow Agentic Robotic Exploration",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autonomous Robot Exploration",
          "brief": "The use of robots to explore and map unknown environments without human intervention"
        },
        {
          "name": "Hierarchical Autonomous Exploration Framework",
          "brief": "A framework that integrates multiple levels of decision-making to control a robot's exploration"
        },
        {
          "name": "Large Language Model (LLM)",
          "brief": "A type of artificial intelligence model that can process and understand human language"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves training an agent to make decisions based on rewards or penalties"
        },
        {
          "name": "Fast-Slow Thinking Paradigm",
          "brief": "A decision-making approach that combines quick, intuitive thinking with slower, more deliberate thinking"
        },
        {
          "name": "Modularity-Based Pruning Mechanism",
          "brief": "A method for reducing redundant structures in a graph to improve efficiency"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Robotics",
          "why_needed": "To understand the context and applications of autonomous robot exploration"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concepts of LLMs and RL"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of RL and its application in robotics"
        },
        {
          "topic": "Graph Theory",
          "why_needed": "To understand the topological graph used in the FARE framework"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the local observations and geometric decision-making in the RL module"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.15876": {
    "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Computer Use Agents (CUA)",
          "brief": "AI models that can interact with computers like humans"
        },
        {
          "name": "Evolutionary Learning",
          "brief": "A machine learning approach that uses self-sustaining cycles to improve agent performance"
        },
        {
          "name": "Synthetic Experience",
          "brief": "Artificially generated data used to train AI models"
        },
        {
          "name": "Scalable Infrastructure",
          "brief": "Large-scale systems that support massive data processing and agent training"
        },
        {
          "name": "Iterative Learning Strategy",
          "brief": "A training approach that dynamically updates policies based on success and failure analysis"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Multimodal AI",
          "why_needed": "To understand the context of computer use agents and their potential applications"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To comprehend the concepts of policy optimization, rewards, and supervision"
        },
        {
          "topic": "Data Generation and Synthesis",
          "why_needed": "To understand how synthetic experience is created and used in AI training"
        },
        {
          "topic": "Distributed Computing",
          "why_needed": "To appreciate the scalability and infrastructure requirements for large-scale AI training"
        },
        {
          "topic": "Error Analysis and Self-Correction",
          "why_needed": "To understand how agents learn from failures and improve their performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.15197": {
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language-Action (VLA) models",
          "brief": "Models that integrate vision, language, and action to enable robot manipulation"
        },
        {
          "name": "Information Collapse",
          "brief": "Phenomenon where language instructions become predictable from visual observations, causing models to degenerate"
        },
        {
          "name": "Bayesian decomposition",
          "brief": "Method to decompose complex models into simpler components using Bayesian inference"
        },
        {
          "name": "Latent Action Queries",
          "brief": "Learnable queries that help estimate vision-only prior and language-conditioned posterior"
        },
        {
          "name": "Conditional Pointwise Mutual Information (PMI)",
          "brief": "Objective function to maximize mutual information between actions and instructions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Robotics and manipulation",
          "why_needed": "To understand the application domain of VLA models"
        },
        {
          "topic": "Deep learning and neural networks",
          "why_needed": "To comprehend the architecture and training of VLA models"
        },
        {
          "topic": "Bayesian inference and probabilistic modeling",
          "why_needed": "To understand the Bayesian decomposition and Latent Action Queries"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand language instructions and their integration with vision and action"
        },
        {
          "topic": "Information theory and mutual information",
          "why_needed": "To understand the concept of Information Collapse and PMI objective function"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-23"
  },
  "2601.11141": {
    "title": "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "End-to-End Spoken Dialogue Systems",
          "brief": "Systems that process and generate speech directly without intermediate text representations"
        },
        {
          "name": "Speech Tokenizers",
          "brief": "Algorithms that convert speech into discrete representations"
        },
        {
          "name": "Neural Audio Codecs",
          "brief": "Techniques for compressing and decompressing audio using neural networks"
        },
        {
          "name": "Personalized Voice Cloning",
          "brief": "Generating speech that mimics a specific individual's voice"
        },
        {
          "name": "Real-Time Processing",
          "brief": "Processing and generating speech in real-time, with low latency"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the neural networks and architectures used in the model"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the text-audio token schedule and dialogue management"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "To grasp the audio processing and neural audio codecs used in the model"
        },
        {
          "topic": "Speech Synthesis",
          "why_needed": "To understand the personalized voice cloning and speech generation aspects"
        },
        {
          "topic": "Computer Vision and Human-Computer Interaction",
          "why_needed": "To appreciate the potential applications and implications of the model"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-24"
  },
  "2601.12346": {
    "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Deep Research Agents (DRAs)",
          "brief": "AI models that generate citation-rich reports via multi-step search and synthesis"
        },
        {
          "name": "Multimodal Understanding",
          "brief": "The ability of AI models to understand and process multiple forms of data, such as text and images"
        },
        {
          "name": "Citation-Grounded Report Generation",
          "brief": "The process of generating reports that are supported by credible sources and citations"
        },
        {
          "name": "Multimodal Evidence Use",
          "brief": "The use of multiple forms of data, such as text and images, as evidence in report generation"
        },
        {
          "name": "Formula-LLM Adaptive Evaluation (FLAE)",
          "brief": "A method for evaluating the quality of generated reports"
        },
        {
          "name": "Trustworthy Retrieval-Aligned Citation Evaluation (TRACE)",
          "brief": "A method for evaluating the alignment of citations with generated reports"
        },
        {
          "name": "Multimodal Support-Aligned Integrity Check (MOSAIC)",
          "brief": "A method for evaluating the integrity of text-visual references in generated reports"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the concepts of text generation, language understanding, and multimodal processing"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of image processing and multimodal understanding"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To understand the concepts of citation retrieval and evidence alignment"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of model training, evaluation, and optimization"
        },
        {
          "topic": "Research Methods",
          "why_needed": "To understand the concepts of research report generation, citation practices, and academic integrity"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-24"
  },
  "2601.15282": {
    "title": "Rethinking Video Generation Model for the Embodied World",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Embodied Intelligence",
          "brief": "The integration of artificial intelligence with robotics to enable robots to perceive, reason, and act in the physical world"
        },
        {
          "name": "Video Generation Models",
          "brief": "Deep learning models that generate videos, often used in robotics to simulate real-world interactions"
        },
        {
          "name": "Robotics Benchmark",
          "brief": "A standardized framework to evaluate the performance of robot-oriented video generation models"
        },
        {
          "name": "Data Pipeline",
          "brief": "A series of processes to collect, annotate, and refine data for training video generation models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the fundamentals of video generation models and their applications in robotics"
        },
        {
          "topic": "Robotics",
          "why_needed": "To comprehend the challenges and requirements of generating realistic robot behaviors in videos"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To grasp the concepts of video analysis, object detection, and scene understanding"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the principles of model evaluation, training data, and performance metrics"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-24"
  },
  "2601.14255": {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Video Matting",
          "brief": "The process of separating a foreground object from its background in a video"
        },
        {
          "name": "Generative Prior",
          "brief": "A type of machine learning model that generates new data samples based on a given distribution"
        },
        {
          "name": "Diffusion Models",
          "brief": "A class of deep learning models that learn to represent data as a sequence of noise-adding operations"
        },
        {
          "name": "Pseudo-Labeling",
          "brief": "A technique used to generate labels for unlabeled data by leveraging a trained model's predictions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of generative models, diffusion models, and pseudo-labeling"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend video matting, image segmentation, and object detection"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, fine-tuning, and evaluation metrics"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-24"
  },
  "2601.15778": {
    "title": "Agentic Confidence Calibration",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agentic Confidence Calibration",
          "brief": "The process of calibrating the confidence of AI agents in their decision-making to ensure reliability and accuracy"
        },
        {
          "name": "Holistic Trajectory Calibration (HTC)",
          "brief": "A novel diagnostic framework for calibrating AI agent confidence by analyzing process-level features across an agent's entire trajectory"
        },
        {
          "name": "General Agent Calibrator (GAC)",
          "brief": "A calibrator that achieves the best calibration across different domains and agent frameworks without requiring retraining"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence (AI) and Machine Learning (ML)",
          "why_needed": "To understand the basics of AI agents, their decision-making processes, and the importance of confidence calibration"
        },
        {
          "topic": "Calibration Methods",
          "why_needed": "To comprehend the limitations of existing calibration methods and the need for novel approaches like HTC"
        },
        {
          "topic": "Autonomous Systems",
          "why_needed": "To appreciate the challenges and requirements of deploying AI agents in high-stakes settings"
        },
        {
          "topic": "Uncertainty and Error Analysis",
          "why_needed": "To understand the concepts of compounding errors, uncertainty, and opaque failure modes in agentic systems"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-25"
  },
  "2601.15549": {
    "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and understand multiple forms of data, such as text, images, and videos"
        },
        {
          "name": "In-Context Learning (ICL)",
          "brief": "A training-free adaptation method that enables models to learn from a few examples or prompts"
        },
        {
          "name": "Density-Uncertainty-Weighted Sampling",
          "brief": "A method for selecting the most informative and representative samples from a dataset"
        },
        {
          "name": "Confidence-Aware Retrieval and Prompting",
          "brief": "Techniques for modeling label reliability and adapting to noisy pseudo-labels"
        },
        {
          "name": "Hybrid Pool",
          "brief": "A combination of labeled and unlabeled data for training and adapting MLLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and their applications"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the concepts of language models and their limitations"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To grasp the fundamentals of image and video processing"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To familiarize oneself with the concepts of supervised, unsupervised, and semi-supervised learning"
        },
        {
          "topic": "Data Annotation and Labeling",
          "why_needed": "To recognize the challenges and importance of high-quality annotations in machine learning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-25"
  },
  "2601.16163": {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Visuomotor Control",
          "brief": "The ability of a robot to control its movements based on visual feedback"
        },
        {
          "name": "Video Models",
          "brief": "Deep learning models that generate or predict video sequences"
        },
        {
          "name": "Policy Learning",
          "brief": "The process of training a model to make decisions in a given environment"
        },
        {
          "name": "Latent Diffusion Process",
          "brief": "A type of generative model that represents data as a sequence of latent variables"
        },
        {
          "name": "Test-Time Planning",
          "brief": "The ability of a model to plan and adapt its actions during deployment"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of video models"
        },
        {
          "topic": "Robotics",
          "why_needed": "To understand the application of visuomotor control and policy learning in robotic systems"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the processing and analysis of visual data in video models"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the concept of policy learning and test-time planning"
        },
        {
          "topic": "Generative Models",
          "why_needed": "To understand the concept of latent diffusion processes and video generation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-25"
  },
  "2601.15165": {
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Large Language Models (dLLMs)",
          "brief": "A type of language model that allows token generation in arbitrary orders, breaking the traditional left-to-right constraint"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning approach used to elicit the reasoning capability of dLLMs"
        },
        {
          "name": "Group Relative Policy Optimization (GRPO)",
          "brief": "An optimization algorithm used to improve the reasoning potential of dLLMs"
        },
        {
          "name": "Arbitrary Order Generation",
          "brief": "The ability of dLLMs to generate tokens in any order, which can lead to a premature collapse of the solution space"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Language Models",
          "why_needed": "To understand the context and limitations of traditional language models"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To comprehend the role of RL in eliciting the reasoning capability of dLLMs"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the underlying concepts and architectures of dLLMs"
        },
        {
          "topic": "Mathematics and Coding",
          "why_needed": "To understand the types of tasks that dLLMs are intended to perform and the importance of reasoning potential"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-25"
  },
  "2601.11387": {
    "title": "Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "AI-supported fact-checking",
          "brief": "Using artificial intelligence to verify the accuracy of information"
        },
        {
          "name": "Evidence-based evaluation",
          "brief": "Assessing the reliability of information based on underlying evidence"
        },
        {
          "name": "Natural language explanations",
          "brief": "Using human-like language to explain AI decisions and predictions"
        },
        {
          "name": "Human-AI interaction",
          "brief": "Understanding how people interact with and trust AI systems"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the basics of AI and its applications in fact-checking"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To comprehend how people interact with AI systems and evaluate their reliability"
        },
        {
          "topic": "Information Literacy",
          "why_needed": "To recognize the importance of evidence in evaluating the accuracy of information"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To understand how people process and make decisions based on information presented by AI systems"
        }
      ],
      "difficulty_level": "intermediate",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-25"
  },
  "2601.16725": {
    "title": "LongCat-Flash-Thinking-2601 Technical Report",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts (MoE) reasoning model",
          "brief": "A type of artificial intelligence model that combines multiple expert models to achieve better performance"
        },
        {
          "name": "Agentic reasoning capability",
          "brief": "The ability of a model to reason and make decisions in complex, dynamic environments"
        },
        {
          "name": "Asynchronous reinforcement learning framework (DORA)",
          "brief": "A framework for training models in multiple environments simultaneously"
        },
        {
          "name": "Environment scaling and principled task construction",
          "brief": "Techniques for designing and optimizing environments and tasks for model training"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial intelligence and machine learning",
          "why_needed": "To understand the context and concepts of the research paper"
        },
        {
          "topic": "Reinforcement learning",
          "why_needed": "To comprehend the training framework and algorithms used in the paper"
        },
        {
          "topic": "Computer vision and robotics",
          "why_needed": "To understand the applications and environments discussed in the paper"
        },
        {
          "topic": "Software engineering and infrastructure",
          "why_needed": "To appreciate the scale and complexity of the model and its training process"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-26"
  },
  "2601.14133": {
    "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language-Action (VLA) models",
          "brief": "Models that integrate vision, language, and action for robotic control"
        },
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that process and understand both visual and linguistic inputs"
        },
        {
          "name": "Asymmetric Mixture-of-Transformers (AsyMoT)",
          "brief": "A novel mechanism for coordinating generalist and specialist VLMs"
        },
        {
          "name": "Embodied proprioception",
          "brief": "The ability of a robot to perceive its own body position and movement"
        },
        {
          "name": "Catastrophic forgetting",
          "brief": "The phenomenon of a model forgetting its pre-trained knowledge when fine-tuned for a new task"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the architecture and mechanisms of VLA models and VLMs"
        },
        {
          "topic": "Robotics and robotic control",
          "why_needed": "To appreciate the application of VLA models in robotic manipulation tasks"
        },
        {
          "topic": "Transformers and attention mechanisms",
          "why_needed": "To comprehend the AsyMoT mechanism and its role in TwinBrainVLA"
        },
        {
          "topic": "Multimodal learning and fusion",
          "why_needed": "To understand how visual, linguistic, and proprioceptive inputs are integrated in VLA models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-26"
  },
  "2601.16973": {
    "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that integrate visual and linguistic information to perform tasks"
        },
        {
          "name": "Multimodal Agents",
          "brief": "Agents that can perceive and interact with their environment through multiple modalities (e.g., vision, language)"
        },
        {
          "name": "Multistep Visual Interactions",
          "brief": "Interactions that require agents to integrate perception, memory, and action over multiple steps"
        },
        {
          "name": "VisGym",
          "brief": "A suite of environments for evaluating and training VLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of VLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand visual perception and processing in VLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand linguistic processing in VLMs"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the training and evaluation of multimodal agents in VisGym"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-26"
  },
  "2601.16296": {
    "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Video-to-Video Diffusion Models",
          "brief": "A type of deep learning model used for editing videos by modifying appearance, motion, or camera movement"
        },
        {
          "name": "Memory-Augmented Models",
          "brief": "A technique that uses external memory to condition the model on prior results"
        },
        {
          "name": "Cross-Consistency in Multi-Turn Video Editing",
          "brief": "The ability of a video editor to maintain consistency across sequential edits"
        },
        {
          "name": "Tokenization and Token Compression",
          "brief": "Techniques used to efficiently represent and compress visual cues in videos"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the fundamentals of video-to-video diffusion models and memory-augmented models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand video editing, novel view synthesis, and text-conditioned video editing"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand text-conditioned video editing and the role of language in video editing"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To understand the retrieval strategy used in Memory-V2V to condition the current editing step on prior results"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-26"
  },
  "2601.14256": {
    "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Implicit Neural Representation (INR)",
          "brief": "A technique for learning compact representations of images using neural networks"
        },
        {
          "name": "Contrastive Learning",
          "brief": "A method for training models to learn useful representations by contrasting positive and negative pairs of samples"
        },
        {
          "name": "Knowledge Distillation",
          "brief": "A technique for transferring knowledge from a large model to a smaller one"
        },
        {
          "name": "Hyper-Networks",
          "brief": "A type of neural network that learns to generate model weights for other networks"
        },
        {
          "name": "Image Representation Learning",
          "brief": "The task of learning compact and useful representations of images"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of image representation, recognition, and generation"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of model training, optimization, and evaluation"
        },
        {
          "topic": "Mathematics (Linear Algebra, Calculus)",
          "why_needed": "To understand the mathematical foundations of deep learning and computer vision"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-26"
  },
  "2601.16276": {
    "title": "GameTalk: Training LLMs for Strategic Conversation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Strategic Decision-Making",
          "brief": "The ability to make informed decisions that achieve long-term goals in complex, dynamic environments"
        },
        {
          "name": "Multi-Agent Settings",
          "brief": "Scenarios where multiple agents or entities interact and make decisions that affect each other"
        },
        {
          "name": "Conversational Fine-Tuning",
          "brief": "A method of training LLMs to optimize their performance in interactive, conversational environments"
        },
        {
          "name": "Game Theory",
          "brief": "The study of how people make decisions when the outcome depends on the actions of multiple individuals or parties"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LLMs"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the challenges of processing and generating human language"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To grasp the concept of reward signals and optimization of long-term objectives"
        },
        {
          "topic": "Multi-Agent Systems",
          "why_needed": "To understand the complexities of interactions between multiple agents or entities"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-26"
  },
  "2601.16093": {
    "title": "SAMTok: Representing Any Mask with Two Words",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Pixel-wise capabilities",
          "brief": "The ability of a system to understand and interact with individual pixels in an image"
        },
        {
          "name": "Multi-modal LLMs (MLLMs)",
          "brief": "Large language models that can process and generate multiple forms of data, such as text and images"
        },
        {
          "name": "Discrete mask tokenizer",
          "brief": "A technique for converting region masks into discrete tokens that can be processed by a language model"
        },
        {
          "name": "SAMTok",
          "brief": "A specific discrete mask tokenizer that converts any region mask into two special tokens"
        },
        {
          "name": "Reinforcement learning",
          "brief": "A type of machine learning where an agent learns to take actions to maximize a reward"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer vision",
          "why_needed": "To understand the concept of pixel-wise capabilities and region masks"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "To understand the concept of large language models and tokenization"
        },
        {
          "topic": "Machine learning",
          "why_needed": "To understand the concept of reinforcement learning and training objectives"
        },
        {
          "topic": "Deep learning architectures",
          "why_needed": "To understand the concept of MLLMs and their limitations"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-26"
  },
  "2601.17761": {
    "title": "AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoregressive Modeling",
          "brief": "A statistical modeling technique where the next value in a sequence is predicted based on past values"
        },
        {
          "name": "Multimodal Learning",
          "brief": "A type of machine learning that involves processing and generating multiple forms of data, such as text, images, and speech"
        },
        {
          "name": "Transformer Architecture",
          "brief": "A type of neural network architecture that is particularly well-suited for sequence-to-sequence tasks"
        },
        {
          "name": "Unified Modeling",
          "brief": "A approach to machine learning where a single model is trained to perform multiple tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the underlying architecture and training mechanisms of the AR-Omni model"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges and opportunities of generating text and speech"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the requirements and challenges of generating images"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of training, inference, and evaluation of the AR-Omni model"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.16515": {
    "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Transformers",
          "brief": "A type of deep learning model for video generation"
        },
        {
          "name": "Sparse Attention Mechanisms",
          "brief": "Techniques to reduce computational complexity in attention-based models"
        },
        {
          "name": "Linear Attention",
          "brief": "An efficient alternative to full attention for reducing computational latency"
        },
        {
          "name": "Input-Dependent Gating Mechanism",
          "brief": "A method to balance multiple branches in a neural network based on input data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Transformer Architecture",
          "why_needed": "To understand the context of attention mechanisms and diffusion models"
        },
        {
          "topic": "Deep Learning for Computer Vision",
          "why_needed": "To comprehend the application of diffusion transformers in video generation"
        },
        {
          "topic": "Optimization Techniques for Neural Networks",
          "why_needed": "To appreciate the importance of efficient training and inference in deep learning models"
        },
        {
          "topic": "Video Generation and Processing",
          "why_needed": "To understand the problem domain and the relevance of diffusion transformers"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.15808": {
    "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Deep Research Agents (DRAs)",
          "brief": "Artificial intelligence models that automate knowledge discovery and problem-solving"
        },
        {
          "name": "Self-Evolving Agents",
          "brief": "Agents that improve their performance through self-evaluation and refinement"
        },
        {
          "name": "Test-Time Rubric-Guided Verification",
          "brief": "A method of evaluating agent outputs using predefined rubrics to guide improvement"
        },
        {
          "name": "Inference-Time Scaling of Verification",
          "brief": "The process of scaling verification to improve agent performance during inference time"
        },
        {
          "name": "DRA Failure Taxonomy",
          "brief": "A systematic classification of agent failures into categories and sub-categories"
        },
        {
          "name": "DeepVerifier",
          "brief": "A rubrics-based outcome reward verifier that evaluates agent outputs and provides feedback for improvement"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of artificial neural networks and their applications"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the context of language models and their evaluation"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To grasp the fundamentals of machine learning and autonomous systems"
        },
        {
          "topic": "Machine Learning Evaluation Metrics",
          "why_needed": "To understand the significance of metrics like F1 score and accuracy in evaluating model performance"
        },
        {
          "topic": "Software Development and Integration",
          "why_needed": "To appreciate the importance of modular design and plug-and-play functionality in DeepVerifier"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.18753": {
    "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Hallucinations in LLMs",
          "brief": "Instances where LLMs produce incorrect or nonsensical output"
        },
        {
          "name": "Data-Driven Hallucinations",
          "brief": "Hallucinations caused by biases or inaccuracies in the training data"
        },
        {
          "name": "Reasoning-Driven Hallucinations",
          "brief": "Hallucinations caused by flaws in the LLM's reasoning or inference process"
        },
        {
          "name": "Neural Tangent Kernel (NTK)",
          "brief": "A mathematical framework for analyzing neural networks"
        },
        {
          "name": "Hallucination Risk Bound",
          "brief": "A theoretical framework for understanding and mitigating hallucinations in LLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the applications and challenges of LLMs in language generation"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To grasp the theoretical foundations of the Hallucination Risk Bound and NTK"
        },
        {
          "topic": "Machine Learning Evaluation Metrics",
          "why_needed": "To understand the benchmarks and baselines used to evaluate HalluGuard"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.18137": {
    "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Long-Horizon Agentic Planning",
          "brief": "Planning that involves making decisions over an extended period, considering various constraints and goals"
        },
        {
          "name": "Verifiable Constraints",
          "brief": "Constraints that can be verified and validated, such as time and financial budgets"
        },
        {
          "name": "DeepPlanning Benchmark",
          "brief": "A benchmark for evaluating the planning abilities of agents in complex, real-world scenarios"
        },
        {
          "name": "Constrained Optimization",
          "brief": "The process of finding the best solution to a problem, subject to various constraints and limitations"
        },
        {
          "name": "Active Information Gathering",
          "brief": "The process of proactively seeking and acquiring information to inform decision-making"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and significance of long-horizon agentic planning"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To appreciate the role of LLMs (Large Language Models) in planning and decision-making"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand the concepts of constrained optimization and how they apply to planning problems"
        },
        {
          "topic": "Cognitive Architectures",
          "why_needed": "To recognize the importance of explicit reasoning patterns and tool use in achieving effective planning"
        },
        {
          "topic": "Benchmarking and Evaluation",
          "why_needed": "To comprehend the purpose and design of the DeepPlanning benchmark"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.14127": {
    "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and understand multiple forms of data, such as images and text"
        },
        {
          "name": "Multi-Image Reasoning",
          "brief": "The ability of AI models to reason and make decisions based on multiple images"
        },
        {
          "name": "Safety Risks in AI",
          "brief": "Potential hazards or negative consequences associated with the use of AI models"
        },
        {
          "name": "MIR-SafetyBench",
          "brief": "A benchmark for evaluating the safety of MLLMs in multi-image reasoning tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the basics of AI models and their applications"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the underlying technologies used in MLLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how AI models process and analyze images"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand how AI models process and generate human-like text"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.18778": {
    "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach where an agent learns by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models trained on vast amounts of text data to generate human-like language"
        },
        {
          "name": "Meta-RL",
          "brief": "A subfield of reinforcement learning that involves learning to learn or adapt to new tasks"
        },
        {
          "name": "Automated Curriculum Learning",
          "brief": "A technique where a model generates its own training curriculum to improve learning efficiency"
        },
        {
          "name": "Self-Improvement Frameworks",
          "brief": "Architectures designed to enable models to improve themselves through self-supervised learning or meta-learning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and how they can be applied to reinforcement learning and language models"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To comprehend the concept of sparse, binary rewards and how they affect learning"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To grasp the context of machine learning, reinforcement learning, and language models"
        },
        {
          "topic": "Curriculum Learning",
          "why_needed": "To understand how automated curriculum learning can be used to improve model performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.17367": {
    "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Transformers",
          "brief": "A type of neural network architecture used for natural language processing tasks"
        },
        {
          "name": "Attention Mechanisms",
          "brief": "A technique used in neural networks to focus on specific parts of the input data"
        },
        {
          "name": "Sparsity",
          "brief": "A measure of the proportion of zero or empty elements in a dataset or model"
        },
        {
          "name": "Elastic Attention",
          "brief": "A proposed method for dynamically adjusting the sparsity of attention mechanisms in transformers"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and their applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the context and importance of transformers and attention mechanisms"
        },
        {
          "topic": "Computer Vision and Scalability",
          "why_needed": "To appreciate the challenges of large language models and long-context scenarios"
        },
        {
          "topic": "GPU Computing and Parallel Processing",
          "why_needed": "To understand the training process and computational requirements of the proposed method"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.16451": {
    "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Foundation Models",
          "brief": "Large-scale pre-trained models for general tasks"
        },
        {
          "name": "Pathology Image Segmentation",
          "brief": "Identifying specific regions or features in histopathology images"
        },
        {
          "name": "Interactive Models",
          "brief": "Models that incorporate user feedback for improved performance"
        },
        {
          "name": "Semantic Segmentation",
          "brief": "Assigning a class label to each pixel in an image"
        },
        {
          "name": "Computational Pathology",
          "brief": "Using computational methods for pathology image analysis"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of neural networks and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Knowledge of image processing and analysis techniques"
        },
        {
          "topic": "Pathology",
          "why_needed": "Familiarity with histopathology images and their characteristics"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Understanding of model training and evaluation metrics"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "Knowledge of user feedback mechanisms and their integration with models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.14243": {
    "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to make decisions in complex environments"
        },
        {
          "name": "Quantized Reinforcement Learning",
          "brief": "A technique to reduce computational costs by using lower precision data types, such as FP8"
        },
        {
          "name": "On-Policy vs Off-Policy RL",
          "brief": "On-policy RL learns from experiences gathered while following the same policy, whereas off-policy RL learns from experiences gathered without following the same policy"
        },
        {
          "name": "Unified Training and Rollout Precision Flow",
          "brief": "Using the same precision for both training and rollout phases to minimize numerical discrepancies"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the context of reinforcement learning and its applications"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To appreciate the computational costs and benefits of quantized reinforcement learning"
        },
        {
          "topic": "Numerical Analysis",
          "why_needed": "To understand the implications of using lower precision data types on numerical stability and accuracy"
        },
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To recognize the importance of reinforcement learning in enhancing complex reasoning capabilities of LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.17958": {
    "title": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Transformers",
          "brief": "A type of neural network architecture introduced in 2017 that relies entirely on self-attention mechanisms"
        },
        {
          "name": "Attention Mechanisms",
          "brief": "A technique used in deep learning models to focus on specific parts of the input data"
        },
        {
          "name": "High-Order Attention Tensors",
          "brief": "A mathematical representation that captures the interactions between different components of a transformer model"
        },
        {
          "name": "Linear Operators",
          "brief": "A mathematical function that maps one vector space to another while preserving certain properties"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and their applications"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To comprehend the mathematical formulations and tensor operations used in the paper"
        },
        {
          "topic": "Transformer Architecture",
          "why_needed": "To grasp the specific components and interactions within a transformer model"
        },
        {
          "topic": "Machine Learning Interpretability",
          "why_needed": "To appreciate the importance of understanding and visualizing model behavior"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-27"
  },
  "2601.18202": {
    "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Deep Search",
          "brief": "A process of answering complex questions by reasoning across multiple documents"
        },
        {
          "name": "Steerable Agentic Data Generation",
          "brief": "A method of generating high-quality data using an agentic pipeline with execution feedback"
        },
        {
          "name": "Execution Feedback",
          "brief": "A mechanism for refining generated data based on the performance of a search agent"
        },
        {
          "name": "Data Generator",
          "brief": "A component that proposes question-answer pairs for a given corpus and target difficulty level"
        },
        {
          "name": "Search Agent",
          "brief": "A component that attempts to solve generated questions and provides execution feedback"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and application of deep search and data generation"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training and evaluation of deep search agents"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To grasp the concept of searching and retrieving information from large corpora"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the broader context of agentic pipelines and autonomous systems"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-28"
  },
  "2601.17737": {
    "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agentic Framework",
          "brief": "A novel framework for dialogue-to-cinematic-video generation"
        },
        {
          "name": "ScripterAgent",
          "brief": "A model that translates coarse dialogue into a fine-grained cinematic script"
        },
        {
          "name": "DirectorAgent",
          "brief": "A model that orchestrates video models for long-horizon coherence"
        },
        {
          "name": "Visual-Script Alignment (VSA) metric",
          "brief": "A metric for evaluating script faithfulness and temporal fidelity"
        },
        {
          "name": "Cross-scene continuous generation strategy",
          "brief": "A strategy for ensuring long-horizon coherence in video generation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of models like ScripterAgent and DirectorAgent"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the dialogue-to-script translation process"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To grasp the video generation and analysis aspects of the framework"
        },
        {
          "topic": "Video Generation",
          "why_needed": "To understand the current state-of-the-art models and their limitations"
        },
        {
          "topic": "Multimodal Learning",
          "why_needed": "To appreciate the integration of text, image, and video data in the framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-01-28"
  },
  "2601.11258": {
    "title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Supervised Fine-Tuning (SFT)",
          "brief": "A method to update model knowledge with labeled data"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves trial and error to learn behaviors"
        },
        {
          "name": "Parametric Skill Transfer (PaST)",
          "brief": "A framework for transferring skills between models"
        },
        {
          "name": "Skill Vector",
          "brief": "A domain-agnostic representation of skills that can be transferred between models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of LLMs and SFT"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the challenges of knowledge incorporation in LLMs"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To grasp the concept of RL and its application in skill acquisition"
        },
        {
          "topic": "Transfer Learning",
          "why_needed": "To understand the idea of transferring knowledge between models"
        },
        {
          "topic": "Question Answering and Decision-Making",
          "why_needed": "To appreciate the importance of adapting LLMs for real-world applications"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-28"
  },
  "2601.19532": {
    "title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Benchmarks",
          "brief": "Standardized tests used to evaluate the performance of LLMs"
        },
        {
          "name": "Omni-MATH dataset",
          "brief": "A dataset used to evaluate the mathematical problem-solving abilities of LLMs"
        },
        {
          "name": "Judge-induced noise",
          "brief": "Errors or inconsistencies in the evaluation of LLMs caused by the judges or evaluators"
        },
        {
          "name": "Dataset quality",
          "brief": "The accuracy, completeness, and reliability of the data used to train and evaluate LLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and purpose of LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training and evaluation processes of LLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the complexities of human language and its interaction with LLMs"
        },
        {
          "topic": "Mathematical Problem-Solving",
          "why_needed": "To understand the specific challenges and requirements of the Omni-MATH dataset"
        },
        {
          "topic": "Data Quality and Evaluation",
          "why_needed": "To recognize the importance of accurate and reliable data in assessing LLM performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-28"
  },
  "2601.18631": {
    "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and understand multiple forms of data, such as text and images"
        },
        {
          "name": "Visual Reasoning",
          "brief": "The ability of AI models to reason and draw conclusions based on visual data"
        },
        {
          "name": "Tool Orchestration",
          "brief": "The process of selecting, sequencing, and composing tools to achieve a specific goal"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning that involves training AI models through trial and error by rewarding desired behaviors"
        },
        {
          "name": "Dynamic Learning Mechanisms",
          "brief": "Adaptive systems that adjust their behavior based on changing conditions or new information"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of MLLMs and visual reasoning"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the reinforcement learning algorithm and adaptive learning mechanisms used in AdaReasoner"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To appreciate the challenges and opportunities of visual reasoning in AI models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the role of language in multimodal models and tool orchestration"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-28"
  },
  "2601.19375": {
    "title": "Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Activation Steering",
          "brief": "Techniques to control LLMs' behavior by modifying their internal activations"
        },
        {
          "name": "Norm-Preserving Control",
          "brief": "Methods to maintain the stability and integrity of LLMs' activations during control"
        },
        {
          "name": "Discriminative Layer Selection",
          "brief": "Selecting specific layers in LLMs where steering is applied based on class alignment"
        },
        {
          "name": "Adversarial Attacks",
          "brief": "Techniques to manipulate LLMs into producing harmful or undesirable outputs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of neural networks and their applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Familiarity with language models and their vulnerabilities"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "Understanding of vector spaces and rotations for norm-preserving control"
        },
        {
          "topic": "Machine Learning Security",
          "why_needed": "Knowledge of adversarial attacks and defense mechanisms"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-28"
  },
  "2601.18790": {
    "title": "MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Reasoning Objectives",
          "brief": "Goals that language models are optimized to achieve, such as solving complex tasks"
        },
        {
          "name": "Emergency Contexts",
          "brief": "Situations that require immediate attention and action to prevent harm or danger"
        },
        {
          "name": "MortalMATH Benchmark",
          "brief": "A set of scenarios used to evaluate language models' ability to balance reasoning objectives with emergency contexts"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the basics of language models and their capabilities"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend how language models process and generate human-like text"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand how language models are trained and optimized for specific tasks"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To appreciate the importance of considering emergency contexts in language model design"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-28"
  },
  "2601.20552": {
    "title": "DeepSeek-OCR 2: Visual Causal Flow",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that combine visual and language understanding"
        },
        {
          "name": "Encoder-Decoder Architecture",
          "brief": "A type of neural network architecture used for sequence-to-sequence tasks"
        },
        {
          "name": "Causal Reasoning",
          "brief": "The ability to reason about cause-and-effect relationships"
        },
        {
          "name": "Visual Tokens",
          "brief": "Representations of visual elements in an image"
        },
        {
          "name": "Positional Encoding",
          "brief": "A technique used to preserve sequence order in neural networks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and implementation of DeepSeek-OCR 2"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the processing of visual tokens and image semantics"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the integration of visual and language understanding in VLMs"
        },
        {
          "topic": "Human Visual Perception",
          "why_needed": "To appreciate the inspiration behind the causal reasoning approach"
        },
        {
          "topic": "Neural Network Architectures",
          "why_needed": "To understand the design and functionality of the DeepEncoder V2"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.20614": {
    "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
          "brief": "A mechanism for enhancing mathematical reasoning in large models"
        },
        {
          "name": "Group Relative Policy Optimization (GRPO)",
          "brief": "An algorithm used for policy optimization"
        },
        {
          "name": "Difficulty-Aware Group Policy Optimization (DGPO)",
          "brief": "A proposed algorithm to rectify implicit imbalance in GRPO"
        },
        {
          "name": "Multi-Aspect Question Reformulation (MQR)",
          "brief": "A strategy to reformulate questions and increase difficulty"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the context of RLVR and its application in mathematical reasoning"
        },
        {
          "topic": "Mathematical Reasoning",
          "why_needed": "To comprehend the importance of refining underdeveloped capabilities in large models"
        },
        {
          "topic": "Policy Optimization",
          "why_needed": "To grasp the concepts of GRPO and DGPO"
        },
        {
          "topic": "Data Augmentation",
          "why_needed": "To understand the role of MQR in expanding the data frontier"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.19228": {
    "title": "Towards Pixel-Level VLM Perception via Simple Points Prediction",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and understand multiple forms of data, including text and images"
        },
        {
          "name": "Pixel-Level VLM Perception",
          "brief": "The ability of visual-language models to understand and interpret visual data at the pixel level"
        },
        {
          "name": "Simple Points Prediction",
          "brief": "A method of predicting sequences of points that delineate object boundaries in images"
        },
        {
          "name": "Reinforcement Learning with IoU-based Reward",
          "brief": "A training method that uses a reward function based on the Intersection over Union (IoU) metric to refine point sequences"
        },
        {
          "name": "Sequence Generation Problem",
          "brief": "A problem formulation where the model generates a sequence of outputs, in this case, points delineating object boundaries"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of MLLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand image segmentation and object detection tasks"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the language processing capabilities of MLLMs"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the training pipeline and IoU-based reward function"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.18292": {
    "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach where agents learn from interactions with an environment to achieve a goal"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and generate human-like language"
        },
        {
          "name": "Safety Alignment",
          "brief": "The process of ensuring LLMs generate safe and non-toxic content"
        },
        {
          "name": "Adversarial Prompt Generation",
          "brief": "The process of generating inputs to test an LLM's vulnerability to toxic or harmful content"
        },
        {
          "name": "Tri-Role Framework",
          "brief": "A collaborative framework involving an attacker, defender, and evaluator to improve LLM safety"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the underlying mechanics of LLMs and reinforcement learning"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the challenges and complexities of generating safe and non-toxic language"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the fundamentals of reinforcement learning and its application to LLM safety alignment"
        },
        {
          "topic": "AI Safety and Ethics",
          "why_needed": "To understand the importance and implications of ensuring LLMs generate safe and responsible content"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.18491": {
    "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "AI Agent Safety and Security",
          "brief": "Ensuring autonomous agents behave safely and securely in complex environments"
        },
        {
          "name": "Guardrail Framework",
          "brief": "A structured approach to monitoring and diagnosing risks in AI agent behavior"
        },
        {
          "name": "Agentic Risk Taxonomy",
          "brief": "A three-dimensional categorization of risks by source, failure mode, and consequence"
        },
        {
          "name": "Diagnostic Guardrail",
          "brief": "A framework for fine-grained monitoring and diagnosis of AI agent safety and security"
        },
        {
          "name": "Agent Alignment",
          "brief": "Ensuring AI agents behave in accordance with human values and intentions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "Understanding AI concepts and challenges is essential for grasping the context of the research"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with machine learning models and techniques is necessary for understanding the implementation of AgentDoG"
        },
        {
          "topic": "Safety and Security in Autonomous Systems",
          "why_needed": "Knowledge of safety and security concerns in autonomous systems is crucial for appreciating the significance of the research"
        },
        {
          "topic": "Risk Analysis and Management",
          "why_needed": "Understanding risk analysis and management principles is important for comprehending the agentic risk taxonomy and diagnostic guardrail framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.19834": {
    "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Chain-of-Thought (CoT) Reasoning",
          "brief": "A type of AI reasoning that approximates human cognitive abilities by manipulating concepts within internal world models"
        },
        {
          "name": "Multimodal World Models",
          "brief": "Unified models that integrate verbal and visual representations to facilitate more human-like reasoning"
        },
        {
          "name": "Visual Superiority Hypothesis",
          "brief": "The idea that visual generation is more effective than verbal reasoning for tasks grounded in the physical world"
        },
        {
          "name": "Internal World Modeling",
          "brief": "The process of constructing and manipulating mental models of the world to facilitate reasoning and problem-solving"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence (AI)",
          "why_needed": "To understand the context and current state of AI research in reasoning and world modeling"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To appreciate the human cognitive abilities that AI systems are trying to replicate, such as internal world modeling and reasoning"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the role of visual generation in multimodal world models and its potential benefits for reasoning"
        },
        {
          "topic": "Mathematics and Programming",
          "why_needed": "To recognize the domains where current AI systems have achieved expert-level performance and those where they still lag behind humans"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.20802": {
    "title": "Reinforcement Learning via Self-Distillation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning where an agent learns by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Self-Distillation",
          "brief": "A technique where a model is used as its own teacher to improve its performance"
        },
        {
          "name": "Rich Feedback",
          "brief": "Detailed feedback provided by an environment, such as textual explanations, to help an agent learn"
        },
        {
          "name": "Policy Optimization",
          "brief": "A method to improve a policy in reinforcement learning by optimizing its parameters"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of large language models"
        },
        {
          "topic": "Reinforcement Learning Fundamentals",
          "why_needed": "To comprehend the concepts of rewards, policies, and value functions"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the role of tokenized feedback and textual explanations in the learning process"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To grasp the policy optimization methods used in the Self-Distillation Policy Optimization (SDPO) algorithm"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.20209": {
    "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A subfield of machine learning that involves training agents to take actions in an environment to maximize a reward"
        },
        {
          "name": "Long-Horizon Tasks",
          "brief": "Tasks that require agents to make decisions over an extended period, often with limited feedback or guidance"
        },
        {
          "name": "Dynamic Branching",
          "brief": "A method of selectively exploring different branches or paths in a decision-making process"
        },
        {
          "name": "Policy-Aware Exploration",
          "brief": "A technique that uses an agent's policy to guide exploration and improve decision-making"
        },
        {
          "name": "Embodied Planning",
          "brief": "A type of planning that involves an agent navigating and interacting with a physical environment"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning Fundamentals",
          "why_needed": "To understand the basics of reinforcement learning and how it applies to long-horizon tasks"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the role of large language models in reinforcement learning and their limitations"
        },
        {
          "topic": "Decision-Making under Uncertainty",
          "why_needed": "To appreciate the challenges of making decisions in complex, dynamic environments"
        },
        {
          "topic": "Resource Allocation",
          "why_needed": "To understand the importance of efficient resource allocation in exploration and decision-making"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.19798": {
    "title": "Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that combine visual and linguistic information for multimodal comprehension"
        },
        {
          "name": "Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm",
          "brief": "A training paradigm that treats visual signals as supervisory targets rather than passive inputs"
        },
        {
          "name": "Autoregressive supervision",
          "brief": "A technique where the model predicts the next token in a sequence based on previous tokens"
        },
        {
          "name": "Multimodal comprehension",
          "brief": "The ability of a model to understand and generate text and images"
        },
        {
          "name": "Vision-centric tasks",
          "brief": "Tasks that require a model to focus on visual information, such as image classification or object detection"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the architecture and training of VLMs"
        },
        {
          "topic": "Computer vision",
          "why_needed": "To understand visual information processing and vision-centric tasks"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "To understand linguistic content processing and text generation"
        },
        {
          "topic": "Machine learning",
          "why_needed": "To understand the optimization objectives and training paradigms used in VLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.17950": {
    "title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Feature Upsampling",
          "brief": "A technique to create denser features from pre-trained visual backbones"
        },
        {
          "name": "Iterative Upsampling",
          "brief": "A method to upsample features through iterative refinement"
        },
        {
          "name": "Cross-Attention-Based Methods",
          "brief": "A method to upsample features using cross-attention mechanisms"
        },
        {
          "name": "Local Attender Operator",
          "brief": "An efficient operator to overcome limitations of prior iterative feature upsampling methods"
        },
        {
          "name": "UPLiFT Architecture",
          "brief": "A proposed architecture for Universal Pixel-dense Lightweight Feature Transforms"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the context of feature upsampling and visual backbones"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the application of feature upsampling in tasks like image generation and processing"
        },
        {
          "topic": "Attention Mechanisms",
          "why_needed": "To grasp the concept of cross-attention-based methods and the proposed Local Attender operator"
        },
        {
          "topic": "Generative Models",
          "why_needed": "To understand the application of UPLiFT in generative downstream tasks like VAE feature upsampling"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-29"
  },
  "2601.18129": {
    "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and understand human language"
        },
        {
          "name": "Sovereign Capability",
          "brief": "The ability of a model to perform high-stakes, region-specific tasks while maintaining control and understanding of model weights and data"
        },
        {
          "name": "Post-Training Methods",
          "brief": "Techniques used to fine-tune and adapt pre-trained language models for specific tasks and domains"
        },
        {
          "name": "Reinforcement Fine-Tuning (RFT)",
          "brief": "A method of fine-tuning models using rewards or penalties to optimize performance"
        },
        {
          "name": "Typhoon-S",
          "brief": "A minimal and open post-training recipe for adapting language models to sovereign settings"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the fundamentals of language models and their applications"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of large language models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model fine-tuning, reinforcement learning, and distillation"
        },
        {
          "topic": "Language Model Adaptation",
          "why_needed": "To understand the challenges and techniques involved in adapting pre-trained models to new domains and tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-30"
  },
  "2601.21872": {
    "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Web Agents",
          "brief": "Software programs that automate complex computer tasks on the web"
        },
        {
          "name": "Process Reward Models (PRMs)",
          "brief": "Models that provide feedback on the decision-making process of web agents"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning that involves training agents to make decisions based on rewards or penalties"
        },
        {
          "name": "Reasoning Distillation",
          "brief": "A technique used to equip models with coherent principle-guided reasoning"
        },
        {
          "name": "Text Generation",
          "brief": "A technique used to produce structured justifications for decision-making"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concept of web agents and their applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the reinforcement learning and reasoning distillation techniques used in the paper"
        },
        {
          "topic": "Web Development",
          "why_needed": "To understand the web environments and tasks used in the benchmark"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the text generation technique used in the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-30"
  },
  "2601.19494": {
    "title": "AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Automated Code Review (ACR)",
          "brief": "The use of artificial intelligence and machine learning to review code for errors and improvements"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models trained on vast amounts of text data to generate human-like language"
        },
        {
          "name": "Repository-Level Context",
          "brief": "The consideration of the entire code repository when reviewing code, rather than just individual files"
        },
        {
          "name": "AI-Assisted Annotation",
          "brief": "The use of AI to assist in the annotation and labeling of data for machine learning model training"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Programming Languages",
          "why_needed": "To understand the context of code review and the importance of multi-language support"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of LLMs, model training, and evaluation"
        },
        {
          "topic": "Software Development",
          "why_needed": "To appreciate the role of code review in the development process and the challenges of evaluating ACR models"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the application of LLMs in code review and the importance of context in NLP tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-30"
  },
  "2601.19325": {
    "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models",
          "brief": "AI models that process and understand multiple forms of data, such as text and images"
        },
        {
          "name": "Scientific Discovery",
          "brief": "Using AI to advance understanding and reasoning in scientific domains"
        },
        {
          "name": "Principled Training Design",
          "brief": "A systematic approach to training AI models, emphasizing transparency and reproducibility"
        },
        {
          "name": "Data Efficiency",
          "brief": "Achieving strong performance with limited data, reducing the need for large-scale pretraining"
        },
        {
          "name": "Generalization",
          "brief": "The ability of a model to perform well on a variety of tasks and datasets"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the fundamentals of large language models and multimodal processing"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the text-based aspects of the model and its applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To grasp the image-based aspects of the model and its general vision capabilities"
        },
        {
          "topic": "Scientific Domains",
          "why_needed": "To appreciate the model's applications and performance in various scientific fields"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the model's training methodology and optimization techniques"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-01-30"
  },
  "2601.22158": {
    "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion/Flow-based Models",
          "brief": "A type of generative model that uses a process to transform noise into a realistic image"
        },
        {
          "name": "Latent Space",
          "brief": "A lower-dimensional representation of the input data, used for generative modeling"
        },
        {
          "name": "Pixel Mean Flows",
          "brief": "A technique for one-step latent-free image generation using a presumed low-dimensional image manifold"
        },
        {
          "name": "Image Manifold",
          "brief": "A geometric space that represents the structure of an image"
        },
        {
          "name": "MeanFlow",
          "brief": "A loss function defined in the velocity space for training generative models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and generative models"
        },
        {
          "topic": "Generative Models",
          "why_needed": "To comprehend the concepts of diffusion/flow-based models and latent space"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand image processing and analysis techniques"
        },
        {
          "topic": "Mathematics (Linear Algebra, Calculus)",
          "why_needed": "To grasp the mathematical formulations of the proposed method"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-30"
  },
  "2601.22054": {
    "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Metric Depth Estimation",
          "brief": "Estimating the depth of objects in a scene using metric units"
        },
        {
          "name": "Pretraining Framework",
          "brief": "A method for training models on large datasets before fine-tuning for specific tasks"
        },
        {
          "name": "Sparse Metric Prompt",
          "brief": "A technique for creating a universal interface for decoupling spatial reasoning from sensor and camera biases"
        },
        {
          "name": "Vision Foundation Models",
          "brief": "Large-scale models that serve as a foundation for various computer vision tasks"
        },
        {
          "name": "Multimodal Large Language Models",
          "brief": "Models that process and generate multiple forms of data, including text, images, and audio"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding of computer vision concepts, such as depth estimation and image processing"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with machine learning concepts, including pretraining, fine-tuning, and model architectures"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Knowledge of deep learning techniques, including convolutional neural networks and transformers"
        },
        {
          "topic": "3D Reconstruction",
          "why_needed": "Understanding of 3D reconstruction techniques and challenges, such as handling noisy and heterogeneous data"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Familiarity with natural language processing concepts, including language models and multimodal processing"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-30"
  },
  "2601.22069": {
    "title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Text Compression",
          "brief": "A technique to compress textual data into compact images for efficient reasoning"
        },
        {
          "name": "Long-Context Reasoning",
          "brief": "A process that enables large language models to tackle complex tasks by considering lengthy contextual information"
        },
        {
          "name": "Vision-Language Models",
          "brief": "AI models that can process and understand both visual and textual data"
        },
        {
          "name": "Optical Memory",
          "brief": "A concept where compact images are used as a form of memory to store and retrieve information in vision-language models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and limitations of long-context reasoning"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the concept of vision-text compression and optical memory"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the challenges and opportunities in long-context reasoning and vision-language models"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To appreciate the training and fine-tuning of vision-language models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-30"
  },
  "2601.18150": {
    "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves training agents to make decisions in complex environments"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        },
        {
          "name": "FP8 (8-bit Floating Point)",
          "brief": "A low-precision data type used to reduce computational cost and memory usage"
        },
        {
          "name": "Quantization",
          "brief": "The process of converting high-precision data to lower precision"
        },
        {
          "name": "Importance Sampling",
          "brief": "A technique used to correct for discrepancies between training and inference environments"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the context of LLMs and RL"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To appreciate the benefits and challenges of using low-precision data types like FP8"
        },
        {
          "topic": "Machine Learning Optimization",
          "why_needed": "To understand the importance of mitigating train-inference mismatch"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the applications and challenges of LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-30"
  },
  "2601.22143": {
    "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Audio-Visual Foundation Models",
          "brief": "Pretrained models that jointly generate sound and visual content"
        },
        {
          "name": "Diffusion Models",
          "brief": "Deep learning models that use a process called diffusion-based image synthesis"
        },
        {
          "name": "LoRA (Low-Rank Adaptation)",
          "brief": "A technique for adapting large pre-trained models to new tasks with minimal additional parameters"
        },
        {
          "name": "Video Dubbing",
          "brief": "The process of replacing the original audio of a video with a new audio track, often in a different language"
        },
        {
          "name": "Multimodal Generation and Editing",
          "brief": "The ability of models to generate and edit multiple forms of data, such as audio and video"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of diffusion models, foundation models, and LoRA"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand video processing, facial motion, and lip synchronization"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the challenges of multilingual video dubbing and the importance of preserving speaker identity"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "To understand audio processing and synchronization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-30"
  },
  "2601.21996": {
    "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mechanistic Interpretability",
          "brief": "A method to understand how large language models (LLMs) work by identifying interpretable circuits"
        },
        {
          "name": "Influence Functions",
          "brief": "A technique to trace the impact of specific training samples on model behavior"
        },
        {
          "name": "Mechanistic Data Attribution (MDA)",
          "brief": "A framework to attribute interpretable units in LLMs to specific training samples"
        },
        {
          "name": "Interpretable Units",
          "brief": "Components of LLMs that can be understood and interpreted"
        },
        {
          "name": "In-context Learning (ICL)",
          "brief": "The ability of LLMs to learn from context and adapt to new tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and application of Mechanistic Interpretability and MDA"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the concepts of Influence Functions and model training"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the relevance of LLMs and ICL in NLP tasks"
        },
        {
          "topic": "Causal Inference",
          "why_needed": "To appreciate the causal validation of targeted interventions in the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-31"
  },
  "2601.21343": {
    "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Pretraining",
          "brief": "Initial training phase for language models using large datasets"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "Machine learning approach that uses rewards to improve model performance"
        },
        {
          "name": "Fine-tuning and Alignment",
          "brief": "Techniques used to adjust and optimize language models for specific tasks"
        },
        {
          "name": "Factuality and Safety",
          "brief": "Critical aspects of language models that ensure generated content is accurate and safe"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of neural networks and machine learning fundamentals"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "Knowledge of language models, text generation, and language understanding"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with training methods, including reinforcement learning and supervised learning"
        },
        {
          "topic": "Computer Science",
          "why_needed": "Basic understanding of programming, data structures, and algorithms"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-31"
  },
  "2601.21268": {
    "title": "Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning where an agent learns by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Meta-Evaluation",
          "brief": "Evaluating the performance of a model using natural-language meta-questions"
        },
        {
          "name": "Language Models",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Group-Relative Policy Optimization",
          "brief": "An optimization technique used to update the generator in RLME"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of language models and reinforcement learning"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the challenges of training language models without ground-truth labels"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of optimization, evaluation, and generalization in AI models"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To understand the use of probability in meta-evaluation and reward derivation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-31"
  },
  "2601.20730": {
    "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Agent-Environment Interaction",
          "brief": "The dynamic relationship between an autonomous agent and its environment"
        },
        {
          "name": "Lateral Thinking Puzzles",
          "brief": "Puzzles that require creative, non-linear thinking to solve"
        },
        {
          "name": "Environment Rollouts",
          "brief": "Simulated interactions between an agent and its environment to evaluate performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of LLMs and agent-environment interaction"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the challenges of language understanding and generation"
        },
        {
          "topic": "Cognitive Architectures",
          "why_needed": "To appreciate the importance of dynamic context management and information synthesis"
        },
        {
          "topic": "Benchmarking and Evaluation",
          "why_needed": "To recognize the need for rigorous testing of autonomous agents"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-31"
  },
  "2601.20465": {
    "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Brain-inspired Multi-Agent Memory (BMAM)",
          "brief": "A memory architecture that models agent memory as functionally specialized subsystems"
        },
        {
          "name": "Cognitive Memory Systems",
          "brief": "The study of how the brain processes and stores information"
        },
        {
          "name": "Episodic Memory",
          "brief": "A type of memory that stores specific events and experiences"
        },
        {
          "name": "Semantic Memory",
          "brief": "A type of memory that stores general knowledge and facts"
        },
        {
          "name": "Salience-aware Memory",
          "brief": "A type of memory that prioritizes important or relevant information"
        },
        {
          "name": "Control-oriented Memory",
          "brief": "A type of memory that guides decision-making and action"
        },
        {
          "name": "Long-horizon Reasoning",
          "brief": "The ability to make decisions based on extended periods of time"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of language-model-based agents and multi-agent systems"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To comprehend the inspiration behind BMAM's architecture and its components"
        },
        {
          "topic": "Neuroscience",
          "why_needed": "To understand the brain's memory systems and their functional specializations"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To appreciate the implementation and evaluation of BMAM"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To grasp the technical aspects of BMAM's design and implementation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-31"
  },
  "2601.20381": {
    "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Object-centric Representation",
          "brief": "A method of representing visual data that focuses on individual objects and their properties"
        },
        {
          "name": "Visual Foundation Models",
          "brief": "Pre-trained models that provide strong perceptual features for robotics tasks"
        },
        {
          "name": "Slot-based Representation",
          "brief": "A way of organizing object-centric representations into a set of semantic-aware slots"
        },
        {
          "name": "Multi-phase Training Strategy",
          "brief": "A training approach that involves multiple stages, including pretraining and joint adaptation"
        },
        {
          "name": "Robotic Manipulation",
          "brief": "The use of robots to manipulate and interact with objects in their environment"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of pre-trained models, object-centric representations, and multi-phase training strategies"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how visual foundation models work and how they can be applied to robotics tasks"
        },
        {
          "topic": "Robotics",
          "why_needed": "To understand the context and applications of robotic manipulation and object-centric representations"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand how language embeddings can be used for visual-semantic pretraining"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of generalization, control performance, and task-aware representations"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-31"
  },
  "2601.21204": {
    "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts (MoE) architectures",
          "brief": "A type of neural network architecture that uses multiple expert models to improve performance"
        },
        {
          "name": "Embedding scaling",
          "brief": "A technique for scaling up language models by increasing the size of embeddings"
        },
        {
          "name": "Sparsity scaling",
          "brief": "A technique for improving the efficiency of large language models by reducing the number of parameters"
        },
        {
          "name": "Pareto frontier",
          "brief": "A concept in multi-objective optimization that refers to the set of optimal solutions that balance competing objectives"
        },
        {
          "name": "Speculative decoding",
          "brief": "A technique for improving the inference speed of language models by predicting the output before the entire input is processed"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the basics of neural networks and language models"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the application of language models and the importance of scaling"
        },
        {
          "topic": "Computer architecture",
          "why_needed": "To understand the system-level bottlenecks and optimizations discussed in the paper"
        },
        {
          "topic": "Optimization techniques",
          "why_needed": "To understand the concepts of Pareto frontier and speculative decoding"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-01-31"
  },
  "2601.21590": {
    "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Reinforcement Learning (RL) post-training",
          "brief": "A method to improve LLMs' performance by training them with rewards or penalties"
        },
        {
          "name": "Distribution Sharpening",
          "brief": "A technique to improve LLMs' performance by refining their output distributions"
        },
        {
          "name": "Markov Chain Monte Carlo (MCMC)",
          "brief": "A computational method for sampling from complex probability distributions"
        },
        {
          "name": "Power Sampling",
          "brief": "A method for sampling from a distribution by raising it to a power"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of LLMs and RL"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To comprehend distribution sharpening and MCMC"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the applications of LLMs"
        },
        {
          "topic": "Mathematics",
          "why_needed": "To follow the theoretical formulation and derivations in the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-01"
  },
  "2601.22101": {
    "title": "ECO: Quantized Training without Full-Precision Master Weights",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Quantization",
          "brief": "A technique to reduce the precision of model weights and activations to improve computational efficiency"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "A type of artificial intelligence model designed to process and understand human language"
        },
        {
          "name": "Sparse Mixture of Experts (SMoE) models",
          "brief": "A type of neural network architecture that uses a mixture of expert models to improve performance and efficiency"
        },
        {
          "name": "Error-Compensating Optimizer (ECO)",
          "brief": "A novel optimizer that eliminates the need for master weights in quantized training"
        },
        {
          "name": "Master weights",
          "brief": "High-precision weight buffers used in traditional quantized training approaches"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning fundamentals",
          "why_needed": "To understand the context and basics of LLMs and quantization"
        },
        {
          "topic": "Neural network architectures",
          "why_needed": "To comprehend the design and functionality of SMoE models"
        },
        {
          "topic": "Optimization techniques",
          "why_needed": "To grasp the concept of ECO and its significance in quantized training"
        },
        {
          "topic": "Computer architecture and memory management",
          "why_needed": "To appreciate the memory overhead introduced by master weights and the benefits of ECO"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-01"
  },
  "2601.21051": {
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Native Reasoning Model",
          "brief": "A type of AI model that can reason and draw conclusions based on given information"
        },
        {
          "name": "Supervised Fine-Tuning (SFT)",
          "brief": "A machine learning technique used to fine-tune a pre-trained model on a specific task"
        },
        {
          "name": "Reinforcement Learning from Verifiable Rewards (RLVR)",
          "brief": "A machine learning technique that uses rewards to train a model to make decisions"
        },
        {
          "name": "Cybersecurity Analysis",
          "brief": "The process of analyzing and identifying potential security threats in a system or network"
        },
        {
          "name": "Multi-Hop Reasoning",
          "brief": "The ability of a model to reason and draw conclusions based on multiple pieces of information"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the training process and techniques used in the paper"
        },
        {
          "topic": "Cybersecurity",
          "why_needed": "To understand the context and applications of the model"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the broader implications and potential applications of the model"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of the model"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the input and output formats of the model"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-01"
  },
  "2601.18005": {
    "title": "Flow-based Extremal Mathematical Structure Discovery",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Flow-based Generative Models",
          "brief": "A type of deep learning model that uses normalizing flows to generate samples from complex distributions"
        },
        {
          "name": "Geometry-aware Conditional Flow-matching Model",
          "brief": "A model that combines geometric knowledge with flow-based generative models to sample high-quality configurations"
        },
        {
          "name": "Reward-guided Policy Optimization",
          "brief": "A reinforcement learning approach that optimizes policies based on reward signals"
        },
        {
          "name": "Stochastic Local Search",
          "brief": "A optimization technique that uses random perturbations to search for better solutions"
        },
        {
          "name": "Extremal Mathematical Structure Discovery",
          "brief": "The process of finding rare and extreme geometric structures in mathematics"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of flow-based generative models and policy optimization"
        },
        {
          "topic": "Geometry and Mathematical Optimization",
          "why_needed": "To appreciate the geometric problems being tackled and the importance of extremal structure discovery"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To grasp the concept of reward-guided policy optimization and its application in this context"
        },
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the differences between FlowBoost and prior approaches that rely on LLMs"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-01"
  },
  "2601.21416": {
    "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Object-Centric Representations",
          "brief": "Representing visual data in a way that focuses on specific objects or entities within a scene"
        },
        {
          "name": "Robotic Manipulation",
          "brief": "The use of robots to manipulate and interact with objects in their environment"
        },
        {
          "name": "Visual Representations",
          "brief": "The way in which visual data is processed and represented for use in robotic systems"
        },
        {
          "name": "Slot-Based Object-Centric Representations (SBOCR)",
          "brief": "A specific type of object-centric representation that groups dense features into a finite set of object-like entities"
        },
        {
          "name": "Generalization in Robotics",
          "brief": "The ability of a robotic system to perform well in new, unseen situations or environments"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of pre-trained encoders, dense features, and global features"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how visual data is processed and represented in robotic systems"
        },
        {
          "topic": "Robotics",
          "why_needed": "To understand the concepts of robotic manipulation and the challenges of generalization in robotics"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of policy learning and generalization in robotic systems"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-01"
  },
  "2601.21181": {
    "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and generate multiple forms of data, such as text, images, and audio"
        },
        {
          "name": "Cross-Modal Hallucinations",
          "brief": "When one modality inappropriately influences generation about another, leading to fabricated output"
        },
        {
          "name": "Modality-Adaptive Decoding (MAD)",
          "brief": "A training-free method that adaptively weights modality-specific decoding branches based on task requirements"
        },
        {
          "name": "Contrastive Decoding",
          "brief": "A method that enables the model to focus on relevant information while suppressing cross-modal interference"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of MLLMs"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the language generation and processing aspects of MLLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the visual processing aspects of MLLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, evaluation, and optimization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-01"
  },
  "2601.22664": {
    "title": "Real-Time Aligned Reward Model beyond Semantics",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning from Human Feedback (RLHF)",
          "brief": "A technique for aligning large language models with human preferences"
        },
        {
          "name": "Reward Overoptimization",
          "brief": "The phenomenon where policy models overfit to the reward model and exploit spurious reward patterns"
        },
        {
          "name": "Real-Time Aligned Reward Model (R2M)",
          "brief": "A novel lightweight RLHF framework that leverages policy feedback to align with the real-time distribution shift of the policy"
        },
        {
          "name": "Policy Distribution Shift",
          "brief": "The change in the policy model's distribution during the reinforcement learning process"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of RL and its applications"
        },
        {
          "topic": "Large Language Models",
          "why_needed": "To comprehend the context and limitations of LLMs"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the concepts of policy models, reward models, and their interactions"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To understand the importance of aligning AI models with human preferences and intent"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.21420": {
    "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "ConceptMoE",
          "brief": "A model that dynamically merges semantically similar tokens into concept representations for adaptive token-to-concept compression"
        },
        {
          "name": "Mixture of Experts (MoE)",
          "brief": "A deep learning architecture that enables multiple models to be trained together and used for a single task"
        },
        {
          "name": "Token-level Compute Allocation",
          "brief": "The process of allocating computational resources to different tokens in a sequence based on their semantic similarity"
        },
        {
          "name": "Learnable Chunk Module",
          "brief": "A module that identifies optimal boundaries for merging tokens into concept representations"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models",
          "why_needed": "To understand the limitations of uniform computation allocation across all tokens"
        },
        {
          "topic": "Deep Learning Architectures",
          "why_needed": "To comprehend the MoE architecture and its applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the importance of semantic similarity in token representation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To appreciate the model's performance on vision-language tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.21358": {
    "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Chain-of-Thought (CoT)",
          "brief": "A method that enables Large Language Models (LLMs) to solve complex problems by generating a sequence of intermediate reasoning steps"
        },
        {
          "name": "Latent Reasoning",
          "brief": "A technique that performs reasoning within continuous hidden states to improve efficiency"
        },
        {
          "name": "Planning with Latent Thoughts (PLaT)",
          "brief": "A framework that decouples reasoning from verbalization by modeling reasoning as a deterministic trajectory of latent planning states"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and generate human-like language"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of LLMs and CoT"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of latent reasoning, planning, and model architecture"
        },
        {
          "topic": "Mathematical Benchmarks",
          "why_needed": "To evaluate the performance of PLaT and understand the trade-offs in reasoning diversity and accuracy"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.21558": {
    "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Tool-Augmented Agents",
          "brief": "Agents that use external tools to make decisions"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "Machine learning approach that involves training agents through trial and error"
        },
        {
          "name": "Supervised Fine-Tuning (SFT)",
          "brief": "Technique to fine-tune pre-trained models on specific tasks"
        },
        {
          "name": "Trajectory Synthesis",
          "brief": "Generating diverse paths for agents to follow"
        },
        {
          "name": "Environment Synthesis",
          "brief": "Creating simulated environments for agents to learn in"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of RL, SFT, and LLMs"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To comprehend the concept of tool-augmented agents and their applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the role of LLMs in generating human-like language"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the concepts of programming, code execution, and software development"
        },
        {
          "topic": "Mathematics",
          "why_needed": "To understand the mathematical foundations of RL and SFT"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.22156": {
    "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Hybrid Transformer architectures",
          "brief": "Combining softmax attention blocks and recurrent neural networks (RNNs) for long-context modeling"
        },
        {
          "name": "Knowledge distillation",
          "brief": "Transferring knowledge from pre-trained models to smaller models"
        },
        {
          "name": "Parameter transfer",
          "brief": "Converting pre-trained softmax attention blocks into RNN blocks"
        },
        {
          "name": "Position encoding schemes",
          "brief": "Techniques for encoding positional information in sequence models"
        },
        {
          "name": "Layer optimization",
          "brief": "Optimizing neural network layers for efficient distillation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Transformer models",
          "why_needed": "Understanding the basics of Transformer models is necessary to comprehend the conversion process"
        },
        {
          "topic": "Recurrent neural networks (RNNs)",
          "why_needed": "Knowledge of RNNs is required to understand the hybrid architecture"
        },
        {
          "topic": "Deep learning",
          "why_needed": "Familiarity with deep learning concepts, such as training and optimization, is necessary to understand the paper"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "Understanding NLP concepts, such as language modeling, is necessary to appreciate the application of the proposed method"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.20833": {
    "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Autonomous Scientific Discovery",
          "brief": "Using AI to automate the scientific research process"
        },
        {
          "name": "Pre-computation-driven Framework",
          "brief": "A approach that relies on pre-processing and storing data for efficient retrieval"
        },
        {
          "name": "Methodological Knowledge Graph",
          "brief": "A structured graph that represents research methods and patterns"
        },
        {
          "name": "Research Intent Alignment",
          "brief": "Aligning user research goals with established research paradigms"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the capabilities and limitations of LLMs"
        },
        {
          "topic": "Scientific Research Methods",
          "why_needed": "To comprehend the research process and the importance of methodological knowledge"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the concepts of language generation and understanding"
        },
        {
          "topic": "Graph Theory",
          "why_needed": "To understand the structure and organization of the methodological knowledge graph"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To appreciate the computational aspects of the pre-computation-driven framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.22153": {
    "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language-Action (VLA) models",
          "brief": "Models that integrate vision, language, and action to manipulate objects"
        },
        {
          "name": "Dynamic object manipulation",
          "brief": "The ability to manipulate objects that are moving or changing"
        },
        {
          "name": "Temporal reasoning",
          "brief": "The ability to reason about time and sequences of events"
        },
        {
          "name": "Closed-loop adaptation",
          "brief": "The ability to adapt to changing situations through continuous feedback and adjustment"
        },
        {
          "name": "Convolutional vision encoder",
          "brief": "A type of neural network architecture for efficient image processing"
        },
        {
          "name": "Continuous Inference",
          "brief": "A technique for overlapping reasoning and execution to reduce latency"
        },
        {
          "name": "Latent-aware Action Streaming",
          "brief": "A technique for bridging the perception-execution gap through temporally aligned action execution"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the architecture and training of VLA models"
        },
        {
          "topic": "Computer vision",
          "why_needed": "To understand the processing and analysis of visual data"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "To understand the integration of language and action in VLA models"
        },
        {
          "topic": "Robotics and control",
          "why_needed": "To understand the manipulation of dynamic objects and the importance of timely adaptation"
        },
        {
          "topic": "Machine learning benchmarks and datasets",
          "why_needed": "To understand the creation and use of the Dynamic Object Manipulation (DOM) benchmark"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.22491": {
    "title": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning paradigm for training agents to take actions in an environment to maximize rewards"
        },
        {
          "name": "Sweet Spot Learning (SSL)",
          "brief": "A novel framework providing differentiated guidance for agent optimization through tiered rewards"
        },
        {
          "name": "Agentic Optimization",
          "brief": "The process of optimizing agent performance in complex tasks"
        },
        {
          "name": "Gradient Signal-to-Noise Ratio",
          "brief": "A measure of the quality of the gradient signal in optimization algorithms"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning Fundamentals",
          "why_needed": "To understand the basics of reinforcement learning and optimization algorithms"
        },
        {
          "topic": "Reinforcement Learning Algorithms",
          "why_needed": "To appreciate the limitations of existing methods and the novelty of SSL"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To comprehend the theoretical analysis of SSL and its implications on solution ordering and gradient signal-to-noise ratio"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the implementation details of SSL and its applications in various tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.23182": {
    "title": "FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Language Models (dLLMs)",
          "brief": "A type of language model that generates text through a process of diffusion, which involves iteratively refining the input until a coherent output is produced"
        },
        {
          "name": "Non-Autoregressive Generation",
          "brief": "A decoding strategy that generates text in parallel, without relying on previous output to generate the next token"
        },
        {
          "name": "Frequency-Domain Analysis",
          "brief": "A method of analyzing signals or data in terms of their frequency components, which can reveal underlying patterns or structures"
        },
        {
          "name": "Fourier Transform",
          "brief": "A mathematical technique for decomposing a signal into its frequency components"
        },
        {
          "name": "Structure-to-Detail Generation",
          "brief": "A generation strategy that prioritizes global structure and long-range dependencies before focusing on local details"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of language models and diffusion processes"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To appreciate the challenges and opportunities of text generation tasks"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "To grasp the concepts of frequency-domain analysis and the Fourier transform"
        },
        {
          "topic": "Autoregressive Models",
          "why_needed": "To compare and contrast with non-autoregressive generation strategies"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.23188": {
    "title": "Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Deep Search",
          "brief": "A technique using large language models for multi-step retrieval and reasoning"
        },
        {
          "name": "Meta-Cognitive Monitoring",
          "brief": "A mechanism inspired by cognitive neuroscience to monitor and regulate reasoning and retrieval states"
        },
        {
          "name": "Hierarchical Meta-Cognitive Monitoring",
          "brief": "A framework integrating fast anomaly detection with selectively triggered, experience-driven reflection"
        },
        {
          "name": "Fast Consistency Monitor",
          "brief": "A lightweight check on the alignment between external evidence and internal reasoning confidence"
        },
        {
          "name": "Slow Experience-Driven Monitor",
          "brief": "A selectively activated monitor guiding corrective intervention based on experience memory from historical agent trajectories"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Cognitive Neuroscience",
          "why_needed": "To understand the inspiration behind meta-cognitive monitoring and its hierarchical organization"
        },
        {
          "topic": "Large Language Models",
          "why_needed": "To comprehend the capabilities and limitations of deep search agents"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the integration of meta-cognitive monitoring with deep search frameworks"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To appreciate the context and applications of deep search and meta-cognitive monitoring"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.20732": {
    "title": "Continual GUI Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Continual Learning",
          "brief": "A machine learning approach that enables agents to learn from a stream of data and adapt to changes over time"
        },
        {
          "name": "GUI Agents",
          "brief": "Software agents that interact with graphical user interfaces (GUIs) to perform tasks"
        },
        {
          "name": "Reinforcement Fine-Tuning",
          "brief": "A technique used to adapt pre-trained models to new tasks or environments through reinforcement learning"
        },
        {
          "name": "GUI-Anchoring in Flux (GUI-AiF)",
          "brief": "A reinforcement fine-tuning framework for continual learning in GUI agents"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of continual learning and reinforcement fine-tuning"
        },
        {
          "topic": "Graphical User Interfaces (GUIs)",
          "why_needed": "To comprehend the complexities of GUI interactions and the challenges of learning in dynamic GUI environments"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To grasp the concepts of rewards, agents, and environments in the context of GUI-AiF"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the underlying techniques used in GUI-AiF, such as neural networks and fine-tuning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.21957": {
    "title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "PaddleOCR-VL-1.5",
          "brief": "An upgraded model for robust in-the-wild document parsing"
        },
        {
          "name": "OmniDocBench v1.5",
          "brief": "A benchmark for evaluating document parsing models"
        },
        {
          "name": "Real5-OmniDocBench",
          "brief": "A benchmark for evaluating robustness against real-world physical distortions"
        },
        {
          "name": "VLM (Visual-Language Model)",
          "brief": "A type of model that combines visual and language understanding"
        },
        {
          "name": "Seal recognition",
          "brief": "A task that involves identifying and recognizing seals in documents"
        },
        {
          "name": "Text spotting",
          "brief": "A task that involves detecting and recognizing text in images"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the architecture and training of PaddleOCR-VL-1.5"
        },
        {
          "topic": "Computer vision",
          "why_needed": "To understand the image processing and feature extraction techniques used in the model"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "To understand the language understanding and text recognition capabilities of the model"
        },
        {
          "topic": "Document analysis",
          "why_needed": "To understand the challenges and requirements of parsing documents in the wild"
        },
        {
          "topic": "Benchmarking and evaluation",
          "why_needed": "To understand the metrics and methodologies used to evaluate the model's performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.22636": {
    "title": "Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Adversarial Risk",
          "brief": "The risk of a model producing harmful or undesirable responses when subjected to malicious input"
        },
        {
          "name": "Best-of-N Sampling",
          "brief": "A method of repeatedly sampling a model to determine its vulnerability to adversarial attacks"
        },
        {
          "name": "Beta Distribution",
          "brief": "A statistical distribution used to model the probability of success in a series of trials"
        },
        {
          "name": "Scaling Law",
          "brief": "A mathematical relationship that describes how a system's behavior changes as its size or complexity increases"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of LLMs and adversarial risk"
        },
        {
          "topic": "Statistical Modeling",
          "why_needed": "To comprehend the use of Beta distribution and scaling law in the research"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the context and applications of LLMs"
        },
        {
          "topic": "Adversarial Attacks",
          "why_needed": "To understand the concept of adversarial risk and its importance in LLM evaluation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.23143": {
    "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Reasoning Models (LRMs)",
          "brief": "Models that achieve high performance on reasoning tasks using reinforcement learning"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves training models to make decisions based on rewards or penalties"
        },
        {
          "name": "Chain-of-Thought (CoT) Reasoning",
          "brief": "A type of reasoning that involves generating a sequence of intermediate steps to arrive at a conclusion"
        },
        {
          "name": "Self-Generated Safety Alignment",
          "brief": "A framework that aligns models with safety mechanisms without relying on external teachers"
        },
        {
          "name": "Teacher Distillation",
          "brief": "A technique that involves training a model to mimic the behavior of a teacher model"
        },
        {
          "name": "Distributional Discrepancy",
          "brief": "A difference in the distribution of data between two models or datasets"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of model training and evaluation"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To comprehend the concept of reward-based learning and its applications"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context of language models and their potential safety risks"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To familiarize oneself with the architecture and training of deep neural networks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-02"
  },
  "2601.22642": {
    "title": "Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Formal Logic Verification",
          "brief": "Method to verify the correctness of logical statements using formal systems"
        },
        {
          "name": "Neuro-Symbolic Methods",
          "brief": "Approaches that combine neural networks with symbolic AI to improve reasoning and logic"
        },
        {
          "name": "Natural Language Generation",
          "brief": "Process of generating human-like language using AI models"
        },
        {
          "name": "Two-Stage Training Pipeline",
          "brief": "Methodology for training AI models in two stages to improve performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of LLMs and their limitations"
        },
        {
          "topic": "Formal Logic",
          "why_needed": "To comprehend the principles of formal verification and its application"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, fine-tuning, and optimization"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the challenges of generating coherent and logical language"
        },
        {
          "topic": "Mathematical Reasoning",
          "why_needed": "To appreciate the evaluation benchmarks used in the research paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2601.22108": {
    "title": "Value-Based Pre-Training with Downstream Feedback",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Value-Based Pre-Training",
          "brief": "A method for controlled continued pretraining that maximizes the value of each gradient step using downstream feedback"
        },
        {
          "name": "Self-Supervised Learning (SSL)",
          "brief": "A type of machine learning where models learn from unlabeled data"
        },
        {
          "name": "Foundation Models",
          "brief": "Large, pre-trained models used as a starting point for a variety of downstream tasks"
        },
        {
          "name": "Modality-Agnostic Learning",
          "brief": "A type of learning that can be applied to different types of data, such as text or images"
        },
        {
          "name": "Downstream Feedback",
          "brief": "Using feedback from downstream tasks to guide the pretraining process"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of self-supervised learning, foundation models, and pretraining"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of supervised and unsupervised learning, as well as the concept of gradient descent"
        },
        {
          "topic": "Natural Language Processing (NLP) and Computer Vision",
          "why_needed": "To understand the applications of value-based pretraining in these fields"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand how to maximize the value of each gradient step during pretraining"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2601.21998": {
    "title": "Causal World Modeling for Robot Control",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Causal World Modeling",
          "brief": "Understanding the causality between actions and visual dynamics to predict the future"
        },
        {
          "name": "Autoregressive Diffusion Framework",
          "brief": "A framework that learns frame prediction and policy execution simultaneously"
        },
        {
          "name": "Mixture-of-Transformers (MoT) Architecture",
          "brief": "An architecture that integrates vision and action tokens in a shared latent space"
        },
        {
          "name": "Closed-Loop Rollout Mechanism",
          "brief": "A mechanism that allows for ongoing acquisition of environmental feedback with ground-truth observations"
        },
        {
          "name": "Asynchronous Inference Pipeline",
          "brief": "A pipeline that parallelizes action prediction and motor execution for efficient control"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Robot Learning",
          "why_needed": "To understand the context and applications of causal world modeling"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the video world modeling and vision-language pre-training aspects"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the autoregressive diffusion framework and Mixture-of-Transformers architecture"
        },
        {
          "topic": "Control Theory",
          "why_needed": "To understand the policy execution and motor control aspects"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To familiarize with the concepts of model training, evaluation, and generalizability"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2601.23184": {
    "title": "ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Chain-of-Thought (CoT)",
          "brief": "A method to enhance Large Language Models (LLMs) performance by providing explicit reasoning chains"
        },
        {
          "name": "Variational Auto-Encoding (VAE) framework",
          "brief": "A deep learning framework used for dimensionality reduction and generative modeling"
        },
        {
          "name": "Latent Reasoning",
          "brief": "A technique to compress reasoning processes into latent space for efficient computation"
        },
        {
          "name": "Rendered CoT-Guided variational Latent Reasoning (ReGuLaR)",
          "brief": "A novel latent learning paradigm proposed in the study to resolve computational redundancy and performance degradation issues"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and application of the proposed method"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the Variational Auto-Encoding (VAE) framework and other related concepts"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the rendering of explicit reasoning chains as images and extraction of dense visual-semantic representations"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the significance of latent reasoning and its impact on LLMs performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2601.21716": {
    "title": "DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Character Image Animation",
          "brief": "Synthesizing high-fidelity videos by transferring motion from a driving sequence to a static reference image"
        },
        {
          "name": "Spatiotemporal In-Context Learning",
          "brief": "Reimagining motion conditioning as an in-context learning problem to address challenges in character image animation"
        },
        {
          "name": "Latent Space Fusion",
          "brief": "Fusing reference appearance and motion cues into a unified latent space to enable joint reasoning about spatial identity and temporal dynamics"
        },
        {
          "name": "Self-Bootstrapped Data Synthesis",
          "brief": "Curating pseudo cross-identity training pairs to facilitate seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of deep learning concepts, such as generative models and latent space representations, is necessary to comprehend the proposed approach"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Knowledge of computer vision techniques, such as image and video processing, is required to understand the character image animation task"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with machine learning concepts, such as in-context learning and self-supervised learning, is necessary to appreciate the proposed framework"
        },
        {
          "topic": "Animation and Graphics",
          "why_needed": "Understanding of animation and graphics principles, such as motion transfer and character modeling, is helpful to appreciate the application of the proposed approach"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2601.22588": {
    "title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Representation-as-a-Judge",
          "brief": "A paradigm that uses internal model representations for evaluation instead of surface-level generation"
        },
        {
          "name": "Semantic Capacity Asymmetry Hypothesis",
          "brief": "The idea that evaluation requires less semantic capacity than generation and can be grounded in intermediate representations"
        },
        {
          "name": "INSPECTOR Framework",
          "brief": "A probing-based framework that predicts aspect-level evaluation scores from small model representations"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of LLMs"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and functionality of LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, evaluation, and representation learning"
        },
        {
          "topic": "Linguistics and Language Theory",
          "why_needed": "To understand the nuances of language and the requirements for effective evaluation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2602.00759": {
    "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
          "brief": "A type of reinforcement learning that uses verifiable rewards to enhance model performance"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        },
        {
          "name": "Adaptive Ability Decomposing (A^2D)",
          "brief": "A method for decomposing complex questions into simpler sub-questions to aid in RLVR"
        },
        {
          "name": "Decomposer",
          "brief": "A model that breaks down complex questions into simpler sub-questions"
        },
        {
          "name": "Reasoner",
          "brief": "A model that uses RLVR and sub-question guidance to improve its reasoning ability"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of RLVR and its applications"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of LLMs and decomposers"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the challenges of processing human language and the role of LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the fundamentals of model training, evaluation, and optimization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2601.22032": {
    "title": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "End-to-End Autonomous Driving",
          "brief": "A type of autonomous driving that relies on a single system to perceive the environment and make decisions"
        },
        {
          "name": "Self-Supervised Video Pretraining",
          "brief": "A method of training models on large amounts of unlabeled video data to learn general features"
        },
        {
          "name": "Multimodal Trajectory Distillation",
          "brief": "A technique for learning diverse behaviors from multiple trajectories"
        },
        {
          "name": "Video Joint-Embedding Predictive Architecture (V-JEPA)",
          "brief": "A framework for learning predictive representations from video data"
        },
        {
          "name": "Proposal-Centric Planner",
          "brief": "A type of planner that generates and evaluates multiple proposals for trajectory planning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of the proposed model"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how the model processes and represents visual data"
        },
        {
          "topic": "Autonomous Driving",
          "why_needed": "To understand the context and challenges of the problem being addressed"
        },
        {
          "topic": "Trajectory Planning",
          "why_needed": "To understand how the model generates and evaluates possible trajectories"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2601.22680": {
    "title": "Visual Personalization Turing Test",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Visual Personalization Turing Test (VPTT)",
          "brief": "A paradigm for evaluating contextual visual personalization based on perceptual indistinguishability"
        },
        {
          "name": "Perceptual Indistinguishability",
          "brief": "The ability of a model to generate content that is indistinguishable from human-created content"
        },
        {
          "name": "Visual Retrieval-Augmented Generator (VPRAG)",
          "brief": "A generator that uses visual retrieval to improve the quality and diversity of generated content"
        },
        {
          "name": "VPTT Score",
          "brief": "A text-only metric for evaluating the performance of visual personalization models"
        },
        {
          "name": "VPTT Framework",
          "brief": "A framework for evaluating and improving visual personalization models using the VPTT paradigm"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of generative models and visual retrieval"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of image and video generation, and visual perception"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the concepts of text-based metrics and evaluation"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To understand the concepts of user experience and perception"
        },
        {
          "topic": "Machine Learning Evaluation Metrics",
          "why_needed": "To understand the concepts of evaluation metrics and benchmarking"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2602.02276": {
    "title": "Kimi K2.5: Visual Agentic Intelligence",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Agentic Model",
          "brief": "A model that combines text and vision to enhance each other"
        },
        {
          "name": "Joint Text-Vision Pre-training",
          "brief": "A technique to pre-train models on both text and vision data simultaneously"
        },
        {
          "name": "Zero-Vision SFT",
          "brief": "A technique to fine-tune models without vision data"
        },
        {
          "name": "Joint Text-Vision Reinforcement Learning",
          "brief": "A technique to train models using reinforcement learning on both text and vision data"
        },
        {
          "name": "Agent Swarm",
          "brief": "A self-directed parallel agent orchestration framework to execute complex tasks concurrently"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of multimodal models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the vision component of the multimodal model"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the text component of the multimodal model"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the training objective of the model"
        },
        {
          "topic": "Parallel Computing",
          "why_needed": "To understand the Agent Swarm framework"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2602.01897": {
    "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Internal Flow Signatures",
          "brief": "A method to audit decision formation in large language models"
        },
        {
          "name": "Self-Checking",
          "brief": "The ability of a model to verify its own outputs"
        },
        {
          "name": "Refinement in LLMs",
          "brief": "The process of improving large language models' performance and accuracy"
        },
        {
          "name": "Depthwise Dynamics",
          "brief": "The analysis of how models process information at different depths or layers"
        },
        {
          "name": "GRU Validator",
          "brief": "A lightweight validator using Gated Recurrent Units to perform self-checking"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and limitations of LLMs"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the concepts of depthwise dynamics and internal flow signatures"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To appreciate the applications and challenges of LLMs in NLP tasks"
        },
        {
          "topic": "Gated Recurrent Units (GRUs)",
          "why_needed": "To understand the architecture and functionality of the GRU validator"
        },
        {
          "topic": "Orthogonal Transport",
          "why_needed": "To grasp the mathematical concepts used in aligning neighboring window frames"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2602.01675": {
    "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Long-Horizon Interactive Agents",
          "brief": "Agents that interact with users over an extended period, handling complex and dynamic scenarios"
        },
        {
          "name": "LLM-based Agents",
          "brief": "Agents powered by Large Language Models, capable of understanding and generating human-like text"
        },
        {
          "name": "Multi-Tool Reasoning",
          "brief": "The ability of agents to utilize multiple tools and resources to achieve a goal"
        },
        {
          "name": "Real-World Scenarios",
          "brief": "Scenarios that mimic real-world situations, making agents more applicable to practical problems"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning where agents learn through trial and error, receiving rewards or penalties for their actions"
        },
        {
          "name": "TRIP-Bench",
          "brief": "A benchmark for evaluating the performance of long-horizon interactive agents in real-world scenarios"
        },
        {
          "name": "GTPO",
          "brief": "An online multi-turn reinforcement learning method for training robust long-horizon agents"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand how LLM-based agents work and interact with users"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of reinforcement learning and multi-tool reasoning"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To grasp the broader context of interactive agents and their applications"
        },
        {
          "topic": "Travel Planning",
          "why_needed": "To understand the specific scenario used in TRIP-Bench and its requirements"
        },
        {
          "topic": "Benchmarking",
          "why_needed": "To appreciate the importance of evaluating agent performance in real-world scenarios"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2602.02343": {
    "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Language Model Parameter Dynamics",
          "brief": "The study of how language model parameters change and adapt during training and fine-tuning"
        },
        {
          "name": "Local Weight Fine-Tuning",
          "brief": "A method for adjusting model weights to fit specific tasks or datasets"
        },
        {
          "name": "LoRA-based Adaptation",
          "brief": "A technique for adapting language models using low-rank updates"
        },
        {
          "name": "Activation-Based Interventions",
          "brief": "Methods for modifying model activations to control output"
        },
        {
          "name": "Preference-Utility Analysis",
          "brief": "A framework for evaluating the trade-off between model preference and utility"
        },
        {
          "name": "Activation Manifold Perspective",
          "brief": "A geometric interpretation of model representations and their relationship to control signals"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of language models and their training"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the applications and challenges of language models"
        },
        {
          "topic": "Linear Algebra and Geometry",
          "why_needed": "To grasp the mathematical concepts underlying the activation manifold perspective"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the principles of model fine-tuning and adaptation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2602.01077": {
    "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Transformers",
          "brief": "A type of neural network architecture used for video and image generation"
        },
        {
          "name": "Sparse Attention",
          "brief": "A technique to reduce computational complexity of attention mechanisms in neural networks"
        },
        {
          "name": "Piecewise Sparse Attention (PISA)",
          "brief": "A novel attention mechanism that combines exact and approximate computation for efficient diffusion transformers"
        },
        {
          "name": "Block-wise Taylor Expansion",
          "brief": "A mathematical technique used to approximate attention scores in non-critical blocks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Transformer Architecture",
          "why_needed": "To understand the context and limitations of attention mechanisms in neural networks"
        },
        {
          "topic": "Attention Mechanisms",
          "why_needed": "To comprehend the role of attention in transformer architectures and its computational complexity"
        },
        {
          "topic": "Sparse Neural Networks",
          "why_needed": "To appreciate the concept of sparsity in neural networks and its application to attention mechanisms"
        },
        {
          "topic": "Mathematical Optimization Techniques",
          "why_needed": "To understand the block-wise Taylor expansion used in PISA for approximating attention scores"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2602.02486": {
    "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Re-TRAC",
          "brief": "A framework for recursive trajectory compression in deep search agents"
        },
        {
          "name": "ReAct Framework",
          "brief": "A linear framework for building deep research agents"
        },
        {
          "name": "Cross-Trajectory Exploration",
          "brief": "A method for generating structured state representations to guide search agents"
        },
        {
          "name": "LLM (Large Language Models)",
          "brief": "A type of artificial intelligence model used in natural language processing"
        },
        {
          "name": "Deep Search Agents",
          "brief": "Artificial intelligence agents that use deep learning to search for information"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of LLMs and deep search agents"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the application of LLMs in text-based search tasks"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To grasp the concept of search agents and their role in information retrieval"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the supervised fine-tuning process mentioned in the paper"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To familiarize oneself with the overall architecture and design of search agents"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2602.02488": {
    "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to take actions in an environment to maximize a reward"
        },
        {
          "name": "Dynamic Environment Adaptation",
          "brief": "The ability of an RL system to adapt its environment during training to improve learning efficiency"
        },
        {
          "name": "Policy and Reward Model Training",
          "brief": "The process of training a policy to take actions and a reward model to evaluate the quality of those actions"
        },
        {
          "name": "Closed-Loop Optimization",
          "brief": "A method of optimization where the system's performance is continuously evaluated and improved through feedback loops"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning Fundamentals",
          "why_needed": "To understand the basics of RL, policy and reward models, and optimization techniques"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the implementation of LLMs and the integration of RL with deep learning techniques"
        },
        {
          "topic": "Reinforcement Learning Algorithms",
          "why_needed": "To understand the different types of RL algorithms and their applications"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To grasp the concepts of closed-loop optimization and feedback mechanisms"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-03"
  },
  "2602.02660": {
    "title": "MARS: Modular Agent with Reflective Search for Automated AI Research",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Modular Agent with Reflective Search (MARS)",
          "brief": "A framework for autonomous AI research"
        },
        {
          "name": "Budget-Aware Planning",
          "brief": "Cost-constrained planning to balance performance and execution expense"
        },
        {
          "name": "Modular Construction",
          "brief": "A pipeline to manage complex research repositories"
        },
        {
          "name": "Comparative Reflective Memory",
          "brief": "Analyzing solution differences to distill high-signal insights"
        },
        {
          "name": "Monte Carlo Tree Search (MCTS)",
          "brief": "A search algorithm for decision-making under uncertainty"
        },
        {
          "name": "Credit Assignment",
          "brief": "The problem of attributing performance to individual components or actions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence (AI)",
          "why_needed": "To understand the context and goals of the MARS framework"
        },
        {
          "topic": "Machine Learning (ML)",
          "why_needed": "To appreciate the challenges of automating AI research and the role of MARS"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To recognize the differences between general software engineering and AI research"
        },
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the limitations of current LLM-based agents in AI research"
        },
        {
          "topic": "Automated AI Research",
          "why_needed": "To grasp the concept of automating AI research and its challenges"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.01753": {
    "title": "ObjEmbed: Towards Universal Multimodal Object Embeddings",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Embedding Models",
          "brief": "Models that align objects with corresponding textual descriptions"
        },
        {
          "name": "Object-Oriented Representation",
          "brief": "Capturing semantic and spatial aspects of objects through complementary embeddings"
        },
        {
          "name": "Visual Understanding Tasks",
          "brief": "Tasks like visual grounding, local image retrieval, and global image retrieval"
        },
        {
          "name": "MLLM Embedding Model",
          "brief": "A type of multimodal embedding model that supports various visual understanding tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "To understand image processing and object detection"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend textual descriptions and semantic matching"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the concepts of embedding models and neural networks"
        },
        {
          "topic": "Multimodal Learning",
          "why_needed": "To understand the integration of visual and textual data"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2601.19103": {
    "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Glance and Focus Reinforcement Learning",
          "brief": "A framework for pan-cancer screening that mimics radiologists' diagnostic strategy"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves an agent learning to take actions to maximize a reward"
        },
        {
          "name": "Pan-cancer Screening",
          "brief": "The process of detecting and localizing various types of cancer in large-scale CT scans"
        },
        {
          "name": "Group Relative Learning Paradigm",
          "brief": "A novel learning paradigm that prioritizes high-advantage predictions and discards low-advantage predictions within sub-volume groups"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of reinforcement learning, convolutional neural networks, and image segmentation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the challenges of image segmentation and object detection in medical imaging"
        },
        {
          "topic": "Medical Imaging",
          "why_needed": "To understand the characteristics of CT scans and the challenges of pan-cancer screening"
        },
        {
          "topic": "Radiology",
          "why_needed": "To appreciate the diagnostic strategies employed by radiologists and how they can be mimicked by AI models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.02477": {
    "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Chain-of-Thought (CoT) Reasoning",
          "brief": "A step-by-step reasoning approach used by LLMs to solve complex problems"
        },
        {
          "name": "Divide-and-Conquer (DAC) Reasoning",
          "brief": "A problem-solving approach that breaks down complex problems into smaller subproblems"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning approach that involves training models through trial and error by receiving rewards or penalties"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training and evaluation of LLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the processing and generation of human language by LLMs"
        },
        {
          "topic": "Problem-Solving Strategies",
          "why_needed": "To appreciate the divide-and-conquer approach and its advantages"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2601.22801": {
    "title": "Clipping-Free Policy Optimization for Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A subfield of machine learning that involves training agents to take actions in an environment to maximize a reward"
        },
        {
          "name": "Large Language Models",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Clipping-Free Policy Optimization (CFPO)",
          "brief": "A proposed method for optimizing large language models without relying on clipping mechanisms"
        },
        {
          "name": "Total Variation divergence constraints",
          "brief": "A mathematical concept used to derive a convex quadratic penalty in CFPO"
        },
        {
          "name": "Policy Optimization",
          "brief": "The process of adjusting the parameters of a policy to maximize a reward signal"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of large language models and reinforcement learning"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To comprehend the derivation of the convex quadratic penalty in CFPO"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the applications and challenges of large language models"
        },
        {
          "topic": "Reinforcement Learning Algorithms",
          "why_needed": "To understand the limitations of existing clipping-based methods and the benefits of CFPO"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.01322": {
    "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Sparse Autoencoders (SAEs)",
          "brief": "A method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms"
        },
        {
          "name": "Polynomial Decoding",
          "brief": "A technique to model feature interactions in SAEs by extending the decoder with higher-order terms"
        },
        {
          "name": "Low-Rank Tensor Factorization",
          "brief": "A method for capturing pairwise and triple feature interactions with small parameter overhead"
        },
        {
          "name": "Compositional Structure",
          "brief": "The ability of a model to capture the relationships between features, such as morphological binding and phrasal composition"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Neural Networks",
          "why_needed": "To understand the context of SAEs and their role in interpreting neural network representations"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To comprehend the mathematical concepts underlying SAEs, polynomial decoding, and low-rank tensor factorization"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the application of PolySAE in language models and its potential impact on language understanding"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of feature interactions, sparse representations, and model interpretability"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.03796": {
    "title": "3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "3D-Aware Implicit Motion Control",
          "brief": "A method for controlling human motion in video generation that uses implicit 3D representations"
        },
        {
          "name": "View-Adaptive Human Video Generation",
          "brief": "Generating human videos that can be viewed from different angles"
        },
        {
          "name": "SMPL (Skeletal Mesh Parameterization of the Human Body)",
          "brief": "A 3D parametric model of the human body used for motion capture and simulation"
        },
        {
          "name": "Cross-Attention Mechanism",
          "brief": "A neural network component that allows the model to attend to different parts of the input data"
        },
        {
          "name": "View-Rich Supervision",
          "brief": "A training method that uses multiple viewpoints to supervise the model's learning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the basics of image and video processing, as well as 3D reconstruction"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the concepts of neural networks, attention mechanisms, and generative models"
        },
        {
          "topic": "Human Motion Capture and Simulation",
          "why_needed": "To understand the challenges of modeling and simulating human motion, as well as the limitations of existing methods"
        },
        {
          "topic": "Geometry and 3D Modeling",
          "why_needed": "To grasp the concepts of 3D space, geometry, and spatial reasoning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.02537": {
    "title": "WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and understand multiple forms of data, such as text and images"
        },
        {
          "name": "Atomic Visual World Knowledge",
          "brief": "The ability of a model to recognize and identify specific objects and entities in visual data"
        },
        {
          "name": "Visual Knowledge Retrieval",
          "brief": "The process of retrieving information from visual data"
        },
        {
          "name": "Grounding and Naming Visual Entities",
          "brief": "The ability of a model to identify and label specific objects and entities in visual data"
        },
        {
          "name": "Stratified Taxonomy",
          "brief": "A hierarchical organization of concepts and objects, ranging from general to specific"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and capabilities of MLLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the challenges of visual knowledge retrieval and grounding"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the role of language in MLLMs and the importance of visual factuality"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To recognize the broader context and applications of MLLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.03320": {
    "title": "MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Medical Image Segmentation",
          "brief": "The process of dividing medical images into their constituent parts to analyze and understand their structure"
        },
        {
          "name": "Multi-turn Agentic Reinforcement Learning",
          "brief": "A type of machine learning that involves an agent making decisions in a multi-step process to achieve a goal"
        },
        {
          "name": "Multi-modal Large Language Models (MLLMs)",
          "brief": "Artificial intelligence models that can process and understand multiple types of data, including text and images"
        },
        {
          "name": "Segment Anything Model (SAM)",
          "brief": "A specialized tool for segmenting objects in images"
        },
        {
          "name": "Hybrid Prompting Strategy",
          "brief": "A technique for generating expert-curated trajectories to guide the decision-making process of a model"
        },
        {
          "name": "Two-stage Training Pipeline",
          "brief": "A method for training a model in two stages, integrating multi-turn outcome verification and process reward design"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of reinforcement learning, large language models, and image segmentation"
        },
        {
          "topic": "Medical Imaging",
          "why_needed": "To understand the context and applications of medical image segmentation"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concepts of autonomous agents, decision-making processes, and machine learning"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of image processing and object segmentation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.01212": {
    "title": "SimpleGPT: Improving GPT via A Simple Normalization Strategy",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Transformer Optimization",
          "brief": "Techniques to improve the performance of Transformer models"
        },
        {
          "name": "Second-Order Geometry",
          "brief": "The study of the curvature of the loss landscape in deep learning"
        },
        {
          "name": "Hessian Matrix",
          "brief": "A matrix of second partial derivatives of the loss function with respect to model parameters"
        },
        {
          "name": "SimpleNorm",
          "brief": "A simple normalization strategy to stabilize intermediate activation scales"
        },
        {
          "name": "Spectral Norm",
          "brief": "The largest singular value of a matrix, used to measure the stability of the Hessian"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the context of Transformer models and optimization techniques"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To comprehend the concepts of Hessian matrix, spectral norm, and singular values"
        },
        {
          "topic": "Optimization Methods",
          "why_needed": "To appreciate the importance of learning rates and stability in deep learning"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the application of Transformer models in language tasks, such as those performed by GPT models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.03845": {
    "title": "Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Parallel Thinking",
          "brief": "A reasoning paradigm that involves exploring multiple branches of thought simultaneously"
        },
        {
          "name": "2D Probing",
          "brief": "An interface that exposes the width-depth dynamics of parallel thinking by eliciting intermediate answers from all branches"
        },
        {
          "name": "Consensus-Based Early Stopping",
          "brief": "A technique used to regulate reasoning depth by stopping the exploration of branches when a consensus is reached"
        },
        {
          "name": "Deviation-Based Branch Pruning",
          "brief": "A technique used to dynamically adjust the width of parallel thinking by pruning branches that deviate from the consensus"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context of parallel thinking and its applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of training, testing, and optimization in the context of parallel thinking"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the basics of algorithms, data structures, and computational complexity"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To understand the human reasoning processes and how they can be modeled using parallel thinking"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.03295": {
    "title": "POP: Prefill-Only Pruning for Efficient Large Model Inference",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "AI models that process and understand both visual and linguistic data"
        },
        {
          "name": "Structured Pruning",
          "brief": "A technique to reduce computational costs of AI models by removing unnecessary components"
        },
        {
          "name": "Prefill-Only Pruning (POP)",
          "brief": "A stage-aware inference strategy that prunes deep layers during the prefill stage"
        },
        {
          "name": "Virtual Gate Mechanism",
          "brief": "A method to analyze the importance of different layers in a neural network"
        },
        {
          "name": "Key-Value (KV) Projections",
          "brief": "A technique to maintain cache integrity during the transition between stages"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and their applications"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the context and applications of LLMs and VLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the context and applications of VLMs"
        },
        {
          "topic": "Model Optimization",
          "why_needed": "To understand the importance of reducing computational costs of AI models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.02220": {
    "title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Embodied Intelligence",
          "brief": "The ability of AI systems to perceive and interact with their environment"
        },
        {
          "name": "Open-Vocabulary Goal Navigation",
          "brief": "The task of navigating to a target based on natural language instructions"
        },
        {
          "name": "Hierarchical Navigation",
          "brief": "Navigation at multiple semantic levels, such as scene, room, region, and instance"
        },
        {
          "name": "Language as a Map (LangMap)",
          "brief": "A large-scale benchmark for evaluating language-driven navigation tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand how language instructions are interpreted by AI agents"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend how agents perceive and navigate their environment"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the training and evaluation of models on the LangMap benchmark"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To appreciate the importance of meaningful communication between humans and AI systems"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.03709": {
    "title": "No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multi-hop Question Answering",
          "brief": "A type of question answering that requires reasoning across multiple pieces of information to arrive at an answer"
        },
        {
          "name": "Cultural Understanding",
          "brief": "The ability to comprehend and reason about cultural context, traditions, and implicit social knowledge"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Question Answering (QA) Benchmarks",
          "brief": "Standardized datasets used to evaluate the performance of question answering models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the concepts and techniques used in question answering and language modeling"
        },
        {
          "topic": "Cultural Studies",
          "why_needed": "To appreciate the importance of cultural understanding and the challenges of assessing it in AI models"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the capabilities and limitations of large language models and their applications"
        },
        {
          "topic": "Indonesian Culture and Language",
          "why_needed": "To understand the specific cultural context and language used in the ID-MoCQA dataset"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-04"
  },
  "2602.04575": {
    "title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agentic Orchestration",
          "brief": "A paradigm for content generation using autonomous synthesis of hierarchical multi-agent workflows"
        },
        {
          "name": "Vibe AIGC",
          "brief": "A new paradigm for content generation that bridges the gap between human imagination and machine execution"
        },
        {
          "name": "Intent-Execution Gap",
          "brief": "The disparity between a creator's high-level intent and the stochastic nature of current single-shot models"
        },
        {
          "name": "Meta-Planner",
          "brief": "A centralized system that deconstructs high-level user input into executable agentic pipelines"
        },
        {
          "name": "Human-AI Collaborative Economy",
          "brief": "A concept that redefines the role of AI in content creation, transforming it into a robust system-level engineering partner"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Generative Artificial Intelligence (AI)",
          "why_needed": "To understand the context and limitations of current AI models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of stochastic inference and model-centric paradigms"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To appreciate the shift in user role from prompt engineering to providing high-level input"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To understand the concept of system-level engineering and the potential of AI in democratizing complex digital asset creation"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To grasp the idea of human imagination and its relationship with machine execution"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.04541": {
    "title": "LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Long-Context Large Language Models (LLMs)",
          "brief": "AI models that process and understand long sequences of text"
        },
        {
          "name": "Hybrid-Head Sparse Decoding",
          "brief": "A method to efficiently decode LLMs by partitioning attention heads"
        },
        {
          "name": "Attention Mechanism",
          "brief": "A technique used in LLMs to focus on specific parts of the input text"
        },
        {
          "name": "Top-k Selection Strategy",
          "brief": "A method to select the most important tokens in the input text"
        },
        {
          "name": "HardKuma-based Mechanism",
          "brief": "A novel mechanism to partition attention heads into retrieval and sparse heads"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of LLMs and attention mechanisms"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of LLMs"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To understand the hardware constraints and optimization techniques"
        },
        {
          "topic": "Machine Learning Optimization",
          "why_needed": "To understand the trade-offs between model performance and computational efficiency"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.02405": {
    "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Imitation Learning",
          "brief": "A machine learning approach where an agent learns by observing and imitating the behavior of an expert"
        },
        {
          "name": "Contrastive Objective",
          "brief": "A learning objective that focuses on distinguishing between similar and dissimilar examples"
        },
        {
          "name": "Distribution Aligned Imitation Learning (DAIL)",
          "brief": "A two-step method for bridging the distributional gap between expert solutions and machine learning models"
        },
        {
          "name": "Reasoning Capabilities",
          "brief": "The ability of a model to draw conclusions, make inferences, and solve problems"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of imitation learning, contrastive objectives, and large language models"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To comprehend the context and applications of LLMs and reasoning capabilities"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges of working with human language and the importance of distributional alignment"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To understand human reasoning and problem-solving strategies, and how they can be applied to machine learning models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.03216": {
    "title": "Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Attention Mechanism",
          "brief": "A technique used in neural networks to focus on specific parts of the input data"
        },
        {
          "name": "Sparse Attention",
          "brief": "A method to reduce the computational complexity of attention mechanisms by selectively weighting input elements"
        },
        {
          "name": "Token-Level Sparsification",
          "brief": "A technique to dynamically select and compress tokens in a sequence to improve computational efficiency"
        },
        {
          "name": "Long-Context Inference",
          "brief": "The process of making predictions or inferences based on a large amount of input data, such as text"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and attention mechanisms"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the context of long-context inference and language models"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To appreciate the implications of computational complexity and optimization techniques"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To understand the mathematical operations involved in attention mechanisms and sparse attention"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.02419": {
    "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "GUI Grounding",
          "brief": "Translating natural language instructions into executable screen coordinates"
        },
        {
          "name": "Uncertainty Calibration",
          "brief": "Calibrating model predictions to quantify uncertainty and improve reliability"
        },
        {
          "name": "False Discovery Rate (FDR) Control",
          "brief": "Statistically controlling the rate of false positives in model predictions"
        },
        {
          "name": "Distribution-aware Uncertainty Quantification",
          "brief": "Quantifying uncertainty in model predictions using distribution-aware methods"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Graphical User Interface (GUI) Interaction",
          "why_needed": "To understand the context and applications of GUI grounding models"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the translation of natural language instructions into executable screen coordinates"
        },
        {
          "topic": "Machine Learning and Model Reliability",
          "why_needed": "To appreciate the importance of uncertainty calibration and FDR control in model predictions"
        },
        {
          "topic": "Statistical Analysis and Calibration",
          "why_needed": "To understand the calibration process and statistical guarantees in SafeGround"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.00747": {
    "title": "Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Model (LLM) Pre-training",
          "brief": "The process of training language models on large datasets to achieve general competence and proficiency on specific tasks"
        },
        {
          "name": "Data Mixing",
          "brief": "The process of combining different datasets to create an optimal mixture for LLM pre-training"
        },
        {
          "name": "Model Merging",
          "brief": "A technique used to combine multiple models into a single model, used in DeMix to predict optimal data ratios"
        },
        {
          "name": "Decouple Searching from Training Mix (DeMix)",
          "brief": "A novel framework that leverages model merging to predict optimal data ratios and decouples search from training costs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of LLM pre-training and model merging"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the importance of data mixing and the challenges of LLM pre-training"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand the trade-offs between sufficiency, accuracy, and efficiency in LLM pre-training"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of model training, evaluation, and merging"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.02495": {
    "title": "Reward-free Alignment for Conflicting Objectives",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Direct Alignment Methods",
          "brief": "Methods used to align large language models with human preferences"
        },
        {
          "name": "Multi-Objective Optimization",
          "brief": "Optimization techniques for handling multiple conflicting objectives"
        },
        {
          "name": "Reward-Free Alignment",
          "brief": "Alignment methods that do not rely on explicit reward models"
        },
        {
          "name": "Conflict-Averse Gradient Descent",
          "brief": "A novel variant of gradient descent that resolves gradient conflicts"
        },
        {
          "name": "Pareto-Critical Points",
          "brief": "Points that represent optimal trade-offs between multiple objectives"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and application of the proposed alignment framework"
        },
        {
          "topic": "Multi-Objective Optimization Techniques",
          "why_needed": "To appreciate the limitations of existing methods and the novelty of the proposed approach"
        },
        {
          "topic": "Gradient Descent and Optimization Algorithms",
          "why_needed": "To understand the technical details of the proposed conflict-averse gradient descent method"
        },
        {
          "topic": "Human Preferences and Alignment",
          "why_needed": "To understand the importance of aligning LLMs with human preferences and the challenges involved"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.04547": {
    "title": "OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Radiological Foundation Model",
          "brief": "A type of artificial intelligence model designed for medical image analysis"
        },
        {
          "name": "Self-Supervised Learning",
          "brief": "A machine learning approach where models learn from unlabeled data"
        },
        {
          "name": "Multi-Task Medical Image Analysis",
          "brief": "The ability of a model to perform multiple tasks such as classification and segmentation on medical images"
        },
        {
          "name": "Transfer Learning",
          "brief": "A technique where a model trained on one task is fine-tuned for another related task"
        },
        {
          "name": "Representation Learning",
          "brief": "A subset of machine learning that focuses on learning meaningful representations of data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of OmniRad"
        },
        {
          "topic": "Medical Imaging",
          "why_needed": "To comprehend the application domain and modalities used in the research"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To grasp the concepts of image analysis and processing"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the principles of self-supervised learning and transfer learning"
        },
        {
          "topic": "Radiology",
          "why_needed": "To appreciate the radiology-inspired principles used in the design of OmniRad"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.04634": {
    "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Multi-Agent Reinforcement Learning (MARL)",
          "brief": "A type of machine learning where multiple agents learn to make decisions in a shared environment"
        },
        {
          "name": "Width Scaling",
          "brief": "A technique to improve model performance by increasing the number of parallel agents or processes"
        },
        {
          "name": "Multi-Agent Systems",
          "brief": "A system where multiple agents interact and work together to achieve a common goal"
        },
        {
          "name": "Lead-Agent-Subagent Framework",
          "brief": "A framework where a lead agent coordinates and guides multiple subagents to achieve a task"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of LLMs and MARL"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To comprehend the learning process of agents in MARL"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the application of LLMs in information seeking tasks"
        },
        {
          "topic": "Distributed Systems",
          "why_needed": "To appreciate the benefits of width scaling and parallel execution"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the broader context of multi-agent systems and their applications"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.04879": {
    "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to take actions in an environment to maximize a reward"
        },
        {
          "name": "Proximal Policy Optimization (PPO)",
          "brief": "A model-free, on-policy reinforcement learning algorithm that is widely used for fine-tuning Large Language Models"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language, typically trained on vast amounts of text data"
        },
        {
          "name": "Trust Region",
          "brief": "A concept in optimization that refers to the region around the current parameters where the objective function can be approximated accurately"
        },
        {
          "name": "Divergence Proximal Policy Optimization (DPPO)",
          "brief": "A proposed algorithm that modifies PPO to use a more principled constraint based on a direct estimate of policy divergence"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and language models"
        },
        {
          "topic": "Reinforcement Learning Fundamentals",
          "why_needed": "To comprehend the concepts of agents, environments, and rewards"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To understand the concepts of probability ratios, Total Variation, and KL divergence"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand the basics of trust regions and constrained optimization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-05"
  },
  "2602.05986": {
    "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Generative Video Models",
          "brief": "AI models that generate videos based on input data"
        },
        {
          "name": "Implicit World Rules",
          "brief": "Unspoken rules that govern the behavior of objects and events in the world"
        },
        {
          "name": "Text-Image-to-Video (TI2V) Synthesis",
          "brief": "The process of generating videos from text and image inputs"
        },
        {
          "name": "Large Multimodal Models (LMMs)",
          "brief": "AI models that can process and generate multiple forms of data, such as text, images, and videos"
        },
        {
          "name": "Reasoning-Oriented Benchmark",
          "brief": "A framework for evaluating the reasoning capabilities of AI models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of generative video models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the visual aspects of video generation and evaluation"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the text-based input and output aspects of TI2V synthesis"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To understand human reasoning and decision-making processes, which are emulated in the evaluation protocol"
        },
        {
          "topic": "Machine Learning Evaluation Metrics",
          "why_needed": "To understand the four metrics used in the evaluation protocol: Reasoning Alignment, Temporal Consistency, Physical Rationality, and Visual Quality"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.04581": {
    "title": "Trust The Typical",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "LLM Safety",
          "brief": "Large Language Model safety, focusing on preventing harmful outputs"
        },
        {
          "name": "Out-of-Distribution Detection",
          "brief": "Identifying data points that significantly deviate from the expected distribution"
        },
        {
          "name": "Semantic Space",
          "brief": "A representation of text data in a high-dimensional space based on meaning and context"
        },
        {
          "name": "Guardrails",
          "brief": "Mechanisms to prevent or mitigate harmful outputs in LLMs"
        },
        {
          "name": "Trust The Typical (T3) Framework",
          "brief": "A novel approach to LLM safety that focuses on understanding safe prompts rather than harmful ones"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of LLM safety"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of out-of-distribution detection and semantic space"
        },
        {
          "topic": "Language Models",
          "why_needed": "To grasp the basics of LLMs and their potential risks"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the training and optimization of models like T3"
        },
        {
          "topic": "Toxicity and Hate Speech Detection",
          "why_needed": "To recognize the importance of LLM safety in preventing harmful content"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.05327": {
    "title": "ProAct: Agentic Lookahead in Interactive Environments",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Model (LLM) agents",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Grounded LookAhead Distillation (GLAD)",
          "brief": "A training paradigm that enables agents to internalize accurate lookahead reasoning"
        },
        {
          "name": "Monte-Carlo Critic (MC-Critic)",
          "brief": "An auxiliary value estimator designed to enhance policy-gradient algorithms"
        },
        {
          "name": "Policy-gradient algorithms (e.g., PPO and GRPO)",
          "brief": "Reinforcement learning methods that optimize policies directly"
        },
        {
          "name": "Model-based value approximation",
          "brief": "A technique used to estimate the value of a state or action in a reinforcement learning environment"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the context and challenges of training LLM agents in interactive environments"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of LLM agents"
        },
        {
          "topic": "Markov Decision Processes",
          "why_needed": "To understand the mathematical framework underlying the interactive environments"
        },
        {
          "topic": "Stochastic and Deterministic Environments",
          "why_needed": "To appreciate the differences in planning and decision-making in various environments"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.05842": {
    "title": "Reinforcement World Model Learning for LLM-based Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A subfield of machine learning that involves training agents to take actions in an environment to maximize a reward"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language, often used for tasks like text generation and language translation"
        },
        {
          "name": "World Modeling",
          "brief": "The ability of an agent to predict the consequences of its actions and understand the dynamics of its environment"
        },
        {
          "name": "Self-Supervised Learning",
          "brief": "A type of machine learning where the model is trained on unlabeled data, using its own predictions as supervision"
        },
        {
          "name": "Sim-to-Real Gap Rewards",
          "brief": "A reward function that encourages the alignment of simulated and real-world outcomes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the language-centric tasks and the role of LLMs in these tasks"
        },
        {
          "topic": "Reinforcement Learning Algorithms",
          "why_needed": "To appreciate the challenges of training agents in complex environments and the need for world modeling capabilities"
        },
        {
          "topic": "Agent-Environment Interactions",
          "why_needed": "To understand how agents perceive and respond to their environment, and the importance of world modeling in this context"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.03955": {
    "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and understand human language"
        },
        {
          "name": "Multi-Agent Systems",
          "brief": "Systems that consist of multiple agents interacting with each other to achieve a common goal"
        },
        {
          "name": "Distillation",
          "brief": "A technique to transfer knowledge from one model to another, often used to reduce computational complexity"
        },
        {
          "name": "Hierarchical Distillation Strategies",
          "brief": "Methods to distill knowledge from multiple models into a single model, including reasoning-enhanced fine-tuning, trajectory-based augmentation, and process-aware distillation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of LLMs and model distillation"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To comprehend the context of multi-agent systems and their applications"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To grasp the fundamentals of programming and software development, necessary for implementing and experimenting with the proposed framework"
        },
        {
          "topic": "Mathematics",
          "why_needed": "To understand the mathematical formulations and optimizations involved in the distillation process"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.04271": {
    "title": "SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "4D Generation",
          "brief": "Generating dynamic 3D objects from input text, images, or videos"
        },
        {
          "name": "Gaussian Skeletonization",
          "brief": "A novel framework for generating editable dynamic 3D Gaussians from monocular video input"
        },
        {
          "name": "Hierarchical Articulated Representation",
          "brief": "Decomposing motion into sparse rigid motion and fine-grained non-rigid motion"
        },
        {
          "name": "Linear Blend Skinning",
          "brief": "A technique for driving rigid motion via a skeleton"
        },
        {
          "name": "Hexplane-based Refinement",
          "brief": "A method for refining non-rigid deformations"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding of image and video processing techniques"
        },
        {
          "topic": "3D Graphics",
          "why_needed": "Knowledge of 3D modeling and animation principles"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with deep learning architectures and training methods"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "Understanding of mathematical concepts such as transformations and skinning"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "Knowledge of filtering and refinement techniques"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.03916": {
    "title": "SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Artificial intelligence models that combine visual and linguistic information to understand and generate human-like responses"
        },
        {
          "name": "Spatial Reasoning",
          "brief": "The ability to think about and understand the relationships between objects in space"
        },
        {
          "name": "SpatiaLab Benchmark",
          "brief": "A comprehensive benchmark for evaluating VLMs' spatial reasoning capabilities in realistic contexts"
        },
        {
          "name": "Deep Learning Models",
          "brief": "A subset of machine learning models that use neural networks to analyze and interpret data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how VLMs process and analyze visual information"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend how VLMs generate and understand human-like language"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the fundamentals of training and evaluating AI models like VLMs"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To appreciate the human spatial reasoning capabilities that VLMs aim to replicate"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.06030": {
    "title": "PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agent-Based Modeling (ABM)",
          "brief": "A computational modeling approach to simulate the behavior of complex systems composed of interacting agents"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models trained on vast amounts of text data to generate human-like language"
        },
        {
          "name": "Neuro-Symbolic Fusion",
          "brief": "A hybrid approach combining neural networks and symbolic reasoning to integrate rich individual-level signals and non-stationary behaviors"
        },
        {
          "name": "Multimodal Neural Transition Model",
          "brief": "A model capturing temporal and interaction dynamics in complex systems"
        },
        {
          "name": "Uncertainty-Aware Epistemic Fusion",
          "brief": "A method to yield calibrated cluster-level transition distributions by accounting for uncertainty in the modeling process"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence and Machine Learning",
          "why_needed": "To understand the fundamentals of LLMs, neural networks, and symbolic reasoning"
        },
        {
          "topic": "Complex Systems and Simulation",
          "why_needed": "To comprehend the application of ABM in various domains such as public health, finance, and social sciences"
        },
        {
          "topic": "Probability and Statistics",
          "why_needed": "To grasp the concepts of uncertainty, calibration, and stochastic processes in the context of PhysicsAgentABM"
        },
        {
          "topic": "Computer Science and Programming",
          "why_needed": "To implement and experiment with the proposed PhysicsAgentABM framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.05023": {
    "title": "Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "AI models that combine visual and language understanding to interpret images and generate text"
        },
        {
          "name": "Contextual Integrity",
          "brief": "The concept of maintaining privacy by considering the context in which information is shared"
        },
        {
          "name": "Multimodal Large Reasoning Models (MLRMs)",
          "brief": "Advanced AI models that can reason about multiple forms of data, such as images and text"
        },
        {
          "name": "Image Geolocation",
          "brief": "The ability of AI models to determine the location where an image was taken"
        },
        {
          "name": "Privacy Risk",
          "brief": "The potential for AI models to reveal sensitive information, such as location, without user consent"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand how VLMs and MLRMs work"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend image geolocation and visual understanding"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp how VLMs generate text and reason about language"
        },
        {
          "topic": "Privacy and Security",
          "why_needed": "To appreciate the importance of contextual integrity and privacy risk"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To understand how users interact with AI models and share information"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.05073": {
    "title": "Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Uncertainty Quantification (UQ)",
          "brief": "The process of estimating and managing uncertainty in machine learning models"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and generate human-like language"
        },
        {
          "name": "Agent UQ",
          "brief": "A framework for quantifying uncertainty in interactive agents, such as those using LLMs"
        },
        {
          "name": "Conditional Uncertainty Reduction",
          "brief": "A novel perspective on UQ that models reducible uncertainty over an agent's trajectory"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of LLMs and UQ"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the complexities of LLMs and their applications"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To grasp the concepts of uncertainty and probability in UQ"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the context and applications of LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2601.22027": {
    "title": "CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Model (LLM) agents",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Real-world uncertainty",
          "brief": "Situations where agents face incomplete or ambiguous information"
        },
        {
          "name": "Consistency and limit-awareness",
          "brief": "Ability of agents to handle uncertainty and adhere to policies"
        },
        {
          "name": "CAR-bench benchmark",
          "brief": "A testing environment for evaluating LLM agents in an in-car assistant domain"
        },
        {
          "name": "Hallucination and Disambiguation tasks",
          "brief": "Types of tasks that test agents' ability to handle missing information and resolve uncertainty"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand how LLM agents process and generate language"
        },
        {
          "topic": "Artificial Intelligence (AI) and Machine Learning (ML)",
          "why_needed": "To comprehend the underlying technologies and concepts"
        },
        {
          "topic": "Human-Computer Interaction (HCI)",
          "why_needed": "To understand the importance of reliable and self-aware LLM agents in real-world applications"
        },
        {
          "topic": "Uncertainty and decision-making in AI",
          "why_needed": "To appreciate the challenges of handling incomplete or ambiguous information"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2602.03338": {
    "title": "Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "LLM Critic Models",
          "brief": "Large Language Models used for failure prediction and intervention"
        },
        {
          "name": "Proactive Interventions",
          "brief": "Actions taken to prevent failures based on predictions"
        },
        {
          "name": "Disruption-Recovery Tradeoff",
          "brief": "The balance between recovering failing trajectories and disrupting successful ones"
        },
        {
          "name": "Pre-deployment Testing",
          "brief": "Evaluating the effectiveness of interventions before full deployment"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand LLM critic models and their applications"
        },
        {
          "topic": "Reliability Engineering",
          "why_needed": "To comprehend the importance of failure prediction and prevention"
        },
        {
          "topic": "Experimental Design",
          "why_needed": "To grasp the concept of pre-deployment testing and pilot studies"
        },
        {
          "topic": "Statistics",
          "why_needed": "To interpret results and understand the significance of performance metrics (e.g., AUROC, pp)"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-06"
  },
  "2601.22345": {
    "title": "Failing to Explore: Language Models on Interactive Tasks",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Language Models",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Interactive Tasks",
          "brief": "Tasks that require models to interact with an environment and make decisions based on feedback"
        },
        {
          "name": "Exploration-Exploitation Trade-off",
          "brief": "The balance between exploring new possibilities and exploiting known solutions to maximize performance"
        },
        {
          "name": "Parametric Tasks",
          "brief": "Tasks with adjustable parameters to control difficulty or complexity"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand language models and their applications"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To comprehend the exploration-exploitation trade-off and interactive tasks"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To grasp the broader context of language models and their limitations"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of model training, evaluation, and optimization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-07"
  },
  "2602.05293": {
    "title": "Fast-SAM3D: 3Dfy Anything in Images but Faster",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "3D Reconstruction",
          "brief": "The process of creating a 3D model from 2D images"
        },
        {
          "name": "SAM3D",
          "brief": "A scalable, open-world 3D reconstruction framework"
        },
        {
          "name": "Inference Latency",
          "brief": "The time it takes for a model to make predictions"
        },
        {
          "name": "Multi-Level Heterogeneity",
          "brief": "The presence of different types of data or processes in a system"
        },
        {
          "name": "Modality-Aware Step Caching",
          "brief": "A technique to optimize computation by caching intermediate results"
        },
        {
          "name": "Joint Spatiotemporal Token Carving",
          "brief": "A method to concentrate refinement on high-entropy regions"
        },
        {
          "name": "Spectral-Aware Token Aggregation",
          "brief": "A technique to adapt decoding resolution based on spectral variance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the context of 3D reconstruction and image processing"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the concepts of inference latency, caching, and token carving"
        },
        {
          "topic": "Geometry and Linear Algebra",
          "why_needed": "To understand the mathematical foundations of 3D reconstruction and spectral variance"
        },
        {
          "topic": "Software Optimization",
          "why_needed": "To appreciate the importance of efficient computation and caching techniques"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-07"
  },
  "2602.04683": {
    "title": "UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Audio Language Models",
          "brief": "Models that process and generate audio data based on linguistic patterns"
        },
        {
          "name": "Text-Aligned Factorized Audio Tokenization",
          "brief": "Method of breaking down audio into tokens that align with text representations"
        },
        {
          "name": "Discrete Audio Codec",
          "brief": "Technique for converting audio into discrete tokens for efficient processing"
        },
        {
          "name": "Autoregressive Architecture",
          "brief": "Type of neural network architecture that generates output sequences one step at a time"
        },
        {
          "name": "Few-shot and Zero-shot Learning",
          "brief": "Ability of a model to perform well on new tasks with limited or no training data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of UniAudio 2.0"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the text-aligned aspect of the audio tokenization"
        },
        {
          "topic": "Audio Signal Processing",
          "why_needed": "To comprehend the audio codec and reconstruction aspects"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concept of few-shot and zero-shot learning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-07"
  },
  "2602.06036": {
    "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoregressive Large Language Models (LLMs)",
          "brief": "Sequential models that generate text one token at a time"
        },
        {
          "name": "Speculative Decoding",
          "brief": "A technique to speed up decoding by using a fast draft model and verifying its outputs in parallel"
        },
        {
          "name": "Diffusion LLMs",
          "brief": "Models that enable parallel generation of text using a process called diffusion"
        },
        {
          "name": "Block Diffusion",
          "brief": "A specific type of diffusion model used for parallel drafting in the DFlash framework"
        },
        {
          "name": "DFlash Framework",
          "brief": "A speculative decoding framework that uses block diffusion for parallel drafting"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of LLMs and speculative decoding"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of LLMs and diffusion models"
        },
        {
          "topic": "Parallel Computing",
          "why_needed": "To appreciate the benefits and challenges of parallelizing decoding processes"
        },
        {
          "topic": "Language Model Evaluation Metrics",
          "why_needed": "To understand the metrics used to compare the performance of different models and decoding methods"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-07"
  },
  "2602.05115": {
    "title": "SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Social Intelligence of Language Agents",
          "brief": "Evaluating language models' ability to understand and interact with humans in a social context"
        },
        {
          "name": "Communication Barriers",
          "brief": "Obstacles that hinder effective communication, such as semantic vagueness, sociocultural mismatch, and emotional interference"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and generate human-like language"
        },
        {
          "name": "Social Learning Environment",
          "brief": "A simulated environment for testing social intelligence of language agents, such as SocialVeil"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the basics of language models and their applications"
        },
        {
          "topic": "Artificial Intelligence (AI)",
          "why_needed": "To comprehend the concepts of machine learning and social intelligence"
        },
        {
          "topic": "Human-Computer Interaction (HCI)",
          "why_needed": "To recognize the importance of effective communication between humans and machines"
        },
        {
          "topic": "Social Psychology",
          "why_needed": "To understand the complexities of human communication and social interactions"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-07"
  },
  "2602.05933": {
    "title": "Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Policy Mirror Descent (PMD)",
          "brief": "A framework for reinforcement learning that iteratively solves KL-regularized policy improvement subproblems"
        },
        {
          "name": "Log-Partition Function",
          "brief": "A mathematical function used in PMD to estimate the partition function"
        },
        {
          "name": "KL--Ï^2 Regularizer",
          "brief": "A type of regularizer that constrains large probability changes and enhances robustness against finite-sample estimation errors"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Advanced language models that require efficient and stable training algorithms"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning (RL)",
          "why_needed": "To understand the context and applications of Policy Mirror Descent"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To comprehend the derivation and analysis of the PMD-mean algorithm"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To understand the concepts of partition functions, KL-divergence, and Ï^2 regularization"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To appreciate the importance of efficient and stable training algorithms for LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-07"
  },
  "2602.02393": {
    "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Interactive World Models",
          "brief": "Models that can generate and predict future frames in a video sequence"
        },
        {
          "name": "Hierarchical Pose-free Memory Compressor (HPMC)",
          "brief": "A module that compresses historical data into a fixed-budget representation"
        },
        {
          "name": "Uncertainty-aware Action Labeling",
          "brief": "A strategy to discretize continuous motion into a tri-state logic for robust action-response learning"
        },
        {
          "name": "Revisit-Dense Finetuning Strategy",
          "brief": "A method to fine-tune the model using a compact dataset for long-range loop-closure capabilities"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of interactive world models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the concepts of pose estimation, visual memory, and video generation"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To grasp the idea of action-response learning and control"
        },
        {
          "topic": "Mathematics (Linear Algebra, Calculus)",
          "why_needed": "To understand the underlying mathematical concepts used in the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-08"
  },
  "2602.06035": {
    "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Generative Control",
          "brief": "A method to generate human-object interactions based on physical and motor priors"
        },
        {
          "name": "Physics-Based Human-Object Interactions",
          "brief": "Simulating interactions between humans and objects using physical laws"
        },
        {
          "name": "Imitation Learning",
          "brief": "Learning from demonstrations or expert behaviors"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "Learning through trial and error by receiving rewards or penalties"
        },
        {
          "name": "Variational Policy",
          "brief": "A type of policy that learns to reconstruct motion from multimodal observations and high-level intent"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Robotics and Humanoid Robotics",
          "why_needed": "To understand the context and application of the research"
        },
        {
          "topic": "Machine Learning and Deep Learning",
          "why_needed": "To comprehend the methods and techniques used in the paper"
        },
        {
          "topic": "Physics and Mechanics",
          "why_needed": "To understand the physical and motor priors used in the research"
        },
        {
          "topic": "Computer Vision and Sensorimotor Control",
          "why_needed": "To understand the multimodal observations and high-level intent used in the research"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-08"
  },
  "2602.05386": {
    "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Intrinsic Risk Sensing (IRS)",
          "brief": "A mechanism that allows agents to maintain latent vigilance and trigger defenses only upon risk perception"
        },
        {
          "name": "Hierarchical Adaptive Screening",
          "brief": "A defense mechanism that trades off efficiency and precision by resolving known patterns via lightweight similarity matching and escalating ambiguous cases to deep internal reasoning"
        },
        {
          "name": "Spider-Sense Framework",
          "brief": "An event-driven defense framework based on IRS that invokes a hierarchical defense mechanism upon risk perception"
        },
        {
          "name": "S^2Bench Benchmark",
          "brief": "A lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks for evaluating agent defense mechanisms"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context of autonomous agents and their security challenges"
        },
        {
          "topic": "Agent Defense Mechanisms",
          "why_needed": "To appreciate the limitations of existing mandatory checking paradigms and the need for intrinsic and selective defense approaches"
        },
        {
          "topic": "Machine Learning and Deep Learning",
          "why_needed": "To understand the concepts of similarity matching, deep internal reasoning, and the trade-offs between efficiency and precision"
        },
        {
          "topic": "Computer Security and Risk Management",
          "why_needed": "To comprehend the importance of risk perception, attack success rate, and false positive rate in evaluating defense mechanisms"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-08"
  },
  "2602.05393": {
    "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Pretraining",
          "brief": "The process of training a model on a large dataset before fine-tuning it for a specific task"
        },
        {
          "name": "Late-to-Early Training (LET) paradigm",
          "brief": "A method that enables LLMs to learn later knowledge in earlier steps and earlier layers"
        },
        {
          "name": "Late-to-early-step learning and late-to-early-layer learning",
          "brief": "Mechanisms that drive LET's effectiveness in accelerating training convergence and enhancing language modeling capabilities"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LLMs"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the applications and challenges of LLMs in language modeling and downstream tasks"
        },
        {
          "topic": "Model Optimization and Efficiency",
          "why_needed": "To appreciate the importance of accelerating training convergence and reducing computational costs"
        },
        {
          "topic": "Transfer Learning",
          "why_needed": "To understand how pretrained models can be leveraged to improve the training of larger models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-08"
  },
  "2602.06040": {
    "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "Models that combine vision and language for perception and reasoning"
        },
        {
          "name": "Hybrid Autoregressive Formulation",
          "brief": "A method that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts"
        },
        {
          "name": "Reasoning Modes",
          "brief": "Different ways of thinking, including text-only, vision-only, and interleaved vision-text reasoning"
        },
        {
          "name": "SwimBird",
          "brief": "A reasoning-switchable MLLM that dynamically switches among different reasoning modes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of MLLMs"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the text-based logical reasoning and language understanding aspects of MLLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To grasp the vision-intensive tasks and visual perception aspects of MLLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the supervised fine-tuning dataset construction and model training process"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-08"
  },
  "2602.05871": {
    "title": "Pathwise Test-Time Correction for Autoregressive Long Video Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoregressive Diffusion Models",
          "brief": "A type of generative model that uses a Markov chain to generate data"
        },
        {
          "name": "Test-Time Optimization (TTO)",
          "brief": "A method to adapt a model at test time to improve performance"
        },
        {
          "name": "Test-Time Correction (TTC)",
          "brief": "A training-free method to correct errors in long video generation"
        },
        {
          "name": "Distilled Autoregressive Diffusion Models",
          "brief": "A type of model that facilitates real-time short video synthesis"
        },
        {
          "name": "Long Video Generation",
          "brief": "The task of generating videos of extended lengths"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of generative models and autoregressive diffusion models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand video generation and analysis"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of model optimization and adaptation"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "To understand the concepts of video sequences and temporal dependencies"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-08"
  },
  "2602.06949": {
    "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "World Models",
          "brief": "Simulating the outcomes of actions in varied environments"
        },
        {
          "name": "Generalist Agents",
          "brief": "Agents capable of performing a wide range of tasks"
        },
        {
          "name": "Dexterous Robotics Tasks",
          "brief": "Tasks requiring precise control and manipulation of objects"
        },
        {
          "name": "Egocentric Human Videos",
          "brief": "Videos recorded from a first-person perspective"
        },
        {
          "name": "Continuous Latent Actions",
          "brief": "Unified proxy actions for enhancing interaction knowledge transfer"
        },
        {
          "name": "Generative World Models",
          "brief": "Models that can generate new scenarios and outcomes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Robotics",
          "why_needed": "Understanding of robotic systems and their applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Knowledge of machine learning concepts, including model training and evaluation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding of video processing and analysis techniques"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "Familiarity with AI concepts, including agent design and decision-making"
        },
        {
          "topic": "Data Preprocessing",
          "why_needed": "Understanding of data preparation and labeling techniques"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.05940": {
    "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multilingual Long Reasoning",
          "brief": "The ability of models to reason and understand questions in multiple languages"
        },
        {
          "name": "Translation-Reasoning Integrated Training (TRIT)",
          "brief": "A framework that integrates translation training into multilingual reasoning"
        },
        {
          "name": "Cross-Lingual Question Alignment",
          "brief": "The ability of models to align questions across different languages"
        },
        {
          "name": "Self-Improving Models",
          "brief": "Models that can improve their performance without external feedback or additional data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the basics of language models and their applications"
        },
        {
          "topic": "Machine Translation",
          "why_needed": "To comprehend the challenges of translating text between languages"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the concepts of model training and optimization"
        },
        {
          "topic": "Multilingualism in AI",
          "why_needed": "To recognize the importance of developing models that can handle multiple languages"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.06960": {
    "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Infinite-Horizon Reasoning",
          "brief": "A type of reasoning that involves making decisions over an indefinite period"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach that involves training agents to make decisions based on rewards or penalties"
        },
        {
          "name": "Iterative Reasoning",
          "brief": "A process of periodically summarizing and refining intermediate thoughts to improve reasoning"
        },
        {
          "name": "Chain-of-Thought",
          "brief": "A paradigm for large reasoning models that involves generating a sequence of intermediate thoughts"
        },
        {
          "name": "Model-Controlled Iteration Boundaries",
          "brief": "A technique that allows models to control when to summarize and resume reasoning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of reinforcement learning and model training"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of large reasoning models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the application of infinite-horizon reasoning in language models"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To appreciate the importance of efficient optimization in reinforcement learning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2601.21037": {
    "title": "Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models",
          "brief": "Models that combine visual and textual information for reasoning tasks"
        },
        {
          "name": "Video Generation Models",
          "brief": "Models that generate video frames for visual reasoning and planning"
        },
        {
          "name": "Visual Context",
          "brief": "The use of visual information to control and guide reasoning and planning"
        },
        {
          "name": "Test-Time Scaling",
          "brief": "The ability to improve performance by increasing the visual inference budget at test time"
        },
        {
          "name": "Zero-Shot Generalization",
          "brief": "The ability of a model to perform well on unseen data without specific fine-tuning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of vision-language models and video generation models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand visual context and test-time scaling"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the textual aspects of vision-language models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of zero-shot generalization and model evaluation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.00298": {
    "title": "Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Emergent Misalignment",
          "brief": "A phenomenon where AI models, especially language models, produce unintended and potentially harmful outputs"
        },
        {
          "name": "Narrow Finetuning",
          "brief": "A process of fine-tuning pre-trained language models on specific datasets to adapt to particular tasks or domains"
        },
        {
          "name": "Backdoor Triggers",
          "brief": "Input prompts designed to elicit specific, often undesirable, responses from AI models"
        },
        {
          "name": "Domain-Level Susceptibility",
          "brief": "The vulnerability of AI models to emergent misalignment across different domains or tasks"
        },
        {
          "name": "Membership Inference Metrics",
          "brief": "Techniques used to predict the degree of possible misalignment in AI models based on their training data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence (AI) and Machine Learning (ML) Fundamentals",
          "why_needed": "To understand the basics of language models, fine-tuning, and the risks associated with AI safety"
        },
        {
          "topic": "Language Models and Natural Language Processing (NLP)",
          "why_needed": "To comprehend the capabilities and limitations of large language models and their applications"
        },
        {
          "topic": "AI Safety and Security",
          "why_needed": "To recognize the importance of assessing and mitigating risks associated with emergent misalignment in AI systems"
        },
        {
          "topic": "Data Science and Statistics",
          "why_needed": "To understand the experimental design, data analysis, and interpretation of results in the research paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.05281": {
    "title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach where an agent learns to take actions to maximize a reward"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        },
        {
          "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
          "brief": "A paradigm that enhances reasoning in LLMs using verifiable rewards"
        },
        {
          "name": "Group Relative Policy Optimization (GRPO)",
          "brief": "A policy optimization method used in RLVR"
        },
        {
          "name": "Advantage Re-weighting Mechanism (ARM)",
          "brief": "A novel mechanism proposed to equilibrate confidence levels across correct responses"
        },
        {
          "name": "Exploration-Exploitation Trade-off",
          "brief": "A fundamental problem in RL that involves balancing exploration and exploitation to achieve optimal results"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of LLMs and RL"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To comprehend the concept of sampling probability dynamics and advantage estimation"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To understand the optimization methods used in RLVR, such as GRPO"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the application of LLMs in reasoning tasks"
        },
        {
          "topic": "Reinforcement Learning Fundamentals",
          "why_needed": "To understand the basics of RL, including exploration-exploitation trade-off and policy optimization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.02016": {
    "title": "DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Shampoo Optimizer",
          "brief": "A variant of approximate second-order optimizers"
        },
        {
          "name": "Batched Block Preconditioning",
          "brief": "A technique to improve GPU utilization by stacking preconditioner blocks into 3D tensors"
        },
        {
          "name": "Efficient Inverse-Root Solvers",
          "brief": "Novel approaches for computing inverse matrix roots, including Newton-DB iteration and Chebyshev polynomial approximations"
        },
        {
          "name": "Distributed Accelerated SHampoo (DASH)",
          "brief": "A faster implementation of Distributed Shampoo"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Linear Algebra",
          "why_needed": "To understand matrix operations, preconditioning, and inverse matrix roots"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To comprehend the context of Shampoo and its applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To appreciate the importance of efficient optimizers in deep learning"
        },
        {
          "topic": "Parallel Computing",
          "why_needed": "To understand the benefits of distributed computing and GPU acceleration"
        },
        {
          "topic": "Mathematical Analysis",
          "why_needed": "To follow the in-depth analysis of matrix scaling and its effect on Shampoo convergence"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.02474": {
    "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Model (LLM)",
          "brief": "A type of artificial intelligence model designed to process and understand human language"
        },
        {
          "name": "Memory Skills",
          "brief": "Learnable and evolvable routines for extracting, consolidating, and pruning information from interaction traces"
        },
        {
          "name": "Self-Evolving Agents",
          "brief": "Agents that can adapt and improve their performance over time through learning and evolution"
        },
        {
          "name": "Agent Skills",
          "brief": "Reusable routines that enable agents to perform specific tasks or functions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of LLMs and self-evolving agents"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the learning and evolution mechanisms employed in MemSkill"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges and opportunities in processing and managing large amounts of language data"
        },
        {
          "topic": "Cognitive Architectures",
          "why_needed": "To understand the design and implementation of agent memory systems and skills"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.06130": {
    "title": "Self-Improving World Modelling with Latent Actions",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Latent Actions",
          "brief": "Treating actions as hidden variables to improve world modelling"
        },
        {
          "name": "Forward World Modelling (FWM)",
          "brief": "Predicting next states based on current states and actions"
        },
        {
          "name": "Inverse Dynamics Modelling (IDM)",
          "brief": "Inferring actions from current and next states"
        },
        {
          "name": "Variational Information Maximisation",
          "brief": "Updating FWM to maximise mutual information between latent actions and next states"
        },
        {
          "name": "ELBO Maximisation",
          "brief": "Updating IDM to explain observed transitions using evidence lower bound"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "Training models using reward signals, specifically GRPO"
        },
        {
          "name": "Self-Improvement Framework",
          "brief": "Alternating between FWM and IDM to improve world modelling"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "Understanding of ML fundamentals, including RL and deep learning"
        },
        {
          "topic": "Probabilistic Modelling",
          "why_needed": "Knowledge of probability theory and variational inference"
        },
        {
          "topic": "World Modelling",
          "why_needed": "Familiarity with internal modelling of the world and its importance in LLMs and VLMs"
        },
        {
          "topic": "Latent Variable Models",
          "why_needed": "Understanding of latent variable models and their applications"
        },
        {
          "topic": "Optimisation Techniques",
          "why_needed": "Knowledge of optimisation methods, including coordinate ascent and gradient-based methods"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.04837": {
    "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Open-Ended Self-Improvement",
          "brief": "The ability of agents to autonomously modify their own structural designs to advance their capabilities"
        },
        {
          "name": "Group-Evolving Agents (GEA)",
          "brief": "A paradigm for open-ended self-improvements that treats a group of agents as the fundamental evolutionary unit"
        },
        {
          "name": "Experience Sharing",
          "brief": "The ability of agents to share and reuse experiences within a group to improve their capabilities"
        },
        {
          "name": "Evolutionary Algorithms",
          "brief": "Algorithms that use principles of evolution to search for optimal solutions"
        },
        {
          "name": "Tree-Structured Evolution",
          "brief": "A type of evolutionary algorithm that uses a tree-like structure to represent the evolution of agents"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of open-ended self-improvement"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of evolutionary algorithms and experience sharing"
        },
        {
          "topic": "Evolutionary Computation",
          "why_needed": "To understand the principles of evolutionary algorithms and their applications"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To understand the context of coding benchmarks and the evaluation of GEA"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the fundamental concepts of algorithms, data structures, and software design"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.06129": {
    "title": "Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Transformers",
          "brief": "A type of deep learning model that integrates heterogeneous data to forecast climate-risk indicators"
        },
        {
          "name": "Spatio-Temporal Modeling",
          "brief": "A technique to analyze data with spatial and temporal components to understand urban dynamics"
        },
        {
          "name": "Climate-Resilient Housing",
          "brief": "The ability of housing to withstand and recover from climate-related disasters"
        },
        {
          "name": "Transportation-Network Structure",
          "brief": "The organization and accessibility of transportation networks in urban areas"
        },
        {
          "name": "Intelligent Vehicles",
          "brief": "Vehicles that use artificial intelligence and sensors to navigate and respond to emergency situations"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the diffusion-transformer framework and its applications"
        },
        {
          "topic": "Urban Planning",
          "why_needed": "To comprehend the importance of climate-resilient housing and transportation-network structure"
        },
        {
          "topic": "Climate Science",
          "why_needed": "To understand the impact of climate hazards on urban areas and the need for climate-risk prediction"
        },
        {
          "topic": "Geographic Information Systems (GIS)",
          "why_needed": "To work with spatial data and understand the integration of heterogeneous urban data"
        },
        {
          "topic": "Probability and Statistics",
          "why_needed": "To understand the probabilistic risk trajectories and uncertainty-aware accessibility layers"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.01734": {
    "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Training Instability",
          "brief": "Phenomenon where model training fails due to sudden gradient explosions"
        },
        {
          "name": "Stable Rank Restoration",
          "brief": "Technique to restore model stability by maintaining a stable rank in weight matrices"
        },
        {
          "name": "Matrix Sign Operations",
          "brief": "Mathematical operations used to restore stable rank in matrices"
        },
        {
          "name": "Optimizer (MSign)",
          "brief": "Proposed optimizer designed to prevent training instability in LLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of large language models and training processes"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To comprehend matrix operations, norms, and rank concepts"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand the role of optimizers in model training and the proposed MSign optimizer"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concept of model training, gradient descent, and instability issues"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.05711": {
    "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts (MoE) architectures",
          "brief": "A type of neural network architecture that combines multiple expert models to improve performance"
        },
        {
          "name": "Atomic Experts",
          "brief": "A fine-grained expert specialization approach that enables vector-level processing"
        },
        {
          "name": "System-algorithm co-design",
          "brief": "A design approach that integrates both system and algorithm aspects to optimize performance"
        },
        {
          "name": "Cartesian Product Router",
          "brief": "A routing algorithm that reduces complexity by decomposing the index space"
        },
        {
          "name": "Expert-Centric Scheduling",
          "brief": "A scheduling approach that optimizes execution order to reduce memory access overhead"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Neural Networks",
          "why_needed": "To understand the basics of MoE architectures and their applications"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the concepts of expert models, parameter efficiency, and routing complexity"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To appreciate the system-algorithm co-design approach and its impact on hardware execution efficiency"
        },
        {
          "topic": "Algorithm Design",
          "why_needed": "To understand the trade-offs between granularity, routing complexity, and memory access"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.06669": {
    "title": "compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Reinforcement Learning from Human Feedback (RLHF)",
          "brief": "A training method that uses human feedback to improve LLMs"
        },
        {
          "name": "Direct Preference Optimization (DPO)",
          "brief": "A training method that optimizes LLMs based on human preferences"
        },
        {
          "name": "Human-Computer Interaction",
          "brief": "The study of how humans interact with computers and AI systems"
        },
        {
          "name": "Natural Language Processing (NLP)",
          "brief": "A subfield of AI that deals with the interaction between computers and humans in natural language"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training methods used for LLMs, such as RLHF and DPO"
        },
        {
          "topic": "Human-Computer Interaction",
          "why_needed": "To appreciate the importance of human feedback and preference data in LLM development"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the challenges and opportunities of working with non-English languages"
        },
        {
          "topic": "Data Privacy and Ethics",
          "why_needed": "To recognize the importance of privacy-preserving methods in data collection and usage"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.06883": {
    "title": "Vision Transformer Finetuning Benefits from Non-Smooth Components",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision Transformer",
          "brief": "A type of neural network architecture that applies transformer models to vision tasks"
        },
        {
          "name": "Finetuning",
          "brief": "The process of adjusting a pre-trained model to fit a new task or dataset"
        },
        {
          "name": "Plasticity",
          "brief": "A measure of how sensitive a model's outputs are to changes in its inputs"
        },
        {
          "name": "Smoothness",
          "brief": "A measure of how stable a model's outputs are to small changes in its inputs"
        },
        {
          "name": "Transfer Learning",
          "brief": "The process of applying a model trained on one task to a different task"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and transformer architectures"
        },
        {
          "topic": "Transformer Models",
          "why_needed": "To comprehend the specific architecture and components of vision transformers"
        },
        {
          "topic": "Generalization and Adversarial Robustness",
          "why_needed": "To appreciate the importance of smoothness and plasticity in machine learning models"
        },
        {
          "topic": "Mathematical Analysis",
          "why_needed": "To follow the theoretical analysis and derivations in the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.06570": {
    "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Clinical Decision Support Systems",
          "brief": "Systems that provide healthcare professionals with clinical decision-making support"
        },
        {
          "name": "Natural Language Processing",
          "brief": "Techniques for computers to understand and generate human language"
        },
        {
          "name": "Deep Learning",
          "brief": "A subset of machine learning that uses neural networks to analyze data"
        },
        {
          "name": "Medical Informatics",
          "brief": "The application of computer technology to healthcare and medicine"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning Fundamentals",
          "why_needed": "To understand the training pipeline and model architecture of Baichuan-M3"
        },
        {
          "topic": "Clinical Workflow and Decision-Making",
          "why_needed": "To appreciate the systematic workflow of a physician and the limitations of existing systems"
        },
        {
          "topic": "Natural Language Processing Techniques",
          "why_needed": "To comprehend the proactive information acquisition and long-horizon reasoning capabilities of Baichuan-M3"
        },
        {
          "topic": "Medical Terminology and Concepts",
          "why_needed": "To understand the clinical context and applications of Baichuan-M3"
        },
        {
          "topic": "Evaluation Metrics and Benchmarks",
          "why_needed": "To interpret the empirical evaluations and comparisons with other models like GPT-5.2"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-09"
  },
  "2602.06694": {
    "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Post-Training Quantization (PTQ)",
          "brief": "A technique to reduce the precision of model weights after training to improve efficiency"
        },
        {
          "name": "Low-Rank Binary Factorization",
          "brief": "A method to approximate a matrix using a low-rank binary representation"
        },
        {
          "name": "Alternating Direction Method of Multipliers (ADMM)",
          "brief": "An optimization algorithm used to solve constrained optimization problems"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "A type of artificial intelligence model designed to process and understand human language"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the context and applications of large language models and quantization techniques"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To comprehend the low-rank binary factorization and ADMM algorithm"
        },
        {
          "topic": "Optimization Methods",
          "why_needed": "To understand the ADMM algorithm and its application in quantization"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To appreciate the implications of model compression on hardware and deployment"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.06869": {
    "title": "Uncovering Cross-Objective Interference in Multi-Objective Alignment",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multi-Objective Alignment",
          "brief": "A technique used in large language models to optimize multiple objectives simultaneously"
        },
        {
          "name": "Cross-Objective Interference",
          "brief": "A phenomenon where improving one objective causes others to degrade"
        },
        {
          "name": "Scalarization Algorithms",
          "brief": "Methods used to combine multiple objectives into a single objective function"
        },
        {
          "name": "Covariance Law",
          "brief": "A mathematical principle describing the relationship between objective rewards and the scalarized score"
        },
        {
          "name": "Covariance Targeted Weight Adaptation (CTWA)",
          "brief": "A method proposed to mitigate cross-objective interference by maintaining positive covariance between objective rewards and the training signal"
        },
        {
          "name": "Polyak--Åojasiewicz Condition",
          "brief": "A mathematical condition used to establish global convergence in non-convex optimization"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and application of multi-objective alignment"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To comprehend scalarization algorithms and the Polyak--Åojasiewicz condition"
        },
        {
          "topic": "Linear Algebra and Covariance",
          "why_needed": "To understand the mathematical principles underlying the covariance law and CTWA"
        },
        {
          "topic": "Machine Learning and Deep Learning",
          "why_needed": "To appreciate the challenges and limitations of multi-objective optimization in LLMs"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.03075": {
    "title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning where models learn by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Mid-Training (Annealing) Phase",
          "brief": "A critical phase in the training process of LLMs where the learning rate decays rapidly"
        },
        {
          "name": "ReMiT (Reinforcement Learning-Guided Mid-Training)",
          "brief": "A novel approach that leverages RL-tuned models to dynamically reweight tokens during the mid-training phase"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the training process of LLMs and the role of mid-training phase"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the applications and challenges of LLMs"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the fundamentals of reinforcement learning and its integration with LLMs"
        },
        {
          "topic": "Mathematics and Code Reasoning",
          "why_needed": "To understand the benchmarks and evaluation metrics used in the paper"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.06554": {
    "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning paradigm for training agents to make decisions in complex environments"
        },
        {
          "name": "Agentic-RL",
          "brief": "A type of RL that focuses on training agents to achieve goals in multi-turn scenarios"
        },
        {
          "name": "Convergence Guarantees",
          "brief": "Mathematical proofs that an algorithm will converge to an optimal solution"
        },
        {
          "name": "Policy Update Mechanisms",
          "brief": "Methods for updating an agent's policy in RL, such as REINFORCE and PPO"
        },
        {
          "name": "Advantage Estimation Methods",
          "brief": "Techniques for estimating the advantage of taking a particular action in RL, such as Group Relative Advantage Estimation (GRAE)"
        },
        {
          "name": "Multi-Turn Scenarios",
          "brief": "Environments where agents interact with each other over multiple turns or episodes"
        },
        {
          "name": "Sequence-Level Agentic-RL",
          "brief": "A type of RL that focuses on training agents to make decisions at the sequence level, rather than individual actions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning Fundamentals",
          "why_needed": "To understand the basics of RL and its applications"
        },
        {
          "topic": "Markov Decision Processes (MDPs)",
          "why_needed": "To understand the mathematical framework underlying RL"
        },
        {
          "topic": "Multi-Agent Systems",
          "why_needed": "To understand how multiple agents interact with each other in complex environments"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand the mathematical techniques used to optimize RL algorithms"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the neural network architectures used in RL, such as large language models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.06540": {
    "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "AgentCPM-Report",
          "brief": "A lightweight framework for generating deep research reports"
        },
        {
          "name": "Writing As Reasoning Policy (WARP)",
          "brief": "A policy that enables models to dynamically revise outlines during report generation"
        },
        {
          "name": "Multi-Stage Agentic Training",
          "brief": "A training strategy for equipping small models with the capability to generate deep research reports"
        },
        {
          "name": "Evidence-Based Drafting",
          "brief": "A process of drafting reports based on evidence"
        },
        {
          "name": "Reasoning-Driven Deepening",
          "brief": "A process of refining knowledge and outlines through reasoning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and challenges of generating deep research reports"
        },
        {
          "topic": "Language Models",
          "why_needed": "To comprehend the limitations of current language models in generating deep research reports"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of the 8B-parameter deep research agent"
        },
        {
          "topic": "Reinforcement Learning (RL)",
          "why_needed": "To understand the Multi-Stage Agentic Training strategy"
        },
        {
          "topic": "Human Writing Process",
          "why_needed": "To appreciate the inspiration behind the AgentCPM-Report framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2601.21363": {
    "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to make decisions in complex environments"
        },
        {
          "name": "Proximal Policy Optimization (PPO)",
          "brief": "An on-policy RL algorithm that enables robust training via large-scale parallel simulation"
        },
        {
          "name": "Off-policy RL",
          "brief": "A type of RL that allows learning from experiences gathered without following the same policy used for deployment"
        },
        {
          "name": "Model-based RL",
          "brief": "A type of RL that involves learning a model of the environment and using it to make decisions"
        },
        {
          "name": "Soft Actor-Critic (SAC)",
          "brief": "An off-policy RL algorithm that combines policy-based and value-based methods"
        },
        {
          "name": "Humanoid Control",
          "brief": "The application of RL to control humanoid robots"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of RL and its applications"
        },
        {
          "topic": "Robotics",
          "why_needed": "To understand the challenges and requirements of humanoid control"
        },
        {
          "topic": "Linear Algebra and Calculus",
          "why_needed": "To understand the mathematical formulations of RL algorithms"
        },
        {
          "topic": "Programming (e.g., Python)",
          "why_needed": "To implement and experiment with RL algorithms"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.08629": {
    "title": "CauScale: Neural Causal Discovery at Scale",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Causal Discovery",
          "brief": "The process of identifying cause-and-effect relationships in data"
        },
        {
          "name": "Neural Architecture",
          "brief": "A type of machine learning model inspired by the structure and function of the human brain"
        },
        {
          "name": "Graph Theory",
          "brief": "The study of graphs, which are collections of nodes and edges that represent relationships between objects"
        },
        {
          "name": "Attention Mechanism",
          "brief": "A technique used in neural networks to focus on specific parts of the input data"
        },
        {
          "name": "Data Embeddings",
          "brief": "A way of representing complex data in a lower-dimensional space"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of neural networks and deep learning"
        },
        {
          "topic": "Graph Neural Networks",
          "why_needed": "To comprehend how neural networks can be applied to graph-structured data"
        },
        {
          "topic": "Causal Inference",
          "why_needed": "To grasp the concepts of causality and how to discover causal relationships in data"
        },
        {
          "topic": "Deep Learning Architectures",
          "why_needed": "To understand how to design and implement efficient neural architectures"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.08658": {
    "title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Deduction",
          "brief": "A reasoning paradigm that involves drawing conclusions based on given premises"
        },
        {
          "name": "Induction",
          "brief": "A reasoning paradigm that involves making generalizations based on specific observations"
        },
        {
          "name": "Abduction",
          "brief": "A reasoning paradigm that involves making educated guesses or hypotheses based on incomplete information"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Out-of-Domain Generalization",
          "brief": "The ability of a model to apply its knowledge to new, unseen tasks or domains"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and capabilities of Large Language Models"
        },
        {
          "topic": "Logical Reasoning",
          "why_needed": "To comprehend the fundamental reasoning paradigms of deduction, induction, and abduction"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand how LLMs process and generate human-like language"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the methods used to fine-tune and train LLMs"
        },
        {
          "topic": "Cognitive Science",
          "why_needed": "To understand human logical thinking and how it relates to the fundamental reasoning paradigms"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.08961": {
    "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "4D VAE",
          "brief": "A type of variational autoencoder that operates on 4D data, used for joint representation of dense 3D point maps and 3D scene flows"
        },
        {
          "name": "Dense Geometry Reconstruction",
          "brief": "The process of estimating the 3D structure of a scene from a monocular video"
        },
        {
          "name": "Dense Motion Estimation",
          "brief": "The process of estimating the 3D motion of objects in a scene from a monocular video"
        },
        {
          "name": "Video Diffusion",
          "brief": "A technique used for generating and reconstructing video data"
        },
        {
          "name": "Joint Representation",
          "brief": "A method of representing multiple types of data (e.g. 3D point maps and scene flows) in a shared coordinate system"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of VAEs, neural networks, and diffusion models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of 3D reconstruction, motion estimation, and image processing"
        },
        {
          "topic": "Mathematics (Linear Algebra, Calculus)",
          "why_needed": "To understand the mathematical formulations and derivations used in the paper"
        },
        {
          "topic": "Python Programming",
          "why_needed": "To implement and experiment with the proposed framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.07040": {
    "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autonomous Scientific Discovery",
          "brief": "Using AI to automatically discover new scientific knowledge"
        },
        {
          "name": "AI Agent",
          "brief": "An artificial intelligence system that performs tasks autonomously"
        },
        {
          "name": "Program Iteration",
          "brief": "Repeatedly improving a program to achieve better performance"
        },
        {
          "name": "State-of-the-Art (SOTA) Results",
          "brief": "Achieving the best known results in a particular field or task"
        },
        {
          "name": "Machine Learning",
          "brief": "A subset of AI that involves training models on data to make predictions or decisions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concept of AI agents and their applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training process and evaluation of machine learning models"
        },
        {
          "topic": "Programming",
          "why_needed": "To understand the concept of program iteration and improvement"
        },
        {
          "topic": "Scientific Research",
          "why_needed": "To appreciate the context and applications of autonomous scientific discovery"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the technical aspects of AI, machine learning, and programming"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.08676": {
    "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "LLaDA2.1",
          "brief": "A text diffusion model that combines Token-to-Token and Mask-to-Token editing for improved decoding speed and generation quality"
        },
        {
          "name": "Token-to-Token (T2T) editing",
          "brief": "A technique for refining output in text diffusion models"
        },
        {
          "name": "Mask-to-Token (M2T) scheme",
          "brief": "A conventional method for text generation in diffusion models"
        },
        {
          "name": "Reinforcement Learning (RL) framework",
          "brief": "A framework for training large language models to improve reasoning precision and instruction-following fidelity"
        },
        {
          "name": "Diffusion dynamics",
          "brief": "The process of generating text through a series of transformations in a diffusion model"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and applications of LLaDA2.1"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the technical aspects of the model architecture and training framework"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To appreciate the challenges and opportunities in text generation and editing"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the specialized techniques used for stable gradient estimation in the RL framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.05946": {
    "title": "f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Preference Alignment (PA) objectives",
          "brief": "Objectives that act as divergence estimators between aligned and unaligned response distributions"
        },
        {
          "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
          "brief": "A setting where only environmental rewards are available for learning"
        },
        {
          "name": "f-divergences",
          "brief": "A class of divergence measures used for comparing probability distributions"
        },
        {
          "name": "Variational representation of f-divergences",
          "brief": "A method for approximating f-divergences using variational techniques"
        },
        {
          "name": "On-policy and off-policy reinforcement learning",
          "brief": "Types of reinforcement learning that involve learning from the current policy or from experiences gathered without following the current policy"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of RL and its applications"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the use of neural networks in implementing the proposed framework"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To grasp the concept of divergence measures and their role in the proposed objectives"
        },
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context of the proposed framework and its applications in LLM alignment"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.05708": {
    "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Retrieval-Augmented Generation (RAG)",
          "brief": "A technique that enhances large language models (LLMs) with external knowledge retrieval for improved reasoning in knowledge-intensive tasks"
        },
        {
          "name": "Entity Matching",
          "brief": "The process of identifying and matching equivalent entities across different data sources"
        },
        {
          "name": "Blocking-based Batch Retrieval and Generation",
          "brief": "A method to reduce computation overhead in RAG pipelines by retrieving and generating data in batches"
        },
        {
          "name": "Cost-Efficient RAG Architecture (CE-RAG4EM)",
          "brief": "A proposed architecture that optimizes RAG for entity matching by minimizing computation overhead"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the foundation of RAG and its applications"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To comprehend the retrieval mechanisms used in RAG"
        },
        {
          "topic": "Data Integration and Entity Matching",
          "why_needed": "To recognize the importance and challenges of entity matching in real-world applications"
        },
        {
          "topic": "Optimization Techniques for Computational Efficiency",
          "why_needed": "To appreciate the motivations behind blocking-based batch retrieval and generation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-10"
  },
  "2602.07120": {
    "title": "Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Anchored Decoding",
          "brief": "A method to reduce copyright risk in language models by suppressing verbatim copying"
        },
        {
          "name": "Language Models (LMs)",
          "brief": "AI models that generate human-like language based on training data"
        },
        {
          "name": "Copyright Risk",
          "brief": "The risk of language models reproducing copyrighted content without permission"
        },
        {
          "name": "Permissively Trained Safe Model",
          "brief": "A language model trained on data with appropriate licenses and permissions"
        },
        {
          "name": "Information Budget",
          "brief": "A constraint on the amount of information used by the language model during generation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the basics of language models and their applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training and inference processes of language models"
        },
        {
          "topic": "Copyright Law",
          "why_needed": "To understand the legal implications of copyright infringement and the need for reducing copyright risk"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the technical aspects of language model architecture and training"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.10063": {
    "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Chain of Mindset (CoM)",
          "brief": "A training-free agentic framework for adaptive cognitive mode orchestration"
        },
        {
          "name": "Mindsets",
          "brief": "Distinct modes of cognitive processing, including Spatial, Convergent, Divergent, and Algorithmic"
        },
        {
          "name": "Meta-Agent",
          "brief": "A dynamic selector of optimal mindsets based on the evolving reasoning state"
        },
        {
          "name": "Context Gate",
          "brief": "A bidirectional filter for cross-module information flow to maintain effectiveness and efficiency"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Existing models that apply a single fixed mindset across all steps"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To understand the concept of mindsets and their role in human problem-solving"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To comprehend the limitations of existing LLMs and the need for adaptive mindset orchestration"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To appreciate the training-free aspect of the CoM framework and its potential applications"
        },
        {
          "topic": "Problem-Solving Strategies",
          "why_needed": "To recognize the importance of integrating multiple mindsets in solving complex tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2601.21235": {
    "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and generate human-like language"
        },
        {
          "name": "Social Harm Analysis",
          "brief": "Evaluating the potential harm caused by AI models in social contexts"
        },
        {
          "name": "Risk Profiles",
          "brief": "Multidimensional evaluation of risk in AI models"
        },
        {
          "name": "Conditional Value at Risk (CVaR95)",
          "brief": "A metric to characterize worst-case model behavior"
        },
        {
          "name": "Bias, Fairness, Ethics, and Epistemic Reliability",
          "brief": "Key dimensions for evaluating social harm in AI models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the basics of LLMs and their applications"
        },
        {
          "topic": "Statistics and Probability",
          "why_needed": "To comprehend the concepts of risk profiles, CVaR95, and distributional statistics"
        },
        {
          "topic": "Social Science and Ethics",
          "why_needed": "To understand the social implications of AI models and the importance of evaluating harm"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model evaluation, bias, and fairness"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.09823": {
    "title": "Covo-Audio Technical Report",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "End-to-End LALM (Large Language Model)",
          "brief": "A type of AI model that can process and generate human-like language, including speech and text"
        },
        {
          "name": "Covo-Audio Architecture",
          "brief": "A specific 7B-parameter end-to-end LALM designed for audio inputs and outputs"
        },
        {
          "name": "Speech-Text Modeling",
          "brief": "The process of converting spoken language into text and vice versa"
        },
        {
          "name": "Full-Duplex Voice Interaction",
          "brief": "A type of voice interaction where both parties can speak and listen simultaneously"
        },
        {
          "name": "Intelligence-Speaker Decoupling Strategy",
          "brief": "A method to separate dialogue intelligence from voice rendering for more efficient deployment"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LALMs"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend speech-text modeling, spoken dialogue, and language understanding"
        },
        {
          "topic": "Audio Signal Processing",
          "why_needed": "To grasp the processing of continuous audio inputs and outputs"
        },
        {
          "topic": "Conversational AI",
          "why_needed": "To appreciate the applications of Covo-Audio in conversational assistant scenarios"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.08234": {
    "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A subfield of machine learning that involves training agents to make decisions in complex environments"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        },
        {
          "name": "Skill Discovery",
          "brief": "The process of automatically identifying and extracting reusable behavioral patterns from experiences"
        },
        {
          "name": "Hierarchical Skill Library",
          "brief": "A structured repository of skills that enables efficient retrieval and evolution of skills"
        },
        {
          "name": "Recursive Evolution Mechanism",
          "brief": "A process that allows the skill library to co-evolve with the agent's policy during reinforcement learning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the fundamentals of LLMs and reinforcement learning"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the application of LLMs in complex tasks"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To grasp the broader context of skill discovery and hierarchical skill libraries"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of reinforcement learning and policy improvement"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.07837": {
    "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Embodied AI",
          "brief": "Artificial intelligence that interacts with and learns from the physical world"
        },
        {
          "name": "Online Policy Learning",
          "brief": "Learning policies in real-time, directly in the physical world"
        },
        {
          "name": "Real-World Reinforcement Learning",
          "brief": "Applying reinforcement learning in real-world, physical environments"
        },
        {
          "name": "Cloud-Edge Communication",
          "brief": "Communication between cloud computing resources and edge devices, such as robots"
        },
        {
          "name": "Asynchronous Learning Framework",
          "brief": "A framework for learning that can handle concurrent, asynchronous data and updates"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of policy learning and its challenges in real-world environments"
        },
        {
          "topic": "Robotics and Embodied Systems",
          "why_needed": "To comprehend the complexities of interacting with the physical world"
        },
        {
          "topic": "Cloud Computing and Edge Devices",
          "why_needed": "To understand the communication and infrastructure requirements for real-world policy learning"
        },
        {
          "topic": "Deep Learning and Neural Networks",
          "why_needed": "To grasp the concepts of CNN/MLP, generative policies, and large vision-language-action (VLA) models"
        },
        {
          "topic": "Distributed Systems and Networking",
          "why_needed": "To appreciate the challenges of scalable data collection, heterogeneous deployment, and long-horizon effective training"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.02285": {
    "title": "Statistical Learning Theory in Lean 4: Empirical Processes from Scratch",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Statistical Learning Theory (SLT)",
          "brief": "A framework for understanding the performance of machine learning algorithms"
        },
        {
          "name": "Empirical Process Theory",
          "brief": "A mathematical framework for analyzing the behavior of random processes"
        },
        {
          "name": "Gaussian Lipschitz Concentration",
          "brief": "A mathematical concept for bounding the deviation of random variables"
        },
        {
          "name": "Dudley's Entropy Integral Theorem",
          "brief": "A mathematical result for bounding the expected value of random processes"
        },
        {
          "name": "Least-Squares Regression",
          "brief": "A statistical method for modeling the relationship between variables"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Mathematical Analysis",
          "why_needed": "To understand the theoretical foundations of SLT and empirical process theory"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To understand the behavior of random processes and variables"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the context and applications of SLT"
        },
        {
          "topic": "Formal Verification",
          "why_needed": "To understand the use of Lean 4 and formal proof construction"
        },
        {
          "topic": "Programming (Lean 4)",
          "why_needed": "To understand the implementation and codebase of the project"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6-12 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.09849": {
    "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language-Action (VLA) models",
          "brief": "Models that integrate visual, linguistic, and action information for decision-making"
        },
        {
          "name": "Embodied agents",
          "brief": "Artificial agents that interact with their environment through sensors and actuators"
        },
        {
          "name": "Long-horizon manipulation",
          "brief": "Tasks that require planning and execution over an extended period"
        },
        {
          "name": "Residual Flow Guidance (RFG)",
          "brief": "A method for guiding action generation using predictive visual features"
        },
        {
          "name": "Pre-trained foundation models",
          "brief": "Large models pre-trained on vast amounts of data for general-purpose understanding and generation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the architecture and training of VLA models"
        },
        {
          "topic": "Computer vision",
          "why_needed": "To comprehend visual forecasting and feature extraction"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "To understand linguistic planning and text generation"
        },
        {
          "topic": "Robotics and control",
          "why_needed": "To grasp the concepts of embodied agents and action execution"
        },
        {
          "topic": "Machine learning",
          "why_needed": "To understand the training and evaluation of BagelVLA and other VLA models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.07839": {
    "title": "TodoEvolve: Learning to Architect Agent Planning Systems",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agent Planning Systems",
          "brief": "Systems that enable agents to plan and navigate complex tasks"
        },
        {
          "name": "Meta-Planning Paradigm",
          "brief": "A high-level approach to generate and adapt planning systems"
        },
        {
          "name": "Modular Design Space",
          "brief": "A standardized framework for diverse planning paradigms"
        },
        {
          "name": "Multi-Objective Reinforcement Learning",
          "brief": "A type of machine learning that optimizes multiple objectives simultaneously"
        },
        {
          "name": "Impedance-Guided Preference Optimization (IGPO)",
          "brief": "A specific reinforcement learning objective that guides the generation of planning systems"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of agent planning systems"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the concepts of reinforcement learning and multi-objective optimization"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To appreciate the design and implementation of modular systems and APIs"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the fundamentals of algorithms, data structures, and software design"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.10099": {
    "title": "Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Transformers",
          "brief": "A type of generative model that uses a process called diffusion-based image synthesis"
        },
        {
          "name": "Representation Encoders",
          "brief": "Neural networks that learn to represent data in a compact and meaningful way"
        },
        {
          "name": "Riemannian Flow Matching with Jacobi Regularization (RJF)",
          "brief": "A method that enables standard diffusion transformers to converge on representation encoders by constraining the generative process to the manifold geodesics"
        },
        {
          "name": "Geometric Interference",
          "brief": "A phenomenon where standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders"
        },
        {
          "name": "Manifold Learning",
          "brief": "A technique that involves learning the underlying structure of high-dimensional data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and generative models"
        },
        {
          "topic": "Differential Geometry",
          "why_needed": "To comprehend the concepts of manifolds, geodesics, and curvature"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To understand the principles of probability distributions and density estimation"
        },
        {
          "topic": "Generative Modeling",
          "why_needed": "To familiarize oneself with the concepts of diffusion-based image synthesis and representation learning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.07755": {
    "title": "Learning to Continually Learn via Meta-learning Agentic Memory Designs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Meta-learning",
          "brief": "A subfield of machine learning that involves training models to learn how to learn from other tasks"
        },
        {
          "name": "Agentic Memory Designs",
          "brief": "Memory modules designed for agentic systems to retain and reuse past experience for continual learning"
        },
        {
          "name": "Continual Learning",
          "brief": "The ability of a system to learn from a stream of data and adapt to new tasks or environments over time"
        },
        {
          "name": "Foundation Models",
          "brief": "Large-scale pre-trained models that serve as a foundation for a wide range of downstream tasks"
        },
        {
          "name": "Executable Code",
          "brief": "Code that can be executed by a computer to perform a specific task or set of tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of meta-learning, continual learning, and foundation models"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concept of agentic systems and their limitations"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the concept of executable code and database schemas"
        },
        {
          "topic": "Sequential Decision-Making",
          "why_needed": "To understand the context in which the proposed framework is applied"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.08382": {
    "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        },
        {
          "name": "Long-Context Processing",
          "brief": "The ability of a model to consider a long sequence of input data when making predictions"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning where an agent learns by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Chunk-Wise Compression",
          "brief": "A method of breaking down input data into smaller chunks and compressing each chunk to reduce computational costs"
        },
        {
          "name": "Selective Memory Recall",
          "brief": "A technique where a model selectively retrieves and processes relevant information from memory to solve a task"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of Large Language Models"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the challenges and applications of long-context processing in language models"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To appreciate the computational costs and memory usage of different models"
        },
        {
          "topic": "Machine Learning Optimization",
          "why_needed": "To understand the end-to-end reinforcement learning approach used in the proposed framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.10090": {
    "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agent World Model (AWM)",
          "brief": "A fully synthetic environment generation pipeline for agentic reinforcement learning"
        },
        {
          "name": "Large Language Model (LLM)",
          "brief": "A type of artificial intelligence model capable of processing and generating human-like language"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach where agents learn to make decisions by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Synthetic Environments",
          "brief": "Artificially generated environments used for training and testing autonomous agents"
        },
        {
          "name": "Multi-turn Interactions",
          "brief": "Complex interactions between agents and environments that require multiple steps or decisions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the context and applications of Agent World Model"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the reinforcement learning approach used in the paper"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the concepts of synthetic environments, databases, and code-driven environments"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To familiarize with large language models and their capabilities"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To appreciate the design and implementation of the Agent World Model pipeline"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.10098": {
    "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language-Action (VLA) policies",
          "brief": "Policies that combine visual, linguistic, and action information for decision-making"
        },
        {
          "name": "Latent World Model",
          "brief": "A model that represents the world in a compact, abstract form, focusing on relevant state transitions rather than raw sensory input"
        },
        {
          "name": "JEPA-style pretraining framework",
          "brief": "A framework for pretraining models that uses a target encoder and a student pathway to predict future states without using future information as input"
        },
        {
          "name": "Leakage-free state prediction",
          "brief": "A method for predicting future states without using future information as input, reducing the risk of information leakage and appearance bias"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the concepts of pretraining, latent representations, and neural networks"
        },
        {
          "topic": "Reinforcement learning",
          "why_needed": "To understand the context of VLA policies and the importance of robust state transitions"
        },
        {
          "topic": "Computer vision",
          "why_needed": "To understand the challenges of working with visual data, such as appearance bias and nuisance motion"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "To understand the role of language in VLA policies and the potential for information leakage"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-11"
  },
  "2602.11144": {
    "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Unified Multimodal Models (UMMs)",
          "brief": "Models that can process and generate multiple forms of data, such as text and images"
        },
        {
          "name": "Generative Fluid Intelligence (GFI)",
          "brief": "The ability of a model to induce patterns, reason through constraints, and adapt to novel scenarios on the fly"
        },
        {
          "name": "Crystallized Intelligence",
          "brief": "The ability to recall accumulated knowledge and learned schemas"
        },
        {
          "name": "Inducing Implicit Patterns",
          "brief": "The ability to infer patterns or preferences from limited information"
        },
        {
          "name": "Executing Ad-hoc Constraints",
          "brief": "The ability to generate outputs based on novel or unconventional constraints"
        },
        {
          "name": "Adapting to Contextual Knowledge",
          "brief": "The ability to reason and generate outputs based on immediate context"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and multimodal models"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concepts of intelligence, reasoning, and adaptation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the generation and processing of visual data"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the generation and processing of text data"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To understand the concepts of human intelligence, reasoning, and cognition"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.11089": {
    "title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language"
        },
        {
          "name": "Data Recipes",
          "brief": "A data processing pipeline to transform raw sources into training corpora for LLMs"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach where an agent learns to make decisions by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Data Curation",
          "brief": "The process of selecting, transforming, and preparing data for use in LLM training"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the fundamentals of LLMs and reinforcement learning"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the applications and challenges of LLMs in processing human language"
        },
        {
          "topic": "Data Processing Pipelines",
          "why_needed": "To grasp the concept of data recipes and their role in LLM training"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To appreciate the broader context and potential implications of automating LLM training"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.10604": {
    "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts (MoE) model",
          "brief": "A type of neural network architecture that combines multiple expert models to improve performance and efficiency"
        },
        {
          "name": "Sparse Mixture-of-Experts",
          "brief": "A variant of MoE that uses sparse connections to reduce computational costs"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning that involves training agents to make decisions based on rewards or penalties"
        },
        {
          "name": "Multi-Token Prediction (MTP-3)",
          "brief": "A technique used to reduce latency and cost in multi-round agentic interactions"
        },
        {
          "name": "Interleaved 3:1 sliding-window/full attention",
          "brief": "A method used to optimize the attention mechanism in neural networks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of the Step 3.5 Flash model"
        },
        {
          "topic": "Neural Networks",
          "why_needed": "To comprehend the underlying concepts of the MoE model and its variants"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To appreciate the context and applications of the Step 3.5 Flash model"
        },
        {
          "topic": "Mathematics and Coding",
          "why_needed": "To understand the tasks and benchmarks used to evaluate the model's performance"
        },
        {
          "topic": "Reinforcement Learning Frameworks",
          "why_needed": "To grasp the scalable framework used to train the model"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.07918": {
    "title": "CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Indirect Prompt Injection (IPI) attacks",
          "brief": "A type of attack where malicious commands are hidden within untrusted content to trick AI agents into performing unauthorized actions"
        },
        {
          "name": "Causal attribution",
          "brief": "A method to attribute the influence of different factors on a decision-making process"
        },
        {
          "name": "Causal ablation",
          "brief": "A technique to analyze the effect of removing a particular factor on the outcome of a decision-making process"
        },
        {
          "name": "Chain-of-Thought masking",
          "brief": "A method to prevent an AI agent from acting on poisoned reasoning traces"
        },
        {
          "name": "Selective defense framework",
          "brief": "A defense strategy that triggers sanitization only when an untrusted segment dominates the user intent"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence (AI) and Machine Learning (ML)",
          "why_needed": "To understand the basics of AI agents, tool-calling capabilities, and decision-making processes"
        },
        {
          "topic": "Computer Security and Cybersecurity",
          "why_needed": "To comprehend the concept of attacks, vulnerabilities, and defense mechanisms"
        },
        {
          "topic": "Causal Inference and Attribution",
          "why_needed": "To grasp the principles of causal attribution and ablation"
        },
        {
          "topic": "Natural Language Processing (NLP) and Human-Computer Interaction",
          "why_needed": "To understand how AI agents interact with users and process untrusted content"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.04802": {
    "title": "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that process and understand both visual and textual inputs"
        },
        {
          "name": "Visualized Text Understanding",
          "brief": "The ability of models to comprehend text embedded in images"
        },
        {
          "name": "Modality Gap",
          "brief": "The difference in performance between models on pure-text and visualized-text inputs"
        },
        {
          "name": "Multimodal Perception and Reasoning",
          "brief": "The ability of models to perceive and reason about multiple forms of input data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of VLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the processing of visual inputs"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the processing of textual inputs"
        },
        {
          "topic": "Machine Learning Evaluation Metrics",
          "why_needed": "To interpret the results of the VISTA-Bench benchmark"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.08503": {
    "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Models (VLMs)",
          "brief": "Models that combine visual and linguistic information for tasks like image captioning and visual question answering"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to take actions in an environment to maximize a reward"
        },
        {
          "name": "Rollout Augmentation",
          "brief": "A technique to generate new training examples by recombining existing rollouts"
        },
        {
          "name": "Self-Correction",
          "brief": "The ability of a model to correct its own mistakes or inaccuracies"
        },
        {
          "name": "Response-Masking Strategy",
          "brief": "A technique to decouple self-correction from direct reasoning in VLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of VLMs"
        },
        {
          "topic": "Reinforcement Learning Fundamentals",
          "why_needed": "To comprehend the challenges of learning self-correction behaviors in VLMs"
        },
        {
          "topic": "Computer Vision and Natural Language Processing",
          "why_needed": "To appreciate the applications and limitations of VLMs"
        },
        {
          "topic": "Machine Learning Optimization Techniques",
          "why_needed": "To understand the importance of sample efficiency and stable optimization in RL"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.10699": {
    "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Autoregressive Models",
          "brief": "A type of generative model that predicts the next item in a sequence based on the previous items"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to make decisions in complex environments"
        },
        {
          "name": "Likelihood-Dominated Decoding",
          "brief": "A decoding strategy that prioritizes locally probable prefixes, often leading to myopic bias"
        },
        {
          "name": "Value-Guided Efficient Decoding (VED)",
          "brief": "A decoding strategy that selectively deepens high-potential prefixes to improve exploration efficiency"
        },
        {
          "name": "Sibling-GRPO",
          "brief": "An advantage estimation method that exploits tree topology to compute sibling-relative advantages"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Generative Recommendation Systems",
          "why_needed": "To understand the context and motivation behind the proposed V-STAR framework"
        },
        {
          "topic": "Reinforcement Learning Fundamentals",
          "why_needed": "To comprehend the challenges of probability-reward mismatch and advantage compression in RL"
        },
        {
          "topic": "Autoregressive Models and Beam Search",
          "why_needed": "To appreciate the limitations of conventional decoding strategies and the need for alternative approaches"
        },
        {
          "topic": "Tree-Structured Advantage Reinforcement",
          "why_needed": "To understand the synergistic components of the V-STAR framework and their contributions to improved exploration and learning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.10999": {
    "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Agentic coding",
          "brief": "A type of coding that requires agents to interact with runtime environments"
        },
        {
          "name": "Command Line Interface (CLI)",
          "brief": "A user interface that allows users to interact with a computer using commands"
        },
        {
          "name": "Environment-intensive tasks",
          "brief": "Tasks that require interaction with runtime environments, such as resolving dependency issues"
        },
        {
          "name": "Dockerfile",
          "brief": "A file that contains instructions for building a Docker image"
        },
        {
          "name": "Agentic task generation",
          "brief": "The process of generating tasks for agents to learn from"
        },
        {
          "name": "Environment inversion",
          "brief": "The process of tracing environment histories to derive tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial intelligence and machine learning",
          "why_needed": "To understand the concept of agentic coding and task generation"
        },
        {
          "topic": "Software development and programming",
          "why_needed": "To understand the context of CLI and environment-intensive tasks"
        },
        {
          "topic": "Docker and containerization",
          "why_needed": "To understand the analogy between Dockerfile and agentic task"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "To understand the concept of fine-tuning models and evaluating performance on benchmarks like Terminal-Bench"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.10224": {
    "title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
          "brief": "A method for enhancing reasoning capabilities of Large Language Models"
        },
        {
          "name": "Meta-Experience Learning (MEL)",
          "brief": "A framework for internalizing reusable knowledge representations into a model's parametric memory"
        },
        {
          "name": "Meta-Experience",
          "brief": "Reusable knowledge representations derived from past errors"
        },
        {
          "name": "Contrastive Analysis",
          "brief": "A method for identifying precise bifurcation points where reasoning errors arise"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models designed to process and understand human language"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of RLVR and its limitations"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of Large Language Models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the application and evaluation of LLMs"
        },
        {
          "topic": "Meta-Learning",
          "why_needed": "To grasp the concept of learning from experience and adapting to new tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.10177": {
    "title": "Towards Autonomous Mathematics Research",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Foundational Models",
          "brief": "Advanced AI models capable of complex reasoning and problem-solving"
        },
        {
          "name": "Math Research Agent",
          "brief": "AI system designed to generate, verify, and revise mathematical solutions"
        },
        {
          "name": "Inference-Time Scaling Law",
          "brief": "Method to extend AI's problem-solving capabilities beyond current limits"
        },
        {
          "name": "Arithmetic Geometry",
          "brief": "Branch of mathematics studying geometric properties of numbers and algebraic structures"
        },
        {
          "name": "Human-AI Collaboration",
          "brief": "Partnership between humans and AI systems to achieve research goals"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Mathematical Olympiad",
          "why_needed": "Understanding the context of competition-level problem-solving"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "Familiarity with AI concepts and their applications"
        },
        {
          "topic": "Mathematical Research",
          "why_needed": "Knowledge of the research process and challenges in mathematics"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Understanding how AI generates and processes human-like language"
        },
        {
          "topic": "Erdos Conjectures",
          "why_needed": "Familiarity with open problems in mathematics and their significance"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.08741": {
    "title": "Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Mixture-of-Experts (MoE) architectures",
          "brief": "A type of neural network architecture that improves scaling efficiency by activating only a small subset of parameters per token"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "A type of artificial intelligence model designed to process and understand human language"
        },
        {
          "name": "Large Language Lobotomy (L^3)",
          "brief": "A training-free, architecture-agnostic attack that compromises safety alignment in MoE LLMs by exploiting expert routing dynamics"
        },
        {
          "name": "Expert silencing",
          "brief": "A technique used in L^3 to adaptively silence the most safety-relevant experts in MoE LLMs until harmful outputs are produced"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep learning",
          "why_needed": "To understand the basics of neural networks and their applications in natural language processing"
        },
        {
          "topic": "Natural language processing",
          "why_needed": "To comprehend the context and implications of LLMs and MoE architectures"
        },
        {
          "topic": "Adversarial attacks",
          "why_needed": "To recognize the importance of safety alignment and the potential vulnerabilities of MoE LLMs"
        },
        {
          "topic": "Computer architecture",
          "why_needed": "To understand the design and routing structure of MoE LLMs and how they can be exploited"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.09014": {
    "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Models",
          "brief": "A type of generative model that uses a process of denoising steps to generate images"
        },
        {
          "name": "Text-to-Image Generation",
          "brief": "The process of generating images from text prompts"
        },
        {
          "name": "Non-Linear Flow Distillation",
          "brief": "A technique used to approximate complex trajectories in generative models"
        },
        {
          "name": "Teacher-Student Framework",
          "brief": "A training paradigm where a pre-trained model (teacher) guides a smaller model (student) to learn"
        },
        {
          "name": "Mixture of Continuous Momentum Processes",
          "brief": "A mathematical formulation to model complex velocity fields"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of generative models and neural networks"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To appreciate the application of text-to-image generation"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To grasp the concept of non-linear flow distillation and trajectory approximation"
        },
        {
          "topic": "Probability Theory",
          "why_needed": "To understand the mixture of continuous momentum processes"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.10231": {
    "title": "Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multi-Objective Reinforcement Learning (RL)",
          "brief": "A type of RL where an agent must optimize multiple objectives simultaneously"
        },
        {
          "name": "Group Relative Policy Optimization (GRPO)",
          "brief": "A method for optimizing policies in RL by comparing the performance of different groups of agents"
        },
        {
          "name": "Blockwise Advantage Estimation",
          "brief": "A method for estimating advantages in multi-objective RL by assigning separate advantages to each objective and applying them only to the relevant text blocks"
        },
        {
          "name": "Outcome-Conditioned Baseline",
          "brief": "A technique for approximating intermediate state values using within-group statistics and stratifying samples according to a prefix-derived intermediate outcome"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of RL and how it applies to multi-objective problems"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the neural network architectures and techniques used in the paper"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To understand the mathematical formulations and derivations presented in the paper"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the application of the method to structured generations and text blocks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-12"
  },
  "2602.11298": {
    "title": "Voxtral Realtime",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Automatic Speech Recognition (ASR)",
          "brief": "Technology used to recognize and transcribe spoken language into text"
        },
        {
          "name": "Streaming ASR",
          "brief": "ASR models that can process audio in real-time, with minimal delay"
        },
        {
          "name": "Delayed Streams Modeling",
          "brief": "Framework for modeling sequential data with delayed feedback"
        },
        {
          "name": "Causal Audio Encoder",
          "brief": "Type of neural network encoder that processes audio sequences in a causal manner"
        },
        {
          "name": "Ada RMS-Norm",
          "brief": "Normalization technique used to improve model stability and performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of the Voxtral Realtime model"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges and applications of ASR"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "To comprehend the audio encoding and processing aspects of the model"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, evaluation, and optimization"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.11598": {
    "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Embodied Navigation",
          "brief": "The ability of an agent to navigate through an environment using its sensors and actuators"
        },
        {
          "name": "Vision-Language-Action (VLA) Foundation Model",
          "brief": "A unified model that integrates vision, language, and action to achieve versatile embodied navigation"
        },
        {
          "name": "Hierarchical Brain-Action Architecture",
          "brief": "An architecture that pairs a cognitive brain for semantic reasoning with an action expert for precise trajectory generation"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "A type of artificial intelligence model trained on large amounts of text data to generate human-like language"
        },
        {
          "name": "Flow Matching",
          "brief": "A technique used for precise, continuous trajectory generation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of the ABot-N0 model"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the vision component of the VLA foundation model"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the language component of the VLA foundation model"
        },
        {
          "topic": "Robotics and Navigation",
          "why_needed": "To appreciate the applications and challenges of embodied navigation"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the broader context and implications of the ABot-N0 model"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.11964": {
    "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Model (LLM) Agents",
          "brief": "Artificial intelligence models that can understand and generate human-like language"
        },
        {
          "name": "Dynamic and Asynchronous Environments",
          "brief": "Realistic scenarios where events evolve independently of agent actions, requiring adaptability and temporal reasoning"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning where agents learn from rewards or penalties to make decisions"
        },
        {
          "name": "Benchmarking",
          "brief": "Evaluating the performance of models or systems using standardized tests or scenarios"
        },
        {
          "name": "Agents Research Environments (ARE) platform",
          "brief": "An open-source framework for developing, testing, and training agent systems"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence (AI) and Machine Learning (ML)",
          "why_needed": "To understand the context and concepts related to LLM agents and reinforcement learning"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To comprehend the capabilities and limitations of LLM agents in generating and understanding human-like language"
        },
        {
          "topic": "Computer Science and Software Engineering",
          "why_needed": "To appreciate the design and implementation of the Gaia2 benchmark and the ARE platform"
        },
        {
          "topic": "Cognitive Architectures and Multi-Agent Systems",
          "why_needed": "To understand the challenges and trade-offs involved in developing practical agent systems that can operate in dynamic environments"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.08934": {
    "title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning that involves training an agent to take actions in an environment to maximize a reward"
        },
        {
          "name": "Paraphrasing Attacks",
          "brief": "A type of adversarial attack that involves generating paraphrased text to evade detection while preserving semantics"
        },
        {
          "name": "Multi-Detector Evasion",
          "brief": "A technique to evade detection by multiple AI-text detectors"
        },
        {
          "name": "Group Relative Policy Optimization (GRPO)",
          "brief": "A reinforcement learning algorithm used to train a paraphrase policy"
        },
        {
          "name": "LoRA Adapters",
          "brief": "A type of adapter used in transformer-based models to improve efficiency and performance"
        },
        {
          "name": "AI-Text Detectors",
          "brief": "Models used to detect and classify text as legitimate or malicious"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of reinforcement learning and AI-text detectors"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the concepts of paraphrasing and text analysis"
        },
        {
          "topic": "Adversarial Attacks",
          "why_needed": "To understand the concept of evasion attacks and their impact on AI models"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture of transformer-based models and their applications"
        },
        {
          "topic": "Information Security",
          "why_needed": "To understand the importance of robustness and security in AI-text detectors"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.11008": {
    "title": "ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Model Compression",
          "brief": "Techniques to reduce model size while maintaining performance"
        },
        {
          "name": "Knapsack Problem",
          "brief": "A problem of optimizing resource allocation with constraints"
        },
        {
          "name": "Sparse Matrix Factorization",
          "brief": "A method to reduce dimensionality of matrices while preserving information"
        },
        {
          "name": "Dictionary Learning",
          "brief": "A technique to learn a compact representation of data"
        },
        {
          "name": "Deep Learning",
          "brief": "A subset of machine learning that uses neural networks to analyze data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning Fundamentals",
          "why_needed": "To understand the context of model compression and the techniques used in ROCKET"
        },
        {
          "topic": "Linear Algebra",
          "why_needed": "To comprehend the mathematical formulations of sparse matrix factorization and dictionary learning"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand the knapsack problem and the optimization methods used in ROCKET"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model performance, compression, and fine-tuning"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.08489": {
    "title": "Beyond Correctness: Learning Robust Reasoning via Transfer",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
          "brief": "A method to strengthen LLM reasoning by focusing on final answer correctness"
        },
        {
          "name": "Reinforcement Learning with Transferable Reward (RLTR)",
          "brief": "A method that operationalizes robustness via transfer reward to encourage stable and interpretable reasoning"
        },
        {
          "name": "Robust Reasoning",
          "brief": "The ability of a reasoning process to remain useful and accurate beyond its original context"
        },
        {
          "name": "Transfer Learning",
          "brief": "A machine learning technique where a model is trained on one task and applied to another related task"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of RLVR and RLTR"
        },
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To comprehend how LLMs are used in the context of RLVR and RLTR"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of transfer learning and robustness in machine learning"
        },
        {
          "topic": "Philosophy of Reasoning",
          "why_needed": "To understand the philosophical view of robust reasoning as a form of meaning transfer"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.11748": {
    "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "In-Context Exploration",
          "brief": "The ability of models to generate, verify, and refine multiple reasoning hypotheses within a single continuous context"
        },
        {
          "name": "State Coverage theory",
          "brief": "A theory that explains how models can achieve broader state coverage by generating longer reasoning trajectories"
        },
        {
          "name": "Shallow Exploration Trap",
          "brief": "A phenomenon where the probability of sampling longer reasoning sequences decays exponentially during autoregressive generation"
        },
        {
          "name": "Length-Incentivized Exploration",
          "brief": "A method that encourages models to explore more by using a length-based reward coupled with a redundancy penalty"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning that involves training models to make decisions based on rewards or penalties"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of autoregressive generation and reinforcement learning"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the context of in-context exploration and its applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the basics of model training and evaluation"
        },
        {
          "topic": "State Space Theory",
          "why_needed": "To understand the concept of state coverage and its importance in exploration"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.12262": {
    "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Large Language Models (DLLMs)",
          "brief": "Language models that enable fast text generation by decoding multiple tokens in parallel"
        },
        {
          "name": "Trajectory Self-Distillation",
          "brief": "A framework that improves few-step decoding by distilling the model's own generative trajectories"
        },
        {
          "name": "Direct Discriminative Optimization (DDO)",
          "brief": "A reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes"
        },
        {
          "name": "Few-Step Decoding",
          "brief": "A technique that reduces the number of refinement steps in language models to improve inference efficiency"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Language Models",
          "why_needed": "To understand the context and applications of DLLMs"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the concepts of self-distillation, DDO, and mode-seeking"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges and opportunities in text generation and language modeling"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To understand the role of DDO in promoting efficient decoding"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.11683": {
    "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Latent Reasoning",
          "brief": "A method of improving reasoning efficiency by using continuous representations in a latent space"
        },
        {
          "name": "Discrete Token Space",
          "brief": "A space where thinking trajectories are represented as discrete tokens"
        },
        {
          "name": "Confidence-Aware Routing",
          "brief": "A mechanism that routes thinking between latent and discrete spaces based on model confidence"
        },
        {
          "name": "ThinkRouter",
          "brief": "A proposed inference-time confidence-aware routing mechanism for efficient reasoning"
        },
        {
          "name": "STEM Reasoning and Coding Benchmarks",
          "brief": "Benchmarks used to evaluate the performance of ThinkRouter"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of latent space, discrete token space, and model confidence"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the application of ThinkRouter in STEM reasoning and coding benchmarks"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of model training, evaluation, and optimization"
        },
        {
          "topic": "Mathematics and Logic",
          "why_needed": "To understand the underlying mathematical and logical concepts used in ThinkRouter"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.12099": {
    "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language-Action (VLA) Models",
          "brief": "Models that predict multi-step action chunks from current observations"
        },
        {
          "name": "World Model-Based Reinforcement Learning",
          "brief": "A learning approach that utilizes pre-trained world models for robust spatiotemporal reasoning and future prediction"
        },
        {
          "name": "Reinforcement Learning via World Model-Conditioned Policy (RAMP)",
          "brief": "A method that integrates world models with reinforcement learning for cross-task adaptation"
        },
        {
          "name": "Robotic Manipulation",
          "brief": "The use of robots to perform complex tasks that require manipulation of objects"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of VLA models and world models"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To comprehend the concept of learning from trial and error and the role of world models in reinforcement learning"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand how VLA models process visual observations and predict actions"
        },
        {
          "topic": "Robotics",
          "why_needed": "To appreciate the application of VLA models and world models in robotic manipulation tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.10367": {
    "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Medical Benchmarking",
          "brief": "Evaluating the performance of LLMs in clinical settings"
        },
        {
          "name": "Data Contamination",
          "brief": "The issue of test data being inadvertently used in model training"
        },
        {
          "name": "Temporal Misalignment",
          "brief": "The problem of medical knowledge evolving faster than model updates"
        },
        {
          "name": "Clinical Curation Framework",
          "brief": "A framework for filtering and validating clinical data"
        },
        {
          "name": "Automated Rubric-based Evaluation",
          "brief": "A method for evaluating LLM performance using case-specific criteria"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand how LLMs work and their applications"
        },
        {
          "topic": "Medical Informatics",
          "why_needed": "To comprehend the clinical context and the importance of accurate medical evaluations"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, testing, and evaluation"
        },
        {
          "topic": "Clinical Decision Support Systems",
          "why_needed": "To understand the role of LLMs in high-stakes clinical settings"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.10106": {
    "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Egocentric Demonstration",
          "brief": "Using human demonstrations from a first-person perspective to learn robot control policies"
        },
        {
          "name": "Loco-Manipulation",
          "brief": "The ability of a robot to both locomote and manipulate objects in its environment"
        },
        {
          "name": "Embodiment Gap",
          "brief": "The differences in physical morphology and viewpoint between humans and robots"
        },
        {
          "name": "View Alignment",
          "brief": "Techniques to reduce visual domain discrepancies between human and robot perspectives"
        },
        {
          "name": "Action Alignment",
          "brief": "Methods to map human motions into a unified action space for humanoid control"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Robotics",
          "why_needed": "Understanding of robot control, locomotion, and manipulation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Knowledge of visual perception, camera systems, and image processing"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Familiarity with policy learning, reinforcement learning, and deep learning"
        },
        {
          "topic": "Human-Robot Interaction",
          "why_needed": "Understanding of human motion, behavior, and interaction with robots"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-13"
  },
  "2602.11075": {
    "title": "RISE: Self-Improving Robot Policy with Compositional World Model",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning approach that involves training agents to take actions in an environment to maximize rewards"
        },
        {
          "name": "Compositional World Model",
          "brief": "A model that predicts future outcomes and evaluates imagined scenarios to improve policy decisions"
        },
        {
          "name": "Vision-Language-Action (VLA) models",
          "brief": "Models that integrate visual, linguistic, and action-related information to perform tasks"
        },
        {
          "name": "Self-Improving Systems",
          "brief": "Systems that can improve their performance over time through experience and learning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Robotics",
          "why_needed": "To understand the application domain of the RISE framework"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the underlying architectures and objectives used in the Compositional World Model"
        },
        {
          "topic": "Control Theory",
          "why_needed": "To grasp the dynamics model and progress value model components of the RISE framework"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the reinforcement learning and policy improvement aspects of the RISE framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-14"
  },
  "2602.12092": {
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Models (LMs)",
          "brief": "Artificial intelligence models with a large number of parameters"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and understand multiple forms of data, including language and images"
        },
        {
          "name": "Safety Evaluation",
          "brief": "Assessing the potential risks and hazards of AI models"
        },
        {
          "name": "Safety Diagnosis",
          "brief": "Identifying the root causes of safety issues in AI models"
        },
        {
          "name": "Safety Alignment",
          "brief": "Ensuring that AI models are aligned with human values and goals"
        },
        {
          "name": "DeepSight",
          "brief": "An open-source toolkit for evaluating and diagnosing the safety of AI models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "Understanding the basics of AI and its applications"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Knowledge of machine learning concepts, including model development and evaluation"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Understanding of NLP concepts, including language models and text processing"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Knowledge of computer vision concepts, including image processing and multimodal models"
        },
        {
          "topic": "Software Development",
          "why_needed": "Understanding of software development principles, including open-source projects and toolkit development"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-14"
  },
  "2602.12205": {
    "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Unified Multimodal Models",
          "brief": "Models that can handle multiple tasks such as image generation and editing"
        },
        {
          "name": "Stacked Channel Bridging (SCB)",
          "brief": "A deep alignment framework for extracting hierarchical features"
        },
        {
          "name": "Data-Centric Training Strategy",
          "brief": "A three-stage training approach for improving model performance"
        },
        {
          "name": "Reinforcement Learning with MR-GRPO",
          "brief": "A method for fine-tuning models using a mixture of reward functions and supervision signals"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of unified multimodal models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend image generation and editing tasks"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the role of text in image-text pairs and editing triplets"
        },
        {
          "topic": "Model Optimization",
          "why_needed": "To appreciate the challenges of training large models and the benefits of lightweight models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-14"
  },
  "2602.12164": {
    "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Co-evolving Paradigms",
          "brief": "Approaches that involve simultaneous evolution of multiple components or systems"
        },
        {
          "name": "Scientific Reasoning",
          "brief": "The process of drawing conclusions based on scientific evidence and principles"
        },
        {
          "name": "Geometric Consensus",
          "brief": "A method for achieving agreement among multiple entities or models using geometric principles"
        },
        {
          "name": "Sparse Supervision",
          "brief": "A learning approach that uses limited labeled data to train models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LLMs"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the application of LLMs in scientific reasoning tasks"
        },
        {
          "topic": "Mathematics and Logic",
          "why_needed": "To grasp the fundamental principles of scientific reasoning and problem-solving"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of supervised, unsupervised, and self-supervised learning"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To appreciate the design and implementation of the Sci-CoE framework"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-14"
  },
  "2602.12056": {
    "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Legal Reasoning",
          "brief": "The process of drawing conclusions based on legal principles and evidence"
        },
        {
          "name": "Deep Learning",
          "brief": "A subset of machine learning that uses neural networks to analyze data"
        },
        {
          "name": "Autonomous Agents",
          "brief": "Systems that can perform tasks independently with minimal human intervention"
        },
        {
          "name": "Natural Language Processing (NLP)",
          "brief": "A field of study focused on the interaction between computers and humans in natural language"
        },
        {
          "name": "Explore-Verify-Memorize Strategy",
          "brief": "A strategy used by LawThinker to verify intermediate reasoning steps in dynamic environments"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the basics of autonomous agents and deep learning"
        },
        {
          "topic": "Legal Studies",
          "why_needed": "To comprehend the principles of legal reasoning and the importance of procedural compliance"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To grasp the concepts of NLP, deep learning, and software development"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the mechanisms of the DeepVerifier module and the memory module"
        },
        {
          "topic": "Judicial Systems",
          "why_needed": "To appreciate the context and applications of LawThinker in dynamic judicial environments"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-14"
  },
  "2602.05827": {
    "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language Navigation",
          "brief": "The ability of agents to navigate through environments based on language instructions"
        },
        {
          "name": "Beyond-the-View Navigation (BVN)",
          "brief": "Navigating to unseen targets without dense guidance"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Video Generation Models",
          "brief": "AI models that generate videos based on input data"
        },
        {
          "name": "Sparse Video Generation",
          "brief": "Generating sparse videos to reduce computational complexity"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of LLMs and video generation models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the processing and generation of visual data"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the processing and generation of language instructions"
        },
        {
          "topic": "Navigation and Robotics",
          "why_needed": "To grasp the application of BVN in real-world scenarios"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-14"
  },
  "2602.10575": {
    "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Visual Reinforcement Learning",
          "brief": "A type of machine learning that involves training agents to make decisions in complex environments using visual inputs"
        },
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that can process and understand multiple forms of data, including text and images"
        },
        {
          "name": "Theory of Mind (ToM)",
          "brief": "The ability to understand and interpret the mental states and intentions of others"
        },
        {
          "name": "Image Metaphor Understanding",
          "brief": "The ability to comprehend and interpret metaphors and nuances in visual content"
        },
        {
          "name": "End-to-End Learning",
          "brief": "A type of machine learning where a single model is trained to perform a task from start to finish"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of visual reinforcement learning and MLLMs"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the challenges of image metaphor understanding and visual reasoning"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the role of language models in image implication tasks"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To understand the concept of Theory of Mind and its relevance to image metaphor understanding"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-14"
  },
  "2602.11509": {
    "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Large Language Models (MLLMs)",
          "brief": "AI models that process and generate text based on multiple forms of input, such as video, audio, and text"
        },
        {
          "name": "Fact-Level Multimodal Attribution",
          "brief": "The ability of a model to provide accurate and reliable sources for its claims, including the specific modality and temporal segment"
        },
        {
          "name": "Verifiable Reasoning",
          "brief": "The process of generating answers with explicit reasoning and precise citations to support factual claims"
        },
        {
          "name": "MuRGAt Benchmark",
          "brief": "A benchmark for evaluating fact-level multimodal attribution in complex multimodal reasoning scenarios"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the basics of language models and their applications"
        },
        {
          "topic": "Multimodal Learning",
          "why_needed": "To comprehend how models process and integrate multiple forms of input"
        },
        {
          "topic": "Evaluation Metrics for AI Models",
          "why_needed": "To understand how to assess the performance of MLLMs in complex reasoning tasks"
        },
        {
          "topic": "Machine Learning and Deep Learning",
          "why_needed": "To grasp the underlying techniques and architectures used in MLLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-15"
  },
  "2602.11451": {
    "title": "LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Looped Transformers",
          "brief": "A class of models for efficient and powerful reasoning in the language domain"
        },
        {
          "name": "Elastic-Depth Looped Transformers",
          "brief": "Transformers that can adapt their computational depth under variable compute budgets"
        },
        {
          "name": "Shortcut Modulation",
          "brief": "A technique to align trajectories of different lengths in looped Transformers"
        },
        {
          "name": "Budget-Conditioned Reasoning",
          "brief": "The ability of a model to adapt its reasoning based on the available compute budget"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Transformer Architecture",
          "why_needed": "To understand the basics of Transformer models and their applications in language modeling"
        },
        {
          "topic": "Language Modeling",
          "why_needed": "To comprehend the context and significance of the proposed LoopFormer model"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To grasp the concepts of neural networks, training schemes, and optimization techniques"
        },
        {
          "topic": "Reasoning and Algorithmic Tasks",
          "why_needed": "To understand the types of tasks that LoopFormer is designed to perform"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-15"
  },
  "2602.10229": {
    "title": "Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Latent Thoughts Tuning (LT-Tuning)",
          "brief": "A framework for constructing and deploying latent thoughts in Large Language Models (LLMs)"
        },
        {
          "name": "Chain-of-Thought (CoT)",
          "brief": "A method for equipping LLMs with strong reasoning capabilities by verbalizing intermediate steps"
        },
        {
          "name": "Context-Prediction-Fusion mechanism",
          "brief": "A mechanism that combines contextual hidden states and predictive semantic guidance for latent thought construction"
        },
        {
          "name": "Curriculum learning pipeline",
          "brief": "A three-stage pipeline for dynamically switching between latent and explicit thinking modes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and application of Latent Thoughts Tuning"
        },
        {
          "topic": "Deep learning and neural networks",
          "why_needed": "To comprehend the underlying mechanisms and architectures used in LT-Tuning"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the importance of reasoning and inference in language models"
        },
        {
          "topic": "Latent space and continuous representation learning",
          "why_needed": "To appreciate the benefits and challenges of reasoning in continuous latent space"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-15"
  },
  "2602.08277": {
    "title": "PISCO: Precise Video Instance Insertion with Sparse Control",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Video Instance Insertion",
          "brief": "Inserting a specific instance into existing footage while maintaining scene integrity"
        },
        {
          "name": "Video Diffusion Model",
          "brief": "A type of generative model for video generation and editing"
        },
        {
          "name": "Sparse Keyframe Control",
          "brief": "Specifying a limited number of keyframes to control the video generation process"
        },
        {
          "name": "Distribution-Preserving Temporal Masking",
          "brief": "A technique to stabilize temporal generation in video diffusion models"
        },
        {
          "name": "Geometry-Aware Conditioning",
          "brief": "A method to adapt the generated video to the scene geometry"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Computer Vision",
          "why_needed": "Understanding of image and video processing techniques"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Familiarity with generative models and neural networks"
        },
        {
          "topic": "Video Editing",
          "why_needed": "Knowledge of professional video editing techniques and requirements"
        },
        {
          "topic": "AI-Assisted Filmmaking",
          "why_needed": "Understanding of the role of AI in video generation and post-processing"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-15"
  },
  "2602.11543": {
    "title": "Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models that process and understand human language"
        },
        {
          "name": "Mixture-of-Experts (MoE) Models",
          "brief": "A type of neural network architecture that combines multiple expert models to improve performance"
        },
        {
          "name": "Distributed Training",
          "brief": "A method of training machine learning models across multiple devices or nodes"
        },
        {
          "name": "Federated Optimization",
          "brief": "A technique for optimizing machine learning models in a decentralized manner"
        },
        {
          "name": "Sparse Expert Synchronization (SPES)",
          "brief": "A memory-efficient decentralized framework for pretraining MoE LLMs"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and language models"
        },
        {
          "topic": "Distributed Computing",
          "why_needed": "To comprehend the concepts of distributed training and federated optimization"
        },
        {
          "topic": "GPU Architecture",
          "why_needed": "To understand the memory limitations of GPUs and the need for memory-efficient training methods"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the applications and challenges of training large language models"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-15"
  },
  "2602.12108": {
    "title": "The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Stateful Language Models",
          "brief": "Language models that can manage their own context and state"
        },
        {
          "name": "Internal Reasoning Loop",
          "brief": "A mechanism that allows models to dynamically engineer their own context"
        },
        {
          "name": "Memory Tools",
          "brief": "Techniques such as context pruning, document indexing, and note-taking used by models to manage their state"
        },
        {
          "name": "Foundation Models",
          "brief": "Large-scale language models that serve as a foundation for various downstream tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the basics of language models and their applications"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the architecture and training of large-scale language models"
        },
        {
          "topic": "Harry Potter Series",
          "why_needed": "To understand the metaphor of the Pensieve and its relation to the concept of stateful language models"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To understand the concepts of document indexing and retrieval"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-15"
  },
  "2602.02192": {
    "title": "ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A machine learning approach where an agent learns by interacting with an environment and receiving rewards or penalties"
        },
        {
          "name": "Distributed Rollout Framework",
          "brief": "A system that allows for the distribution of rollout execution across multiple machines or workers to improve efficiency"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Artificial intelligence models designed to process and understand human language, often requiring significant computational resources"
        },
        {
          "name": "Policy Dissemination",
          "brief": "The process of distributing updated policies or models to multiple workers or agents in a distributed system"
        },
        {
          "name": "Overlap-based Capacity Model",
          "brief": "A mathematical model that relates training time, dissemination latency, and rollout throughput to optimize system performance"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the fundamentals of reinforcement learning and large language models"
        },
        {
          "topic": "Distributed Systems",
          "why_needed": "To comprehend the challenges and opportunities of distributing rollout execution across multiple workers"
        },
        {
          "topic": "Computer Networks",
          "why_needed": "To understand the impact of wide-area bandwidth regimes on system performance"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "To appreciate the importance of optimizing system performance and cost efficiency"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-15"
  },
  "2602.07885": {
    "title": "MemFly: On-the-Fly Memory Optimization via Information Bottleneck",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Information Bottleneck",
          "brief": "A principle used to balance the trade-off between compressing redundant information and maintaining precise retrieval"
        },
        {
          "name": "Long-term Memory in LLMs",
          "brief": "The ability of large language models to store and retrieve information from historical interactions"
        },
        {
          "name": "Memory Optimization",
          "brief": "The process of improving the efficiency and effectiveness of memory storage and retrieval in LLMs"
        },
        {
          "name": "Stratified Memory Structure",
          "brief": "A hierarchical organization of memory to facilitate efficient storage and retrieval"
        },
        {
          "name": "Hybrid Retrieval Mechanism",
          "brief": "A combination of semantic, symbolic, and topological pathways for retrieving information from memory"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and applications of MemFly"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To comprehend the information bottleneck principle and its role in MemFly"
        },
        {
          "topic": "Machine Learning and Optimization",
          "why_needed": "To understand the gradient-free optimizer and the overall optimization framework"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To appreciate the challenges and opportunities in optimizing memory for LLMs"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-15"
  },
  "2602.12176": {
    "title": "Single-minus gluon tree amplitudes are nonzero",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Single-minus gluon tree amplitudes",
          "brief": "Tree-level n-gluon scattering amplitudes with one minus-helicity gluon and n-1 plus-helicity gluons"
        },
        {
          "name": "Klein space",
          "brief": "A mathematical space used to describe certain configurations of gluon momenta"
        },
        {
          "name": "Weinberg's soft theorem",
          "brief": "A theoretical framework for describing the behavior of gluon scattering amplitudes in the soft limit"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Quantum Field Theory (QFT)",
          "why_needed": "To understand the context of gluon scattering amplitudes and the relevance of tree-level calculations"
        },
        {
          "topic": "Particle Physics",
          "why_needed": "To be familiar with the concept of gluons, helicity, and scattering amplitudes"
        },
        {
          "topic": "Mathematical Physics",
          "why_needed": "To understand the mathematical tools and techniques used to derive the closed-form expression for the decay of a single minus-helicity gluon"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.11236": {
    "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Embodied Agents",
          "brief": "Robots that can interact with their environment and learn from experiences"
        },
        {
          "name": "Action Manifold Learning",
          "brief": "A technique to predict efficient and stable robot actions by projecting onto low-dimensional manifolds"
        },
        {
          "name": "Unified Pre-training",
          "brief": "A method to improve knowledge transfer and generalization across different robot platforms and tasks"
        },
        {
          "name": "DiT Backbone",
          "brief": "A deep learning architecture used for predicting continuous action sequences"
        },
        {
          "name": "Modular Perception",
          "brief": "A dual-stream mechanism that integrates visual and other sensory information for robot perception"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Robotics",
          "why_needed": "Understanding of robotic systems, including hardware and software components"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "Familiarity with deep learning concepts, including neural networks and training strategies"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Knowledge of visual perception and processing techniques for robots"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Understanding of machine learning principles, including supervised and unsupervised learning"
        },
        {
          "topic": "Mathematics",
          "why_needed": "Familiarity with mathematical concepts, including linear algebra and differential geometry"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.11769": {
    "title": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion-based Generative Models",
          "brief": "A type of deep learning model used for generating and manipulating images and videos"
        },
        {
          "name": "4D Video Relighting",
          "brief": "The process of changing the lighting in a 4D video, which includes 3D space and time"
        },
        {
          "name": "Disentangled Flow Guidance",
          "brief": "A time-aware strategy for controlling lighting in the latent space of a generative model"
        },
        {
          "name": "Temporal Consistent Attention",
          "brief": "A technique for maintaining temporal consistency in video processing"
        },
        {
          "name": "IC-Light Architecture",
          "brief": "A specific architecture used for image and video relighting"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of diffusion-based generative models and their applications"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the concepts of image and video processing, including relighting and viewpoint changes"
        },
        {
          "topic": "Linear Algebra and Geometry",
          "why_needed": "To understand the mathematical concepts underlying 3D and 4D spaces"
        },
        {
          "topic": "Programming (Python)",
          "why_needed": "To implement and experiment with the proposed framework, Light4D"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.04315": {
    "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language-Action (VLA) Models",
          "brief": "A hierarchical model that combines vision, language, and action to enable robots to understand and interact with their environment"
        },
        {
          "name": "Knowledge-Guided Trajectory Planning",
          "brief": "A method that uses knowledge to plan and generate trajectories for robots to follow"
        },
        {
          "name": "Affordance Segmentation Module (ASM)",
          "brief": "A module that perceives image keypoint affordances of the scene to understand the environment"
        },
        {
          "name": "3D-Aware Control Policy",
          "brief": "A policy that enables precise manipulation of robots in 3D space"
        },
        {
          "name": "Zero-Shot Learning",
          "brief": "The ability of a model to generalize to unseen scenarios without requiring additional training data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the foundation models and VLA models"
        },
        {
          "topic": "Robotics",
          "why_needed": "To understand the application of VLA models in robotics and the challenges of generalization"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the perception and understanding of the environment through vision"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the language component of VLA models and how they interact with the environment"
        },
        {
          "topic": "Trajectory Planning",
          "why_needed": "To understand how robots plan and generate trajectories to achieve tasks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.12705": {
    "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Medical Vision-Language Foundation Model",
          "brief": "A model that combines medical images and text to advance medical understanding and reasoning"
        },
        {
          "name": "Entity-Aware Continual Pretraining",
          "brief": "A framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach that enables medical expert-level reasoning and interaction"
        },
        {
          "name": "Tool-Augmented Agentic Training",
          "brief": "A training method that enables multi-step diagnostic reasoning with verifiable decision traces"
        },
        {
          "name": "Multimodal Systems",
          "brief": "Systems that integrate multiple forms of data, such as images and text, to improve medical understanding and reasoning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of MedXIAOHE"
        },
        {
          "topic": "Medical Imaging",
          "why_needed": "To comprehend the application of MedXIAOHE in medical diagnosis and reasoning"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the text-based components of MedXIAOHE and its ability to generate reports"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To grasp the decision-making and reasoning capabilities of MedXIAOHE"
        },
        {
          "topic": "Medical Informatics",
          "why_needed": "To appreciate the clinical applications and potential impact of MedXIAOHE"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.08683": {
    "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Codec-Aligned Sparsity",
          "brief": "A principle that aligns deep learning architectures with the fundamental structure of data for effective compression"
        },
        {
          "name": "Multimodal Intelligence",
          "brief": "The ability of a model to process and understand multiple forms of data, such as vision and language"
        },
        {
          "name": "Information-Theoretic Principles",
          "brief": "Fundamental principles that govern the compression and representation of data"
        },
        {
          "name": "Predictive Visual Structure",
          "brief": "The idea that visual signals can be compressed into semantic meaning by focusing on predictive residuals"
        },
        {
          "name": "Codec Patchification",
          "brief": "A method of dividing video into patches to focus computation on regions rich in signal entropy"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of neural networks, convolutional neural networks, and transformer models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand the principles of image and video processing, and the challenges of visual understanding"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To understand the fundamental principles of data compression and representation"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concept of artificial general intelligence and its relation to compression and representation"
        },
        {
          "topic": "Signal Processing",
          "why_needed": "To understand the concepts of signal entropy and predictive residuals"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.11910": {
    "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Audio Diffusion Models",
          "brief": "AI models that generate high-fidelity music from text"
        },
        {
          "name": "Activation Patching",
          "brief": "Technique to understand internal mechanisms of AI models"
        },
        {
          "name": "Contrastive Activation Addition",
          "brief": "Method to improve control over generated audio"
        },
        {
          "name": "Sparse Autoencoders",
          "brief": "Technique to enable precise control over musical elements"
        },
        {
          "name": "Attention Layers",
          "brief": "Components of AI models that control high-level concepts"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of AI models and their internal mechanisms"
        },
        {
          "topic": "Audio Signal Processing",
          "why_needed": "To comprehend the generation of high-fidelity music"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the text-to-music synthesis process"
        },
        {
          "topic": "Music Theory",
          "why_needed": "To appreciate the concepts of instruments, vocals, and genre characteristics"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.11757": {
    "title": "Code2Worlds: Empowering Coding LLMs for 4D World Generation",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Coding LLMs",
          "brief": "Large Language Models for coding tasks"
        },
        {
          "name": "4D World Generation",
          "brief": "Generating dynamic 3D scenes that change over time"
        },
        {
          "name": "Spatial Intelligence",
          "brief": "Ability to understand and generate spatial environments"
        },
        {
          "name": "Physics-Aware Simulation",
          "brief": "Simulations that adhere to physical laws"
        },
        {
          "name": "Dual-Stream Architecture",
          "brief": "Architecture for disentangling object generation and environmental orchestration"
        },
        {
          "name": "Closed-Loop Mechanism",
          "brief": "Mechanism for iteratively refining simulation code"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "Understanding of neural networks and language models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "Knowledge of 3D scene generation and visual understanding"
        },
        {
          "topic": "Physics and Dynamics",
          "why_needed": "Understanding of physical laws and dynamic simulations"
        },
        {
          "topic": "Programming and Coding",
          "why_needed": "Familiarity with coding concepts and languages"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Understanding of machine learning concepts and algorithms"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.12984": {
    "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "SciAgentGym",
          "brief": "A scalable interactive environment for benchmarking multi-step scientific tool-use in LLM agents"
        },
        {
          "name": "SciAgentBench",
          "brief": "A tiered evaluation suite for stress-testing agentic capabilities in scientific tool-use"
        },
        {
          "name": "SciForge",
          "brief": "A data synthesis method for generating logic-aware training trajectories to improve multi-step workflow execution"
        },
        {
          "name": "LLM Agents",
          "brief": "Large Language Model agents that can perform scientific reasoning and tool-use"
        },
        {
          "name": "Multi-Step Workflow Execution",
          "brief": "The ability of agents to orchestrate tools for rigorous scientific workflows"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the concept of LLM agents and their applications"
        },
        {
          "topic": "Scientific Reasoning",
          "why_needed": "To comprehend the importance of integrating toolkits for domain-specific knowledge navigation"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the capabilities and limitations of current LLM models"
        },
        {
          "topic": "Benchmarking and Evaluation",
          "why_needed": "To understand the need for tiered evaluation suites like SciAgentBench"
        },
        {
          "topic": "Data Synthesis and Generation",
          "why_needed": "To appreciate the role of methods like SciForge in improving agent performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.03120": {
    "title": "Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Post-Training Quantization (PTQ)",
          "brief": "A technique to reduce the precision of model weights for efficient deployment"
        },
        {
          "name": "Evolution Strategies (ES)",
          "brief": "A optimization method that doesn't rely on backpropagation"
        },
        {
          "name": "Quantized Evolution Strategies (QES)",
          "brief": "A novel optimization paradigm for fine-tuning quantized models"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "Complex AI models for natural language processing tasks"
        },
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A subfield of machine learning that involves training agents to make decisions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the context of LLMs and the challenges of quantization"
        },
        {
          "topic": "Optimization Methods",
          "why_needed": "To appreciate the novelty of QES and its advantages over traditional methods"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To understand the importance of memory constraints and low-precision computing"
        },
        {
          "topic": "Mathematical Optimization",
          "why_needed": "To grasp the mathematical foundations of ES and QES"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.12612": {
    "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Recommender Systems",
          "brief": "Systems that suggest items to users based on their preferences and behavior"
        },
        {
          "name": "Neural Architecture Search (NAS)",
          "brief": "A method for automating the design of neural network architectures"
        },
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models trained on vast amounts of text data to generate human-like language"
        },
        {
          "name": "Self-Evolving Systems",
          "brief": "Systems that can adapt and improve themselves over time through feedback and learning"
        },
        {
          "name": "Model Diagnosis and Evaluation",
          "brief": "Techniques for analyzing and improving the performance of machine learning models"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and their application in recommender systems"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the role of LLMs in generating directional feedback"
        },
        {
          "topic": "Information Retrieval",
          "why_needed": "To grasp the evaluation metrics used in recommender systems, such as NDCG and Hit Ratio"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "To appreciate the concept of code evolution and the importance of model diagnosis and evaluation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.04163": {
    "title": "BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Bit-Plane Decomposition Quantization (BPDQ)",
          "brief": "A technique for efficient serving of large language models by constructing a variable quantization grid via bit-planes and scalar coefficients"
        },
        {
          "name": "Post-Training Quantization (PTQ)",
          "brief": "A method for reducing the precision of model weights while maintaining high fidelity"
        },
        {
          "name": "Variable Quantization Grid",
          "brief": "A grid that can be adjusted to minimize quantization errors"
        },
        {
          "name": "Hessian-Induced Geometry",
          "brief": "A mathematical framework for analyzing the optimization objective of quantization processes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models (LLMs)",
          "why_needed": "To understand the context and application of BPDQ"
        },
        {
          "topic": "Quantization Techniques",
          "why_needed": "To appreciate the limitations of existing methods and the benefits of BPDQ"
        },
        {
          "topic": "Deep Learning and Neural Networks",
          "why_needed": "To comprehend the underlying concepts and mathematics of LLMs and quantization"
        },
        {
          "topic": "Linear Algebra and Optimization",
          "why_needed": "To understand the mathematical framework of Hessian-induced geometry and the optimization objective of BPDQ"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.12684": {
    "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vision-Language-Action (VLA) Model",
          "brief": "A model that integrates vision, language, and action to enable robots to understand and interact with their environment"
        },
        {
          "name": "Real-Time Execution",
          "brief": "The ability of a model to process and respond to inputs in real-time, without significant delay"
        },
        {
          "name": "Cross-Embodiment Robot Trajectories",
          "brief": "Data that represents the movements and actions of robots with different physical bodies or embodiments"
        },
        {
          "name": "Asynchronous Execution",
          "brief": "A technique that allows a model to process multiple tasks or inputs simultaneously, improving efficiency and reducing latency"
        },
        {
          "name": "Catastrophic Forgetting",
          "brief": "A phenomenon where a model forgets its previously learned knowledge or skills when trained on new data or tasks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of VLA models"
        },
        {
          "topic": "Robotics",
          "why_needed": "To appreciate the applications and challenges of vision-language-action models in robotic systems"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the visual perception and understanding aspects of VLA models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the language understanding and generation components of VLA models"
        },
        {
          "topic": "Real-Time Systems",
          "why_needed": "To appreciate the importance of low-latency and efficient processing in real-time robotic applications"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.12036": {
    "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning with Verifiable Rewards (RLVR)",
          "brief": "A method that uses verifiable prompts to train large language models"
        },
        {
          "name": "Composition-RL",
          "brief": "A technique that composes multiple problems into new verifiable questions for RL training"
        },
        {
          "name": "Large Language Models",
          "brief": "AI models that process and generate human-like language"
        },
        {
          "name": "Verifiable Prompts",
          "brief": "Prompts used to train RL models, which can be verified for correctness"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To understand the basics of RL and its application to large language models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To comprehend the challenges of training large language models and the importance of verifiable prompts"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the architecture and training of large language models"
        },
        {
          "topic": "Curriculum Learning",
          "why_needed": "To appreciate the concept of gradually increasing compositional depth in Composition-RL"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.12116": {
    "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Personalized Generative Reward Model (P-GenRM)",
          "brief": "A model that adapts responses to individual user preferences using reinforcement learning and test-time user-based scaling"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A machine learning approach that involves training agents to make decisions based on rewards or penalties"
        },
        {
          "name": "Test-time User-based Scaling",
          "brief": "A mechanism that scales and aggregates user preferences at test time to improve personalized alignment"
        },
        {
          "name": "User Prototypes",
          "brief": "Clusters of users with similar preferences, used to enhance generalization to unseen users"
        },
        {
          "name": "Dual-granularity Scaling Mechanism",
          "brief": "A mechanism that scales user preferences at both individual and prototype levels"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models",
          "why_needed": "To understand the context of personalized alignment and the challenges of obtaining accurate user-specific reward signals"
        },
        {
          "topic": "Reinforcement Learning",
          "why_needed": "To comprehend the basics of reinforcement learning and its application in personalized reward models"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the importance of personalized alignment in generating human-like responses"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the concepts of model training, evaluation, and generalization"
        },
        {
          "topic": "User Modeling",
          "why_needed": "To understand the concept of user prototypes and their role in enhancing generalization to unseen users"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.12500": {
    "title": "Favia: Forensic Agent for Vulnerability-fix Identification and Analysis",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Vulnerability-fix identification",
          "brief": "Identifying commits that fix security vulnerabilities in software"
        },
        {
          "name": "Forensic agent-based framework",
          "brief": "Using an agent-based approach to analyze and identify vulnerability-fixing commits"
        },
        {
          "name": "Large language models (LLMs)",
          "brief": "Utilizing LLMs to evaluate and rank commits for vulnerability-fix identification"
        },
        {
          "name": "ReAct-based LLM agent",
          "brief": "Employing a ReAct-based agent to navigate codebases and establish causal alignment between code changes and vulnerability root causes"
        },
        {
          "name": "Scalable candidate ranking",
          "brief": "Using efficient ranking methods to narrow down the search space of commits"
        },
        {
          "name": "Deep and iterative semantic reasoning",
          "brief": "Applying rigorous evaluation and reasoning to identify vulnerability-fixing commits"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Software security and maintenance",
          "why_needed": "To understand the importance of identifying vulnerability-fixing commits"
        },
        {
          "topic": "Machine learning and large language models",
          "why_needed": "To comprehend the limitations and potential of existing automated approaches"
        },
        {
          "topic": "Code analysis and navigation",
          "why_needed": "To appreciate the challenges of identifying indirect, multi-file, and non-trivial fixes"
        },
        {
          "topic": "Data structures and algorithms",
          "why_needed": "To understand the efficient ranking and evaluation methods used in Favia"
        },
        {
          "topic": "Artificial intelligence and agent-based systems",
          "why_needed": "To grasp the concept of using an agent-based framework for vulnerability-fix identification"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.12628": {
    "title": "RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Reinforcement Learning (RL)",
          "brief": "A type of machine learning that involves training agents to take actions in an environment to maximize a reward"
        },
        {
          "name": "Sim-Real Co-Training",
          "brief": "A method that combines simulation and real-world data for training models"
        },
        {
          "name": "Vision-Language-Action (VLA) Models",
          "brief": "Models that integrate visual, linguistic, and action-based inputs for decision-making"
        },
        {
          "name": "Supervised Fine-Tuning (SFT)",
          "brief": "A technique for fine-tuning models using labeled data"
        },
        {
          "name": "Catastrophic Forgetting",
          "brief": "A phenomenon where a model forgets its previously learned knowledge when fine-tuned on new data"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of neural networks and their applications"
        },
        {
          "topic": "Robotics and Computer Vision",
          "why_needed": "To comprehend the context of VLA models and sim-real co-training"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To grasp the fundamentals of reinforcement learning and supervised fine-tuning"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To appreciate the broader implications of RL-based sim-real co-training"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-16"
  },
  "2602.14699": {
    "title": "Qute: Towards Quantum-Native Database",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Quantum Computing",
          "brief": "A new paradigm for computing that uses quantum-mechanical phenomena to perform operations on data"
        },
        {
          "name": "Quantum Database",
          "brief": "A database system designed to store and manage quantum data, leveraging quantum computing principles"
        },
        {
          "name": "SQL Compilation",
          "brief": "Translating SQL queries into quantum circuits for execution on quantum processors"
        },
        {
          "name": "Hybrid Optimization",
          "brief": "Dynamically selecting between quantum and classical execution plans for optimal performance"
        },
        {
          "name": "Quantum Indexing",
          "brief": "Techniques for efficiently storing and retrieving quantum data"
        },
        {
          "name": "Fidelity-Preserving Storage",
          "brief": "Methods for mitigating errors in quantum storage due to qubit constraints"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Classical Database Systems",
          "why_needed": "Understanding traditional database concepts to appreciate the innovations in Qute"
        },
        {
          "topic": "Quantum Mechanics and Quantum Information",
          "why_needed": "Grasping fundamental principles of quantum computing to comprehend Qute's quantum-native approach"
        },
        {
          "topic": "Programming Languages and Compiler Design",
          "why_needed": "Familiarity with programming languages and compiler design to understand the compilation of SQL into quantum circuits"
        },
        {
          "topic": "Optimization Techniques",
          "why_needed": "Knowledge of optimization methods to appreciate the hybrid optimizer in Qute"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.11609": {
    "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models that process and understand human language"
        },
        {
          "name": "Single-Cell RNA-seq Data",
          "brief": "Genetic data from individual cells"
        },
        {
          "name": "Omics-Native Reasoning",
          "brief": "Using LLMs to analyze and reason about biological data"
        },
        {
          "name": "Bioinformatics Tools",
          "brief": "Software used to analyze biological data"
        },
        {
          "name": "Cell-Type Annotation",
          "brief": "Identifying cell types based on genetic data"
        },
        {
          "name": "Developmental-Trajectory Reconstruction",
          "brief": "Reconstructing cell development paths"
        },
        {
          "name": "Transcription-Factor Targeting",
          "brief": "Identifying genes regulated by transcription factors"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Molecular Biology",
          "why_needed": "Understanding genetic data and biological processes"
        },
        {
          "topic": "Bioinformatics",
          "why_needed": "Familiarity with bioinformatics tools and data analysis"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Understanding LLMs and their applications"
        },
        {
          "topic": "Programming",
          "why_needed": "Using code to implement and analyze scPilot"
        },
        {
          "topic": "Data Analysis",
          "why_needed": "Interpreting results and evaluating model performance"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.11715": {
    "title": "DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Large Language Models (dLLMs)",
          "brief": "A type of language model that generates text in parallel, rather than sequentially"
        },
        {
          "name": "Autoregressive (AR) LLMs",
          "brief": "A type of language model that generates text one token at a time, in sequence"
        },
        {
          "name": "CUDA Kernels",
          "brief": "Small programs that run on NVIDIA GPUs, used for general-purpose computing"
        },
        {
          "name": "Curated Reinforcement Learning (C-RL)",
          "brief": "A type of machine learning that involves training an agent to make decisions in a complex environment"
        },
        {
          "name": "Bi-phase Curated Reinforcement Learning (BiC-RL)",
          "brief": "A specific type of C-RL that involves two stages of training"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the basics of language models and their applications"
        },
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the concepts of language models and text generation"
        },
        {
          "topic": "Computer Architecture",
          "why_needed": "To understand the basics of CUDA kernels and GPU computing"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of reinforcement learning and supervised fine-tuning"
        },
        {
          "topic": "Programming",
          "why_needed": "To understand the basics of code generation and CUDA kernel development"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.13344": {
    "title": "FireRed-Image-Edit-1.0 Techinical Report",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Diffusion Transformers",
          "brief": "A type of neural network architecture for image editing tasks"
        },
        {
          "name": "Instruction-Based Image Editing",
          "brief": "A technique for editing images based on text instructions"
        },
        {
          "name": "Data Curation",
          "brief": "The process of collecting, cleaning, and preparing data for training machine learning models"
        },
        {
          "name": "Multi-Stage Training Pipeline",
          "brief": "A training methodology that involves multiple stages of pre-training, fine-tuning, and reinforcement learning"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning that involves training models through trial and error"
        },
        {
          "name": "Asymmetric Gradient Optimization",
          "brief": "A technique for stabilizing optimization and enhancing controllability in neural networks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the underlying neural network architectures and training methodologies"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand image editing tasks and techniques"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand text-based instruction handling and processing"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the concepts of data curation, model training, and evaluation"
        }
      ],
      "difficulty_level": "expert",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.14234": {
    "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Long-Horizon Search Agents",
          "brief": "Agents that perform complex tasks requiring multiple steps and decision-making"
        },
        {
          "name": "Large Language Models",
          "brief": "AI models trained on vast amounts of text data to generate human-like language"
        },
        {
          "name": "Scalable Task Synthesis",
          "brief": "Generating complex tasks efficiently for training search agents"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "Training agents using rewards and penalties to learn optimal behaviors"
        },
        {
          "name": "Tool-Augmented Queries",
          "brief": "Using external tools to enhance search agent capabilities"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "Understanding AI fundamentals is necessary to comprehend the context of search agents and large language models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "Knowledge of machine learning concepts, such as reinforcement learning, is required to understand the training process of search agents"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Familiarity with NLP is necessary to understand the role of large language models in search agent optimization"
        },
        {
          "topic": "Graph Theory",
          "why_needed": "Understanding graph topology is necessary to comprehend the task synthesis process in REDSearcher"
        },
        {
          "topic": "Software Engineering",
          "why_needed": "Knowledge of software development is necessary to understand the implementation of tool-augmented queries and simulated environments"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.14178": {
    "title": "UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Unified Multimodal Large Language Models (MLLMs)",
          "brief": "Models that process and generate multiple forms of data, such as text and images"
        },
        {
          "name": "Visual Tokenizers",
          "brief": "Algorithms that convert images into discrete tokens for processing by MLLMs"
        },
        {
          "name": "Binary Codebook",
          "brief": "A large set of binary vectors used for quantizing and representing visual data"
        },
        {
          "name": "Pre-Post Distillation",
          "brief": "A training framework that enhances semantic extraction and generative capabilities of discrete tokens"
        },
        {
          "name": "Convolution-Attention Hybrid Architecture",
          "brief": "A neural network architecture that combines convolutional and attention-based layers"
        },
        {
          "name": "SigLu Activation Function",
          "brief": "A activation function that bounds encoder output and stabilizes semantic distillation"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of neural networks, tokenization, and generative models"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand image processing, representation, and generation"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand language models, text generation, and semantic extraction"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To understand concepts of entropy, quantization, and coding theory"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.13367": {
    "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Generalist Language Model",
          "brief": "A language model that can perform a wide range of tasks, including reasoning, code generation, and agentic behavior"
        },
        {
          "name": "Reward Modeling",
          "brief": "A technique used to align model outputs with human preferences and values"
        },
        {
          "name": "Reinforcement Learning",
          "brief": "A type of machine learning that involves training models to make decisions based on rewards or penalties"
        },
        {
          "name": "Deep Search",
          "brief": "A technique used to synthesize complex data and perform long-horizon planning"
        },
        {
          "name": "Tool Interaction",
          "brief": "The ability of a model to interact with external tools and systems to achieve complex goals"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and applications of language models"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To comprehend the training and optimization techniques used in the paper"
        },
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To appreciate the broader implications and potential applications of generalist language models"
        },
        {
          "topic": "Computer Science",
          "why_needed": "To understand the technical details of model architecture, training, and evaluation"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.14367": {
    "title": "InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Large Language Models (LLMs)",
          "brief": "AI models capable of generating human-like language"
        },
        {
          "name": "Knowledge-Grounded Reasoning",
          "brief": "Using relevant knowledge to inform decision-making"
        },
        {
          "name": "Multi-Perspective Reasoning",
          "brief": "Considering multiple viewpoints to evaluate ideas"
        },
        {
          "name": "Innovation Evaluation",
          "brief": "Assessing the novelty and potential impact of research ideas"
        },
        {
          "name": "Deep Learning",
          "brief": "A subset of machine learning using neural networks"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Artificial Intelligence",
          "why_needed": "To understand the capabilities and limitations of LLMs"
        },
        {
          "topic": "Scientific Research Methods",
          "why_needed": "To comprehend the importance of rigorous evaluation in scientific research"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To grasp the role of language models in generating and evaluating text"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the application of deep learning in innovation evaluation"
        },
        {
          "topic": "Cognitive Psychology",
          "why_needed": "To appreciate the human-like reasoning and decision-making aspects of InnoEval"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.14689": {
    "title": "Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Open-Weight Models",
          "brief": "Large language models with accessible weights, vulnerable to internal manipulation"
        },
        {
          "name": "Prefill Attacks",
          "brief": "Attacks that exploit the prefilling feature in open-weight models to manipulate their responses"
        },
        {
          "name": "Red-Teaming Research",
          "brief": "Research focused on identifying and exploiting vulnerabilities in language models"
        },
        {
          "name": "Internal Safeguards",
          "brief": "Defenses built into open-weight models to prevent harmful behavior"
        },
        {
          "name": "Parameter-Level Manipulations",
          "brief": "Techniques used to manipulate model parameters for malicious purposes"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Large Language Models",
          "why_needed": "Understanding the basics of large language models is necessary to comprehend the context of the research"
        },
        {
          "topic": "Machine Learning Security",
          "why_needed": "Knowledge of machine learning security concepts, such as adversarial attacks, is required to appreciate the significance of prefill attacks"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "Familiarity with NLP concepts, such as language generation and tokenization, is necessary to understand the prefilling feature and its exploitation"
        },
        {
          "topic": "Model Vulnerabilities",
          "why_needed": "Understanding common vulnerabilities in language models, such as input-based jailbreaking, is necessary to appreciate the novelty of prefill attacks"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.12221": {
    "title": "Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Multimodal Reasoning",
          "brief": "The ability of a model to understand and generate multiple forms of data, such as text and images"
        },
        {
          "name": "Discrete Flow Matching",
          "brief": "A technique used to match and align different forms of data in a unified framework"
        },
        {
          "name": "Task-specific Low-rank Adapters",
          "brief": "A method to decouple understanding and generation tasks while avoiding objective interference"
        },
        {
          "name": "Reference-based Multimodal Preference Alignment",
          "brief": "A technique to optimize relative outcomes under identical conditioning, improving faithfulness and controllability"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of multimodal reasoning and generation"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To comprehend the applications of the proposed framework, such as image generation and editing"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand the text-based applications of the proposed framework"
        },
        {
          "topic": "Information Theory",
          "why_needed": "To grasp the concept of discrete flow matching and its applications"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.13191": {
    "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Video Language Models (VideoLMs)",
          "brief": "AI systems that understand temporal dynamics in videos"
        },
        {
          "name": "Codec Primitives",
          "brief": "Video codec primitives such as motion vectors and residuals that encode video redundancy and sparsity"
        },
        {
          "name": "Transformer-based Encoders",
          "brief": "Lightweight encoders that aggregate codec primitives and align their representations with image encoder embeddings"
        },
        {
          "name": "Pre-training Strategy",
          "brief": "A strategy that accelerates convergence during end-to-end fine-tuning"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Deep Learning",
          "why_needed": "To understand the concepts of transformer-based encoders and pre-training strategies"
        },
        {
          "topic": "Computer Vision",
          "why_needed": "To understand video processing, image encoding, and video understanding benchmarks"
        },
        {
          "topic": "Video Coding",
          "why_needed": "To understand video codec primitives such as motion vectors and residuals"
        },
        {
          "topic": "Natural Language Processing",
          "why_needed": "To understand language models and their application to video understanding"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-17"
  },
  "2602.14060": {
    "title": "LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts",
    "prerequisites": {
      "core_concepts": [
        {
          "name": "Definition Modeling",
          "brief": "A task in natural language processing that involves generating definitions for words or phrases"
        },
        {
          "name": "Sparse Mixture-of-Experts Architecture",
          "brief": "A type of neural network architecture that combines multiple expert models to improve performance"
        },
        {
          "name": "Semantic Expert Learning",
          "brief": "A technique that involves training small language models as domain experts to improve definition modeling"
        },
        {
          "name": "Data Clustering",
          "brief": "A technique used to group similar data points into clusters, in this case, to enable fine-grained expert specialization"
        },
        {
          "name": "BLEU Score",
          "brief": "A metric used to evaluate the quality of generated text, such as definitions"
        }
      ],
      "background_knowledge": [
        {
          "topic": "Natural Language Processing (NLP)",
          "why_needed": "To understand the context and goals of definition modeling"
        },
        {
          "topic": "Deep Learning",
          "why_needed": "To comprehend the sparse mixture-of-experts architecture and semantic expert learning"
        },
        {
          "topic": "Language Models",
          "why_needed": "To understand how language models can be used as domain experts in definition modeling"
        },
        {
          "topic": "Machine Learning",
          "why_needed": "To understand the clustering strategy and model merging techniques used in LM-Lexicon"
        }
      ],
      "difficulty_level": "advanced",
      "estimated_study_time": "3-6 months"
    },
    "last_updated": "2026-02-17"
  }
}